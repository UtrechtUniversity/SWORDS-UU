html_url_repository,readme,date
https://github.com/a-quei/bert-chinese-classifier,# bert-chinese-classifier,2022-08-12
https://github.com/a-quei/guanyi.cls,"**guanyi.cls** is a costumised article class.<br>
**beamerthemeGuanyi.sty** is a costumised beamer theme. When using the theme, it is better to include beamer class using ``\documentclass[t]{beamer}``.",2022-08-12
https://github.com/a-quei/mtuna-annotated,"# mtuna-annotated
The annotated MTUNA corpus
",2022-08-12
https://github.com/a-quei/neuralreg-re-evaluation,# neuralreg-re-evaluation,2022-08-12
https://github.com/a-quei/probe-neuralreg,"# probe-neuralreg
",2022-08-12
https://github.com/a-quei/qtuna,"# QTUNA

## Introduction

QTUNA is a series of elicitation experiments in which human speakers were asked to perform a linguistic task that invites the use of quantified expressions in order to inform a possible Natural Language Generation algorithms that mimic humans' use of quantified expressions. This work was inspired by a line of work focusing one specific class of noun phrases, i.e., referring expression, where they focus on corpora of referring expressions  that  were  elicited  under  experimentally  controlled  conditions (e.g., [TUNA](https://www.abdn.ac.uk/ncs/departments/computing-science/tuna-318.php) experiment), but aiming  this  time  to  gain  insights into quantified noun phrases.

## Data

#### Scene

The scenes we used for the QTUNA experiements can be dount in the `scenes.pdf`.

#### Raw Data

The plain texts elicitated in the QTUNA experiments can be found in the `raw_data` folder, each of which is in the format of:
```
1	1	1	There are only blue squares.
2	1	2	No circles, half red
3	1	3	Equally devided blue objects
4	1	4	Equal amount of red circles and blue squares
5	1	5	Most of them are squares, all blue

```
where the second and the third colomns are the subject ID and the scene ID respectively.
",2022-08-12
https://github.com/a-quei/quantified-description-generation,"# Quantified Description Generation

## Introduction

This code inplements two quantified description generation (QDG) algorithms, namely, the incremental algorithm and the greedy algorithm, for simulating human production of quantified expressions, and for reproducing descriptions in the [QTUNA dataset](https://github.com/a-quei/quantified-description-generation). 

<p align=""center""><img width=""85%"" src=""framework.png""/></p> 

Given a target scene with its domain knowledge, the generator constructs a set containing all possible scenes in the same domain as the target scene. The generator then calls a QDG algorithm to construct a description containing a set of QEs. The algorithm selects from a set of candidate quantified patterns, based on the common knowledge by mimicking how human beings did so in the QTUNA experiment. Finally, a simple template-based surface realiser is called to map the description (in logical form) into natural language text.

## Usage

To run the code, we, in the first place, need to define the scene in the `generator.py`. For example, 
```
s = {""BS"": 0, ""BC"": 1, ""RS"": 0, ""RC"":3}
``` 
stands for a scene with one blue circle and three red circles. To run the DQG algorithms, use the following command:
```
python3 generator.py
```

#### Example Output

By runing the code, we get outputs like:
```
IA: (True) Every object is circle. Most of the objects are red circles.
Greedy: (True) Most of the objects are red circles. There are only circles.
```
where the `True` mark in the brackets indicates the generated description is logical complete.


",2022-08-12
https://github.com/a-quei/simplenlg-zh,"# SimpleNLG-ZH: a Linguistic Realisation Engine for Mandarin

SimpleNLG-ZH(ongwen) is a realisation engine for Mandarin Chinese, which was built in the tradition of SimpleNLG. Please found more details of which grammar has been implemented in the paper titled, [*SimpleNLG-ZH: a Linguistic Realisation Engine for Mandarin*](https://www.aclweb.org/anthology/W18-6506.pdf), which has been published in the 11th international conference on natural language generation, Tilburg, the Netherlands.

## Documentation

TBC

## Citation

```
@inproceedings{chen-etal-2018-simplenlg,
    title = ""{S}imple{NLG}-{ZH}: a Linguistic Realisation Engine for {M}andarin"",
    author = ""Chen, Guanyi  and
      van Deemter, Kees  and
      Lin, Chenghua"",
    booktitle = ""Proceedings of the 11th International Conference on Natural Language Generation"",
    month = nov,
    year = ""2018"",
    address = ""Tilburg University, The Netherlands"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/W18-6506"",
    doi = ""10.18653/v1/W18-6506"",
    pages = ""57--66"",
}
```
",2022-08-12
https://github.com/AJueling/AMWG,"# CESM's AMWG analysis package for IMAU's simulations

The cshell scripts are adapted to be used for analyzing the CESM simulations performed by the IMAU ocean group and stored on the file system of Surfsara's Cartesius HPC.

Need to make sure there is write access to the test_path_climo/diag folders.

## lines to be adjusted:
- [107ff.]  set location of files
- [126f.]   whether to compare to obs or other simulation 
- [140ff.]  if comparison with other simulations set these paths

## cases
- HIGH vs. LOW: 1/10 degree (CESM 1.04) vs. 1 degree (CESM 1.12) simulations
- ""RCP"":  RCP8.5 like (GHG only) forcing,  years (2070-2099)-(2000-2029)
- ""CTRL_vs_OBS"": compare year 2000 control constant forcing (years 200-229 (500-529) for HIGH (LOW)) to observations
",2022-08-12
https://github.com/AJueling/CESM,"# DEPENDENCIES

All computations are done on Cartesius of Surfsara.

The python 3.7 virtual environment used to run the analysis is saved as conda_list.txt in the documentation folder.

The 'run' scripts that enable scheduled (parallel) computing on Cartesius use sbatch.


# WORKFLOW

1. time averaging
    selected fields of monthly output .nc files to yearly averaged .nc files
2. analysis scripts



# FILE OVERVIEW

## /doc: Documentation
(non-tracked files, including model documentation pdf's)


## /src/  (source code)

python files

- constants.py
- grid.py
- maps.py
- OHC.py
- paths.py
- plotting.py
- read_binary.py
- regions.py
- timesries.py
- xr_DataArrays
- xr_integration
- xr_regression

ipython notebooks
- currents
- data_overview
- geometry
- GMST
- OHC
- SHF
- SST
- timeseries
- winds


### /src/ipynb/  (ipython notebooks including output)

- copy_stripped_notebooks
    makes a copy of the ipynb's with output and strips it out

### /src/run/  (scripts and python files to run in parallel)

- run_OHC_integrals
- run_yrly_avg

### /src/test/  (test suite)

- test_xr_integrals.py

",2022-08-12
https://github.com/AJueling/EC-Earth3-data,"# EC-Earth-data
intake-esm catalogues of EC-Earth3(P)-(HR) and some HighResMIP data at JASMIN/CEDA, ECMWF, KNMI

The `MIPVariableNames.csv` files comes from

## 1. intake-esm catalogues

While these catalogues can be read with intake-esm, you need to have access to the servers in order to load any of the data.

### 1.1 CEDA CMIP6

The catalogue is `https://raw.githubusercontent.com/cedadev/cmip6-object-store/master/catalogs/ceda-zarr-cmip6.json`
This is unfortunately neither complete not up to date.

### 1.2 CEDA HighResMIP netCDF: `ceda_nc_highresmip.json`

from both CMIP6 and PRIMAVERA archives

### 1.3 JASMIN HighResMIP netCDF: `jasmin_nc_highresmip.json`

from both CMIP6 and PRIMAVERA archives 

A catalogue of file that are stored both in:

`/gws/nopw/j04/primavera2/stream1/PRIMAVERA/HighResMIP/` and `/gws/nopw/j04/primavera2/stream1/CMIP6/HighResMIP/`

The json file is created manually (from the `ceda-zarr-cmip6.json` above).
The csv is compiled with `os.walk` in `JASMIN_catalogue.py` (excuted from within jupyterlab with the activated `venv-cmip6-zarr` virtual environment). 

### 1.4 ECMWF


### 1.5 KNMI EC-Earth3(P)-(HR): `knmi_nc_ece3.json`

Some ECE3 data is stored at `/usr/people/sager/nb2/PRIMAVERA/stream2-from-jasmin/`


### 1.5 Google CMIP6

`https://storage.googleapis.com/cmip6/pangeo-cmip6.json`

---

## 2. data availability

",2022-08-12
https://github.com/AJueling/FW-code,"# Code for the Ocean Science publication [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4537845.svg)](https://doi.org/10.5281/zenodo.4537845)
### The Atlantic's Freshwater Budget under Climate Change in the Community Earth System Model with Strongly Eddying Oceans
### (doi:10.5194/os-2020-76)
#### by André Jüling, Xun Zhang, Daniele Castellana, Anna S. von der Heydt, and Henk A. Dijkstra

## Code Structure

This repository holds the files creating the figures of the publication. Original model files and many derived files are not included due to size limitation (single month CESM output files are 56 GB in the high resolution setup). There is more functionality in this code than what is used for the figures alone, but dependencies should be clear from the code.

There is an `environment.yml` file that can be used to create the conda environment used.

For questions regarding the code please contact me: a.juling@uu.nl

```
FW-code
│   README.md
│   LICENSE
│
└───doc               (documentation)
|   │   enviroment.yml              conda environment used (see https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)
│
└───src
    │
    │   Fig{1-10}.ipynb             jupyter notebooks with Figures from paper
    │   GM_eddy_fluxes.ipynb        notebook to create supplementary GM eddy flux figure
    │
    │   aa_derivation_fields.py     creating derived fields
    │   constants.py                defining reused physical constants
    │   curly_bracket.py            plots curly brackets
    │   filters.py                  filters for time series
    │   FW_budget.py                calculate freshwater budget terms
    │   FW_plots.py                 functions to create some of the plots
    │   FW_transport.py             calculate freshwater transport terms
    │   grid.py                     deals with CESM grid
    │   maps.py                     creates maps
    │   mimic_alpha.py              auxiliary plotting function to fake transparancy in .eps
    │   MOC.py                      calculate AMOC
    │   obs_cesm_maps.py            world maps of observations and biases
    │   paths.py                    collection of file paths and names
    │   plotting.py                 auxiliary plotting functions
    │   read_binary.py              functions to read CESM binary output
    │   regions.py                  define world ocean regions
    │   timeseries.py               function to loop over CESM files
    │   xr_DataArrays.py            xarray auxiliary functions
    │   xr_integrate.py             xarray integration functions
    │   xr_regression.py            xarray regression functions
```
",2022-08-12
https://github.com/AJueling/LEC,"https://doi.org/10.5281/zenodo.1414359 v1.0

""Energetics of the Southern Ocean Mode""
submitted to Journal of Geophysical Research: Oceans

Project Lorenz Energy Cycle (LEC)
by Andr\'e J\""uling, July 2018

Calculates and analyses Lorenz Energy Cycle (LEC) of a high resolution Parallel 
Ocean Program (POP) model run that showed OHC oscillations in the Southern Ocean 
(Le Bars, GRL 2016).

In addition to the LEC other quantities are analysed, such as 
  (a) the barotropic and overturning stream functions,
  (b) the mixed layer depths,
  (c) ocean heat content / temperature anaomalies.


================================================================================
Workflow for the LEC calculation
================================================================================

  (1) set up POP to write out required fields
      ajusted output fields in [advection_BY_JAN.f90]

  (2) calculate geometry files

  (3) run POP

  (4) calculate desired temporal averages (in our case: 5 years) as well as a 
      full run average (in our case: 51 years = one SOM cycle) [TAVG.f90]

  (5) analyse output:
      (a) [LEC.f90]
          to calculate all LEC terms plus kinetic energy advection;
          output is stored as 3D binary files and global integrals as text files
      (b) [analyze_LEC.f90]
          perform regional/phase integrals;
          output is stored as text files
      (c) [OSF_BSF.f90]
          calculates overturning and barotropic streamfunctions
      (d) [MXL.f90]
          calculates max MLD
      (e) [OHC.f90]
          calculates TMP anomaly

  (6) plotting
      (a) [LEC-paper-timeseries.py]
          time series
          Figure 1    : overturning/barotropic streamfunction, SOM index, max MLD
          Figure 10   : LEC components  
      (b) [LEC-paper-boxplots.py]
          Figure 3    : energy flow box plots
      (c) [LEC-paper-maps.py]
          Figure 2    : map of max MLD, depth-time temperature anomaly
          Figures 4-9 : polar maps of LEC components


================================================================================
/doc
================================================================================

[t.t0.1_42l_nccs01.007601.hdr]
- example of a file containing metadata of the original model output [12GB]

[t.t0.1_42l_nccs01.027601.hdr]
- example of full output metadata (necessary for LEC calculations) [56GB]

[LEC_bin.hdr]
- metadata for the derived LEC binary files

[pop_in]
- namelist provided to the POP model

[pop_in_and_meaning_parameters.pdf]
- explanations of parameters in pop_in

[start_run.sc ]
- script to start model run

[movie_contents / tavg_contents] 
- which field to output daily / monthly

================================================================================
/src
================================================================================

subfolders according to file types:
f90: ""*.f90""  (Fortran)  for all computations and writing output
do : ""do_*""   (cshell)   scripts to compile and execute Fortran programs
run: ""run_*""  (bash)     scripts to run do scripts in parallel via sbatch
py : ""*.py""   (python)   scripts and notebooks to analyse and plot results


================================================================================
/scr/f90: Programs
================================================================================

[TAVG.f90]
- time averaging of monthly output files
- subroutine files needed:
- output: time averaged binary files with same structure as original output files

[geometry.f90]
- does geometrical calculations from model files
- subroutine files needed:
- output: real binary files: (1) (imt,jmt,4+2*km) , 
                             (2) (6,km) depth dependent quantities

[ref_state.f90]
- creates reference background state from 51 year average file
- subroutine files needed:
    load_3D_field (5), state (6), area_avg(3), area_avg_weighted(3), vert_der(2)
- output: unformatted binary file '/input/ref_state' with these quantities:
          ! k, dz, tdepth, area, p,
          ! <T>, <S>, <RHO>, <PD>, <D(D,T,0)>, <D(D,T,p)>, <Q>,
          ! dz(<PD>), dz(<D(S,T,0)>), dz(<D(S,T,p)>)

[LEC.f90]
- calculates all LEC terms
- subroutine files needed:
- output: total/level terms, binary with LEC terms as 2D/3D fields
- takes about 3:20 mins to run for one file

[analyze_LEC.f90]
- surface/volume integrals over specific areas
- subroutine files needed:
- output: text files for each year with specific area/volume integrals of LEC terms
- takes a little less than 40 sec

[OSF_BSF.f90]
- overturning stream function and barotropic stream function
- import geometry file
- subroutine files needed: none
- output: (3,nt=251)       year,BSF drake, BSF Weddell gyre
          (imt,jmt,nt=251) yearly BSF fields, mean & std fields of last 51 years
          (jmt, km,nt=251) yearly OSF fields, mean & std fields of last 51 years
          in '/projects/0/samoc/jan/Andree/'//'OSF/' and //'BSF/' 
- uses run file and takes somewhat less than 7 hours

[MXL.f90]
- mixed layer depths (HMXL: mean, XMXL: maximum, TMXL: minimum)
- output: (imt,jmt,nt=251 years) 

[mer_adv.f90]
- meridional advection of energy reservoir terms
- subroutine files needed:
- output: (jmt,nt=51 years)

[int_test.f90]
- tests different integration methods
- subroutine files needed:
- output:
- takes about 30 sec

[WVEL_test.f90]
- tests whether the surface integrals of WVEL are 0
- output: 3 files for monthly, 5yr, and 51 yr average file with
          k, WVEL_sint(k), WVEL_sint_abs(k), TTT_WVEL_sint(k), TTT_WVEL_sint_abs(k)
          in 'results/WVEL_test/'
- takes 30 sec

[OHC_adv.f90]
- calculates heat advection across lon/lat stretches; for now along the
  boundaries of a box around the Weddell sinking region [-78S,-60S]x[35W,80E]
- output: /results/OHC_adv/
  - (imin:imax/jmin:jmax,km,nt=251yrs) 2D sections of heat transport across a 
    vertical plane along lat/lon
  - (imin:imax,nt) vertically integrated heat transport
  - (km,nt) zonally/meridionally integrated heat transport
- takes about 10 sec/month so in total 8.3 hours


================================================================================
/scr/f90: Subroutines
================================================================================

(1) [sub_calculations.f90]
       mer_advection      meridional advection
       osf                overturning stream function
(2) [sub_derivatives.f90]
       nabla_hvel         divergence of horizontal velocities
       grad_rho           density gradient
       vert_der           vertical derivative (for n0 calc.)
(3) [sub_integrals.f90]
       vol_int            volume-weighted integral using internal summation
       vol_int2           volume-weighted integral using Kahan summation
       surf_int           simple area-weighted surface integral
       area_avg           simple area-weighted level/area average
       area_avg_weighted  partial bottom cell depth weighted level/area average
       masked_avg         avg over mask (for MXL layer masked TEMP)
       vert_int           vertical integral (along k)
       zonal_int          ""zonal"" integral (along i)
(4) [sub_interpolation.f90]
       wtt2tt             WTT to TTT grid
       uu2tt              horizontal UU to TT grid weighted by DZU
       uu2tt_3d           calls uu2tt for 3d field
       uu2tt_scalar       like uu2tt but unweighted by DZU
       uu2tt_scalar_3D    same as uu2tt_3d for uu2tt_scalar
       interp_mom_fluxes  used for flux formulation approach
(5) [sub_model.f90]
       load_3D_field      loads 3D field from model output file
       load_2D_field      loads 2D field from model output file
       n/e/s/w_rshift     shifts array, used for geometry
(6) [sub_physics.f90]
       pressure           calculated the pressure [bar] at a given depth [m]
       state              calculates the potential density - rho_0 [g/cm^3]
                          (referenced to level k; rho_0=4.1/3.996*1000)

================================================================================
/results
================================================================================

[analyze_LEC/analysis_LEC_5_[year]_[region].out]
- results of analyse_LEC.f90

[SOM/POP_SOM_index.csv]
- SOM index file

[BSF_OSF/BSF_OSF.out]
- oversturning and barotropic streamfunction data


================================================================================
additional files:
================================================================================

model output files are 56GB per month and are consequently not stored here
the data is stroed on the Cartesius supercomputer at Surfsara (surfsara.nl)
access be requested from the authors or Michael Kliphuis (m.kliphuis(at)uu.nl)

derived files saved in /projects/0/samoc/jan/Andree include
- time averaged POP output files (monthly -> 1 year, 5 year, 11 year, 51 year)
- LEC fields, as binary fields
- cPK[m/e]_[1/5] files
- /MXL: mixed layer derived files
- /Psi: overturning stream function derived files
",2022-08-12
https://github.com/AJueling/Marit_DSL_thesis,"# Notebooks for Marit Verbeek's Bachelor thesis ""Evaluating Dynamic Sea Level along the Dutch coast in an Earth System Model""

supervision by Iris Keizer and André Jüling

## folder and file structure
```
.
├── README.md                               (this file)
├── code
│   ├── Second\ location\ (Hoek\ v.\ Holland).ipynb
│   ├── Thesis.ipynb                        (final thesis figures)
│   ├── Thesis_gmzos-corrected.ipynb        (corrections with gmzos)
│   ├── Third\ location\ (Delfzijl).ipynb
│   ├── data_Marit.ipynb                    (data generation from CMIP output)
│   └── iterate.py
├── data                                    (not included here)
│   ├── gmzos_2exps.nc
│   ├── gmzos_3exps_EC-Earth3P-HR.nc
│   ├── gmzos_3exps_EC-Earth3P.nc
│   ├── regridded
│   │   ├── uas_NorthSea_SR_regridded.nc
│   │   ├── vas_NorthSea_SR_regridded.nc
│   │   └── zos_NorthSea_SR_regridded.nc
│   ├── uas_NorthSea_HR.nc
│   ├── uas_NorthSea_SR.nc
│   ├── vas_NorthSea_HR.nc
│   ├── vas_NorthSea_SR.nc
│   ├── zos_NorthSea_HR-SR.nc
│   ├── zos_NorthSea_HR.nc
│   ├── zos_NorthSea_SR.nc
│   └── zos_denhelder
│       └── zos_DenHelder.nc
└── results                                 (empty here, but Figs. in notebooks)
```",2022-08-12
https://github.com/AJueling/melt,"# `melt` project: comparing ice shelf melt models
## Intercomparison of different models of basal melt

### Models considered:
1. Simple: simple parametrizations based on thermal driving, i.e. difference between pressure freezing temperature and in-situ ocean temperature (e.g. Favier _et al._ (2019))
1. Plume: analytical approximtation of the 1D plume model (Lazeroms _et al._ (2019))
1. PICO: box model of cavity circulation (Reese _et al._ (2018))
1. PICOP: combination the two abovementioned models (Pelle _et al._ (2019))
1. Layer: 2D (vertically integratd) plume model (Hellmer _et al._ (1989))
### Example Ice Shelf: Totten Glacier

<img src=""results/Bedmachine/TottenIS_geometry.png"">

__Figure 1:__  Geometry of the Totten Ice Shelf. The data in the top row is provided by the Bedmachine dataset. Left to right: surface elevation, bed topography/bathymetry, and ice thickness. Bottom left, derived draft of the ice shelf (elevation-thickness) together with the grounding line (blue) and the ice shelf front (red). Bottom center and right: minimum distance to the grounding line/ice shelf front.

## Repository structure
```
melt
│   README.md
│   LICENSE
│
└───data
│   └───BedMachine           (Antarctic topography data; not committed here)
│   │   │   BedMachineAntarctica_2020-07-15_v02.nc
│   │
│   └───IceVelocity          (Radar derived surface velocities; not committed here)
│   │   │   antarctic_ice_vel_phase_map_v01.nc
│   │
│   └───mask_polygons        (polygons to select areas; created vi QGIS)
│       │   *_grl.geojson    (polygons around grounding lines, continent only)
│       │   *_grl2.geojson   (polygons around grounding lines, including islands)
│       │   *_isf.geojson    (polygons around ice shelf front)
│       │   *_polygon.geojson(polygons around whole ice shelf)
│
└───doc                      (documentation)
|   |   environment.yml      (conda environments file)
│
└───results                  (images & netcdf files, mostly not committed)
│   
└───src
    └───ipynb                (notebooks with output; not committed)
    │   │   BedMachineAntarctica_2020-07-15_v02.nc
    │
    └───notebooks            (notebooks without output; version controlled)
    │   │   example.ipynb    (recreate figure from README.md)
    │   │   ...
    │
    │   forcing.py        │
    │   ideal_geometry.py │  (set up model domains)
    │   real_geometry.py  │
    │   PICO.py         │
    │   PICOP.py        │
    │   Plume.py        │    (models)
    │   sheet.py        │
    │   Simple.py       │
    │   advect.py         │
    │   constants.py      │  (auxiliary functions)
    │   sheet_utils.py    │
```

### How to run models

0. install the necessary packages\
   e.g., using `conda env create -f environment.yml` [creates a conda environment](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-from-an-environment-yml-file) named `ice` which should be able to execute all the code

1. choose geometry
    ```python
    from ideal_geometry import IdealGeometry
    ds = IdealGeometry('test3').create()
    ```
    realistic ice shelf geometries (with tht `RealGeometry` calss in `real_geometry.py`) require downloading the BedMachine topography and ice velocity data and placing it in the correct folder (see structure above)
2. choose forcing (temperature and salinity profiles)
    ```python
    from forcing import Forcing
    pdict = dict(Tdeep=-2, ztcl=500)
    ds = Forcing(ds).tanh(**pdict)
    ```
    other forcing profiles are part of the `Forcing` class, including `constant`, `tanh2`, `favier`, `isomip`, `isomipfrac`, `holland`, `holland07`
3. pass those to model of choice
    ```python
    from Simple import SimpleModels
    ds = SimpleModels(ds).compute()
    ```

<img src=""results/example/Simple_tanh_test3.png"">

__Figure 2:__ The results from following the example above: applying the `Simple` melt parametrizations on the `test3` with a `tanh` forcing profile.
The draft (a) is prescribed, the hydrostatic pressure (b) is linearly related, the mask (c) specifies grounding line (2), ice shelf (3), and ice shelf front (0), the number of boxes is given but the distribution (d) is calculated.
Panels (e-g) show quantities needed for one or more melt model: the local slope angle along the flowline (here simply left to right), the distance to the grounding line, and the depth at the plume origin.
Panel (h) shows the local freezing point, (i/j) the spatial distribution of temperature and salinity, (k/l) the temperature and salinity depth profiles.
Panel (m) shows the thermal forcing (temperature minus local freezing point).
Finally (n-p) show the three simple melt parameterizations: the local linear, local quadratic, and non-local quadratic ones, respectively

## Disclaimer
This is work in progress and we do not guarantee anything. Contact us if you have questions, and if you find this useful also let us know.

## References
> Favier,  Lionel et al.  (June  2019a). “Assessment of  sub-shelf melting  parameterisations using  the ocean–ice-sheet coupled model NEMO(v3.6)–Elmer/Ice(v8.3)”. Geoscientific  Model  Development 12.6., pp. 2255–2283.doi:10.5194/gmd-12-2255-2019.
>
>Hellmer, H. H. and D. J. Olbers (Dec. 1989). “A two-dimensional model for the thermohaline circulation under an iceshelf”. Antarctic Science 1.4. pp. 325–336.issn: 1365-2079, 0954-1020. doi:10.1017/S0954102089000490
>
> Lazeroms, Werner M.J. et al. (2019b). “An analytical derivation of ice-shelf basal melt based on the dynamics of meltwater plumes”. Journal of Physical Oceanography 49.4, pp. 917–939. doi:10.1175/JPO-D-18-0131.1
>
> Pelle, Tyler, Mathieu Morlighem, and Johannes H. Bondzio (Apr. 2019a). “Brief communication: PICOP, a new ocean melt parameterization under ice shelves combining PICO and a plume model”. The Cryosphere 13.3. pp. 1043–1049. doi:10.5194/tc-13-1043-2019
>
> Reese,  Ronja  et  al.  (June  2018a).  “Antarctic  sub-shelf  melt  rates  via PICO”.  The  Cryosphere 12.6. pp. 1969–1985. doi:10.5194/tc-12-1969-2018.
",2022-08-12
https://github.com/AJueling/MV-code,"# MV-code
Code for Ocean Science publication ""Effects of strongly eddying oceans on multidecadal climate variability in the CESM""

I did my best to share the complete code here and document it reasonably well. There are many functions that are not used for the final figures, but function calls should be obvious. Unfortunately, without access to the data on SurfSara's Cartesius computer, it will not be possible to recreate the plots. The total CESM output is several terabytes and so no easily made publicly available.

There is an `environment.yml` from which a conda environment can (in principle) be recreated. I have found that this often fails in practice, but you can simply create a new conda environment and install the necessary packages (https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html), should there be an error you can specify the version as in the `environment.yml` file.

## Repository Structure

```
MV-code
│   README.md
│   LICENSE
│   environment.yml      (conda environments file)
│
└───src
│    (start here)
│    │MV-paper.ipynb             notebook to create main figures
│    │SST_index_generation.py    pipeline to recreate all derived data files
│
│    (other notebooks)
│    │comparing_spectra.ipynb    notebook exploring different ways to compare spectra
│    │other_SST_products.ipynb   notebook to create appendix figure
│
│    (auxiliary functions)
│    │ab_derivation_SST.py
│    │ba_analysis_dataarrays.py
│    │bb_analysis_timeseries.py
│    │bc_analysis_fields.py
│    │bd_analysis_indices.py
│    │constants.py
│    │filters.py
│    │grid.py
│    │maps.py
│    │OHC.py
│    │paths.py
│    │plotting.py
│    │read_binary.py
│    │regions.py
│    │timeseries.py
│    │xr_DataArrays.py
│    │xr_integrate.py
│    │xr_regression.py
```",2022-08-12
https://github.com/AJueling/python_climate_physics,"# Python for Climate Physics  [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AJueling/python_climate_physics.git/master?urlpath=lab)
This is a collection of jypyter notebooks for an introductory workshop series on python for the Climate Physics master programme at the Institute for Marine and Atmospheric research Utrecht (IMAU), Utrecht University, held in September/October 2020.

_To interact with the notebooks you can launch a binder instance by clicking the binder link button in the heading. This can take a short while to initialize. Also beware that it will shut down after 10 minutes of inactivity (and all changed contents will be lost)._

## Purpose
We noticed in the last years that the programming background of everyone varies with some having zero experience and others having quite a lot already.
In addition, there are common specificities in working with (geo-) scientific data which you could all discover by yourselves, but why reinvent the wheel!?

## Workshop structure
There are four workshops.
The first two cover the basics of programming in python and if you feel comfortable with simple programming and plotting you will probably know this already and won’t want to attend.
The second two workshops focus on packages specific to our field and best practices for computational research projects.
I would thus recommend that all of you at least look at the contents of the last two workshops.
Below there is a list of resources that are worth checking out.

We meet Mondays during the after lunch time slot.
I will be there from 13:00 for those who need help with their installations.
The official workshop time is 13:15-15:00.

This is the current course outline:
1. [14.9.20 13:15-15:00] basic programming in python: data types, lists, dictionaries, loops, conditionals, functions, basic unix commands
2. [21.9.20 13:15-15:00] common python packages and basic plotting: numpy, matplotlib, scipy
3. [28.9.20 13:15-15:00] working with geoscientic data: netcdf data format, pandas, xarray
4. [5.10.20 13:15-15:00] mapping with cartopy, open science, and best practices

I will use jupyter notebooks for teaching because they can combine high quality documentation, code and output. For you to participate in the workshops you need to have a way to execute them. Please install Anaconda/Miniconda before the first workshop (see instruction below)!

The workshops are held online in MS Teams.
You can join the team `Python for Climate Physics` with the code I shared by email.
I will shortly introduce the content lecture-style, but most of the time is reserved for exercises and questions that may arise.
This is the first time I teach online, so things may change and even more than in normal times, I would like your feedback!
I will make the workshop materials available in advance, also so that you can decide whether there is anything new to you.
I will sent one more email before the first workshop, the rest of the communication will be via Teams.


---
## Python/Jupyter installation instructions:

For coherence, I strongly recommend we all use a Python installation with the conda package manager.
You have two (equally valid) options:
1. Anaconda: https://docs.anaconda.com/anaconda/install/ or 
2. Miniconda:  https://docs.conda.io/en/latest/miniconda.html
(NB: If you are on Linux or MacOS, you have a system python, but it is a good idea to install a separate Python version. Changing/adding to the system Python can lead to silly errors that are annoying to fix.)

The difference between the conda distributions is that Anaconda includes many packages (including some you will never use) and consequently uses up quite some disk space. It features a graphical user interface (the ![Anaconda Navigator](https://docs.anaconda.com/anaconda/navigator/)). Miniconda, on the other hand, only includes the bare necessities and packages need to installed as they are needed, thus using a lot less disk space. 

Once you have installed one of the two condas, there are two ways to work with jupyter notebooks:
1. jupyter notebook app: the basic, but sufficient option
2. jupyter lab app: the more advanced option

With Anaconda you can choose either app from the Navigator.

With Miniconda you need to install jupyter first.
You can do this by running `conda install jupyter jupyterlab` and subsequently confirming.
Then you can type in the terminal `jupyter notebook` or `jupyter lab` to start either of the two apps.
Also see the documentation that is linked in the paragraph above.

### Managing packages

The strength of Python is its extensive ecosystem of packages. There are pre-installed system libraries, but most packages need to be installed explicitely by you. Common packages we use include `numpy` (for array calculations) and `matplotlib` (for plotting). There are (again) several ways you can install packages:
1. in the terminal type `conda install {package}`, you can install several at the same time `conda install {package1} {package2} {package3}`
2. if you have Anaconda, you can use the Anaconda Navigator to search for and install packages

You can list all the packages in the current environment by typing `conda list`.

Sometimes conda does not ""know"" a package. Often these packages can be installed using an alternative channel (list of packages). Many smaller scientific packages are on the conda-forge channel and you can install them with `conda install -c conda-forge {special_package}`

---
## Resources
* [Earth and Environmental Data Science](https://earth-env-data-science.github.io/intro) full semester course by Ryan Abernathey at Columbia University from which much content is copied
* [IMAU's Python for Lunch workshop series](https://github.com/UU-IMAU/Python-for-lunch-Notebooks) a collection of workshop materials on topics of interest from IMAU students and researchers
",2022-08-12
https://github.com/AJueling/thesis-figures,"# thesis-figures
Figures of Chapters 1 (Introduction), 2 (CESM overview), and 6 (CESM reponse) of my thesis entitled ""Climate Variability and Response in High-Resolution Earth System Models""

Files:
- `*.ipynb` notebooks with code to produce figures and the figures themselves, some contain superfluous code
- `*.py` python files with auxiliary functions, likely contain unused functions
- `environment.yml` conda environment file

The Figures are unfortunately not immediately reproducible for several reasons:
- Surfsara's Cartesius is currently replaced by the new HPC Snellius, so paths will need to be adjusted and the environment.yml file may not produce a viable conda environment
- the CESM output data is too large to publicly store, but is in principle available upon request
- data is downloaded from the respective publications
",2022-08-12
https://github.com/aldertzomer/cgmlst,"[![License: GPL v2](https://img.shields.io/badge/License-GPL%20v2-blue.svg)](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html) ![](https://img.shields.io/badge/Language-Perl_5-steelblue.svg)

# cgmlst

Fork of Torsten Seemanns excellent [mlst](https://github.com/tseemann/mlst) tool modified for cgMLST. Schemes supported are campylobacter, ecoli and Lmono. Others may work as well. 
Text below was crudely adapted from the readme of mlst

## Quick Start

    % cgmlst --scheme=ecoli contigs.fa > output.tsv

## Installation

### Source

    % cd $HOME
    % git clone https://github.com/aldertzomer/cgmlst.git
    % cd cgmlst
    % bash getdb.sh #needs wget. May take some time on slow connections. 

    
### Dependencies

* [NCBI BLAST+ blastn](https://www.ncbi.nlm.nih.gov/books/NBK279671/) 
  * You probably have `blastn` already installed already.
* Perl modules *Moo* and *List::MoreUtils*
  * Debian: `sudo apt-get install libmoo-perl liblist-moreutils-perl`
  * Redhat: `sudo apt-get install perl-Moo perl-List-MoreUtils`
  * Most Unix: `sudo cpan Moo List::MoreUtils`
* Wget
  * Debian: `sudo apt-get install wget`

## Usage

Simply just give it a genome file in FASTA or GenBank file!

    % cgmlst --scheme=ecoli contigs.fa

It returns a tab-separated line containing
* the filename
* the ST (sequence type)
* the allele IDs

## Available schemes

To see which PubMLST schemes are supported:

    % cgmlst --list
    
    Lmono campylobacter ecoli 

## Missing data

cgmlst does not just look for exact matches to full length alleles. 
It attempts to tell you as much as possible about what it found using the
notation below:

Symbol | Meaning
--- | ---
`n` | exact intact allele
`~n` | novel allele similar to n
`n?` | partial match to known allele
`n,m` | multiple alleles
`-` | allele missing

## Bugs

Please submit via the Github Issues page: https://github.com/aldertzomer/cgmlst/issues

## License

GPLv2: https://raw.githubusercontent.com/aldertzomer/cgmlst/master/LICENSE

",2022-08-12
https://github.com/aldertzomer/essentials,"# essentials

ESSENTIALS: Software for rapid analysis of transposon insertion sequencing data

Aldert Zomer1,2, Peter Burghout1, Hester Bootsma1, Peter WM Hermans1 and Sacha van Hijum2,3,4,5

1 Laboratory of Pediatric Infectious Diseases, Radboud University Nijmegen Medical Centre, P.O. Box 9101, 6500 HB, Nijmegen, The Netherlands.
2 Centre for Molecular and Biomolecular Informatics, Nijmegen Centre for Molecular Life Sciences, Radboud University Nijmegen Medical Centre, P.O. Box 9101, 6500 HB, Nijmegen, The Netherlands.
3 NIZO food research, Kluyver Centre for Genomics of Industrial Fermentation, P.O. Box 20, 6710 BA Ede, The Netherlands
4 TI Food and Nutrition, P.O. Box 557, 6700 AN Wageningen, The Netherlands
5 Netherlands Bioinformatics Centre, 260 NBIC, P.O. Box 9101, 6500 HB Nijmegen, the Netherlands

High-throughput analysis of genome-wide transposon mutant libraries is a powerful tool for (conditional) essential gene discovery. Recently, several next generation sequencing approaches, e.g. Tn-seq, INseq and TraDIS, have been developed that accurately map the site of transposon insertions by mutant-specific amplification and sequence readout of DNA flanking the transposon insertions site, assigning a measure of essentiality based on the number of reads per gene or per mutant. However, analysis of these large and complex datasets is hampered by the lack of an easy to use and automated tool for transposon insertion sequencing data.

To fill this gap, we developed ESSENTIALS, an open source, web-based software tool for researchers in the genomics field utilizing transposon insertion sequencing analysis. It accurately predicts (conditionally) essential genes and offers the flexibility of using different sample normalization methods, genomic location bias correction, data preprocessing steps, appropriate statistical tests and various visualizations to examine the results, while requiring only a minimum of input and hands-on work from the researcher.

We successfully applied ESSENTIALS to in-house and published Tn-seq, TraDIS and HITS datasets and we show that the various pre- and post-processing steps on the sequence reads and count data with ESSENTIALS markedly improve the sensitivity and specificity of predicted gene essentiality.

Essentials at CMBI

A working version of the ESSENTIALS pipeline can be accessed at http://bamics2.cmbi.ru.nl/websoftware/essentials/essentials_start.php

-> First time I wrote perl code, 7 years ago - don't judge. 
",2022-08-12
https://github.com/aldertzomer/FastDeMe,"# FastDeMe

A fast, easy solution for metagenomic data analysis. An outline of the program is given below:

![Screenshot](Metagenomics_pipeline2.png)

A report of FastDeme containing some benchmarks can be found here: [Sander Vermeulen internship report](https://github.com/aldertzomer/FastDeMe/blob/master/Report%20Sander%20Vermeulen.pdf)

## Installation

The program can be downloaded as an archive or with the following git command:

`git clone https://github.com/aldertzomer/FastDeMe.git`

Next, the databases should be downloaded. This can be done by running `getdb.sh` located in the `db/` directory. This will download the four databases that are needed for the program to run and unzip them. Please make sure enough space is available on the drive, since the combined size of the databases is ~165 GB. The combined download size is ~116 GB.

Other requirements include Numpy:

`pip3 install numpy`

After downloading, the program can be invoked with `wrapper.py`.

## Usage
The program has two mandatory arguments, `--inp` and `--output`. To use the basic version of the program, the following command can be ran:

`./wrapper.py --inp file.fastq.gz --output /path/to/output/folder/ --trimming`

This will only result in the file getting trimmed.

To trim, screen the input files for host contamination, perform taxonomic identification with Kaiju and analyse the resistome with GROOT the following command can be used:

`./wrapper.py --pe --inp file_R1.fastq.gz file_R2.fastq.gz --output /path/to/output/folder/ --trimming --screening --kaiju --groot`

The flag `--pe` is needed when using paired end files. `--kaiju`, `--groot`, `--kraken` `--trimming`, `--kma` and `--screening` turn on the respective modules. 

16 CPU cores will be used by default. To limit or increase the amount of CPU cores used, one can use `--threads`. Note that trimming will not use more than 16 cores, even when more are specified.

## Database information
### GROOT

The GROOT database consist of a mixture of the ResFinder, ARG-ANNOT and CARD databases. See the [GROOT documentation](https://groot-documentation.readthedocs.io/en/latest/groot-databases.html) for more details.

### KMA

The KMA database consist of the ResFinder database.

### Kaiju

The Kaiju database was made with assembled and annotated bacterial reference genomes from the NCBI RefSeq database.

### Kraken2

The Kraken2 database was made with the complete bacterial reference genomes from the NCBI RefSeq database.

### Mash/BioBloomCategorizer

The Mash database was made from the complete vertebrate_mammalian and vertebrate_other databases from NCBI RefSeq. Since the bloom filters BioBloomCategorizer uses for filtering the host reads are quite large, only filters for common host species are included in the standard database to reduce download size.

Species in standard database|GCF ID
--------------|----------------
*Bos indicus*|GCF_000247795.1
*Bos mutus*|GCF_000298355.1
*Bos taurus*|GCF_002263795.1
*Canis lupus familiaris*|GCF_000002285.3
*Capra hircus*|GCF_001704415.1
*Chinchilla lanigera*|GCF_000276665.1
*Equus caballus*|GCF_002863925.1
*Felis catus*|GCF_000181335.3
*Gorilla gorilla gorilla*|GCF_000151905.2
*Homo sapiens*|GCF_000001405.38
*Mus musculus*|GCF_000001635.26
*Ovis aries*|GCF_000298735.2
*Ovis aries musimon*|GCF_000765115.1
*Pan troglodytes*|GCF_002880755.1
*Rattus norvegicus*|GCF_000001895.5, GCF_000002265.2
*Sus scrofa*|GCF_000003025.6
*Danio rerio*|GCF_000002035.6
*Gallus gallus*|GCF_000002315.5
*Meleagris gallopavo*|GCF_000146605.2

In case contamination is detected and the host is not in the standard database, the corresponding bloom filter will be downloaded automatically.

## Output
Output obviously depends on what modules are used for analysis. The following files are expected as output for each module:

### Trimming
QC report

`<prefix>_QC_results.txt`

For paired end:

`<prefix>_trimmed_1.fastq.gz`

`<prefix>_trimmed_2.fastq.gz`

For single end:

`<prefix>_trimmed.fastq.gz`

### Screening
For paired end:

`<prefix>_noMatch_1.fastq`

`<prefix>_noMatch_2.fastq`

`<prefix>_GCF*.fna.gz_1.`

`<prefix>_GCF*.fna.gz_2.`

For single end:

`<prefix>_noMatch.fastq`

`<prefix>_GCF*.fna.gz.`

These files contain the (trimmed) fastq files with removed host DNA (`noMatch`) and the removed host reads (`GCF`).

### Kaiju
Output for Kaiju depends on which taxonomic rank is selected. 

`<prefix>_<tax_rank>_kaiju_summary.txt`

These files contain the names and abundance of the selected taxonomic ranks in the samples.

### Kraken2
Output for Kraken2 depends on which taxonomic rank is selected.

`<prefix>_<tax_rank>.bracken`

Contains output of Bracken, sorted by taxonomic rank.

`<prefix>_kraken_report.txt`

Contains output of Kraken2.

`<prefix>_kraken_report_bracken.txt`

Contains all output of Bracken. 

### GROOT
`<prefix>_groot_report.txt`

Contains information about the found antibiotic resistance genes.

`/output_folder/<prefix>_groot/`

Folder with `.gfa` files of the found antibioitc resistance genes.

### KMA
`<prefix>_kma.aln`

Contains  alignments of resistance genes against input.

`<prefix>_kma.fsa`

Contains sequences of found resistance genes in FASTA format.

`<prefix>_kma.res`

Contains information about the found antibiotic resistance genes.

## Options
```
usage: wrapper.py --inp file.fastq.gz --output /path/to/output/folder/ [OPTIONS]

  -h, --help           show this help message and exit  
  --pe                 specify paired-end data, default is single end  
  --inp INP [INP ...]  input files in fastq.gz format, if paired-end input  
                       both files with a space between them                       
  --threads THREADS    specify number of threads to be used, default is max
                       available threads up to 16 threads                       
  --kaiju              use kaiju for taxonomic identification  
  --kraken             use kraken2 for taxonomic identification  
  --groot              use groot for resistome analysis
  --kma                use kma for resistome analysis
  --tax_rank TAX_RANK  set taxonomic rank for output. choose one: phylum,
                       class, order, family, genus, species, default is all
                       ranks.                       
  --prefix PREFIX      prefix for all output files, default is name of input
                       file(s)                       
  --trimming           turn on trimming with fastp  
  --screening          turn on host contamination screening with mash and
                       BioBloomCategorizer                       
  --output OUTPUT      set output directory
```
## License

## Acknowledgments

### Fastp
Shifu Chen, Yanqing Zhou, Yaru Chen, Jia Gu; fastp: an ultra-fast all-in-one FASTQ preprocessor,  Bioinformatics, Volume 34, Issue 17, 1 September 2018, Pages i884–i890, [paper](https://doi.org/10.1093/bioinformatics/bty560)

[GitHub](https://github.com/OpenGene/fastp)

### Mash
Mash: fast genome and metagenome distance estimation using MinHash. Ondov BD, Treangen TJ, Melsted P, Mallonee AB, Bergman NH, Koren S, Phillippy AM. Genome Biol. 2016 Jun 20;17(1):132. [paper](https://doi.org/10.1186/s13059-016-0997-x)

[GitHub](https://github.com/marbl/Mash)

### BioBloom Tools
BioBloom tools: fast, accurate and memory-efficient host species sequence screening using bloom filters.
Justin Chu, Sara Sadeghi, Anthony Raymond, Shaun D. Jackman, Ka Ming Nip, Richard Mar, Hamid Mohamadi, Yaron S. Butterfield, A. Gordon Robertson, Inanç Birol. Bioinformatics 2014; 30 (23): 3402-3404. [paper](https://doi.org/10.1093/bioinformatics/btu558)

[GitHub](https://github.com/bcgsc/biobloom)

### Kraken2
Wood DE, Salzberg SL: Kraken: ultrafast metagenomic sequence classification using exact alignments. Genome Biology 2014, 15:R46. [paper](https://doi.org/10.1186/gb-2014-15-3-r46) 

[GitHub](https://github.com/DerrickWood/kraken2)

### Bracken
Lu J, Breitwieser FP, Thielen P, Salzberg SL. 2017. Bracken: estimating species abundance in metagenomics data. PeerJ Computer Science 3:e104 [paper](https://doi.org/10.7717/peerj-cs.104)

[GitHub](https://github.com/jenniferlu717/Bracken)

### Kaiju
Menzel, P. et al. (2016) Fast and sensitive taxonomic classification for metagenomics with Kaiju. Nat. Commun. 7:11257 [paper](https://doi.org/10.1038/ncomms11257)

[GitHub](https://github.com/bioinformatics-centre/kaiju)

### GROOT
Will P M Rowe, Martyn D Winn; Indexed variation graphs for efficient and accurate resistome profiling, Bioinformatics, Volume 34, Issue 21, 1 November 2018, Pages 3601–3608, [paper](https://doi.org/10.1093/bioinformatics/bty387)

[GitHub](https://github.com/will-rowe/groot)

### KMA
Philip T.L.C. Clausen, Frank M. Aarestrup & Ole Lund, ""Rapid and precise alignment of raw reads against redundant databases with KMA"", BMC Bioinformatics, 2018;19:307. [paper](https://doi.org/10.1186/s12859-018-2336-6)

[BitBucket](https://bitbucket.org/genomicepidemiology/kma)


",2022-08-12
https://github.com/aldertzomer/Microbial-Genomics-2019,"Course Microbial Genomics 2019
==============

This lessons are partially based on the [Data Carpentry Genomics lesson](http://www.datacarpentry.org/genomics-workshop/) and are styled with the [Software Carpentry lesson template](https://swcarpentry.github.io/lesson-example/) which are made available under the (Creative Commons Attribution license](https://creativecommons.org/licenses/by/4.0/) adapted by Anita Schuerch.
 
If you would like to contribute, here are a few useful links: 
 - For instruction how the lessons are organized, please visit [this site](https://swcarpentry.github.io/lesson-example/03-organization/index.html)
 - On [formatting](https://swcarpentry.github.io/lesson-example/04-formatting/index.html)
 - Do not hesitate to contact [me](mailto:a.l.zomer@uu.nl)
",2022-08-12
https://github.com/aldertzomer/Microbial-Genomics-2021,"Course Microbial Genomics 2021
==============

This lessons were partially based on the [Data Carpentry Genomics lesson](http://www.datacarpentry.org/genomics-workshop/) and are styled with the [Software Carpentry lesson template](https://swcarpentry.github.io/lesson-example/) which are made available under the (Creative Commons Attribution license](https://creativecommons.org/licenses/by/4.0/) adapted by Anita Schuerch, Bas Dutilh and Aldert Zomer.
 
If you would like to contribute, here are a few useful links: 
 - For instruction how the lessons are organized, please visit [this site](https://swcarpentry.github.io/lesson-example/03-organization/index.html)
 - On [formatting](https://swcarpentry.github.io/lesson-example/04-formatting/index.html)
 - Do not hesitate to contact [me](mailto:a.l.zomer@uu.nl)
",2022-08-12
https://github.com/aldertzomer/ResistNanome,"# ResistNanome
A pipeline-tool to easily extract the resistome information of a metagenomic sample sequenced using the Nanopore technique.

This pipeline should be reasonably fast in giving the antibiotic resistance genes present in the metagenomic Nanopore dataset, combined with the bacteria. 
Getting this data is the main objective, but a multitude of tools are build in to improve on the output.

![schemetic drawing pipeline](PipelineResistNanome.png)

-------------------------------------------

## Installation

ResitNanome contains most of what is needed. Only bokeh is to be pre-installed for the use of nanoQC.

To install ResistNanome through Unix:

```
git clone https://github.com/AStieva/ResistNanome
```

To include the databases:
```
cd ResistNanome/database
./getDB.sh
```
To delete what's not needed after downloading the databases
```
rm *.tar.gz
```
If you want to use nanoQC, please install bokeh like this: `pip3 install bokeh`

## Usage

When calling the program, two arguments are mandatory, so should always be given: `--inp` (or `-i`) and `--outdir` (or `-o`). After putting down these arguments, the (path to the) input data and the path to the output folder should be put in, respectively. For example when only calling the resistome  analysis: 

`./ResistNanomewrapper.py --inp file.fastq.gz --outdir /path/to/output/folder/ --resistome`

The output from this will be a pdf file which gives the top 10 reads and a tab seperated values file with all information. However, the output doesn't give any information about the quality of the reads and doesn't filter out anything.

## Options

```
usage: ./ResistNanomeWrapper.py --inp file.fastq.gz --outdir /path/to/output/folder/ [options]

-h, --help        Show this help message and exit
-i, --inp         Set the input file, in fastq or fastq.gz format, please make sure to input the whole path
-o, --outdir      Set the output directory, please make sure to input the whole path
-p, --prefix      Set the prefix for all output files, default is the name of the input files. This name can't contain 'temp_' as this is used to define temporary data
-t, --threads     Set the number of threads used, default is maximum available up to 16 threads
--keep            If called, all the intermediate data is also kept. Otherwise only outputs with new information wil be kept
--demux           Execute demultiplexing with Porechop, the verbose will be saved to a pdf file
-fl, --filtlong   Execute filtlong, add --minlen [bp] to add a custom minimum length of reads, default for this is 1000
--minlen          The option to put in the minimum length of the reads saved in bp, default is 1000. Needs filtlong to be used
-qc, --QC         Execute the quality control
--host            Execute the host contamination screening, the database has a lot of vertebrate, but if yours isn't in it, please feel free to add it
--lvl             The classification level for determining the abundance levels with Bracken. Default is all. Options are: K (kingdom level), P (phylum), C (class), O (order), F (family), G (genus), and S (species)
-ar, --resistome  Execute the resistome analysis/antibiotic resistance screening
-N, --repN        Replace the resistancy gene with a series of N. This can take a long time (up to multiple days for a few GB data)
--phred           The option of giving a minimum phred score for resistome filtering
--threading       Option to turn on threading. Both taxonomy and resistome will run at the same time, it uses more memory but is faster
-cs, --taxonomy   Execute the bacterial community screening
-gz, --gz         gzip-ing the fastq file(s) left at the end
```

## Implemented tools

### NanoQC
Wouter De Coster, Svenn D’Hert, Darrin T Schultz, Marc Cruts, Christine Van Broeckhoven, ""NanoPack: visualizing and processing long-read sequencing data"", Bioinformatics, Volume 34, Issue 15, 01 August 2018, Pages 2666–2669, [paper](https://doi.org/10.1093/bioinformatics/bty149)

Tool for creating Quality Controll graphs.

[GitHub](https://github.com/wdecoster/nanoQC)

>#### bokeh
 >[Information page](https://docs.bokeh.org/en/latest/index.html)

 >Nanoqc uses bokeh for the visualisation

 >[GitHub](https://github.com/bokeh/bokeh)

### Filtlong
Ryan Wick filtlong [GitHub](https://github.com/rrwick/Filtlong)

Tool for filtering Nanopore data for length and quality.

### Porechop
Ryan R. Wick, Louise M. Judd, Claire L. Gorrie, Kathryn E. Holt, ""Completing bacterial genome assemblies with multiplex MinION sequencing"", MICROBIAL GENOMICS Volume 3, Issue 10, First Published: 14 September 2017 [paper](https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000132)

Tool for demultiplexing and cutting off barcodes.

[GitHub](https://github.com/rrwick/Porechop)

### Minimap2
Heng Li, ""Minimap2: pairwise alignment for nucleotide sequences"", Bioinformatics, Volume 34, Issue 18, 15 September 2018, Pages 3094–3100, [paper](https://doi.org/10.1093/bioinformatics/bty191)

Tool for alignment.

[GitHub](https://github.com/lh3/minimap2)

### Kraken2
Derrick E. Wood, Jennifer Lu, Ben Langmead, ""Improved metagenomic analysis with Kraken 2"", bioRxiv 762302 [paper](https://doi.org/10.1101/762302)
 
Tool for community screening.
 
[GitHub](https://github.com/DerrickWood/kraken2)
 
### Bracken
Jennifer Lu, Florian P. Breitwieser, Peter Thielen, Steven L. Salzberg, ""Bracken: estimating species abundance in metagenomics data"", Article in Computer Science, published January 2017, [paper](https://peerj.com/articles/cs-104/)

Tool for giving an abundance estimate based of a kraken(2) report.

[GitHub](https://github.com/jenniferlu717/Bracken)

### PyFPDF
Olivier Plathey [FPDF](http://www.fpdf.org/) Original FPDF for PHP
Max Pat, Mariano Reingart, Roman Kharin [readthedocs](https://pyfpdf.readthedocs.io/en/latest/) Python version of FPDF. Original python version by Max Pat, forked version used.

Tool for making PDF-files with code. Used to write output directly to PDF.

[GitHub FPDF](https://github.com/Setasign/FPDF)

[GitHub Python](https://github.com/reingart/pyfpdf)

### pysam
Andreas Heger, Kevin Jacobs et al. 2009 [readthedocs](https://pysam.readthedocs.io/en/latest/#)

Tool for manipulating SAM files.

[GitHub](https://github.com/pysam-developers/pysam)

### KMA
Philip T.L.C. Clausen, Frank M. Aarestrup & Ole Lund, ""Rapid and precise alignment of raw reads against redundant databases with KMA"", BMC Bioinformatics, 2018;19:307, [paper](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2336-6)

Tool for k-mer alignment. Often used for determenation of the resistome.

[Bitbucket](https://bitbucket.org/genomicepidemiology/kma/src/master/)

-------------------------------------------

###### Developed during my internship at Universiteit Utrecht under the guidance of Dr. A.L. Zomer
",2022-08-12
https://github.com/aldertzomer/RFPlasmid,"[![License: GPL v3](https://img.shields.io/badge/License-GPL%20v3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)
[![PIP](https://img.shields.io/pypi/v/RFPlasmid)](https://pypi.org/project/rfplasmid/)
[![CONDA](https://img.shields.io/conda/v/bioconda/rfplasmid)](https://anaconda.org/bioconda/rfplasmid)

# RFPlasmid
Predicting plasmid contigs from assemblies

## Webinterface

A web-interface to test single fasta files is available here: http://klif.uu.nl/rfplasmid/

## Table of Contents

* [Abstract](#abstract)
* [How to run for the impatient](#running-rfplasmid)
* [Getting the software](#getting-the-software)
    * [Conda](#using-conda)
    * [Pip](#using-pip)
      * [Getting CheckM databases](#required-if-you-have-never-installed-checkm-before)
      * [Dependencies](#dependencies-you-need-to-install-when-installing-rfplasmid-using-pip)
    * [Installing development version from Github as regular user](#for-advanced-users-that-want-to-install-the-latest-version-from-github)
    * [Systemwide install of development version from Github](#systemwide-install)
    * [Google Colab notebook](#google-colab-notebook)
* [Output files](#output-files)
* [Output explained](#output-explained)
* [Training models](#training-your-own-model)

## Abstract

**Predicting plasmid contigs from assemblies using single copy marker genes, plasmid genes, kmers**

Linda van der Graaf-van Bloois, Jaap Wagenaar, Aldert Zomer

**Introduction:** Antimicrobial resistant (AMR) genes in bacteria are often carried on plasmids. Since
these plasmids can spread the AMR genes between bacteria, it is important to know if the genes
are located on highly transferable plasmids or in the more stable chromosomes. Whole genome
sequence (WGS) analysis makes it easy to determine if a strain contains a resistance gene,
however, it is not easy to determine if the gene is located on the chromosome or on a plasmid as
genome sequence assembly generally results in 50-300 DNA fragments (contigs). With our newly
developed prediction tool, we analyze the composition of these contigs to predict their likely
source, plasmid or chromosomal. This information can be used to determine if a resistant gene is
chromosomally located or on a plasmid. The tool is optimized for 19 different bacterial species,
including *Campylobacter*, *E. coli*, and *Salmonella*, and can also be used for metagenomic
assemblies.

**Methods:** The tool identifies the number of chromosomal marker genes, plasmid replication genes
and plasmid typing genes using CheckM and DIAMOND Blast, and determines pentamer
frequencies and contig sizes per contig. A prediction model was trained using Random Forest on
an extensive set of plasmids and chromosomes from 19 different bacterial species and validated
on separate test sets of known chromosomal and plasmid contigs of the different bacteria.

**Results:** We show that RFplasmid is able to predict chromosomal and plasmid contigs with error rates ranging from 0.002% to 4.66% and that the use of taxon specific models can be superior to a general plasmid prediction model. Single copy chromosomal marker genes, plasmid genes, k-mer content and length of contig all appear to be informative, however k-mer content is highly specific for taxa. Prediction of small contigs remains unreliable, since these contigs consists primarily of repeated sequences present in both plasmid and chromosome, e.g. transposases or because k-mer content or marker genes cannot be easily identified. 

**Conclusion:** The newly developed tool is able to determine if contigs are chromosomal or plasmid
with a very high specificity and sensitivity (up to 99%) and can be very useful to analyze WGS
data of bacterial genomes and their antimicrobial resistance genes.

## Running RFPlasmid

```
$ rfplasmid --initialize # Only once after installing it. See ""Getting the software"" below
$ rfplasmid

Error; no arguments. Required to specificy --input and --species
usage: rfplasmid.py [-h] [--species SPECIES] [--input INPUT] [--specieslist] [--jelly] [--out OUT] [--debug] [--training] [--threads THREADS] [--version]

optional arguments:
  -h, --help         show this help message and exit
  --species SPECIES  define species (required)
  --input INPUT      directory with input fasta files (required)
  --specieslist      list of available species models
  --jelly            run jellyfish as kmer-count (faster)
  --out OUT          specify output directory
  --debug            no cleanup of intermediate files
  --training         trainings mode Random Forest
  --threads THREADS  specify number of threads to be used, default is max available threads up to 16 threads
  --version          print version number

# Example
rfplasmid --species Campylobacter --input inputfolder --jelly --threads 8 --out outputfolder

```
A folder containing .fasta file is required as input.

--jelly requires a functional jellyfish install. Greatly speeds up the analysis. Strongly recommended as our kmer profiling method in Python is slow

Read specieslist.txt or run rfplasmid --specieslist for species specific models. We have a general Enterobacteriaceae model instead of a species model. All others are species except for the ""Generic"" model which can be used for unknown or metagenomics samples.

## Getting the software

### Using Conda 
thanks to https://github.com/rpetit3. Installs CheckM database as well. A Google Colab notebook in this repository gives an example. The script rfplasmid is placed in ~/.local/bin and assumes that is in your PATH, which is according to the systemd specification (https://www.freedesktop.org/software/systemd/man/file-hierarchy.html). If not, please run the export PATH line. 
```
$ conda install -c bioconda rfplasmid 
$ # or alternatively: conda create -n rfplasmid -c conda-forge -c bioconda rfplasmid ; conda activate rfplasmid
$ rfplasmid --initialize # Bash helper script to locate rfplasmid.py and initialize the plasmid databases
$ export PATH=$PATH:~/.local/bin/ #only necessary if you have not included ~/.local/bin in your path (unusual but it has been observed). 
$ rfplasmid
```

### Using Pip 
Installs most requirements except DIAMOND and JellyFish and R (see below). You need to download HMMER, Prodigal and a database for CheckM if you have never installed it. A Google Colab notebook in this repository gives an example if you want to do this systemwide. This is much more work than Conda and requires more skills. 

```
$  pip3 install rfplasmid
$  export PATH=$PATH:~/.local/bin # pip installs in ~/.local/bin and it should be in your path but some distros don't have this set (even though they should).
$  rfplasmid --initialize #We makes use of a bash helper script to locate the rfplasmid.py file and to download the plasmid databases as they are too large for pip
$  export PATH=$PATH:~/.local/bin/ #only necessary if you have not included ~/.local/bin in your path (unusual but it has been observed). 
$  rfplasmid
```

### Required if you have never installed CheckM before

CheckM relies on a number of precalculated data files which can be downloaded from https://data.ace.uq.edu.au/public/CheckM_databases/. Decompress the file to an appropriate folder and run the following to inform CheckM of where the files have been placed. The example below uses wget to download the an archive of file and installs them in your homedir.
```
$  sudo apt install hmmer # CheckM needs HMMER. See http://hmmer.org/documentation.html for other methods of downloading and installing it
$  cd ~
$  wget https://github.com/hyattpd/Prodigal/releases/download/v2.6.3/prodigal.linux
$  cp prodigal.linux ~/bin/prodigal #we assume you have a ~/bin/ folder and it's in your path. 
$  chmod +x ~/bin/prodigal  
$  mkdir .checkm
$  cd .checkm
$  wget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz
$  tar xzvf checkm_data_2015_01_16.tar.gz
$  checkm data setRoot ~/.checkm
```

### Dependencies you need to install when installing RFPlasmid using Pip
RandomForest package in R ( https://cran.r-project.org/web/packages/randomForest/index.html ) (likely already installed). 
```
$  R
> install.packages(""randomForest"") #likely also already installed
```

DIAMOND ( https://github.com/bbuchfink/diamond ) 
```
$ wget http://github.com/bbuchfink/diamond/releases/download/v0.9.24/diamond-linux64.tar.gz
$ tar xzf diamond-linux64.tar.gz
$ cp diamond ~/bin/diamond
```

Strongly recommended: Jellyfish ( http://www.genome.umd.edu/jellyfish.html )
```
$ wget https://github.com/gmarcais/Jellyfish/releases/download/v2.2.10/jellyfish-linux
$ cp jellyfish-linux ~/bin/jellyfish
$ chmod +x ~/bin/jellyfish
```

### For advanced users that want to install the latest version from Github

You can get the source and using git and run from the folder you downloaded it to. You will need to install the requirements by hand as well
```
$ git clone https://github.com/aldertzomer/RFPlasmid.git
$ cd RFPlasmid
$ bash getdb.sh # downloads and formats the plasmid DBs
$ python3 rfplasmid.py
```

Installing requirements. Assumes you have ~/bin/ in your PATH. Depending on your setup you may need to follow the systemwide version (see further below)

Python 3 with pandas ( https://pandas.pydata.org/)
```
$  pip3 install pandas
```
CheckM ( https://ecogenomics.github.io/CheckM/ ). According to the github page of CheckM:
```
$  pip3 install numpy
$  pip3 install scipy
$  pip3 install pysam
$  pip3 install checkm-genome
$  cd ~
$  mkdir checkm_data
$  cd checkm_data
$  wget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz
$  tar xzvf checkm_data_2015_01_16.tar.gz
$  checkm data setRoot ~/checkm_data
```

RandomForest package in R ( https://cran.r-project.org/web/packages/randomForest/index.html )
```
$  R
> install.packages(""randomForest"")
```

DIAMOND ( https://github.com/bbuchfink/diamond )
```
$ wget http://github.com/bbuchfink/diamond/releases/download/v0.9.24/diamond-linux64.tar.gz
$ tar xzf diamond-linux64.tar.gz
$ cp diamond ~/bin/diamond
```

Strongly recommended: Jellyfish ( http://www.genome.umd.edu/jellyfish.html )
```
$ wget https://github.com/gmarcais/Jellyfish/releases/download/v2.2.10/jellyfish-linux
$ cp jellyfish-linux ~/bin/jellyfish
$ chmod +x ~/bin/jellyfish
```

## Systemwide install
Only if you are a system administrator and you know what you are doing. 
```
$ sudo pip3 install rfplasmid
$ sudo rfplasmid --initialize # Should install the databases as well provided all requirements are met
$ rfplasmid
```

Requirements if you want to install the requirements systemwide. Let your system administrator do this. 

Python 3 with pandas ( https://pandas.pydata.org/)
```
$ sudo pip3 install pandas
```
CheckM ( https://ecogenomics.github.io/CheckM/ ). According to the Wiki page of CheckM:

```
$  sudo su -
$  apt install hmmer # CheckM needs HMMER. See http://hmmer.org/documentation.html for other methods of downloading and installing it
$  wget https://github.com/hyattpd/Prodigal/releases/download/v2.6.3/prodigal.linux
$  cp prodigal.linux /usr/local/bin/prodigal
$  chmod +x /usr/local/bin/prodigal 
$  sudo pip3 install numpy
$  sudo pip3 install scipy
$  sudo pip3 install pysam
$  sudo pip3 install checkm-genome 
$  mkdir /usr/local/checkm_data
$  cd /usr/local/checkm_data
$  wget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz
$  tar xzvf checkm_data_2015_01_16.tar.gz
$  checkm data setRoot/usr/local/checkm_data
```

RandomForest package in R ( https://cran.r-project.org/web/packages/randomForest/index.html )
```
$ sudo R
> install.packages(""randomForest"")
```

DIAMOND ( https://github.com/bbuchfink/diamond )
```
$ wget http://github.com/bbuchfink/diamond/releases/download/v0.9.24/diamond-linux64.tar.gz
$ tar xzf diamond-linux64.tar.gz
$ sudo cp diamond /usr/local/bin/diamond
```

Strongly recommended: Jellyfish ( http://www.genome.umd.edu/jellyfish.html )
```
$ wget https://github.com/gmarcais/Jellyfish/releases/download/v2.2.10/jellyfish-linux
$ sudo cp jellyfish-linux /usr/local/bin/jellyfish
$ sudo chmod +x /usr/local/bin/jellyfish
```
## Google Colab Notebook

An alternative to installing RFPlasmid on your own server is running it from Google Colab. We have provided a Notebook suitable for Google Colab. Please navigate to 

https://github.com/aldertzomer/RFPlasmid/blob/master/RFPlasmid_Conda_install_and_run.ipynb 

and press ""Open in Google Colab"" and follow the instructions on the Google Colab notebook. 

The direct link is available here but only works if you have a signed in to your Google account: https://colab.research.google.com/github/aldertzomer/RFPlasmid/blob/master/RFPlasmid_Conda_install_and_run.ipynb

## Output Files

| File | Description |
| --------- | ----------- |
| prediction.csv | This is the primary output as comma delimited file. c = chromosome, p=plasmid. |
| prediction_full.csv | This is the primary output as comma delimited file including all input data. |
| classification.RData | The R object containing input and output. |
| outputdataframe.csv | Output from the various scripts used as input by the randomForest classifier. |
| other files | Temporary input files. Can be safely removed. |

## Output explained

The file  prediction.csv contains the contig number (column 1), the prediction wether it's chromosomal or plasmid (column 2), the votes for chromosome or plasmids (columns 3 and 4, and the original contig ID (column 5). 

| Contig | Prediction | Votes chromosomal | Votes plasmid | ContigID |
| --------- | ----------- | ----------- | ----------- | ----------- |
| Kp1_1 | p | 0.100 | 0.900 | Kp1_ctg1 |


The file  prediction_full.csv contains the data as before, but also includes the input data.

| Contig     | prediction | Votes chromosomal | Votes plasmid | contigID | Genome | contig length | % SCM | % plasmid genes | %ID plasmidfinder | Number of kmers | SCM genes | plasmid genes | kmer fractions | etc |
|------------|------------|-------------------|---------------|----------|--------|---------------|-------|-----------------|-------------------|-----------------|-----------|---------------|----------------|-----|
| Kp1_1 | p          | 0.101             | 0.899         | Kp1_ctg1 | Kp1    | 50000         | 0.12  | 0.51            | 69.5              | 138845          | 12        | 32            | 0.01477        |     |
|            |            |                   |               |          |        |               |       |                 |                   |                 |           |               |                |     |

All headers are explained below

| Header              | Header contents   | Explanation                                                                                                                |
|---------------------|-------------------|----------------------------------------------------------------------------------------------------------------------------|
|                     | Contig            | Contig number                                                                                                              |
| prediction          | prediction        | Plasmid or chromosome                                                                                                      |
| votes chromosomal   | Votes chromosomal | Random forest votes for the chromosome class (0-1)                                                                         |
| votes plasmid       | Votes plasmid     | Random forest votes for the plasmid class (0-1)                                                                            |
| contigID            | contigID          | Original contig ID                                                                                                         |
| genome              | Genome            | Name of genome                                                                                                             |
| contig_length       | contig length     | length of contig in bases                                                                                                  |
| SCM_genes           | % SCM             | Percentage of genes that is classified as chromosomal marker gene based   on CheckM                                        |
| plasmid_genes_genes | % plasmid genes   | Percentage of genes that is classified as chromosomal marker gene based   on alignment against a plasmid proteins database |
| plasmidcge_id       | %ID plasmidfinder | Highest identity percentage of hits against the PlasmidFinder database                                                     |
| kmer_number         | Number of kmers   | Number of kmers in the contig                                                                                              |
| SCM                 | SCM genes         | Number of genes that is classified as chromosomal marker gene based on   CheckM                                            |
| plasmid_genes       | plasmid genes     | Number of genes that is classified as chromosomal marker gene based on   alignment against a plasmid proteins database     |
| kmer1               | kmer fractions    | Fractions of the 1024 4-mers from AAAA to TTTTT                                                                            |
| etc                 |                   |                                                                                                                            |

## Training your own model

RFPlasmid comes with an option to train your own model if you provide it with contigs of chromosomes and plasmids. The following steps will generate the model for you and make it available for your RFplasmid installation. We will use the genus Lactococcus as an example.

The following steps need to be taken.
1. Get and install RFPlasmid from Github
2. Prepare a folder containing your plasmid contigs and your chromosome contigs. Draft genome assemblies are highly recommended, don't use completed genomes, as the genomes you want to analyze also won't be complete. Make sure that the plasmid contig file(s) start with ""p"" and the file(s) with the chromosomal contigs start with ""c"". In the example they are called plasmids.fasta and chromosomes.fasta, but as long as the files start with p or c your are good. They may also be separate files per genome/plasmid. You need about 20 plasmids and 20 chromosomes for a decent classification.
3. Add the name of your genus/family/etc to specieslist.txt. Use the name exactly as it is listed in CheckM for the appropriate taxon level. It is not recommended to use a species level model, else you will have to deal with spaces in names. Use only genus level or higher. 
4. Run RFplasmid in training mode. If you have very large numbers of contigs (>5000) it is recommended to adjust the sampsize option in the training.R script. It is automatically set to the maximum value (2/3rds of the smallest number of contigs per class).
5. Move the resulting training.rfo R object in the output folder to the RFplasmid folder and name it appropriately 
6. Test your new plasmid prediction model

Below an example of the commands. Replace Lactococcus with the genus of interest and ofcourse change ""/mnt/data/files"" to the folder containing the input
```
$ git clone https://github.com/aldertzomer/RFPlasmid.git
$ cd RFPlasmid
$ bash getdb.sh
$ mkdir Lactococcus
$ cat /mnt/data/files/p*.fasta > Lactococcus/plasmids.fasta
$ cat /mnt/data/files/c*.fasta > Lactococcus/chromosomes.fasta
$ checkm taxon_list |grep Lactococcus
$ echo ""Lactococcus genus"" >> specieslist.txt # this is important. Don't skip this step.
$ python3 rfplasmid.py --training --jelly --input Lactococcus --threads 16 --out Lactococcus.out --species Lactococcus
$ cp Lactococcus.out/training.rfo Lactococcus.rfo
```

It is recommended to load the object in R and explore it to check how well it performs using the OOB output and the confusion matrix. Generally you want to have the number of chromosome contigs that are predicted as being plasmid contigs as low as possible. It is also recommended to check the sizes of the contigs that are incorrectly predicted, as contigs <1 kb are more difficult to predict and at some point no improvements are possible. Modifying the ratio of the contigs sampled in the sampsize option in the training.R script may push the values to more chromosomal or plasmid contigs correctly predicted. In all our models the ratio was kept 1:1. 

```
$ R
> library(randomForest)
> load(""Lactococcus.rfo"")
> rf
```


Test your model using 

```
python3 rfplasmid.py --jelly --input Lactococcus --threads 16 --out Lactococcus.out2 --species Lactococcus
```

## Training data
Plasmid databases can be downloaded from: http://klif.uu.nl/download/plasmid_db/
Data used for training can be downloaded here: http://klif.uu.nl/download/plasmid_db/trainingsets2/
",2022-08-12
https://github.com/aldertzomer/scripts,"# scripts
Bash scripts that may be useful to others


iterative_pilon.sh : Run multiple pilon rounds on your data. Needs both long and short read data
",2022-08-12
https://github.com/aldertzomer/vibrio_parahaemolyticus_genomoserotyping,"## Development of Kaptive databases for Vibrio parahaemolyticus O- and K-antigen serotyping

Linda van der Graaf – van Bloois<sup>1,2</sup>, Hongyou Chen<sup>3</sup>, Jaap A. Wagenaar<sup>1,2,4</sup> and Aldert L. Zomer<sup>1,2</sup>

<sup>1</sup> Department of Infectious Diseases and Immunology, Faculty of Veterinary Medicine, Utrecht University, the Netherlands<p>
<sup>2</sup> WHO Collaborating Centre for Campylobacter / OIE Reference Laboratory for Campylobacteriosis<p>
<sup>3</sup> Shanghai Municipal Center for Disease Control and Prevention, No. 1380, Zhong Shan Xi Rd., Shanghai 200336, PR China<p>
<sup>4</sup> Wageningen Bioveterinary Research, Lelystad, the Netherlands<p>


Running title: Development of V. parahaemolyticus O- and K-antigen Kaptive databases

Corresponding author: Aldert Zomer
 
## Abstract

Vibrio parahaemolyticus is an important food-borne human pathogen and is divided in 16 O-serotypes and 71 K-serotypes. Agglutination tests are still the gold standard for serotyping, but many V. parahaemolyticus isolates are not typable by agglutination. An alternative for agglutination tests is serotyping using whole genome sequencing data. In this study, V. parahaemolyticus isolates are serotyped and sequenced, and all known and several novel O- and K-loci are identified. We developed Kaptive databases for all O- and K-loci after manual curation of the loci. These Kaptive databases with the identified V. parahaemolyticus O- and K -loci can be used to identify the O- and K-serotypes of V. parahaemolyticus isolates from genome sequences. 


## Getting Kaptive
Download Kaptive from https://github.com/katholt/Kaptive and install the software following the instructions on that page

## Using the Vibrio parahaemolyticus database
```
$ git clone https://github.com/aldertzomer/vibrio_parahaemolyticus_genomoserotyping.git
$ kaptive -k vibrio_parahaemolyticus_genomoserotyping.git/VibrioPara_Kaptivedb_K.gbk -a path/to/assemblies/*.fasta -o output
$ kaptive -k vibrio_parahaemolyticus_genomoserotyping.git/VibrioPara_Kaptivedb_O.gbk -a path/to/assemblies/*.fasta -o output
```


",2022-08-12
https://github.com/amices/callmice,"<!-- README.md is generated from README.Rmd. Please edit that file -->
callmice
========

Importing `mice` functionality into your own package
----------------------------------------------------

The [`mice`](https://cran.r-project.org/package=mice) package implements a method to deal with missing data. The `callmice` pachage tests and demonstrates how to incorporate functionality from the `mice` package into your own package.

Installation
------------

The `callmice` package can be installed from GitHub as follows:

``` r
install.packages(""devtools"")
devtools::install_github(repo = ""stefvanbuuren/callmice"")
```

Minimal example
---------------

``` r
library(callmice)

imp <- myimpute(m = 2, print = FALSE)
imp
#> Class: mids
#> Number of multiple imputations:  2 
#> Imputation methods:
#>   age   bmi   hyp   chl 
#>    """" ""pmm"" ""pmm"" ""pmm"" 
#> PredictorMatrix:
#>     age bmi hyp chl
#> age   0   1   1   1
#> bmi   1   0   1   1
#> hyp   1   1   0   1
#> chl   1   1   1   0
```
",2022-08-12
https://github.com/amices/dsMice,"<!-- README.md is generated from README.Rmd. Please edit that file -->

dsMice
======

Multivariate Imputation by Chained Equations for DataSHIELD - Server
--------------------------------------------------------------------

The [`mice`](https://github.com/stefvanbuuren/mice) package creates multiple imputations (replacement values) for multivariate missing data.

The [`DataSHIELD`](https://github.com/datashield) framework is a platform for federated data analysis that brings the algorithm to the data.

The [`dsMiceClient`](https://github.com/stefvanbuuren/dsMiceClient) package is an add-on to `mice` that makes multiple imputation available for federated data systems. This is the package that the `DataSHIELD` end user installs locally.

The [`dsMice`](https://github.com/stefvanbuuren/dsMice) package is part of the `DataSHIELD` infrastructure. This is the package that the `DataSHIELD` node owner installs on the server.

Installation
------------

The following code installs the `dsMice` package on the node server:

Install the `opaladmin` dependence using the R console.

```R
install.packages('opaladmin', repos=c('http://cran.rstudio.com/', 'http://cran.obiba.org'), dependencies=TRUE)
```
Call the `opal` and `opaladmin` library.
```R
library(opal)
library(opaladmin)
```
Connect to data nodes.
```R
o <- opal.login(username = 'user', password = 'pass', url = ""https://node-address-1"")
o2 <- opal.login(username = 'user', password = 'pass', url = ""https://node-address-2"")
o3 <- opal.login(username = 'user', password = 'pass', url = 'https://node-address-3')
```

Remove old version of the package (if you have installed it before).
```R
if (""dsMice"" %in% rownames(installed.packages())) dsadmin.remove_package(o, 'dsMice')
if (""dsMice"" %in% rownames(installed.packages())) dsadmin.remove_package(o2, 'dsMice')
if (""dsMice"" %in% rownames(installed.packages())) dsadmin.remove_package(o3, 'dsMice')
```

Install the package **devtools** into data node (only for the first run).
```R
oadmin.install_devtools(o)
oadmin.install_devtools(o2)
oadmin.install_devtools(o3)
```

Download the package from Git repository to data nodes.
```R
cmd <- paste('devtools::install_github(""stefvanbuuren/dsMice"")')
opal.execute(o, cmd)
opal.execute(o2, cmd)
opal.execute(o3, cmd)
```

Install the package into data nodes
```R
dsadmin.install_package(o, 'dsMice')
dsadmin.install_package(o2, 'dsMice')
dsadmin.install_package(o3, 'dsMice')
```

Publish the package's DataSHIELD methods
```R
dsadmin.set_package_methods(o, 'dsMice')
dsadmin.set_package_methods(o2, 'dsMice')
dsadmin.set_package_methods(o3, 'dsMice')
```

Logout from Opal
```R
opal.logout(o)
opal.logout(o2)
opal.logout(o3)
```

In order to work well, the end user should that the [`dsMiceClient`](https://github.com/stefvanbuuren/dsMiceClient) package installed locally.

Note
----

Warning: This is an experimental feature. These function do not yet actually work. If you have ideas about the integration of `mice` and `DataSHIELD` feel free to join in.

Related initiative
------------------

Related work appears in [`gflcampos/dsMice`](https://github.com/gflcampos/dsMice) and [`gflcampos/dsMiceClient`](https://github.com/gflcampos/dsMiceClient).

Minimal example
---------------

Include minimal example here using public DataSHIELD nodes.
",2022-08-12
https://github.com/amices/dsMiceClient,"<!-- README.md is generated from README.Rmd. Please edit that file -->
dsMiceClient
============

Multivariate Imputation by Chained Equations for DataSHIELD - Client
--------------------------------------------------------------------

The [`mice`](https://github.com/stefvanbuuren/mice) package creates multiple imputations (replacement values) for multivariate missing data.

The [`DataSHIELD`](https://github.com/datashield) framework is a platform for federated data analysis that brings the algorithm to the data.

The [`dsMiceClient`](https://github.com/stefvanbuuren/dsMiceClient) package is an add-on to `mice` that makes multiple imputation available for federated data systems. This is the package that the `DataSHIELD` end user installs locally.

The [`dsMice`](https://github.com/stefvanbuuren/dsMice) package is part of the `DataSHIELD` infrastructure. This is the package that the `DataSHIELD` node owner installs on the server.

Installation
------------

The following code installs the `dsMiceClient` package:

``` r
install.packages(""devtools"")
devtools::install_github(""stefvanbuuren/dsMiceClient"")
```

In order to work well, the `DataSHIELD` nodes should have installed the server site functions available from [`dsMice`](https://github.com/stefvanbuuren/dsMice)

Note
----

Warning: This is an experimental feature. These function do not yet actually work. If you have ideas about the integration of `mice` and `DataSHIELD` feel free to join in.

Related initiative
------------------

Related work appears in [`gflcampos/dsMice`](https://github.com/gflcampos/dsMice) and [`gflcampos/dsMiceClient`](https://github.com/gflcampos/dsMiceClient).

Minimal example
---------------

    Include minimal example here using public DataSHIELD nodes.
",2022-08-12
https://github.com/amices/ggmice,"
<!-- README.md is generated from README.Rmd. Please edit that file and render with devtools::build_readme() -->

# `ggmice` <a href='https://amices.org/ggmice/'><img src=""man/figures/logo.png"" align=""right"" height=""139"" /></a>

<!-- badges: start -->

[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/ggmice)](https://cran.r-project.org/package=ggmice)
[![Total CRAN
downloads](https://cranlogs.r-pkg.org/badges/grand-total/ggmice)](https://cranlogs.r-pkg.org/badges/grand-total/ggmice)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6532702.svg)](https://doi.org/10.5281/zenodo.6532702)

[![GitHub R package
version](https://img.shields.io/github/r-package/v/amices/ggmice?color=yellow&label=dev)](https://github.com/amices/ggmice/blob/main/DESCRIPTION)
[![Lifecycle:
experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![R-CMD-check](https://github.com/amices/ggmice/workflows/R-CMD-check/badge.svg)](https://github.com/amices/ggmice/actions)
<!-- badges: end -->

## Visualizations for `mice` with `ggplot2`

Enhance a [`mice`](https://amices.org/mice) imputation workflow with
visualizations for incomplete and/or imputed data. The `ggmice`
functions produce
[`ggplot`](https://ggplot2.tidyverse.org/reference/ggplot) objects which
may be easily manipulated or extended. Use `ggmice` to inspect missing
data, develop imputation models, evaluate algorithmic convergence, or
compare observed versus imputed data.

## Installation

You can install the latest `ggmice` release from
[CRAN](https://CRAN.R-project.org/package=ggmice) with:

``` r
install.packages(""ggmice"")
```

Alternatively, you could install the development version of `ggmice`
from [GitHub](https://github.com/amices) with:

``` r
# install.packages(""devtools"")
devtools::install_github(""amices/ggmice"")
```

## Example

Inspect the missing data in an incomplete dataset and subsequently
evaluate the imputed data points against observed data. See the [Get
started](https://amices.org/ggmice/articles/ggmice.html) vignette for an
overview of all functionalities. Example data from
[`mice`](https://amices.org/mice/reference/boys).

``` r
# load packages
library(ggplot2)
library(mice)
library(ggmice)
# load some data
dat <- boys
# visualize the incomplete data
ggmice(dat, aes(age, bmi)) + geom_point()
```

<img src=""man/figures/README-example-1.png"" width=""100%"" />

``` r
# impute the incomplete data
imp <- mice(dat, m = 1, seed = 1, printFlag = FALSE)
# visualize the imputed data
ggmice(imp, aes(age, bmi)) + geom_point() 
```

<img src=""man/figures/README-example-2.png"" width=""100%"" />

## Acknowledgements

The `ggmice` package is developed with guidance and feedback from Gerko
Vink, Stef van Buuren, Thomas Debray, Valentijn de Jong, Johanna Muñoz,
Thom Volker, Mingyang Cai and Anaïs Fopma. The `ggmice` hex is based on
designs from the `ggplot2` hex and the `mice` hex (by Jaden Walters).

This project has received funding from the European Union’s Horizon 2020
research and innovation programme under ReCoDID grant agreement No
825746.

## Code of Conduct

You are invited to join the improvement and development of `ggmice`.
Please note that the project is released with a [Contributor Code of
Conduct](https://amices.org/ggmice/CODE_OF_CONDUCT.html). By
contributing to this project, you agree to abide by its terms.

[![licence](https://img.shields.io/github/license/amices/ggmice?color=blue)](https://www.gnu.org/licenses/gpl-3.0.en.html)
[![Codecov test
coverage](https://codecov.io/gh/amices/ggmice/branch/main/graph/badge.svg)](https://app.codecov.io/gh/amices/ggmice?branch=main)
[![CII Best
Practices](https://bestpractices.coreinfrastructure.org/projects/6036/badge)](https://bestpractices.coreinfrastructure.org/projects/6036)
[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu)
",2022-08-12
https://github.com/amices/mice,"<!-- README.md is generated from README.Rmd. Please edit that file -->

# mice <a href='https://amices.github.io/mice/'><img src='man/figures/MICE_sticker_SMALL.png' align=""right"" height=""139"" /></a>

[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/mice)](https://cran.r-project.org/package=mice)
[![](https://cranlogs.r-pkg.org/badges/mice)](https://cran.r-project.org/package=mice)
[![](https://img.shields.io/badge/github%20version-3.14.7-orange.svg)](https://amices.github.io/mice/)

## [Multivariate Imputation by Chained Equations](https://amices.github.io/mice/)

The [`mice`](https://cran.r-project.org/package=mice) package implements
a method to deal with missing data. The package creates multiple
imputations (replacement values) for multivariate missing data. The
method is based on Fully Conditional Specification, where each
incomplete variable is imputed by a separate model. The `MICE` algorithm
can impute mixes of continuous, binary, unordered categorical and
ordered categorical data. In addition, MICE can impute continuous
two-level data, and maintain consistency between imputations by means of
passive imputation. Many diagnostic plots are implemented to inspect the
quality of the imputations.

## Installation

The `mice` package can be installed from CRAN as follows:

``` r
install.packages(""mice"")
```

The latest version can be installed from GitHub as follows:

``` r
install.packages(""devtools"")
devtools::install_github(repo = ""amices/mice"")
```

## Minimal example

``` r
library(mice, warn.conflicts = FALSE)

# show the missing data pattern
md.pattern(nhanes)
```

![Missing data pattern of `nhanes` data. Blue is observed, red is
missing.](man/figures/README-pattern-1.png)

    #>    age hyp bmi chl   
    #> 13   1   1   1   1  0
    #> 3    1   1   1   0  1
    #> 1    1   1   0   1  1
    #> 1    1   0   0   1  2
    #> 7    1   0   0   0  3
    #>      0   8   9  10 27

The table and the graph summarize where the missing data occur in the
`nhanes` dataset.

``` r
# multiple impute the missing values
imp <- mice(nhanes, maxit = 2, m = 2, seed = 1)
#> 
#>  iter imp variable
#>   1   1  bmi  hyp  chl
#>   1   2  bmi  hyp  chl
#>   2   1  bmi  hyp  chl
#>   2   2  bmi  hyp  chl

# inspect quality of imputations
stripplot(imp, chl, pch = 19, xlab = ""Imputation number"")
```

![Distribution of `chl` per imputed data
set.](man/figures/README-stripplot-1.png)

In general, we would like the imputations to be plausible, i.e., values
that could have been observed if they had not been missing.

``` r
# fit complete-data model
fit <- with(imp, lm(chl ~ age + bmi))

# pool and summarize the results
summary(pool(fit))
#>          term estimate std.error statistic    df p.value
#> 1 (Intercept)     9.08     73.09     0.124  4.50  0.9065
#> 2         age    35.23     17.46     2.017  1.36  0.2377
#> 3         bmi     4.69      1.94     2.417 15.25  0.0286
```

The complete-data is fit to each imputed dataset, and the results are
combined to arrive at estimates that properly account for the missing
data.

## `mice 3.0`

Version 3.0 represents a major update that implements the following
features:

1.  `blocks`: The main algorithm iterates over blocks. A block is simply
    a collection of variables. In the common MICE algorithm each block
    was equivalent to one variable, which - of course - is the default;
    The `blocks` argument allows mixing univariate imputation method
    multivariate imputation methods. The `blocks` feature bridges two
    seemingly disparate approaches, joint modeling and fully conditional
    specification, into one framework;

2.  `where`: The `where` argument is a logical matrix of the same size
    of `data` that specifies which cells should be imputed. This opens
    up some new analytic possibilities;

3.  Multivariate tests: There are new functions `D1()`, `D2()`, `D3()`
    and `anova()` that perform multivariate parameter tests on the
    repeated analysis from on multiply-imputed data;

4.  `formulas`: The old `form` argument has been redesign and is now
    renamed to `formulas`. This provides an alternative way to specify
    imputation models that exploits the full power of R’s native
    formula’s.

5.  Better integration with the `tidyverse` framework, especially for
    packages `dplyr`, `tibble` and `broom`;

6.  Improved numerical algorithms for low-level imputation function.
    Better handling of duplicate variables.

7.  Last but not least: A brand new edition AND online version of
    [Flexible Imputation of Missing Data. Second
    Edition.](https://stefvanbuuren.name/fimd/)

See [MICE: Multivariate Imputation by Chained
Equations](https://amices.github.io/mice/) for more resources.

I’ll be happy to take feedback and discuss suggestions. Please submit
these through Github’s issues facility.

## Resources

### Books

1.  Van Buuren, S. (2018). [Flexible Imputation of Missing Data. Second
    Edition.](https://stefvanbuuren.name/fimd/). Chapman & Hall/CRC.
    Boca Raton, FL.

### Course materials

1.  [Handling Missing Data in `R` with
    `mice`](https://amices.github.io/Winnipeg/)
2.  [Statistical Methods for combined data
    sets](https://stefvanbuuren.github.io/RECAPworkshop/)

### Vignettes

1.  [Ad hoc methods and the MICE
    algorithm](https://gerkovink.github.io/miceVignettes/Ad_hoc_and_mice/Ad_hoc_methods.html)
2.  [Convergence and
    pooling](https://gerkovink.github.io/miceVignettes/Convergence_pooling/Convergence_and_pooling.html)
3.  [Inspecting how the observed data and missingness are
    related](https://gerkovink.github.io/miceVignettes/Missingness_inspection/Missingness_inspection.html)
4.  [Passive imputation and
    post-processing](https://gerkovink.github.io/miceVignettes/Passive_Post_processing/Passive_imputation_post_processing.html)
5.  [Imputing multilevel
    data](https://gerkovink.github.io/miceVignettes/Multi_level/Multi_level_data.html)
6.  [Sensitivity analysis with
    `mice`](https://gerkovink.github.io/miceVignettes/Sensitivity_analysis/Sensitivity_analysis.html)
7.  [Generate missing values with
    `ampute`](https://rianneschouten.github.io/mice_ampute/vignette/ampute.html)

### Code from publications

1.  [Flexible Imputation of Missing Data. Second
    edition.](https://github.com/stefvanbuuren/fimdbook)

## Acknowledgement

The cute mice sticker was designed by Jaden M. Walters. Thanks Jaden!

## Code of Conduct

Please note that the mice project is released with a [Contributor Code
of Conduct](https://amices.github.io/mice/CODE_OF_CONDUCT.html). By
contributing to this project, you agree to abide by its terms.
",2022-08-12
https://github.com/amices/shinymice,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# shinymice

<!-- badges: start -->

[![Lifecycle:
experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
<!-- badges: end -->

This repository serves the development of `{shinymice}`, an interactive
evaluation device for missing data by Hanne Oberman, guided by Gerko
Vink and Stef van Buuren.

![shinymice hex sticker](inst/app/www/logo.png)

For a demo version of the app, see
<https://hanneoberman.shinyapps.io/shinymice-demo/>.

-Hanne (16-12-2021)

<!-- The goal of shinymice is to ... -->
<!-- ## Installation -->
<!-- You can install the released version of shinymice from [CRAN](https://CRAN.R-project.org) with: -->
<!-- ``` r -->
<!-- install.packages(""shinymice"") -->
<!-- ``` -->
<!-- And the development version from [GitHub](https://github.com/) with: -->
<!-- ``` r -->
<!-- # install.packages(""devtools"") -->
<!-- devtools::install_github(""amices/shinyMice"") -->
<!-- ``` -->
<!-- ## Example -->
<!-- This is a basic example which shows you how to solve a common problem: -->
<!-- ```{r example} -->
<!-- library(shinymice) -->
<!-- ## basic example code -->
<!-- ``` -->
<!-- What is special about using `README.Rmd` instead of just `README.md`? You can include R chunks like so: -->
<!-- ```{r cars} -->
<!-- summary(cars) -->
<!-- ``` -->
<!-- You'll still need to render `README.Rmd` regularly, to keep `README.md` up-to-date. -->
<!-- You can also embed plots, for example: -->
<!-- ```{r pressure, echo = FALSE} -->
<!-- plot(pressure) -->
<!-- ``` -->
<!-- In that case, don't forget to commit and push the resulting figure files, so they display on GitHub! -->
",2022-08-12
https://github.com/amices/Synthemice,"# Synthetic data with mice

The folder `mice_synthesizing` contains the simulation studies of interest, and includes a ReadMe file of its own, to guide the reader through any work produced in this folder. The `synthpop_synthesizing` folders contains a number of files related to simulations concerning synthesizing data with `synthpop`, but these are not of interest in the project anymore. The folder `gerko` contains some files that are used to assess how to obtain confidence valid variance estimates from bootstrapped data. 
",2022-08-12
https://github.com/amices/Winnipeg,"<!-- README.md is generated from README.Rmd. Please edit that file -->

[Winnipeg workshop: Handling missing data in `R` with `mice`](http://amices.github.io/Winnipeg/)
================================================================================================

Overview
--------

This site contains materials for the Biostatistics Workshop *Handling
missing data in `R` with `mice`* the 45th Annual Meeting of the
Statistical Society of Canada, dated Sunday, June 11, 2017, located in
Winnipeg E3 - 270 (EITC).

Motivation
----------

Nearly all data analytic procedures in R are designed for complete data,
and many will fail if the data contain missing values. Typically,
procedures simply ignore any incomplete rows in the data, or use ad-hoc
procedures like replacing missing values with some sort of “best value”.
However, such fixes may introduce biases in the ensuing statistical
analysis.

Multiple imputation is a principled solution for this problem. The aim
of this workshop is to enable participants to perform and evaluate
multiple imputation using the R package `mice`.

Contents
--------

The workshop will consist of 5 sessions, each of which comprises a
lecture followed by a computer practical using `R`:

1.  Session I: Introduction, issues raised by missing data, and towards
    a systematic approach
2.  Session II: Introduction to multiple imputation
3.  Session III: Multivariate missing data (joint model approach,
    chained equations)
4.  Session IV: Imputation in practice (large data sets, hierarchical
    data, non-linearities, interactions)
5.  Session V: After imputation, guidelines for analysis and reporting

How to prepare
--------------

Please remember to bring your own laptop computer and make sure that you
have write-access to that machine (some corporate computers do not allow
write access) or that you have the following software and packages
pre-installed.

------------------------------------------------------------------------

1.  Download and install the latest version of `R` from [the R-Project
    website](https://cloud.r-project.org)
2.  Download and install the most recent version of
    `RStudio Desktop (Free License)` from [RStudio’s
    website](https://www.rstudio.com/products/rstudio/download3/). This
    is not necessary, per se, but it is highly recommended as `RStudio`
    delivers a tremendous improvement to the user experience of base
    `R`.
3.  Install the following packages:
    [`mice`](https://cran.r-project.org/web/packages/mice/index.html),
    and
    [`lattice`](https://cran.r-project.org/web/packages/lattice/index.html)

-   You can simply install packages from within `RStudio` by navigating
    to `Tools > Install Packages` in the upper menu and entering
    `mice, lattice` into the `Packages` field. Make sure that the button
    `Install dependencies` is selected. Once done, click `Install` and
    you’re all set.
-   Or, from within `R` or `RStudio`, copy, paste and enter the
    following code in the console window (by default the top-right
    window in `RStudio` / the only window in `R`):

``` r
install.packages(""mice"")
install.packages(""lattice"")
```

------------------------------------------------------------------------

Workshop materials
------------------

1.  [Lectures](Lectures/Winnipeg.pdf)
2.  [Handout](Lectures/WinnipegHandout.pdf)
3.  [Practical I](Practicals/Practical_I.html)
4.  [Practical II](Practicals/Practical_II.html)
5.  [Practical III](Practicals/Practical_III.html)
6.  [Practical IV](Practicals/Practical_IV.html)
",2022-08-12
https://github.com/anastasiaGiachanou/Processing-Instagram-Packages,"This scripts have been created in order to process and extract information from the Instagram Download Packages. The current version processes messages and media json files. 

The DDP_dump contains a sample folder that can be used to check the code. More dump packages can be found at the following link: https://zenodo.org/record/4472606#.YNwmkTYzZTY 

",2022-08-12
https://github.com/anastasiaGiachanou/Programming-with-Python,"# Programming-with-Python
Contains material regarding day 3 and day 4 of the summer school course of Utrecht University ""Data Science: Programming with Python""

 Lecture notes and exercises for:
 - File input and output with Python
 - Data manipulation with Pandas (introduction and advanced)
 - Date and time manipulations with datetime and timestamp
 - Visualisations with matplotlib
 - Numerical operations with numPy
",2022-08-12
https://github.com/anastasiaGiachanou/Text-Analysis-of-National-Artificial-Intelligence-Policies,"# Document-Analysis-of-National-Artificial-Intelligence-Policies

This repository contains the code used for the text analysis performed for the following paper:
A Systematic Assessment of National Artificial Intelligence Policies: Perspectives from the Nordics and Beyond.
NordiCHI '20. 2020

The script topicModel.py was used for the experiments on topic extraction from the collection. 
The USE.py calculates the similarity aming the documents based on the Universal Sentence Encoder.
The scropts plot.py and utils.py contain additional functions that were used for the execution of the experiments

",2022-08-12
https://github.com/annalenalamprecht/Coffee-Partner-Lottery,"# Coffee Partner Lottery
A small project from Covid-19 times, for generating random pairings of people who can have a virtual chat and coffee together. The Python script in this repo takes care of the generation, my whole process for the lottery is as follows: 

1. Let interested people sign up. I use MS Forms for the Coffee Partner Lottery at UU's Department of Information and Computing Sciences (template available [here](https://forms.office.com/Pages/ShareFormPage.aspx?id=oFgn10akD06gqkv5WkoQ51EXCAYj7jZCpuwTHAmfcRhUQk1ZOEtTTDJBQVozU0tVN0ZSNlFGWDEwNC4u&sharetoken=NNfcIwyZoQl07ZdXpHkZ)), but Google Forms or another similar tool will also do. You can download the responses and save them as a CSV file (recommended name ""Coffee Partner Lottery participants.csv""), which is the input for the Python script.  
2. Run the Python script to generate a set of pairs. It will store all pairs ever generated in another CSV file (""Coffee Partner Lottery all pairs.csv""), to keep track of already generated pairs and thus making sure that new people meet each time. The new set of pairs is written to a separate CSV file (""Coffee Partner Lottery new pairs.csv""). 
3. Use Thunderbird's MailMerge plugin to automatically generate e-mails with the information from ""Coffee Partner Lottery new pairs.csv"", to inform people that they have been paired. I haven't tried, but probably MS Outlook and other e-mail clients have similar functionality. Alternatively, you can just copy the list of pairs from the program output or from the file ""Coffee Partner Lottery new pairs.txt"" and send them to a Teams chat, group mailing list, or similar.

# Other Versions
Different people have implemented variations of the same idea. Some that I am aware of: 
* [Et9797's extension](https://github.com/Et9797/mysterycoffee), written in Python and using Google services for participant signup and e-mail notifications 
* Barbara Vreede's [Mystery Coffee](https://github.com/bvreede/mystery_coffee), written in R
",2022-08-12
https://github.com/annalenalamprecht/CoTaPP,"# CoTaPP
Course material of the joint courses Computational Thinking, Programming with Python and Programming with Data at Utrecht University, abbreviated as CoTaPP. Comprising (to be extended):
* Lecture notes and exercises (previously in PDF format, now as interactive Jupyter notebooks)
* Project assignments (for four two-week projects, done in groups of 3-4 students)

## License
<a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by/4.0/88x31.png"" /></a><br />This work is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/"">Creative Commons Attribution 4.0 International License</a>.
",2022-08-12
https://github.com/annalenalamprecht/instructional-code,"# instructional-code
Repository for collecting pieces of instructional code from different CS courses.
",2022-08-12
https://github.com/aromanowski/2021-03-19-carpentries-test,"[![Build Status](https://travis-ci.com/carpentries/workshop-template.svg?branch=gh-pages)](https://travis-ci.com/carpentries/workshop-template)

# The Carpentries Workshop Template

This repository is The Carpentries' ([Software Carpentry][swc-site], [Data Carpentry][dc-site], and
[Library Carpentry][lc-site]'s) template for creating websites for workshops.

1. **Please _do not fork this repository directly on GitHub._** Instead, please use GitHub's
   ""template"" function following [the instructions below](#creating-a-repository) to copy this
   `workshop-template` repository and customize it for your workshop.

2. Please *do your work in your repository's `gh-pages` branch*, since that is what is
   [automatically published as a website by GitHub][github-project-pages].

3. Once you are done, please also [let us know][email] the workshop URL. If this is a self-organised
   workshop, you should also [fill in the self-organized workshop
   form][self-organized-workshop-form] (if you have not already done so), so we can keep track of
   all workshops. We build the list of workshops on our websites from the data included in your
   `index.md` page. We can only do that if you [customize][customization] that page correctly *and*
   let us know the workshop URL.

If you run into problems,
or have ideas about how to make this process simpler,
please [get in touch](#getting-and-giving-help).
The pages on [customizing your website][customization],
the [FAQ][faq],
and the [design notes][design] have more detail on what we do and why.
And please note:
if you are teaching Git,
please [create a separate repository](#setting-up-a-separate-repository-for-learners)
for your learners to practice in.

## Video Tutorial

There is a [YouTube video](https://www.youtube.com/watch?v=_Ag1JiZzyUQ) that demonstrates how to
create a workshop website.

## Creating a Repository

1.  Log in to GitHub.
    (If you do not have an account, you can quickly create one for free.)
    You must be logged in for the remaining steps to work.

2.  On this page (<https://github.com/carpentries/workshop-template>),
    click on the green ""Use this template"" button (top right)

    ![screenshot of this repository's GitHub page with an arrow pointing to the the 'use this template' button on the top left](fig/select-github-use-template.png?raw=true)

3.  Select the owner for your new repository.
    (This will probably be you, but may instead be an organization you belong to.)

4.  Choose a name for your workshop website repository.
    This name should have the form `YYYY-MM-DD-site`,
    e.g., `2016-12-01-oomza`,
    where `YYYY-MM-DD` is the start date of the workshop.
    If your workshop is held online, then the respository name should have `-online` in the end.
    e.g., `2016-12-01-oomza-online`

5.  Make sure the repository is public, leave ""Include all branches"" unchecked, and click
on ""Create repository from template"".
You will be redirected to your new copy of the workshop template respository.

6. Your new website will be rendered at `https://your_username.github.io/YYYY-MM-DD-site`.
For example, if your username is `gvwilson`, the website's URL will be
`https://gvwilson.github.io/2016-12-01-oomza`.

If you experience a problem, please [get in touch](#getting-and-giving-help).

## Customizing Your Website (Required Steps)

There are two ways of customizing your website. You can either:

- edit the files directly in GitHub using your web browser
- clone the repository on your computer and update the files locally

### Updating the files on GitHub in your web browser

1.  Go into your newly-created repository,
    which will be at `https://github.com/your_username/YYYY-MM-DD-site`.
    For example,
    if your username is `gvwilson`,
    the repository's URL will be `https://github.com/gvwilson/2016-12-01-oomza`.

3.  Ensure you are on the gh-pages branch by clicking on the branch under the drop
    down in the menu bar (see the note below):

    ![screenshot of this repository's GitHub page showing the ""Branch"" dropdown menu expanded with the ""gh-pages"" branch selected](fig/select-gh-pages-branch.png?raw=true)

3.  Edit the header of `index.md` to customize the list of instructors,
    workshop venue, etc.
    You can do this in the browser by clicking on it in the file view on GitHub
    and then selecting the pencil icon in the menu bar:

    ![screenshot of top menu bar for GitHub's file interface with the edit icon highlighted in the top right](fig/edit-index-file-menu-bar.png?raw=true)

    Editing hints are embedded in `index.md`,
    and full instructions are in [the customization instructions][customization].

4.  Remove the notice about using the workshop template in the `index.md` file. You can safely
    delete everything between the `{% comment %}` and `{% endcomment %}` (included) as indicated
    below (about from line 35 to line 51):

    ```jekyll
    {% comment %} <------------ remove from this line
    8< ============= For a workshop delete from here =============
    For a workshop please delete the following block until the next dashed-line
    {% endcomment %}

    <div class=""alert alert-danger"">
      ....
    </div>

    {% comment %}
     8< ============================= until here ==================
    {% endcomment %} <--------- until this line
    ```

4.  Edit `_config.yml` to customize certain site-wide variables, such as: `carpentry` (to tell your
    participants the lesson program for your workshop), `curriculum` and `flavor` for the
    curriculum  taught in your workshop, and `title` (overall title for all pages).

    Editing hints are embedded in `_config.yml`,
    and full instructions are in [the customization instructions][customization].

5. Edit the `schedule.html` file to edit the schedule for your upcoming workshop. This file is
   located in the `_includes` directory, make sure to choose the one from the appropriate `dc` (Data
   Carpentry workshop), `lc` (Library Carpentry), or `swc` (Software Carpentry) subdirectory.

### Working locally

> Note: you don't have to do this, if you have already updated your site using the web interface.


If you are already familiar with Git, you can clone the repository to your desktop, edit `index.md`,
`_config.yml`, and `schedule.html` following the instruction above there, and push your changes back to the repository.

```shell
git clone https://github.com/your_username/YYYY-MM-DD-site
```

In order to view your changes once you are done editing, if you have bundler installed (see the
[installation instructions below](#installing-software)), you can preview your site locally with:

```shell
make serve
```
and go to <http://0.0.0.0:4000> to preview your site.

Before pushing your changes to your repository, we recommend that you also check for any potential
issues with your site by running:

```shell
make workshop-check
```

Once you are satisfied with the edits to your site, commit and push the changes to your repository.
A few minutes later, you can go to the GitHub Pages URL for your workshop site and preview it. In the example above, this is `https://gvwilson.github.io/2016-12-01-oomza`. [The finished
page should look something like this](fig/completed-page.png?raw=true).


## Optional but Recommended Steps


### Update your repository description and link your website

At the top of your repository on GitHub you'll see

~~~
No description, website, or topics provided. — Edit
~~~

Click 'Edit' and add:

1.  A very brief description of your workshop in the ""Description"" box (e.g., ""Oomza University workshop, Dec. 2016"")

2.  The URL for your workshop in the ""Website"" box (e.g., `https://gvwilson.github.io/2016-12-01-oomza`)

This will help people find your website if they come to your repository's home page.

### Update the content of the README file

You can change the `README.md` file in your website's repository, which contains these instructions,
so that it contains a short description of your workshop and a link to the workshop website.


## Additional Notes

**Note:**
please do all of your work in your repository's `gh-pages` branch,
since [GitHub automatically publishes that as a website][github-project-pages].

**Note:**
this template includes some files and directories that most workshops do not need,
but which provide a standard place to put extra content if desired.
See the [design notes][design] for more information about these.

Further instructions are available in [the customization instructions][customization].
This [FAQ][faq] includes a few extra tips (additions are always welcome)
and these notes on [the background and design][design] of this template may help as well.


## Creating Extra Pages

In rare cases,
you may want to add extra pages to your workshop website.
You can do this by putting either Markdown or HTML pages in the website's root directory
and styling them according to the instructions give in
[the lesson template][lesson-example].


## Installing Software

If you want to set up Jekyll so that you can preview changes on your own machine before pushing them
to GitHub, you must install the software described in the lesson example [setup
instructions](https://carpentries.github.io/lesson-example/setup.html#jekyll-setup-for-lesson-development).

## Setting Up a Separate Repository for Learners

If you are teaching Git,
you should create a separate repository for learners to use in that lesson.
You should not have them use the workshop website repository because:

* your workshop website repository contains many files that most learners don't need to see during
  the lesson, and

* you probably don't want to accidentally merge a damaging pull request from a novice Git user into
  your workshop's website while you are using it to teach.

You can call this repository whatever you like, and add whatever content you need to it.

## Getting and Giving Help

We are committed to offering a pleasant setup experience for our learners and organizers.
If you find bugs in our instructions,
or would like to suggest improvements,
please [file an issue][issues]
or [mail us][email].

[email]: mailto:team@carpentries.org
[customization]: https://carpentries.github.io/workshop-template/customization/index.html
[dc-site]: https://datacarpentry.org
[design]: https://carpentries.github.io/workshop-template/design/index.html
[faq]: https://carpentries.github.io/workshop-template/faq/index.html
[github-project-pages]: https://help.github.com/en/github/working-with-github-pages/creating-a-github-pages-site
[issues]: https://github.com/carpentries/workshop-template/issues
[lesson-example]: https://carpentries.github.io/lesson-example/
[self-organized-workshop-form]: https://amy.carpentries.org/forms/self-organised/
[swc-site]: https://software-carpentry.org
[lc-site]: https://librarycarpentry.org
",2022-08-12
https://github.com/aromanowski/binder,"# Original source: [Binder / Alan-Turing-Institute](https://github.com/alan-turing-institute/the-turing-way/edit/master/workshops/boost-research-reproducibility-binder/workshop-presentations/zero-to-binder-python.md)

# From Zero to Binder in Python!

Sarah Gibson, _The Alan Turing Institute_

[**The Turing Way**](https://github.com/alan-turing-institute/the-turing-way) - making reproducible Data Science ""too easy not to do""

Based on Tim Head's _Zero-to-Binder_ workshops which can be found here: <http://bit.ly/zero-to-binder> and <http://bit.ly/zero-to-binder-rise>

To follow these instructions on your own machine, follow this link: **<http://bit.ly/zero-to-binder-python>**

Binder can take a long time to load, but this doesn't necessarily mean that your Binder will fail to launch.
You can always refresh the window if you see the ""... is taking longer to load, hang tight!"" message.

- [Running Code is more complicated than Displaying Code](#running-code-is-more-complicated-than-displaying-code)
- [What Binder Provides](#what-binder-provides)
- [1. Creating a repo to Binderize](#1-creating-a-repo-to-binderize)
  - [Why did the repo have to be public?](#why-did-the-repo-have-to-be-public)
- [2. Launch your first repo!](#2-launch-your-first-repo)
  - [What's happening in the background? - Part 1](#whats-happening-in-the-background---part-1)
- [3. Run `hello.py`](#3-run-hellopy)
- [4. Pinning Dependencies](#4-pinning-dependencies)
  - [What's happening in the background? - Part 2](#whats-happening-in-the-background---part-2)
  - [More on pinning dependencies](#more-on-pinning-dependencies)
- [5. Check the Environment](#5-check-the-environment)
- [6. Sharing your Work](#6-sharing-your-work)
- [7. Accessing data in your Binder](#7-accessing-data-in-your-binder)
  - [Small public files](#small-public-files)
  - [Medium public files](#medium-public-files)
  - [Large public files](#large-public-files)
  - [Private files](#private-files)
- [8. Get data with `postBuild`](#8-get-data-with-postbuild)
- [Beyond Notebooks...](#beyond-notebooks)
- [Now over to you!](#now-over-to-you)

---

## Running Code is more complicated than Displaying Code

GitHub is a great service for sharing code, but the contents are **static**.

How could you _run_ a GitHub repository **without installing complicated requirements**?
Or even **in your browser**?

To run code, you need:

- Hardware on which to run the code
- Software, including:
  - The code itself
  - The programming language (e.g. Python, R, Julia, and so on)
  - Relevant packages (e.g. pandas, matplotlib, tidyverse, ggplot)

## What Binder Provides

[Binder](https://mybinder.org) is a service that provides your code and the hardware and software to execute it.

You can create a link to a **live, interactive** version of your code!

- An example binder link:
  > **<https://mybinder.org/v2/gh/trekhleb/homemade-machine-learning/master?filepath=notebooks%2Fanomaly_detection%2Fanomaly_detection_gaussian_demo.ipynb>**
  From this repo: **<https://github.com/trekhleb/homemade-machine-learning>**
  - Notice that the Binder link has a similar structure to the GitHub repo link (`<github-username>/<github-repo-name>`)
  - The ""filepath"" argument opens a specific notebook (`.ipynb` file) in the repo

## 1. Creating a repo to Binderize

**TO DO:** :vertical_traffic_light:

1) Create a new repo on GitHub called ""my-first-binder"".
   - Make sure the repository is **public**, _not private_!
   - Don't forget to initialise with a README!
2) Create a file called `hello.py` via the web interface with `print(""Hello from Binder!"")` on the first line and commit to the `main` branch

### Why did the repo have to be public?

<mybinder.org> cannot access private repositories as this would require a secret token.
The Binder team choose not to take on the responsibility of handling secret tokens as <mybinder.org> is a public service and proof of technological concept.
If accessing private repositories is a feature you/your team need, we advise that you look into building your own [BinderHub](https://binderhub.readthedocs.io).

## 2. Launch your first repo!

**TO DO:** :vertical_traffic_light:

1) Go to **<https://mybinder.org>**
2) Type the URL of your repo into the ""GitHub repo or URL"" box.
   It should look like this:
   > **<https://github.com/your-username/my-first-binder>**
3) As you type, the webpage generates a link in the ""Copy the URL below..."" box
   It should look like this:
   > **<https://mybinder.org/v2/gh/your-username/my-first-binder/HEAD>**
4) Copy it, open a new browser tab and visit that URL.
   - You will see a ""spinner"" as Binder launches the repo.

If everything ran smoothly, you'll see a Jupyter Notebook interface.

### What's happening in the background? - Part 1

While you wait, BinderHub (the backend of Binder) is:

- Fetching your repo from GitHub
- Analysing the contents
- Creating a Docker file based on your repo
- Launching that Docker image in the Cloud
- Connecting you to it via your browser

## 3. Run `hello.py`

**TO DO:** :vertical_traffic_light:

1. In the top right corner, click ""New"" :arrow_right: ""Terminal""
2. In the new tab with the terminal, type `python hello.py` and press return

`Hello from Binder!` should be printed to the terminal.

## 4. Pinning Dependencies

It was easy to get started, but our environment is barebones - let's add a **dependency**!

**TO DO:** :vertical_traffic_light:

1) In your repo, create a file called `requirements.txt`
2) Add a line that says: `numpy==1.14.5`
3) Check for typos! Then commit to the `main` branch.
4) Visit **<https://mybinder.org/v2/gh/your-username/my-first-binder/HEAD>** again in a new tab

This time, click on ""Build Logs"" in the big, horizontal, grey bar.
This will let you watch the progress of your build.
It's useful when your build fails or something you think _should_ be installed is missing.

**N.B.:** Sometimes Binder's build logs prints things in red font, such as warnings that `pip` is not up-to-date (`pip` is often out of date because it's regularly updated!) or installation messages, especially if you're using R.
These red messages don't necessarily mean there's a problem with your build and it will fail - it's just an unfortunate font colour choice!

### What's happening in the background? - Part 2

This time, BinderHub will read `requirements.txt` and install version `1.14.5` of the `numpy` package.

### More on pinning dependencies

In the above example, we used two equals signs (`==`) to pin the version of `numpy`.
This tells Binder to install that _specific_ version.

Another way to pin a version number is to use the greater than or equal to sign (`>=`) to allow any version above a particular one to be installed.
This is useful when you have a lot of dependencies that may have dependencies on each other and allows Binder to find a configuration of your dependencies that do not conflict with one another whilst avoiding any earlier versions which may break or change your code.

Finally, you could not provide a version number at all (just the name of the library/package) and Binder will install the latest version of that package.

**N.B.:** These operations to pin dependencies are most likely specific to Python.
Each language has it's own quirks and a link to the different types of configuration files (which is what `requirements.txt` is) is given at the bottom of this document.

## 5. Check the Environment

**TO DO:** :vertical_traffic_light:

1) In the top right corner, click ""New"" :arrow_right: ""Python 3"" to open a new notebook

2) Type the following into a new cell:

   ```python
   import numpy
   print(numpy.__version__)
   numpy.random.randn()
   ```

   **Note the two underscores either side of `version`!**

3) Run the cell to see the version number and a random number printed out
   - Press either SHIFT+RETURN or the ""Run"" button in the Menu bar

**N.B.:** If you save this notebook, it **will not** be saved to the GitHub repo.
Pushing changes back to the GitHub repo through the container is not possible with Binder.
**Any changes you have made to files inside the Binder will be lost once you close the browser window.**

## 6. Sharing your Work

Binder is all about sharing your work easily and there are two ways to do it:

- Share the **<https://mybinder.org/v2/gh/your-username/my-first-binder/HEAD>** URL directly
- Visit **<https://mybinder.org>**, type in the URL of your repo and copy the Markdown or ReStructured Text snippet into your `README.md` file.
  This snippet will render a badge that people can click, which looks like this: ![Binder](https://mybinder.org/badge_logo.svg)

**TO DO:** :vertical_traffic_light:

1) Add the **Markdown** snippet from **<https://mybinder.org>** to the `README.md` file in your repo
   - The grey bar displaying a binder badge will unfold to reveal the snippets.
     Click the clipboard icon next to the box marked with ""m"" to automatically copy the Markdown snippet.
2) Click the badge to make sure it works!

## 7. Accessing data in your Binder

Another kind of dependency for projects is **data**.
There are different ways to make data available in your Binder depending on the size of your data and your preferences for sharing it.

### Small public files

The simplest approach for small, public data files is to add them directly into your GitHub repository.
They are then directly encapsulated into the environment and versioned along with your code.

This is ideal for files up to **10MB**.

### Medium public files

To access medium files **from a few 10s MB up to a few hundred MB**, you can add a file called `postBuild` to your repo.
A `postBuild` file is a shell script that is executed as part of the image construction and is only executed once when a new image is built, not every time the Binder is launched.

See [Binder's `postBuild` example](https://mybinder.readthedocs.io/en/latest/using/config_files.html#postbuild-run-code-after-installing-the-environment) for more uses of the `postBuild` script.

**N.B.:** New images are only built when Binder sees a new commit, not every time you click the Binder link.
Therefore, the data is only downloaded once when the Docker image is built, not every time the Binder is launched.

### Large public files

It is not practical to place large files in your GitHub repo or include them directly in the image that Binder builds.
The best option for large files is to use a library specific to the data format to stream the data as you're using it or to download it on demand as part of your code.

For security reasons, the outgoing traffic of your Binder is restricted to HTTP or GitHub connections only. You will not be able to use FTP sites to fetch data on mybinder.org.

### Private files

There is no way to access files which are not public from mybinder.org.
You should consider all information in your Binder as public, meaning that:

- there should be no passwords, tokens, keys etc in your GitHub repo;
- you should not type passwords into a Binder running on mybinder.org;
- you should not upload your private SSH key or API token to a running Binder.

In order to support access to private files, you would need to create a local deployment of [BinderHub](https://binderhub.readthedocs.io/en/latest/) where you can decide the security trade-offs yourselves.

**N.B.:** Building a BinderHub is not a simple task and is usually taken on by IT/RSE groups for reasons around managing maintenance, security and governance.
However, that is not to say that they are the _only_ groups of people who should/could build a BinderHub.

## 8. Get data with `postBuild`

**TO DO:** :vertical_traffic_light:

1) Go to your GitHub repo and create a file called `postBuild`
2) In `postBuild`, add a single line reading: `wget -q -O gapminder.csv http://bit.ly/2uh4s3g`
   - `wget` is a program which retrieves content from web servers. This line extracts the content from the bitly URL and saves it to the filename denoted by the `-O` flag (capital ""O"", not zero), i.e. `gapminder.csv`.
     The `-q` flag tells `wget` to do this quietly, i.e. don't print anything to the console.
3) Update your `requirements.txt` file by adding a new line with `pandas` on it and another new line with `matplotlib` on it
   - These packages aren't necessary to download the data but we will use them to read the CSV file and make a plot
4) Click the binder badge in your README to launch your Binder

Once the Binder has launched, you should see a new file has appeared that was not part of your repo when you clicked the badge.

Now visualise the data by creating a new notebook (""New"" :arrow_right: ""Python 3"") and run the following code in a cell.

```python
%matplotlib inline

import pandas

data = pandas.read_csv(""gapminder.csv"", index_col=""country"")

years = data.columns.str.strip(""gdpPercap_"")  # Extract year from last 4 characters of each column name
data.columns = years.astype(int)              # Convert year values to integers, saving results back to dataframe

data.loc[""Australia""].plot()
```

See this [Software Carpentry lesson](https://swcarpentry.github.io/python-novice-gapminder/09-plotting/index.html) for more info.

## Beyond Notebooks...

**JupyterLab** is installed into your containerized repo by default.
You can access the environment by changing the URL you visit to:

> **<https://mybinder.org/v2/gh/your-username/my-first-binder/HEAD?urlpath=lab>**

**N.B.:** We've already seen how the `?filepath=` argument can link to a specific file in the [""What Binder Provides""](#what-binder-provides) section at the beginning of this workshop.

Here you can access:

- Notebooks
- IPython consoles
- Terminals
- A text editor

If you use R, you can also open **RStudio** using `?urlpath=rstudio`.

## Now over to you!

Now you've binderized (bound?) this demo repo, it's time to binderize the example script and data you brought along!

**Some useful links:**

- Choosing languages:
  - **<https://mybinder.readthedocs.io/en/latest/howto/languages.html>**
- Configuration files:
  - **<https://mybinder.readthedocs.io/en/latest/using/config_files.html>**
- Example Binder repos:
  - **<https://mybinder.readthedocs.io/en/latest/sample_repos.html>**
- Getting data:
  - With `wget`: **<https://github.com/binder-examples/getting-data>**
  - With `quilt`: **<https://github.com/binder-examples/data-quilt>**
  - From remote storage: **<https://github.com/binder-examples/remote_storage>**
",2022-08-12
https://github.com/aromanowski/Circadian_rhythms_and_alternative_splicing,"# Circadian rhythms and alternative splicing

Here you will find the R scripts used to analyse the RNA-seq timeseries count data, as presented in the **Global transcriptome analysis reveals circadian control of splicing events in Arabidopsis thaliana** manuscript.


All enquiries should be sent to Dr. Andres Romanowski: aromanowski (at) leloir.org.ar
",2022-08-12
https://github.com/aromanowski/Circadian_rhythms_and_alternative_splicing-atspf30-1-scripts-,"# Circadian rhythms and alternative splicing (atspf30-1 analysis)

Here you will find the R scripts used to analyse the RNA-seq atspf30-1 count data, as presented in the **Global transcriptome analysis reveals circadian control of splicing events in Arabidopsis thaliana** manuscript.


All enquiries should be sent to Dr. Andres Romanowski: aromanowski (at) leloir.org.ar
",2022-08-12
https://github.com/aromanowski/Elgg,"Bienvenidos al Github Elgg-UNQ

En este sitio Uds. pueden encontrar el repositorio del sitio Elgg
configurado de acuerdo a lo expuesto en LabSOR - 2do Cuat 2011

Para poder bajar el sitio deberan crearse una cuenta en GitHub
y seguir las indicaciones

El contenido del dump inicial de la base de datos se encuentra en 
el archivo elgg_DB.tar.gz 

El sitio esta en la carpeta site y debe ser copiado a /var/www
",2022-08-12
https://github.com/aromanowski/group-website,"# group-website
Repo for learning how to make websites with Jekyll

## Learning Markdown

Vanilla text may contain *italics* and **bold words**.

This paragraph is separated from the previous one by a blank line.
Line breaks  
are caused by two trailing spaces at the end of a line.

[Carpentries Webpage](https://carpentries.org)


### Carpentries Lesson Programs:
- Software Carpentry
- Data Carpentry
- Library Carpentry

## My second level heading
My text with ~~strikethrough~~

### My third level heading
1. First item
2. Second item
3. Third item

![alt text](https://github.com/carpentries/carpentries.org/blob/main/images/TheCarpentries-opengraph.png ""Image of the carpentries that I inserted here"")

Using inline links: [Carpentries Webpage][carpentries]

[carpentries]: https://carpentries.org/
",2022-08-12
https://github.com/aromanowski/leaf3_dev,"# Light regulation of leaf blade architecture

Here you will find the R scripts used to analyse the RNA-seq timeseries count data, as presented in the **Light regulation of leaf blade architecture** manuscript.


All enquiries should be sent to Dr. Andres Romanowski: Andrew.Romanowski (at) ed.ac.uk
",2022-08-12
https://github.com/aromanowski/LeafDev-app,"# LeafDev-app (Light regulation of leaf blade architecture)

Here you will find the R scripts used to create the Leaf Dev Shiny App to increase ease of access to the RNA-seq timeseries count data presented in the **Light regulation of leaf blade architecture** manuscript.

The app can be accessed at https://aromanowski.shinyapps.io/leafdev-app/

All enquiries should be sent to Dr. Andres Romanowski: Andrew.Romanowski (at) ed.ac.uk
",2022-08-12
https://github.com/aromanowski/old_faithful,"# old_faithful

Just a test

changed this in RStudio
",2022-08-12
https://github.com/aromanowski/roar_workshop,"# roar_workshop
Robust Open Analysis in R

Useful README file

this is markdown

I changed this in RStudio

# Cool data to follow
",2022-08-12
https://github.com/asreview/.github,"# .github
Repo for the organization profile
",2022-08-12
https://github.com/asreview/asreview,"<p align=""center"">
  <a href=""https://github.com/asreview/asreview"">
    <img width=""60%"" height=""60%"" src=""https://raw.githubusercontent.com/asreview/asreview-artwork/master/LogoASReview/SVG/GitHub_Repo_Card_Transparent.svg"">
  </a>
</p>

## ASReview: Active learning for Systematic Reviews

[![PyPI version](https://badge.fury.io/py/asreview.svg)](https://badge.fury.io/py/asreview) [![Build Status](https://img.shields.io/endpoint.svg?url=https%3A%2F%2Factions-badge.atrox.dev%2Fasreview%2Fasreview%2Fbadge%3Fref%3Dmaster&style=flat)](https://actions-badge.atrox.dev/asreview/asreview/goto?ref=master) [![Documentation Status](https://readthedocs.org/projects/asreview/badge/?version=latest)](https://asreview.readthedocs.io/en/latest/?badge=latest) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3345592.svg)](https://doi.org/10.5281/zenodo.3345592) [![Downloads](https://pepy.tech/badge/asreview)](https://github.com/asreview/asreview#installation) [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/4755/badge)](https://bestpractices.coreinfrastructure.org/projects/4755)

Systematically screening large amounts of textual data is time-consuming and
often tiresome. The rapidly evolving field of Artificial Intelligence (AI) has
allowed the development of AI-aided pipelines that assist in finding relevant
texts for search tasks. A well-established approach to increasing efficiency
is screening prioritization via [Active
Learning](https://asreview.readthedocs.io/en/latest/guides/activelearning.html).

The Active learning for Systematic Reviews (ASReview) project, published in
[*Nature Machine Intelligence*](https://doi.org/10.1038/s42256-020-00287-7)
implements different machine learning algorithms that interactively query the
researcher. ASReview LAB  is designed to accelerate the step of screening
textual data with a minimum of records to be read by a human with no or very
few false negatives. ASReview LAB will save time, increase the quality of
output and strengthen the transparency of work when screening large amounts of
textual data to retrieve relevant information. Active Learning will support 
decision-making in any discipline or industry.

ASReview software implements three different modes:

- **Oracle** :crystal_ball: Screen textual data in
  interaction with the active learning model. The reviewer is the 'oracle',
  making the labeling decisions.
- **Exploration** :triangular_ruler: Explore or
  demonstrate ASReview LAB with a completely labeled dataset. This mode is
  suitable for teaching purposes.
- **Simulation** :chart_with_upwards_trend: Evaluate
  the performance of active learning models on fully labeled data. Simulations
  can be run in ASReview LAB or via the command line interface with more
  advanced options.


## Installation

The ASReview software requires Python 3.7+. Detailed step-by-step instructions
to install Python and ASReview are available for
[Windows](https://asreview.ai/installation-guide-windows/) and
[macOS](https://asreview.ai/installation-guide-macos/) users.

```bash
pip install asreview
```

Upgrade ASReview with the following command:

```bash
pip install --upgrade asreview
```

To install ASReview LAB with Docker, see [Install with Docker](https://asreview.readthedocs.io/en/latest/installation.html).

## How it works

[![ASReview LAB explained - animation](https://img.youtube.com/vi/k-a2SCq-LtA/0.jpg)](https://www.youtube.com/watch?v=k-a2SCq-LtA)


## Getting started

[Getting Started with ASReview
LAB](https://asreview.readthedocs.io/en/latest/about.html).

[![ASReview LAB](https://github.com/asreview/asreview/blob/master/images/ASReviewWebApp.png?raw=true)](https://asreview.readthedocs.io/en/latest/lab/overview_lab.html ""ASReview LAB"")

## Citation

The following publication in [Nature Machine
Intelligence](https://doi.org/10.1038/s42256-020-00287-7) can be used to cite
the project.

> van de Schoot, R., de Bruin, J., Schram, R. et al. An open source machine
  learning framework for efficient and transparent systematic reviews.
  Nat Mach Intell 3, 125–133 (2021). https://doi.org/10.1038/s42256-020-00287-7

For citing the software, please refer to the specific release of
the ASReview software on Zenodo https://doi.org/10.5281/zenodo.3345592. The menu on the
right can be used to find the citation format of prevalence.

For more scientific publications on the ASReview software, go to
[asreview.ai/papers](https://asreview.ai/papers/).

## Contact

For an overview of the team working on ASReview, see [ASReview Research Team](https://asreview.ai/about).
ASReview LAB is maintained by
[Jonathan de Bruin](https://github.com/J535D165) and [Yongchao Terry Ma](https://github.com/terrymyc).

The best resources to find an answer to your question or ways to get in
contact with the team are:

- Documentation - [asreview.readthedocs.io](https://asreview.readthedocs.io/)
- Newsletter - [asreview.ai/newsletter/subscribe](https://asreview.ai/newsletter/subscribe)
- Quick tour - [ASReview LAB quick tour](https://asreview.readthedocs.io/en/latest/lab/overview_lab.html)
- Issues or feature requests - [ASReview issue tracker](https://github.com/asreview/asreview/issues)
- FAQ - [ASReview Discussions](https://github.com/asreview/asreview/discussions?discussions_q=sort%3Atop)
- Donation - [asreview.ai/donate](https://asreview.ai/donate)
- Contact - [asreview@uu.nl](mailto:asreview@uu.nl)

## License

The ASReview software has an Apache 2.0 [LICENSE](LICENSE). The ASReview team
accepts no responsibility or liability for the use of the ASReview tool or any
direct or indirect damages arising out of the application of the tool.
",2022-08-12
https://github.com/asreview/asreview-artwork,"[![ASReview](LogoASReview/SVG/GitHub_Repo_Card_Transparent.svg)](https://github.com/asreview/asreview/)

# ASReview artwork

This repository contains the artwork for the [ASReview](https://github.com/asreview/asreview/) 
project. The artwork and logos were made by [Joukje Willemsen](https://joukjewillemsen.github.io/). The work is licensed under Creative Commons License (CC BY-NC-ND 4.0).

* The folder **ElasAdventures** contains vectorart of Elas on different adventures:
<img src=""ElasAdventures/RepoCardElasAdventures.png"" width=900>

* The folder **FlowChart** contains the vectorart illustrating the ASReview LAB flow, how it compares with a traditional screening process, how they fit into the systematic review process and other potential uses of ASReview LAB.
<img src=""FlowChart/RepoCardFlowChart.png"" width=900>

* The folder **Illustrations** contains Illustrations that illustrate the workflow of systematic reviews with and without the use of ASReview.

<img src=""Illustrations/RepoCardIllustrations.png"" width=900>

* The folder **LogoASReview** contains raw and compiled files of our logo:

<img src=""LogoASReview/ContentOverview.png"" width=900>

* The folder **LottieAnimation** contains animated json files created with [Lottie](https://github.com/airbnb/lottie-web) 

![](LottieAnimation/Training/TrainingElasJump.gif)

![](LottieAnimation/SetUpToReview/SetUpToReviewJump.gif)

* The folder **QuickTour** contains the artwork used in the quicktour

<img src=""QuickTour/RepoCardQuickTour.png"" width=900>

* The folder **Stickers** contains AI files of stickers that were designed a while back (hence a little outdated)
<img src=""Stickers/3stickers.png"" width=600>

* The folder **SubLogos** contains raw and compiled files of the ASReviewLAB and ASReviewRESEARCH logos:

<img src=""SubLogos/RepoCardSubLogos.png"" width=900>

* The folder **SVGicons** contains svg's that can be used on the website or in the app

<img src=""SVGicons/RepoCardSVGicons.png"" width=900>
",2022-08-12
https://github.com/asreview/asreview-covid19,"![ASReview for COVID19](https://github.com/asreview/asreview/blob/master/images/intro-covid19-small.png?raw=true)

Extension to add publications on COVID-19 to [ASReview](https://github.com/asreview/asreview).

# ASReview against COVID-19 (Deprecated)

## This extension is deprecated. It still works for version 0.x of ASReview but datasets are no longer updated.

[![Downloads](https://pepy.tech/badge/asreview-covid19)](https://pepy.tech/project/asreview-covid19) [![PyPI version](https://badge.fury.io/py/asreview-covid19.svg)](https://badge.fury.io/py/asreview-covid19) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3764749.svg)](https://doi.org/10.5281/zenodo.3764749) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

The Active learning for Systematic Reviews software [ASReview](https://github.com/asreview/asreview) implements learning algorithms that interactively query the researcher during the title and abstract reading phase of a systematic search. This way of interactive training is known as active learning. ASReview offers support for classical learning algorithms and state-of-the-art learning algorithms like neural networks. The software can be used for traditional systematic reviews for which the user uploads a dataset of papers, or one can make use of the built-in datasets.

To help combat the COVID-19 crisis, the ASReview team released an extension that integrates the latest scientific datasets on COVID-19 in the ASReview software. Experts can **start reviewing the latest scientific literature on COVID-19 immediately!** See [datasets](#datasets) for an overview of the datasets (daily updates).


## Installation, update, and usage

The COVID-19 plug-in requires ASReview 0.9.4 or higher. Install ASReview by following the instructions in [Installation of ASReview](https://asreview.readthedocs.io/en/latest/installation.html).

Install the extension with pip:

```bash
pip install asreview-covid19
```

The datasets are immediately available after starting ASReview (`asreview oracle`). The datasets are selectable in Step 2 of the project initialization. For more information on the usage of ASReview, please have a look at the [Quick Tour](https://asreview.readthedocs.io/en/latest/quicktour.html).

Older versions of the plugin are no longer supported by ASReview>=0.9.4. Please update the plugin with: 

```bash
pip install --upgrade asreview-covid19
```


## Datasets

The following datasets are available:

- [CORD-19 dataset](#cord-19-dataset)
- [COVID19 preprints dataset](#covid19-preprints-dataset)

:exclamation: The datasets are checked for updates every couple of hours such that the latest collections are available in the ASReview COVID19 plugin and ASReview software.

[![ASReview CORD19 datasets](https://github.com/asreview/asreview/blob/master/docs/images/asreview-covid19-screenshot.png?raw=true)](https://github.com/asreview/asreview-covid19)

### CORD-19 dataset
The [CORD-19 dataset](https://pages.semanticscholar.org/coronavirus-research) is a dataset with scientific publications on COVID-19 and coronavirus-related research (e.g. SARS, MERS, etc.) from PubMed Central, the WHO COVID-19 database of publications, the preprint servers bioRxiv, medRxiv and arXiv, and papers contributed by specific publishers (currently Elsevier). The dataset is compiled and maintained by a collaboration of the Allen Institute for AI, the Chan Zuckerberg Initiative, Georgetown University’s Center for Security and Emerging Technology, Microsoft Research, and the National Library of Medicine of the National Institutes of Health. The full dataset contains metadata of more than **100K publications** on COVID-19 and coronavirus-related research. **The CORD-19 dataset receives daily updates and is directly available in the ASReview software.** The most recent versions of the dataset can be found here: https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html

### COVID19 preprints dataset
The [COVID19 preprints dataset](https://github.com/nicholasmfraser/covid19_preprints) is created by [Nicholas Fraser](https://github.com/nicholasmfraser) and [Bianca Kramer](https://github.com/bmkramer), by collecting metadata of COVID19-related preprints from over 15 preprint servers with DOIs registered with Crossref or DataCite, and from arXiv. The dataset contains metadata of >10K preprints on COVID-19 and coronavirus-related research. All versions are archived on [Figshare](https://doi.org/10.6084/m9.figshare.12033672). The COVID19 preprints dataset receives weekly updates.

The most recent version of the dataset can be found here:[https://github.com/nicholasmfraser/covid19_preprints/blob/master/data/covid19_preprints.csv](https://github.com/nicholasmfraser/covid19_preprints/blob/master/data/covid19_preprints.csv).

## License, citation and contact

The ASReview software and the plugin have an Apache 2.0 LICENSE. For the datasets, please see the license of the CORD-19 dataset https://pages.semanticscholar.org/coronavirus-research. The COVID19 preprints dataset has a [CC0 license](https://creativecommons.org/publicdomain/zero/1.0/).

Visit https://doi.org/10.5281/zenodo.3764749 to get the citation style of your preference.

This project is coordinated by by Rens van de Schoot (@Rensvandeschoot) and Daniel Oberski (@daob) and is part of the research work conducted by the Department of Methodology & Statistics, Faculty of Social and Behavioral Sciences, Utrecht University, The Netherlands. Maintainers are Jonathan de Bruin (@J535D165) and Raoul Schram (@qubixes).

Got ideas for improvement? For any questions or remarks, please send an email to asreview@uu.nl.
",2022-08-12
https://github.com/asreview/asreview-datatools,"# ASReview Datatools

[![PyPI version](https://badge.fury.io/py/asreview-datatools.svg)](https://badge.fury.io/py/asreview-datatools) [![Downloads](https://pepy.tech/badge/asreview-datatools)](https://pepy.tech/project/asreview-datatools) ![PyPI - License](https://img.shields.io/pypi/l/asreview-datatools) ![Deploy and release](https://github.com/asreview/asreview-datatools/workflows/Deploy%20and%20release/badge.svg) ![Build status](https://github.com/asreview/asreview-datatools/workflows/test-suite/badge.svg) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6625879.svg)](https://doi.org/10.5281/zenodo.6625879)

ASReview Datatools is an extension to [ASReview
LAB](https://github.com/asreview/asreview) that can be used for describing
basic properties of a dataset (e.g., number of papers, number of inclusions,
the amount of missing data and duplicates), converting file formats via the
command line, and cleaning your (input) data by removing duplicate records.

❣️ ASReview Datatools is the successor to
[ASReview-statistics](https://pypi.org/project/asreview-statistics/). ASReview
datatools is available for version 1 or later. Use ASReview statistics for
versions 0.x.

## Installation

The ASReview Datatools extensions requires Python 3.7+ and [ASReview
LAB](https://github.com/asreview/asreview) version 1 or later.

The easiest way to install the datatools extension is to install from PyPI:

``` bash
pip install asreview-datatools
```

After installation of the datatools extension, `asreview` should automatically
detect it. Test this with the following command:

```bash
asreview --help
```

The extension is successfully installed if it lists `asreview data`.

## Getting started

ASReview-datatools is a command line tool that extends ASReview LAB. Each
subsection below describes one of the tools. The structure is

```bash
asreview data NAME_OF_TOOL
```

where `NAME_OF_TOOL` is the name of one of the tools below (i.e., `describe`)
followed by positional arguments and optional arguments.

Each tool has its own help description which is available with

```bash
asreview data NAME_OF_TOOL -h
```

### `data describe`

Describe the content of a dataset

```bash
asreview data describe MY_DATASET.csv
```

Export the results to a file (`output.json`)

```bash
asreview data describe MY_DATASET.csv -o output.json
```

Describe the `van_de_schoot_2017` dataset from the [benchmark
platform](https://github.com/asreview/systematic-review-datasets).

```bash
asreview data describe benchmark:van_de_schoot_2017 -o output.json
```

```
{
  ""asreviewVersion"": ""1.0"",
  ""apiVersion"": ""1.0"",
  ""data"": {
    ""items"": [
      {
        ""id"": ""n_records"",
        ""title"": ""Number of records"",
        ""description"": ""The number of records in the dataset."",
        ""value"": 6189
      },
      {
        ""id"": ""n_relevant"",
        ""title"": ""Number of relevant records"",
        ""description"": ""The number of relevant records in the dataset."",
        ""value"": 43
      },
      {
        ""id"": ""n_irrelevant"",
        ""title"": ""Number of irrelevant records"",
        ""description"": ""The number of irrelevant records in the dataset."",
        ""value"": 6146
      },
      {
        ""id"": ""n_unlabeled"",
        ""title"": ""Number of unlabeled records"",
        ""description"": ""The number of unlabeled records in the dataset."",
        ""value"": 0
      },
      {
        ""id"": ""n_missing_title"",
        ""title"": ""Number of records with missing title"",
        ""description"": ""The number of records in the dataset with missing title."",
        ""value"": 5
      },
      {
        ""id"": ""n_missing_abstract"",
        ""title"": ""Number of records with missing abstract"",
        ""description"": ""The number of records in the dataset with missing abstract."",
        ""value"": 764
      },
      {
        ""id"": ""n_duplicates"",
        ""title"": ""Number of duplicate records (basic algorithm)"",
        ""description"": ""The number of duplicate records in the dataset based on similar text."",
        ""value"": 104
      }
    ]
  }
}
```

### `data convert`

Convert the format of a dataset. For example, convert a RIS dataset into a
CSV, Excel, or TAB dataset.

```
asreview data convert MY_DATASET.ris MY_OUTPUT.csv
```

### `data dedup`

Remove duplicate records with a simple and straightforward deduplication
algorithm (see [source
code](https://github.com/asreview/asreview-datatools/blob/master/asreviewcontrib/datatools/dedup.py)).
The algorithm concatenates the title and abstract, whereafter it removes all
non-alphanumeric tokens. Then the duplicates are removed.

```
asreview data dedup MY_DATASET.ris
```

Export the deduplicated dataset to a file (`output.csv`)

```
asreview data dedup MY_DATASET.ris -o output.csv
```

Using the `van_de_schoot_2017` dataset from the [benchmark
platform](https://github.com/asreview/systematic-review-datasets).

```
asreview data dedup benchmark:van_de_schoot_2017 -o van_de_schoot_2017_dedup.csv
```

## License

This extension is published under the [MIT license](/LICENSE).

## Contact

This extension is part of the ASReview project ([asreview.ai](https://asreview.ai)). It is maintained by the
maintainers of ASReview LAB. See [ASReview
LAB](https://github.com/asreview/asreview) for contact information and more
resources.
",2022-08-12
https://github.com/asreview/asreview-extension-vocab-extractor,"# Depricated

 This repository has been depricated following https://github.com/asreview/asreview/discussions/1163.
 
 Since version 1.0, it's possible to extract features via the API.
 
",2022-08-12
https://github.com/asreview/asreview-hyperopt,"# ❗Highly experimental❗ ASReview-hyperopt

❗ This extension is experimental and works only with old versions of ASReview. Functionality is limited and should be considered ""research-only"".

![Deploy and release](https://github.com/asreview/asreview-hyperopt/workflows/Deploy%20and%20release/badge.svg)![Build status](https://github.com/asreview/asreview-hyperopt/workflows/test-suite/badge.svg)

Hyper parameter optimization extension for 
[ASReview](https://github.com/asreview/asreview). It uses the 
[hyperopt](https://github.com/hyperopt/hyperopt) package to quickly optimize parameters
of the different models. The hyper parameters and their sample space are defined in the
[ASReview](https://github.com/asreview/asreview) package, and 
automatically used for hyper parameter optimization.

### Installation

The easiest way to install the hyper parameter optimization package is to use the command line:

``` bash
pip install asreview-hyperopt
```

After installation of the visualization package, asreview should automatically detect it.
Test this by:

```bash
asreview --help
```

It should list three new entry points: `hyper-active`, `hyper-passive` and `hyper-cluster`.

### Basic usage

The three entry-points are used in a roughly similar fashion. The main difference between them is
the types of models that have to be supplied:

- hyper-cluster: feature_extraction
- hyper-passive: model, balance\_strategy, feature\_extraction
- hyper-active: model, balance\_strategy, query\_strategy, feature\_extraction


To get help for entry points type:

```bash
asreview hyper-active --help
```

Which results in the following options:

```bash
usage: hyper-active [-h] [-n N_ITER] [-r N_RUN] [-d DATASETS] [--mpi]
                    [--data_dir DATA_DIR] [--output_dir OUTPUT_DIR]
                    [--server_job] [-m MODEL] [-q QUERY_STRATEGY]
                    [-b BALANCE_STRATEGY] [-e FEATURE_EXTRACTION]

optional arguments:
  -h, --help            show this help message and exit
  -n N_ITER, --n_iter N_ITER
                        Number of iterations of Bayesian Optimization.
  -r N_RUN, --n_run N_RUN
                        Number of runs per dataset.
  -d DATASETS, --datasets DATASETS
                        Datasets to use in the hyper parameter optimization
                        Separate by commas to use multiple at the same time
                        [default: all].
  --mpi                 Use the mpi implementation.
  --data_dir DATA_DIR   Base directory with data files.
  --output_dir OUTPUT_DIR
                        Output directory for trials.
  --server_job          Run job on the server. It will incur less overhead of
                        used CPUs, but more latency of workers waiting for the
                        server to finish its own job. Only makes sense in
                        combination with the flag --mpi.
  -m MODEL, --model MODEL
                        Prediction model for active learning.
  -q QUERY_STRATEGY, --query_strategy QUERY_STRATEGY
                        Query strategy for active learning.
  -b BALANCE_STRATEGY, --balance_strategy BALANCE_STRATEGY
                        Balance strategy for active learning.
  -e FEATURE_EXTRACTION, --feature_extraction FEATURE_EXTRACTION
                        Feature extraction method.

```

### Data structure

The extension will by default search for datasets in the `data` directory, relative to the current
working directory. Either put your datasets there, or specify and data directory.

The output of the runs will by default be stored in the `output` directory, relative to
the current path.

An example of a structure that has been created:

```bash
output/
├── active_learning
│   ├── nb_max_double_tfidf
│   │   └── hall_ace_ptsd_nagtegaal
│   │       ├── best
│   │       │   ├── ace
│   │       │   ├── hall
│   │       │   ├── nagtegaal
│   │       │   └── ptsd
│   │       ├── current
│   │       │   ├── ace
│   │       │   ├── hall
│   │       │   ├── nagtegaal
│   │       │   └── ptsd
│   │       └── trials.pkl
│   └── nb_max_random_double_tfidf
│       └── nagtegaal
│           ├── best
│           │   └── nagtegaal
│           ├── current
│           │   └── nagtegaal
│           └── trials.pkl
├── cluster
│   └── doc2vec
│       ├── ace
│       │   ├── best
│       │   │   └── ace
│       │   ├── current
│       │   │   └── ace
│       │   └── trials.pkl
│       ├── hall_ace_ptsd_nagtegaal
│       │   └── current
│       │       ├── ace
│       │       ├── hall
│       │       ├── nagtegaal
│       │       └── ptsd
│       └── nagtegaal
│           └── current
│               └── nagtegaal
└── passive
    └── nb_double_tfidf
        └── ptsd
            ├── best
            │   └── ptsd
            ├── current
            │   └── ptsd
            └── trials.pkl
```

The files with name `trials.pkl` are special files that contain data on which trials were run.

To list these trials, use the following command:

```bash
asreview show $SOME_DIRECTORY/trials.pkl
```

It should give a list of trials sorted by the loss (lower is better). The column names (apart
from the loss) are prefixed with the kind of parameter it is:

- `mdl`: Model parameter
- `bal`: Balance strategy parameter
- `qry`: Query strategy parameter
- `fex`: Feature extraction parameter

### Options

The default number of iterations is 1, which you'll probably want to increase. It depends on the
number of hyper-parameters that need to be optimized, but several hundred iterations is probably
a good estimate for most combinations to get reasonably close to the optimum. In all cases,
use good common sense; if the loss is still going down at a quick pace, do a few more iterations.

The hyperopt extension has built-in support for MPI. MPI is used for parallelization of runs. On
a local PC with an MPI-implementation (like OpenMPI) installed, one could run with 4 cores:

```bash
mpirun -n 4 asreview hyper-active --mpi
```

If you want to be slightly more efficient on a machine with a low number of cores, you can run
jobs on the MPI server as well:

```bash
mpirun -n 4 asreview hyper-active --mpi --server_job
```

On super computers one should sometimes replace `mpirun` with `srun`.
",2022-08-12
https://github.com/asreview/asreview-insights,"# ASReview Insights

[![PyPI version](https://badge.fury.io/py/asreview-insights.svg)](https://badge.fury.io/py/asreview-insights) [![Downloads](https://pepy.tech/badge/asreview-insights)](https://pepy.tech/project/asreview-insights) ![PyPI - License](https://img.shields.io/pypi/l/asreview-insights) ![Deploy and release](https://github.com/asreview/asreview-insights/workflows/Deploy%20and%20release/badge.svg) ![Build status](https://github.com/asreview/asreview-insights/workflows/test-suite/badge.svg) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6626069.svg)](https://doi.org/10.5281/zenodo.6626069)


This official extension to [ASReview
LAB](https://github.com/asreview/asreview) extends the software with tools for
[plotting](#plot-types) and extracting the [statistical results](#metrics) of
several [performance metrics](#performance-metrics). The extension is
especially useful in combination with the [simulation
functionality](https://asreview.readthedocs.io/en/latest/simulation_overview.html)
of ASReview LAB.


❣️ ASReview Insights is the successor to
[ASReview-visualization](https://pypi.org/project/asreview-visualization/).
ASReview insights is available for [ASReview
LAB](https://github.com/asreview/asreview/discussions/975) version 1 or later.
Use ASReview visualization for versions 0.x.

## Installation

ASReview Insights can be installed from PyPI:

``` bash
pip install asreview-insights
```

After installation, check if the `asreview-insights` package is listed as an
extension. Use the following command:

```bash
asreview --help
```

It should list the 'plot' subcommand and the 'metrics' subcommand.

## Performance metrics

The ASReview Insights extension is useful for measuring the performance of
active learning models on collections of binary labeled text. The extension
can be used after performing a simulation study that involves mimicking the
screening process with a specific model. As it is already known which records
are labeled relevant, the simulation can automatically reenact the screening
process as if a screener were using active learning. The performance of one or
multiple models can be measured by different metrics and the
ASReview Insights extension can plot or compute the values for such metrics
from ASReview project files.

The recall is the proportion of relevant records that have been found at a
certain point during the screening phase. It is sometimes also called the
proportion of Relevant Record Found (RRF) after screening an X% of the total
records. For example, the RRF@10 is the recall (i.e., the proportion of the
total number of relevant records) at screening 10% of the total number of
records available in the dataset.

A variation is the Extra Relevant records Found (ERF), which is the proportion
of relevant records found after correcting for the number of relevant records
found via random screening (assuming a uniform distribution of relevant
records).

The Work Saved over Sampling (WSS) is a measure of ""the work saved over and
above the work saved by simple sampling for a given level of recall"" ([Cohen
et al., 2006]((https://doi.org/10.1197/jamia.m1929)). It is defined as the
proportion of records a screener does **not** have to screen compared to
random reading after providing the prior knowledge used to train the first
iteration of the model. The WSS is typically measured at a recall of .95
(WSS@95), reflecting the proportion of records saved by using active learning
at the cost of failing to identify .05 of relevant publications.

The following plot illustrates the differences between the metrics Recall
(y-axis), WSS (blue line), and ERF (red line). The dataset contains 1.000
hypothetical records with labels. The stepped line on the diagonal is the
naive labeling approach (screening randomly sorted records).

![ASReview metrics explained](https://github.com/asreview/asreview-insights/blob/master/docs/stats_explainer.png)


Both recall and WSS are sensitive to the position of the cutoff value and the
distribution of the data. Moreover, the WSS makes assumptions about the
acceptable recall level whereas this level might depend on the research
question at hand. Therefore, [Ferdinands et al.
(2020)](https://doi.org/10.31219/osf.io/w6qbg) proposed two new metrics: (1)
the Time to Discover a relevant record as the fraction of records needed
to screen to detect this record (TD); and (2) the Average Time to Discover
(ATD) as an indicator of how many records need to be screened on average to
find all relevant records in the dataset.


## Basic usage

The ASReview Insights package extends ASReview LAB with two new subcommands
(see `asreview --help`): [`plot`](#plot) and [`metrics`](#metrics). The plots
and metrics are derived from an ASReview project file. The ASReview file
(extension `.asreview`) can be
[exported](https://asreview.readthedocs.io/en/latest/manage.html#export-project)
from ASReview LAB after a
[simulation](https://asreview.readthedocs.io/en/latest/simulation_overview.html),
or it is generated from running a [simulation via the command
line](https://asreview.readthedocs.io/en/latest/simulation_cli.html).

For example, an ASReview can be generated with:


```python
asreview simulate benchmark:van_de_schoot_2017 -s sim_van_de_schoot_2017.asreview --init_seed 535
```

To use the most basic options of the ASReview Insights extension, run

```bash
asreview plot recall YOUR_ASREVIEW_FILE.asreview
```
where `recall` is the type of the plot, or

```bash
asreview metrics sim_van_de_schoot_2017.asreview
```

More options are described in the sections below. All options can be
obtained via `asreview plot --help` or `asreview metrics --help`.

## `Plot`

### Plot types

#### `recall`

The recall is an important metric to study the performance of active learning
algorithms in the context of information retrieval. ASReview Insights
offers a straightforward command line interface to plot a ""recall curve"". The
recall curve is the recall at any moment in the active learning process.

To plot the recall curve, you need a ASReview file (extension `.asreview`).To
plot the recall, use this syntax (Replace `YOUR_ASREVIEW_FILE.asreview` by
your ASReview file name.):

```bash
asreview plot recall YOUR_ASREVIEW_FILE.asreview
```

The following plot is the result of simulating the [`van_de_schoot_2017`](https://github.com/asreview/systematic-review-datasets/tree/master/datasets/van_de_Schoot_2017) in
the benchmark platform (command `asreview simulate
benchmark:van_de_schoot_2017 -s sim_van_de_schoot_2017.asreview`).

![Recall plot of Van de Schoot 2017](https://github.com/asreview/asreview-insights/blob/master/figures/tests_recall_sim_van_de_schoot_2017_1.png)

On the vertical axis, you find the recall (i.e, the proportion of the relevant
records) after every labeling decision. The horizontal axis shows the
proportion of  total number of records in the dataset. The steeper the recall
curve, the higher the performance of active learning when comparted to random
screening. The recall curve can also be used to estimate stopping criteria, see
the discussions in [#557](https://github.com/asreview/asreview/discussions/557) and [#1115](https://github.com/asreview/asreview/discussions/1115).


```bash
asreview plot recall YOUR_ASREVIEW_FILE.asreview
```

#### `wss`

The Work Saved over Sampling (WSS) metric is an useful metric to study the
performance of active learning alorithms compared with a naive (random order)
approach at a given level of recall. ASReview Insights offers a
straightforward command line interface to plot the WSS at any level of recall.

To plot the WSS curve, you need a ASReview file (extension `.asreview`). To
plot the WSS, use this syntax (Replace `YOUR_ASREVIEW_FILE.asreview` by your
ASReview file name.):

```bash
asreview plot wss YOUR_ASREVIEW_FILE.asreview
```

The following plot is the result of simulating the [`van_de_schoot_2017`](https://github.com/asreview/systematic-review-datasets/tree/master/datasets/van_de_Schoot_2017) in
the benchmark platform (command `asreview simulate
benchmark:van_de_schoot_2017 -s sim_van_de_schoot_2017.asreview`).

![Recall plot of Van de Schoot 2017](https://github.com/asreview/asreview-insights/blob/master/figures/tests_wss_default_sim_van_de_schoot_2017_1.png)

On the vertical axis, you find the WSS after every labeling decision. The
recall is displayed on the horizontal axis. As shown in the figure, the
WSS is linearly related to the recall.


#### `erf`

The Extra Relevant Records found is a derivative of the recall and presents
the proportion of relevant records found after correcting for the number of
relevant records found via random screening (assuming a uniform distribution
of relevant records).

To plot the WSS curve, you need a ASReview file (extension `.asreview`). To
plot the WSS, use this syntax (Replace `YOUR_ASREVIEW_FILE.asreview` by your
ASReview file name.):


```bash
asreview plot erf YOUR_ASREVIEW_FILE.asreview
```

The following plot is the result of simulating the [`van_de_schoot_2017`](https://github.com/asreview/systematic-review-datasets/tree/master/datasets/van_de_Schoot_2017) in
the benchmark platform (command `asreview simulate
benchmark:van_de_schoot_2017 -s sim_van_de_schoot_2017.asreview`).

![Recall plot of Van de Schoot 2017](https://github.com/asreview/asreview-insights/blob/master/figures/tests_erf_default_sim_van_de_schoot_2017_1.png)

On the vertical axis, you find the ERF after every labeling decision. The
horizontal axis shows the proportion of  total number of records in the
dataset. The steep increase of the ERF in the beginning of the process is
related to the steep recall curve.

### Plotting CLI

Optional arguments for the command line are `--priors` to include prior
knowledge, `--x_absolute` and `--y_absolute` to use absolute axes.

See `asreview plot -h` for all command line arguments.


### Plotting API

To make use of the more advanced features, you can make use of the Python API.
The advantage is that you can tweak every single element of the plot in the
way you like. The following examples show how the Python API can be used. They
make use of matplotlib extensively. See the [Introduction to
Matplotlib](https://matplotlib.org/stable/tutorials/introductory/usage.html)
for examples on using the API.

The following example show how to plot the recall with the API and save the
result. The plot is saved using the matplotlib API.

```python
import matplotlib.pyplot as plt

from asreview import open_state
from asreviewcontrib.insights.plot import plot_recall

with open_state(""example.asreview"") as s:

    fig, ax = plt.subplots()

    plot_recall(ax, s)

    fig.savefig(""example.png"")
```

Other options are `plot_wss` and `plot_erf`.

#### Example: Customize plot

It's straightforward to customize the plots if you are familiar with
`matplotlib`. The following example shows how to update the title of the plot.

```python
import matplotlib.pyplot as plt

from asreview import open_state
from asreviewcontrib.insights.plot import plot_wss

with open_state(""example.asreview"") as s:

    fig, ax = plt.subplots()
    plot_wss(ax, s)

    plt.title(""WSS with custom title"")

    fig.savefig(""example_custom_title.png"")
```

![WSS with custom title](https://github.com/asreview/asreview-insights/blob/master/docs/example_custom_title.png)

#### Example: Prior knowledge

It's possible to include prior knowledge to your plot. By default, prior
knowledge is excluded from the plot.

```python
import matplotlib.pyplot as plt

from asreview import open_state
from asreviewcontrib.insights.plot import plot_wss

with open_state(""example.asreview"") as s:

    fig, ax = plt.subplots()
    plot_wss(ax, s, priors=True)

```

#### Example: Relative versus absolute axes

By default, all axes in ASReview Insights are relative. The API can be used to
change this behavior. The arguments are identical for each plot function.

```python
import matplotlib.pyplot as plt

from asreview import open_state
from asreviewcontrib.insights.plot import plot_wss

with open_state(""example.asreview"") as s:

    fig, ax = plt.subplots()
    plot_wss(ax, s, x_absolute=True, y_absolute=True)

    fig.savefig(""example_absolute_axis.png"")
```

![Recall with absolute axes](https://github.com/asreview/asreview-insights/blob/master/docs/example_absolute_axes.png)


#### Example: Multiple curves in one plot

It is possible to have multiple curves in one plot by using the API,
and add a legend.

```python
import matplotlib.pyplot as plt

from asreview import open_state
from asreviewcontrib.insights.plot import plot_recall


fig, ax = plt.subplots()

with open_state(""tests/asreview_files/sim_van_de_schoot_2017_1.asreview"") as s1:
    plot_recall(ax, s1)

with open_state(""tests/asreview_files/""
                ""sim_van_de_schoot_2017_logistic.asreview"") as s2:
    plot_recall(ax, s2)

ax.lines[0].set_label(""Naive Bayes"")
ax.lines[2].set_label(""Logistic"")
ax.legend()

fig.savefig(""docs/example_multiple_lines.png"")
```
![Recall with multiple lines](https://github.com/asreview/asreview-insights/blob/master/docs/example_multiple_lines.png)

## `metrics`

The `metrics` subcommand in ASReview Insights can be used to compute metrics
at given values. The easiest way to get compute metrics for a ASReview project
file is with the following command don the command line:

```
asreview metrics sim_van_de_schoot_2017.asreview
```

which results in

```
    ""asreviewVersion"": ""1.0"",
    ""apiVersion"": ""1.0"",
    ""data"": {
        ""items"": [
            {
                ""id"": ""recall"",
                ""title"": ""Recall"",
                ""value"": [
                    [
                        0.1,
                        1.0
                    ],
                    [
                        0.25,
                        1.0
                    ],
                    [
                        0.5,
                        1.0
                    ],
                    [
                        0.75,
                        1.0
                    ],
                    [
                        0.9,
                        1.0
                    ]
                ]
            },
            {
                ""id"": ""wss"",
                ""title"": ""Work Saved over Sampling"",
                ""value"": [
                    [
                        0.95,
                        0.8913851624373686
                    ]
                ]
            },
            {
                ""id"": ""erf"",
                ""title"": ""Extra Relevant record Found"",
                ""value"": [
                    [
                        0.1,
                        0.9047619047619048
                    ]
                ]
            },
            {
                ""id"": ""atd"",
                ""title"": ""Average time to discovery"",
                ""value"": 101.71428571428571
            },
            {
                ""id"": ""td"",
                ""title"": ""Time to discovery"",
                ""value"": [
                    [
                        3898,
                        22
                    ],
                    [
                        284,
                        23
                    ],
                    [
                        592,
                        25
                    ],
                    ...
                    [
                        2382,
                        184
                    ],
                    [
                        5479,
                        224
                    ],
                    [
                        3316,
                        575
                    ]
                ]
            }
        ]
    }
}
```

Each available item has two values. The first value is the value at which the
metric is computed. In the plots above, this is the x-axis. The second value
is the results of the metric. Some metrics are computed for multiple values.

| Metric | Description pos. 1 | Description pos. 2 | Default |
|---|---|---|---|
| `recall` | Labels | Recall | 0.1, 0.25, 0.5, 0.75, 0.9 |
| `wss` | Recall | Work Saved over Sampling at recall | 0.95 |
| `erf` | Labels | ERF | 0.10 |
| `atd` | Average time to discovery (in label actions) | - | - |
| `td` | Row number (starting at 0) | Number of records labeled | - |


### Override default values

It is possible to override the default values of `asreview metrics`. See
`asreview metrics -h` for more information or see the example below.

```
asreview metrics sim_van_de_schoot_2017.asreview --wss 0.9 0.95
```

```
{
    ""asreviewVersion"": ""1.0"",
    ""apiVersion"": ""1.0"",
    ""data"": {
        ""items"": [
            {
                ""id"": ""recall"",
                ""title"": ""Recall"",
                ""value"": [
                    [
                        0.1,
                        1.0
                    ],
                    [
                        0.25,
                        1.0
                    ],
                    [
                        0.5,
                        1.0
                    ],
                    [
                        0.75,
                        1.0
                    ],
                    [
                        0.9,
                        1.0
                    ]
                ]
            },
            {
                ""id"": ""wss"",
                ""title"": ""Work Saved over Sampling"",
                ""value"": [
                    [
                        0.9,
                        0.8474220139001132
                    ],
                    [
                        0.95,
                        0.8913851624373686
                    ]
                ]
            },
            {
                ""id"": ""erf"",
                ""title"": ""Extra Relevant record Found"",
                ""value"": [
                    [
                        0.1,
                        0.9047619047619048
                    ]
                ]
            },
            {
                ""id"": ""atd"",
                ""title"": ""Average time to discovery"",
                ""value"": 101.71428571428571
            },
            {
                ""id"": ""td"",
                ""title"": ""Time to discovery"",
                ""value"": [
                    [
                        3898,
                        22
                    ],
                    [
                        284,
                        23
                    ],
                    [
                        592,
                        25
                    ],
                    ...
                    [
                        2382,
                        184
                    ],
                    [
                        5479,
                        224
                    ],
                    [
                        3316,
                        575
                    ]
                ]
            }
        ]
    }
}
```

### Save metrics to file

Metrics can be saved to a file in the JSON format. Use the flag `-o` or
`--output`.

```
asreview metrics sim_van_de_schoot_2017.asreview -o my_file.json
```

### Metrics CLI

Optional arguments for the command line are `--priors` to include prior
knowledge, `--x_absolute` and `--y_absolute` to use absolute axes.

See `asreview metrics -h` for all command line arguments.

### Metrics API

Metrics are easily accesible with the ASReview Insights API.

Compute the recall after reading half of the dataset.

```python

from asreview import open_state
from asreviewcontrib.insights.metrics import recall

with open_state(""example.asreview"") as s:

    print(recall(s, 0.5))
```

Other metrics are available like `wss` and `erf`.

#### Example: Prior knowledge

It's possible to include prior knowledge to your metric. By default, prior
knowledge is excluded from the metric.

```python

from asreview import open_state
from asreviewcontrib.insights.metrics import recall

with open_state(""example.asreview"") as s:

    print(recall(s, 0.5, priors=True))
```

## License

This extension is published under the [MIT license](/LICENSE).

## Contact

This extension is part of the ASReview project ([asreview.ai](https://asreview.ai)). It is maintained by the
maintainers of ASReview LAB. See [ASReview
LAB](https://github.com/asreview/asreview) for contact information and more
resources.
",2022-08-12
https://github.com/asreview/asreview-multilingual-feature-extractor,"# ASReview multilingual feature extractor

This extension to ASReview implements a multilingual feature extractor algorithm.
This algorithm allows for the usage of records in multiple languages. These 
languages are:

Arabic, Chinese, Dutch, English, French, German, Italian, Korean, Polish, Portuguese, Russian, Spanish, Turkish. 

The extension implements [`sentence-transformers/distiluse-base-multilingual-cased-v1`](https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v1).
This is a sentence-transformers model and maps sentences to a 512 dimensional dense
vector space and is multilingual. For more information about the feature extraction
method, see 

> Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. ArXiv, abs/1908.10084. https://arxiv.org/abs/1908.10084


## Installation

Install the multilingual feature extractor with:

```bash
pip install .
```

or

```bash
pip install git+https://github.com/asreview/asreview-multilingual-feature-extractor.git
```

## Usage

### ASReview LAB

ASReview LAB users can select the model in the
[Model Selection](https://asreview.readthedocs.io/en/latest/features/pre_screening.html#select-model)
step of the project setup. Select ""Multilingual Sentence Transformer"" under
""Feature extraction"". 

### Simulation

The new feature extractor `Multilingual Sentence Transformer` is defined in
[`asreviewcontrib/models/distiluse-base-multilingual.py`](asreviewcontrib/models/distiluse-base-multilingual.py) 
and can be used in a simulation.

```bash
asreview simulate example_data_file.csv -e multilingual
```

Test the feature extractor with:

```bash
asreview simulate benchmark:van_de_Schoot_2017 -e multilingual -m svm
```

## License

[MIT license](/LICENSE)

## Contact

For any questions or remarks, please send an email to asreview@uu.nl or open an issue.
",2022-08-12
https://github.com/asreview/asreview-project-files-testing,"# ASReview Projects for testing
This repository has asreview project files created in different versions for testing compatibility in the latest version.

## Move test projects to `.asreview/` folder
MacOS code to download project files and move to `./asreview` folder.
```
curl -o ~/Downloads/projects.zip -LOk https://github.com/asreview/asreview-project-files-testing/archive/master.zip
unzip ~/Downloads/projects.zip -d ~/Downloads/asreview_projects/
rm -f ~/Downloads/projects.zip

mkdir -p ~/Downloads/.asreview
for dir in ~/Downloads/asreview_projects/asreview-project-files-testing-master/*/
do
    dir=${dir%*/}
    mv ${dir}/* ~/.asreview
done

```

## License 

All work in this repository is CC0 licensed.

##  Contributors

- [Yongchao Terry Ma](https://www.linkedin.com/in/yongchao-ma/) ([@terrymyc](https://github.com/terrymyc))
- [Jonathan de Bruin](https://www.uu.nl/staff/JdeBruin1) ([@J535D165](https://github.com/J535D165))

",2022-08-12
https://github.com/asreview/asreview-ranking-stability,"# DEPRECATED: No longer supported. Only works with version 0 of ASReview.

# asreview-ranking-stability

This is a repository containing some code to plot and investigate the rank stability of the predictions coming out of
ASReview.

First create a probability matrix from a .h5 state file. Then use the other functions to create plots. Note that if you
want to customize a bit more, add a title for example, you will have to change the functions a bit.

## Probability plot:
![prob-plot](images/probability.png)

## Rank plot:
![rank-plot](images/rank.png)

## Spearman's rho plot:
![rho-plot](images/rho.png)

# License and Contact
This repository is under an Apache 2.0 license. Contact: @PeterLombaers
",2022-08-12
https://github.com/asreview/asreview-simulation-time,"# DEPRECATED: No longer supported. Only works with version 0 of ASReview.

# asreview-simulation-time

This is a repository containing python functions for creating time related plots for asreview. At the moment there are two kinds of plots you can create. The 'time_from_start'-plot and the 'time_between_queries'-plot. The first type of plot as the queries on the x-axis and the time between the start of the simulation to the current query on the y-axis. The second type of plot has the same x-axis, and the time from the current to the next query on the y-axis.

## Time-from-start plots
![nb-start](images/nb_start.png)
![log-start](images/log_start.png)
![svm-start](images/svm_start.png)
![rf-start](images/rf_start.png)

## Time-between-queries plots
![nb-between](images/nb_between.png)
![log-between](images/log_between.png)
![svm-between](images/svm_between.png)
![rf-between](images/rf_between.png)

# License and Contact
This repository is under an Apache 2.0 license. Contact: @PeterLombaers
",2022-08-12
https://github.com/asreview/asreview-wordcloud,"# ASReview-wordcloud

[![PyPI version](https://badge.fury.io/py/asreview-wordcloud.svg)](https://badge.fury.io/py/asreview-wordcloud) [![Downloads](https://pepy.tech/badge/asreview-wordcloud)](https://pepy.tech/project/asreview-wordcloud) ![Deploy and release](https://github.com/asreview/asreview-wordcloud/workflows/Deploy%20and%20release/badge.svg) ![Build status](https://github.com/asreview/asreview-wordcloud/workflows/test-suite/badge.svg) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4672241.svg)](https://doi.org/10.5281/zenodo.4672241)


ASReview-wordcloud is an extension for the [ASReview
LAB](https://github.com/asreview/asreview) software. It offers an easy way to
create a visual impression of the contents of datasets.

## Installation

The easiest way to install the wordcloud extension is to install from PyPI:

``` bash
pip install asreview-wordcloud
```

After installation of the wordcloud extension, `asreview` should automatically
detect it. Test this by:

```bash
asreview --help
```

It should list the `wordcloud` subcommand.

## Basic usage

The dataset should contain a column containing titles and/or abstracts.
For specific requirements check the [ASReview documentation](https://asreview.readthedocs.io/en/latest/intro/datasets.html).
To use your data use:

```
asreview wordcloud MY_DATA.csv
```

The following shows the [Schoot et al. (2017) dataset:](https://asreview.readthedocs.io/en/latest/intro/datasets.html#benchmark-datasets)

![All texts](https://github.com/asreview/asreview-wordcloud/blob/main/figures/ptsd_all.png?raw=true)

To make a wordcloud on titles only, use the `title` flag.

```
asreview wordcloud MY_DATA.csv --title
```

![Titles only](https://github.com/asreview/asreview-wordcloud/blob/main/figures/ptsd_title.png?raw=true)

To make a wordcloud on abstracts only, use the `abstract` flag.

```
asreview wordcloud MY_DATA.csv --abstract
```

![Abstracts only](https://github.com/asreview/asreview-wordcloud/blob/main/figures/ptsd_abstract.png?raw=true)

To make a wordcloud on relevant (inclusions) only, use the `relevant` flag.

```
asreview wordcloud MY_DATA.csv --relevant
```

### Save figure

Save the wordcloud to a file with the `-o`  flag.

```
asreview wordcloud MY_DATA.csv -o MY_DATA_WORDCLOUD.png
```

## License

This extension is published under the [MIT license](/LICENSE).

## Contact

This extension is part of the ASReview project ([asreview.ai](https://asreview.ai)). It is maintained by the
maintainers of ASReview LAB. See [ASReview
LAB](https://github.com/asreview/asreview) for contact information and more
resources.
",2022-08-12
https://github.com/asreview/automated-systematic-review-simulations,"# Automated Systematic Review - Simulation [DEPRECATED]

This project contains the code of the simulation study for the [Automated
Systematic Review](https://github.com/asreview/automated-systematic-review)
project. It contains code to run batches of simulation runs in parallel using MPI.

We make use of the SURFSara HPC infrastructure. But with some modifications,
the code can be run on other HPC facilities as well.

Some of the code in the repository is old an no longer maintained. Batch functionality has been (or will soon be) integrated into the core ASReview project. Other scripts will be copied to more suitable repositories.

## Installation 

The Automated Systematic Review project requires Python 3.6+. To run the code you
also need an implementation of the MPI standard. The most well known standard is OpenMPI.
This is not a python package and should be installed separately.

The simulation project itself can be directly installed with: 

```bash
pip install --user git+https://github.com/asreview/automated-systematic-review-simulations
```
Dependencies are automatically installed.

## Running a batch

To run a batch of simulations on 4 cores and 12 runs, use the following command:

```bash
mpirun -n 4 asreview batch ${DATA_SET} --state_file ${DIR}/results.json --n_runs 12
```

It will create 12 files in the ${DIR} directory, while running on 4 cores in parallel.


## Related packages

- asreview-visualization
	Package for visualization of log files.

- asreview-hyperopt
	Package for optimizing ASReview hyperparameters.
",2022-08-12
https://github.com/asreview/citation-file-formatting,"# Citation File Formatting
A collection of documentation highlighting quirks around the file formatting for citation and reference managers.

## Table of Contents
* [Introduction](#introduction)
* [Usage](#usage)
  * [File Formats](#file-formats)
  * [Citation and Reference Managers](#citation-and-reference-managers)
  * [Search Engines](#search-engines)
  * [Systematic Review Software](#systematic-review-software)
* [Contributing](#contributing)
* [License](#license)

## Introduction
There are many software solutions and tools that allow you to manage your scientific work. Often times, you need to use multiple solutions to cover your whole workflow. This is where things get interesting but also confusing. Some tools just do not work well with others.

In this open repository, we have tried to simplify the process of setting up a scientific workflow for research. We have described the interrelations between popular citation and reference managers, search engines and systematic review software. Additionally, we have gathered [relevant datasets](Datasets/) to show capabilities of each tool more specifically.

## Usage
This project is meant to serve as a guide in the field of scientific research, namely dealing with systematic reviews and related software solutions. Or you can check it out for inspiration and learning about said software!

### File Formats
Below, most commonly used file formats in dealing with systematic review are described.

#### CSV
A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas. The use of the comma as a field separator is the source of the name for this file format. A CSV file typically stores tabular data (numbers and text) in plain text, in which case each line will have the same number of fields.

*The CSV file format is not fully standardized.*

Learn more about [CSV at Wikipedia](https://en.wikipedia.org/wiki/Comma-separated_values).

#### RIS
RIS is a standardized tag format developed by Research Information Systems, Incorporated (the format name refers to the company) to enable citation programs to exchange data. The RIS file format — two letters, two spaces and a hyphen — is a tagged format for expressing bibliographic citations. According to the specifications, the lines must end with the ASCII carriage return and line feed characters. Multiple citation records can be present in a single RIS file. A record ends with an ""end record"" tag ER - with no additional blank lines between records.

*The RIS file format is standardized but sometimes improperly implemented.*

Learn more about [RIS at Wikipedia](https://en.wikipedia.org/wiki/RIS_(file_format)).

#### TSV
A tab-separated values (TSV) file is a simple text format for storing data in a tabular structure, e.g., a database table or spreadsheet data, and a way of exchanging information between databases. Each record in the table is one line of the text file. Each field value of a record is separated from the next by a tab character. The TSV format is thus a type of the more general delimiter-separated values format.
TSV is an alternative to the common comma-separated values (CSV) format, which often causes difficulties because of the need to escape commas – literal commas are very common in text data, but literal tab stops are infrequent in running text. The IANA standard for TSV achieves simplicity by simply disallowing tabs within fields.

*The TSV file format is not fully standardized.*

Learn more about [TSV at Wikipedia](https://en.wikipedia.org/wiki/Tab-separated_values).

#### XLSX
The Office Open XML Workbook (XLSX) file is part of a set of file formats that can be used to represent electronic office documents. While this specific format is dedicated to spreadsheets, there are other formats dedicated for word processing documents, presentations as well as specific formats for material such as mathematical formulae, graphics, bibliographies etc.

*The XLSX file format is standardized.*

Learn more about [XLSX at Wikipedia](https://en.wikipedia.org/wiki/Office_Open_XML).

### Citation and Reference Managers
Below, most commonly used citation and reference managers are listed.
- [Citavi](https://www.citavi.com/en)
- [Endnote](https://endnote.com)
- [Mendeley](https://www.mendeley.com)
- [Refworks](https://www.refworks.com/)
- [Zotero](https://www.zotero.org)

### Search Engines
Below, most commonly used search engines are listed.
- [CINAHL Database](https://guides.library.uab.edu/CINAHL)
- [Cochrane](https://www.cochrane.org/search/)
- [Embase](https://www.embase.com)
- [Google Scholar](https://scholar.google.com)
- [Google Scholar Button for Chrome](https://chrome.google.com/webstore/detail/google-scholar-button/ldipcbpaocekfooobnbcddclnhejkcpn)
- [ERIC Ovid®](https://www.wolterskluwer.com/en/solutions/ovid/85)
- [Psycinfo Ovid®](https://www.wolterskluwer.com/en/solutions/ovid/139)
- [Pubmed](https://pubmed.ncbi.nlm.nih.gov)
- [ResearchGate](https://researcgate.net)
- [Scopus](https://www.scopusjournals.com)
- [Web of Science](https://apps.webofknowledge.com/)

### Systematic Review software
Below, most commonly used search engines are listed.
- [abstrackr](http://abstrackr.cebm.brown.edu/account/login)
- [ASReview](https://asreview.nl)
- [Covidence](https://www.covidence.org)
- [DistillerSR](https://www.evidencepartners.com/products/distillersr-systematic-review-software)
- [Eppi-Reviewer](http://eppi.ioe.ac.uk/cms/Default.aspx?alias=eppi.ioe.ac.uk/cms/er4)
- [Rayyan](https://www.rayyan.ai)
- [RobotReviewer](https://www.robotreviewer.net)

## Contributing
Details are described in the [contributing guide](https://github.com/asreview/citation-file-formatting/blob/main/CONTRIBUTING.md).

## License
[CC0 1.0 Universal](https://github.com/asreview/citation-file-formatting/blob/main/LICENSE)
",2022-08-12
https://github.com/asreview/paper-asreview,"# Scripts for '*ASReview: Open Source Software for Efficient and Transparent Active Learning for Systematic Reviews*'

This repository contains the scripts of the simulation study found in the paper *ASReview: Open Source Software for Efficient and Transparent Active Learning for Systematic Reviews*. This paper introduces the [ASReview project](https://github.com/asreview) and the [ASReview LAB software](https://github.com/asreview/asreview). The results of the simulation study are available via: [10.17605/OSF.IO/2JKD6](https://www.doi.org/10.17605/OSF.IO/2JKD6).

:raised_hand: The scripts in this repository make use of ASReview version 0.7.2. Many improvements were made to the ASReview software afterward. We encourage you to use the [latest version](https://pypi.org/project/asreview/) for new studies. See the [extensive ASReview documentation](https://asreview.readthedocs.io/en/latest/) for all features of ASReview for screening and simulating. 

:arrows_counterclockwise: A persistent version of the scripts can be found on Zenodo `Ferdinands et al. (2020, September 11). Scripts for 'ASReview: Open Source Software for Efficient and Transparent Active Learning for Systematic Reviews' (Version v1.0.1). Zenodo.` http://doi.org/10.5281/zenodo.4024122.

## Installation

Running this simulation study requires Python 3.6+. The results in this repository are generated with ASReview v0.7.2. To run the code in parallel, you also need an implementation of the MPI standard. The most well known standard is [OpenMPI](https://www.open-mpi.org/) (this is not a python package and should be installed separately).

Install the python depencies with
```
pip install -r requirements.txt
```

## Simulation study

This directory contains scripts to reproduce reproduce the results of simulation study for the paper *ASReview: Open Source Software for Efficient and Transparent Active Learning for Systematic Reviews*. It consists of three parts:

1) [optimizing hyperparameters](Hyperparameter_optimization)
2) [simulating systematic reviews](Simulation)
3) [visualizing the simulation results](Visualization)

The figure in the paper displays the performance of an Automated Systematic Review model (Naive Bayes + TF-IDF + Certainty + Dynamic Supersampling) over 15 runs on four already labeled datasets. See paper for details on the datasets.

The simulation output is available on the [Open Science Framework](https://www.doi.org/10.17605/OSF.IO/2JKD6).

## License

The scripts in this repository are MIT licensed. 

## Contact

Questions or remarks? Please send an email to asreview@uu.nl. 

",2022-08-12
https://github.com/asreview/paper-depression-brouwer-simulation-ATD,"[![DOI](https://zenodo.org/badge/339765080.svg)](https://zenodo.org/badge/latestdoi/339765080)

# Scripts for simulation study on obtaining average time to discovery 

This repository contains the scripts for a simulation study performed on data from the systematic review by [Brouwer et al. (2019)](https://doi.org/10.1016/j.cpr.2019.101773).

The simulation study has the following characteristics:

- The number of runs is equal to the number of inclusions in the dataset;
- Every run starts with 1 prior inclusion and 10 prior exclusions;
- The prior inclusion is different for every run, e.g. all inclusions in the data are used as a prior inclusion once;
- The 10 prior exclusions are the same for every run, and they are randomly sampled from the dataset.


## Installation

The scripts in this repository require Python 3.6+ and R. Install the dependencies with (in the command line):

```
# install required software (such as ASReview)
pip install requirements.txt
Rscript install.R
```

## Data

The systematic review data used for this study can be found on the [Open
Science Framework](https://osf.io/r45yz/). Download the file
`brouwer_deduplicated.csv` and put it in the `/data` folder.


## Execute simulation study

To reproduce the results presented in this study, run the following in your
terminal:

```
# run all simulations
sh run_all.sh
```

## Extract results 

Extract the results from the simulation output by executing the
following in your CLI:

```
Rscript scripts/extract_simulation_results.R
```

The output (in the folder `output/`) consists of
- All inclusions in the dataset, ordered by their time to discovery - the `time_to_discovery.csv` file. The column *row* corresponds to the position of the inclusion in the original dataset. Note that the position starts at 1;
- A table with WSS and RRF values for every run - the `wss_and_rrf_values.csv` file;
- A plot with recall curves for all runs of this simulation study - the `recall.png` file.


# License
The content in this repository is published under the MIT license.

# Funding
This project is funded by a grant from the Centre for Urban Mental Health, University of Amsterdam, The Netherlands

# Contact
For any questions or remarks, please send an email to the [ASReview-team](mailto:asreview@uu.nl) or [Marlies Brouwer](https://orcid.org/0000-0002-9972-9058).

# References
Brouwer, M. E., Williams, A. D., Kennis, M., Fu, Z., Klein, N. S., Cuijpers,
P., & Bockting, C. L. H. (2019). Psychological theories of depressive relapse
and recurrence: A systematic review and meta-analysis of prospective studies.
Clinical Psychology Review, 74, 101773. https://doi.org/10.1016/j.cpr.2019.101773
",2022-08-12
https://github.com/asreview/paper-evaluating-models-across-research-areas,"# Active learning for screening prioritization in systematic reviews - a simulation study
This repository provides supplementary material on the paper _Active learning for screening prioritization in systematic reviews - a simulation study_.

- Authors: Gerbrich Ferdinands, Raoul Schram, Jonathan de Bruin, Ayoub Bagheri, Daniel Oberski, Lars Tummers, Rens van de Schoot
- Maintainer: Gerbrich Ferdinands
- Date: 30-07-2020
- Version 1.0.0
- Data collection and simulation period: January - May 2020
- Manuscript submitted on 10-08-2020.

This repository contains scripts and data required to reproduce the results: the systematic review datasets and their preprocessing scripts, scripts for the simulations, scripts for the processing and analysis of the results of the simulations, and scripts for producing the figures and tables.

# Content
This archive is organized as such that it follows the steps needed to reproduce the study that was carried out:

1. `datasets` -  collection and preprocessing of six systematic review datasets, production of Table 1.
2. `simulation_study` - simulating seven active learning models on the systematic review datasets.
3. `results` - the simulation study output was processed and analyzed to arrive at the results discussed in the manuscript (Figure 1 and 2, Table 2, 3, and 4).

Please follow the order above if you want to reproduce this study. Every subdirectory contains its own `readme` that will guide you through the process of that particular step.

This repository contains one additional folder called `other`, containing the ethical approval form by the FETC, and the grant approval by SURFsara.

### Disclaimer
Simulating a systematic review produces files that are very large in size. The total amount of storage needed for all the raw output files is over 900 GB. GitHub does not allow for repositories of this size, therefore the raw datafiles of the simulation are stored on the Open Science Framework instead. Due to a bug in OSF, the files had to be distributed over 2 projects, https://osf.io/7mr2g/ and https://osf.io/ag2xp/.

# Requirements
All computations were run on macOS Catalina 10.15.2. The steps in this repository assume you have R (3.6.1) and RStudio (1.2.5042) installed. Moreover, Python version 3.7 or higher is required. If you don't have python you can follow [this installation guide](https://asreview.nl/#!/quick-start). Any further requirements are discussed in the `simulation_study` readme.

# Privacy
Only openly available data was used in this study. The study has been approved by the Ethics Committee of the Faculty of Social and Behavioural Sciences of Utrecht University, filed as an amendment under study 20-104. The approval document can be found in the `other` directory of this repository.

# Permission and access
This research archive is openly published on GitHub, https://github.com/asreview/paper-evaluating-models-across-research-areas under MIT license. Therefore it is 'Open Access' and thus available for anyone. This repository will remain online for at least 10 years.

# Contact
This repository is maintained by Gerbrich Ferdinands. For any further questions, please e-mail me at `g.ferdinands@uu.nl`.
",2022-08-12
https://github.com/asreview/paper-guidelines-KIFMS,"# Scripts for paper on ""Towards up-to-date medical guidelines""

[![DOI](https://zenodo.org/badge/379924501.svg)](https://zenodo.org/badge/latestdoi/379924501)

The purpose of this study was to evaluate the performance and feasibility of active learning to support the selection of relevant publications within the context of medical guideline development. This repository contains scripts to run and analyze simulations for 14 datasets openly published on the Dutch database for [medical guidelines](https://www.richtlijnendatabase.nl). The results are published in the paper ""Artificial intelligence supports literature screening in medical guideline development: towards up-to-date medical guidelines"". 


## Installation

The scripts in this repository require Python 3.6+. Install the extra dependencies with (in the command line):

```
pip install -r requirements.txt
```

## Datasets

The raw data can be obtained via the Open Science Framework [OSF](https://osf.io/vt3n4/) and contains 14 published guidelines from the [Dutch Medical Guideline Database](https://richtlijnendatabase.nl/). The following files should be obtained from OSF and put in a folder `raw_data`:

```
Distal_radius_fractures_approach.csv
Distal_radius_fractures_closed_reduction.csv
Hallux_valgus_prognostic.csv
Head_and_neck_cancer_bone.csv
Head_and_neck_cancer_imaging.csv
Obstetric_emergency_training.csv
Post_intensive_care_treatment.csv
Pregnancy_medication.csv
Shoulder_replacement_diagnostic.csv
Shoulder_replacement_surgery.csv
Shoulderdystocia_positioning.csv
Shoulderdystocia_recurrence.csv
Total_knee_replacement.csv
Vascular_access.csv
```

Each dataset contains

```
title
abstract
```

and three columns with labeling decisions titled:

```
noisy_inclusion
expert_inclusion
fulltext_inclusion
```

The datasets in *raw_data* are split into three columns with labeling decisions. The resulting 42 datasets are generated by executing `job_splitfiles.sh`. The results are stored in the subfolder *data*. 


## Descriptive dataset statistics

To create descriptive statistics for each dataset run:

```
sh generate_dataset_characteristics.sh
```

The results are stored in `output/simulation/[NAME_DATASET]/descriptives/*.json`, are merged into one table (*csv* and *excel*) by running `python scripts/merge_descriptives.py`, and stored in `output/table/data_descriptives.*`. 


## Create wordclouds

To create wordclouds for each dataset run:

```
sh wordcloud_jobs.sh
```

The results are stored in `output/simulation/[NAME_DATASET]/descriptives/wordcloud`. 
There are three version of the wordcloud available, a wordcloud based on the title/abstract words for:

- the entire set of records;
- for the relevant records only;
- for the irrelevant records only. 


## Simulation

The simulation was conducted for each dataset with an equal amount of runs as the number of relevant records in the dataset with each relevant record being a prior inclusion and 10 randomly chosen irrelevant records. In each run, and for every dataset, the same 10 irrelevant records have been used. To extract information about the records that have been used, run `python scripts/get_prior_knowledge.py`, and the result is stored in `output/tables`. 


To obtain the result of the simulation, run: 

```
sh run_simulation.sh
```

The results are stored in `output/simulation`. The dataset characteristics are obtained with `python scripts/merge_descriptives.py` and stored in `output/tables`. The metrics resulting from the simulation study per run, can be obtained with `python scripts/merge_metrics.py` and stored in `output/tables`.

The raw `h5` files are 28.4Gb and are available on request, see the contact details. However, it is straightforward to obtain the results by running the simulation again by using ASReview v0.16. Seed values are set in  `run_simulation.sh`. 

## Analyses

The Jupyter notebook [analyses/analyses_guidelines_KIFMS.ipynb](analyses/analyses_guidelines_KIFMS.ipynb)
 contains a detailed, step-by-step analysis of the simulations performed in this project. For more information about the analysis, read the [README](analyses). 

## Licence 

The content in this repository is published under the MIT license.

## Contact

For any questions or remarks, please send an email to asreview@uu.nl.

",2022-08-12
https://github.com/asreview/paper-megameta-hyperparameter-training,"[![DOI](https://zenodo.org/badge/424153223.svg)](https://zenodo.org/badge/latestdoi/424153223)

# Hyperparameter-training for the Mega-Meta project

The repository is part of the so-called, Mega-Meta study on reviewing factors
contributing to substance use, anxiety, and depressive disorders. The study
protocol has been pre-registered at
[Prospero](https://www.crd.york.ac.uk/prospero/display_record.php?ID=CRD42021266297).
The procedure for obtaining the search terms, the exact search query, and
selecting key papers by expert consensus can be found on the [Open Science
Framework](https://osf.io/m5uhy/). 

The screening was conducted in the software ASReview ([Van de Schoot et al.,
2020](https://www.nature.com/articles/s42256-020-00287-7) using the protocol
as described in [Hofstee et al. (2021)](https://osf.io/3znar/). The server
installation is described in [Melnikov
(2021)](https://github.com/valmelnikov/asreview_server_setup), 
and the post-processing is described by [van de Brand et al., 2021](https://github.com/asreview/paper-megameta-postprocessing-screeningresults). 
The data can be found on DANS [LINK NEEDED].

This repository stores the scripts and plugins that were used for the creation
of the three final project files. These final project files were created using a
classifier based on a [convolutional neural
network](https://github.com/JTeijema/asreview-plugin-model-cnn-17-layer),
optimized using [Optuna](https://github.com/optuna/optuna).

## Content
The `Plugins` folder contains the 2 used ASReview plugins.
[`asreview-cnn-hpo`](https://github.com/BartJanBoverhof/asreview-cnn-hpo) was
used to find the optimal settings for each dataset, and
[`asreview-model-cnn-17-layer`](https://github.com/JTeijema/asreview-plugin-model-cnn-17-layer)
was used to implement these settings.

The `scripts` folder contains a Jupyter Notebook that can be run in [Google
Colab](https://colab.research.google.com/). This script installs the plugins
found in the `plugins` folder and then creates an ASReview lab instance that can
be used to employ these plugins, find the optimal hyperparameters, and create
the final project files.

## Step-by-step quickguide
1. Create a project file from one of the input files.
2. Select the HPO-CNN optimizer as classifier for the created project file.
3. Train the project file to recieve the optimal CNN hyper parameters. The
   optimal parameters appear in the console used to start ASReview.
4. Plug these parameters into the CNN model (not the HPO-CNN model).
5. Train the CNN model.
6. Start screening records with the optimized CNN model.

## Step-by-step guide
This guide details how the CNN was optimized and trained.

1. The process started with 3 different excel files containing ASReview output.


2. In Google Colab, upload and run the
   `cnn_training_script_for_use_in_google_colab.ipynb` file in found in the
   folder called `scripts`. This file is used to optimize the CNNs for the later
   processing.

    - A custom-made version of the HPO-CNN was used to determine the optimal
      hyperparameters for each of the project files. These hyperparameters were
      then applied to the CNN classifier model.

        - To use this custom-made version of the HPO-CNN, upload the folder
          containing the HPO-CNN version to Colab. A quick way of doing this is
          by zipping-up the folder and uploading this zip file.

        - The notebook contains code for unzipping, and then installing the
          plugin automatically.

    - The notebook uses NGROK to access the ASReview frontend. A personal NGROK
      token is needed for the `NGROK_AUTH_TOKEN` variable. A link with
      instructions on where to get such a token can be found in the notebook.

    ```python
    !pip install pyngrok --quiet
    from pyngrok import ngrok

    # Terminate open tunnels if exist
    ngrok.kill()

    # Setting the authtoken (optional)
    # Get your authtoken from https://dashboard.ngrok.com/auth
    NGROK_AUTH_TOKEN = ""fill token here""
    ngrok.set_auth_token(NGROK_AUTH_TOKEN)
    ```

    - After running the notebook, there will be a link produced by NGROK. This
      link will open the ASReview frontend.  

    ```python
    ngrok.connect(port=""80"", proto=""http"")

    <NgrokTunnel: ""http://d3c5-35-197-26-146.ngrok.io"" -> ""http://localhost:80"">
    ```

    - In this front end, create a new project file with one of the Excel files.

    - When the training of the new project file is finished (after a long time),
      the still processing colab cell running ASReview will print the optimal
      parameters in the output.  

    ```
    Hpo trail:  77/80
    Hpo trail:  77/80
    Hpo trail:  77/80
    Hpo trail:  77/80
    FOUND HYPERPARAMETERS:  {'nlayers': 3, 'nfilters':  209}
    ```

3. These parameters are used to optimize the CNN used for the final project
   files.

    - For the training of these project files, the parameters that resulted as
      an output of the HPO are used for the CNN. The output is set by filling in
      the results into the `nlayers` and `nfilters` variables in the
      `asreview-plugin-model-cnn-17-layer\asreviewcontrib\models\cnn.py` file.

    ```python
    def _create_dense_nn_model(_size):
      def model_wrapper():
        backend.clear_session()

        nfilters = 209

        model = Sequential()
    ```

4. Install this newly created CNN. Using this new optimized CNN, create the
   projects files for screening.


## Funding
This project is funded by a grant from the Centre for Urban Mental Health, University of Amsterdam, The Netherlands

## Licence
The content in this repository is published under the MIT license.

## Contact
For any questions or remarks, please send an email to the [ASReview-team](mailto:asreview@uu.nl).
",2022-08-12
https://github.com/asreview/paper-megameta-postprocessing-screeningresults,"[![DOI](https://zenodo.org/badge/416328702.svg)](https://zenodo.org/badge/latestdoi/416328702)


# Scripts for Post-Processing Mega-Meta Results

The repository is part of the so-called, Mega-Meta study on reviewing factors
contributing to substance use, anxiety, and depressive disorders. The study
protocol has been pre-registered at
[Prospero](https://www.crd.york.ac.uk/prospero/display_record.php?ID=CRD42021266297).
The procedure for obtaining the search terms, the exact search query, and
selecting key papers by expert consensus can be found on the [Open Science
Framework](https://osf.io/m5uhy/).

The screening was conducted in the software ASReview ([Van de Schoot et al.,
2020](https://www.nature.com/articles/s42256-020-00287-7) using the protocol
as described in [Hofstee et al. (2021)](https://osf.io/3znar/). The server
installation is described in [Melnikov
(2021)](https://github.com/valmelnikov/asreview_server_setup), and training of the
hyperparameters for the CNN-model is described by [Tijema et al
(2021)](https://github.com/asreview/paper-megameta-hyperparameter-training). The
data can be found on DANS [LINK NEEDED].

The current repository contains the post-processing scripts to:

1.	Merge the three output files after screening in
ASReview;
2.	Obtain missing DOIs;
3.	Apply another round of de-duplication ([the first round](https://osf.io/m5uhy/) of de-duplication was applied before the screening started).
4. Deal with noisy labels corrected in two rounds of quality checks;

The scripts in the current repository result in one single dataset that can be
used for future meta-analyses. The dataset itself is available on DANS[NEEDS
LINK].  

## Datasets


### Test Data
The `/data` folder contains test-files which can be used to test the pipeline.

```
NOTE: When you want to use these test files; please make sure that the empirical
data is not saved in the `/data` folder because the next step will overwrite these files.
```

1. Open the `pre-processing.Rproject` in Rstudio;
2. Open `scrips/change_test_file_names.R` and run the script. The test files
will now have the same file names as those of the empirical data.
3. Continue with **Running the complete pipeline**.

#### Results of the test data
To check whether the pipeline worked correctly on the test data, check the following
values in the output:

- Within the `crossref_doi_retrieval.ipynb` script 33/42 doi's should be retrieved.
- After two rounds of deduplication in `master_script_deduplication.R` the total number of relevant papers
(sum of the values in the composite_label column) should be 21.
- After running the quality_check function in  `master_script_quality_check.R` the number of changed labels
should be:
  - Quality check 1: 7
  - Quality check 2: 6

### Empirical Data

The empricial data is available on DANS[NEEDS LINK]. Request access, donwload the files,
and add the required data into the `/data` folder.

### Data Files Names

The following nine datasets should be available in `/data`:

The three export-datasets with the partly labelled data after screening in
ASReview:  
  - `anxiety-screening-CNN-output.xlsx`
  - `depression-screening-CNN-output.xslx`
  - `substance-screening-CNN-output.xslx`

The three datasets resulting from Quality Check 1:
 - `anxiety-incorrectly-excluded-records.xlsx`
 - `depression-incorrectly-excluded-records.xlsx`
 - `substance-incorrectly-excluded-records.xlsx`

The three datasets resulting from Quality Check 2:
 - `anxiety-incorrectly-included-records`
 - `depression-incorrectly-included-records`
 - `substance-incorrectly-included-records`

## Requirements to get started

To get started:
1. Open the `pre-processing.Rproject` in Rstudio;
2. Open `scripts/master_script_merging_after_asreview.R`;
3. Install, if necessary, the packages required by uncommenting the lines and running them.
4. Make sure that at least the following columns are present in the data:
    - `title`
    - `abstract`
    - `included`
    - `year` (may be spelled differently as this can be changed within `crossref_doi_retrieval.ipynb`)


## Running the complete pipeline

1. Open the `pre-processing.Rproject` in Rstudio and run the `master_script_merging_after_asreview.R` to merge the datasets.
At the end of the merging script, the file `megameta_asreview_merged.xlsx` is created and saved in `/output`.
2. Run the `scripts/crossref_doi_retrieval.ipynb` in [jupyter notebook](https://jupyter.org/install) to retrieve the missing doi's (you might need to install the package tqdm first: `pip install tqdm`). The output from the doi retrieval is stored in `/output`:
   `megameta_asreview_doi_retrieved.xlsx`. Note: This step might take some time!
   To significantly decrease run time, follow the steps in the [Improving DOI retrieval
   speed](#Improving-DOI-retrieval-speed) section.
3. For the deduplication part, open and run `scripts/master_script_deduplication.R`
back in the Rproject in Rstudio. This result is stored in `/output`: `megameta_asreview_deduplicated.xslx`
4. Two quality checks are performed. Manually change the labels
    1. of incorrectly excluded records to included.
    2. of incorrectly included records to excluded.  
   The data which should be corrected is available on DANS. 
   This step should add the following columns to the dataset:
- `quality_check_1(0->1)` (1, 2, 3, NA):
  This column indicates for which subjects a record was falsely excluded:
  - 1 = anxiety
  - 2 = depression
  - 3 = substance-abuse
- `quality_check_2(1->0)` (1, 2, 3, NA):
  This column indicates for which subjects a record was falsely included:
  - 1 = anxiety
  - 2 = depression
  - 3 = substance-abuse
- `depression_included_corrected` (0, 1, NA):
  Combining the information from the depression_included and quality_check columns,
  this column contains the inclusion/exclusion/not seen labels after correction.
- `substance_included_corrected` (0, 1, NA):
    Combining the information from the substance_included and quality_check columns,
    this column contains the inclusion/exclusion/not seen labels after correction.
- `anxiety_included_corrected` (0, 1, NA):
  Combining the information from the anxiety_included and quality_check columns,
  this column contains the inclusion/exclusion/not seen labels after correction.
- `composite_label_corrected` (0, 1, NA):
  A column indicating whether a record was included in at least one of the
  corrected_subject columns: The results after taking the quality checks into account.
5. OPTIONAL: Create ASReview plugin-ready data by running the script `master_script_process_data_for_asreview_plugin.R`.
This script creates a new folder in the output folder, `data_for_plugin`, containing several versions
of the dataset created from step 4. See [Data for the ASReview plugin](#data-for-the-asreview-plugin) for more information.

### Improving DOI retrieval speed
It is possible to improve the speed of the doi retrieval by using the following steps:

1. Split the dataset into smaller chunks by running the `split_input_file.ipynb`
   script. Within this script is the option to set the amount of chunks. If the
   records aren't split evenly, the last chunk might be smaller than the others.
2. For each chunk, create a copy of the `chunk_0.py` file, and place it in the
   `split` folder. Change the name `chunk_0.py` to `chunk_1.py`, `chunk_2.py`,
   etc, for each created chunk.
3. Within each file, change `script_number` = ""0"" to `script_number` = ""1"",
   `script_number` = ""2"", etc.
4. Run each `chunk_*.py` file in the `split` folder simultaneously from a
   separate console. The script stores the console output to a respective
   `result_chunk_*.txt` file.
5. Use the second half of `merge_files.ipynb` to merge the
   results of the `chunk_*.py` scripts.
6. The resulting file will be stored in the same way as the
   `crossref_doi_retrieval.ipynb` would.

The split folder should look like this after each chunk has been run:
![Split folder](img/split_example.png)

## Deduplication strategy

Keeping in mind that deduplication is never perfect,
`scripts/master_script_deduplication.R` contains a function to deduplicate the
records in a very conservative way. It is assumed that it is better to miss
duplicates within the data, than to falsely deduplicate records.

Therefore deduplication within the `master_script_deduplication.R` is based on
two different rounds of deduplication. The first round uses the digitial
object identifier (doi) to identify duplicates. However, many doi's, even
after doi-retrieval, are still missing. Or in some cases the doi's may be
different for otherwise seemingly identical records. Therefore, an extra round
of deduplication is applied to the data. This conservative strategy was
devised with the help of @bmkramer. The code used a deduplication script by
@terrymyc as inspiration.


The exact strategy of the second deduplication round is as follows:
1. Set all necessary columns (see below) for deduplication to lowercase characters and remove any punctuation marks.
2. Count duplicates identified using conservative deduplication strategy. This strategy will identify duplicates based on:
  - Author
  - Title
  - Year
  - Journal or issn (if either journal or issn is an exact match, together with the above, the record is marked as a duplicate)
3. Count duplicates identified using a less conservative deduplication strategy. This strategy will identify duplicates based on:
  - Author
  - Title
  - Year
4. Deduplicate using the strategy from 2.

The deduplication script will also print the number of identified duplicates
for both the conservative strategy and a less conservative strategy based on
only authors, title, and year. In this way, we can compare the impact of
different duplication strategies.

## Data for the ASReview plugin.
The script `master_script_process_data_for_asreview_plugin.R` creates a new folder in the output folder, `data_for_plugin`, containing several versions of the dataset created from [step 4](#running-the-complete-pipeline).

1. `megameta_asreview_partly_labelled`:
A dataset where a column called `label_included` is added, which is an exact copy of the composite_label_corrected.
2. `megameta_asreview_only_potentially_relevant`:
A dataset with only those records which have a 1 in composite_label_corrected
3. `megameta_asreview_potentially_relevant_depression`:
A dataset with only those records which have a 1 in depression_included_corrected
4. `megameta_asreview_potentially_relevant_substance`:
A dataset with only those records which have a 1 in substance_included_corrected
5. `megameta_asreview_potentially_relevant_anxiety`:
A dataset with only those records which have a 1 in anxiety_included_corrected

[INSTRUCTIONS FOR PLUGIN?]

## Post-processing functions

-  `change_test_file_names.R` - With this script the filenames of the test files are converted to the empirical datafile names.
-  `merge_datasets.R` - This script contains a function to merge the datasets. An unique included column is added for each dataset before the merge.
-  `composite_label.R` - This script contains a function to create a column with the final inclusions.
- `print_information_datasets.R` - This script contains a function to print information on datasets.
-  `identify_duplicates.R` - This script contains a function to identify duplicate records in the dataset.
-  `deduplicate_doi.R` - This script contains a function to deduplicate the records, based on doi, while maintaining all information.
-  `deduplicate_conservative.R` - this script contains a function to deduplicate the records in a  conservative way based on title, author, year and journal/issn

## Result

The result of running all master scripts up until [step 4](#running-the-complete-pipeline)
in this repository is the file
`output/megameta_asreview_quality_checked.xslx`. In this dataset the following
columns have been added:

- `index` (1-165045):
  A simple indexing column going from 1-165045. Some numbers are not present
  because they have been removed after deduplication.
- `unique_record` (0, 1, NA):
  Indicating whether the column has a unique DOI. This is NA when there is no
  DOI present.
- `depression_included` (0, 1, NA):
  A column indicating whether a record was included in depression.
- `anxiety_included` (0, 1, NA):
  A column indicating whether a record was included in anxiety.
- `substance_included` (0, 1, NA):
  A column indicating whether a record was included in substance_abuse.
- `composite_label` (0, 1, NA):
  A column indicating whether a record was included in at least one of the
  subjects.
- `quality_check_1(0->1)` (1, 2, 3, NA):
  This column indicates for which subjects a record was falsely excluded:
  - 1 = anxiety
  - 2 = depression
  - 3 = substance-abuse
- `quality_check_2(1->0)` (1, 2, 3, NA):
  This column indicates for which subjects a record was falsely included:
  - 1 = anxiety
  - 2 = depression
  - 3 = substance-abuse
- `depression_included_corrected` (0, 1, NA):
  Combining the information from the depression_included and quality_check columns,
  this column contains the inclusion/exclusion/not seen labels after correction.
- `substance_included_corrected` (0, 1, NA):
    Combining the information from the substance_included and quality_check columns,
    this column contains the inclusion/exclusion/not seen labels after correction.
- `anxiety_included_corrected` (0, 1, NA):
  Combining the information from the anxiety_included and quality_check columns,
  this column contains the inclusion/exclusion/not seen labels after correction.
- `composite_label_corrected` (0, 1, NA):
  A column indicating whether a record was included in at least one of the
  corrected_subject columns: The results after taking the quality checks into account.

For all columns where there are only 0's 1's and NA's, a `0` indicates a negative
(excluded for example), while `1` indicates a positive (included for example). `NA`
means `Not Available`.




## Funding
This project is funded by a grant from the Centre for Urban Mental Health, University of Amsterdam, The Netherlands

## Licence
The content in this repository is published under the MIT license.

## Contact
For any questions or remarks, please send an email to the [ASReview-team](mailto:asreview@uu.nl) or [Marlies Brouwer](https://orcid.org/0000-0002-9972-9058).
",2022-08-12
https://github.com/asreview/paper-simulating-risk-analysis-documents-KNMP,"[![DOI](https://zenodo.org/badge/403880543.svg)](https://zenodo.org/badge/latestdoi/403880543)

# Scripts for simulation report on risk analysis documents
This repository contains scripts to run simulation studies to evaluate the performance of Active Learning to conduct systematic reviews for the Royal Dutch Pharmacists Association (Koninklijke Nederlandse Maatschappij ter bevordering der Pharmacie, KNMP in Dutch). Here, systematic reviews are conducted for the development of risk analysis documents for the Medicine Information Centre (Geneesmiddel Informatie Centrum, GIC in Dutch). This study was conducted as commissioned by and in cooperation with the KNMP.

The project aims to answer three research questions:

1. How much time can be saved by using active learning on different risk analysis documents of the GIC?
2. Is it useful to include the relevant papers from the previous studies as prior information in ASReview when updating an existing risk analysis document?
3. Is it useful to include relevant papers from a previous risk analysis document within a certain group of medication as prior information in ASReview for a different risk analysis document from the same group?

## Installation

The scripts in this repository require Python 3.6+. Install the extra dependencies with (in the command line):

```
pip install -r requirements.txt
```

## Datasets

The raw and pre-processed data can be obtained via the Open Science Framework ([OSF](https://osf.io/f7mev)) and contains 3 existing risk analysis documents from the KNMP. The following files should be obtained from OSF and put in a folder using the following construction `simulations/data/output` :

```
asreview_result_clopidogrel-search1-recoded.csv
asreview_result_clopidogrel-search2-recoded.csv
clopidogrel.csv
clopidogrel_sim2.csv
fentanyl.csv
morfine.csv
morfine_1turns0.csv
morfine_fentanyl.csv
morfine_fentanyl_morfine_removed.csv
morfine_fentanyl_morfine_removed_no150.csv
morfine_fentanyl_no150.csv
morfine_minus1.csv
```

Most important columns within each file:

```
title
abstract
final_included
```

Information about the pre-processing of these datasets can be found on the OSF page.

## Descriptive statistics

Descriptive statistics on the three original datasets are automatically obtained when generating the report.

## Simulation

The simulation was conducted for each dataset with 20 iterations, using 1 randomly chosen relevant and one randomly chosen irrelevant records.
After obtaining the pre-processed data and running `pip install requirements.txt`, one can obtain the result of the simulations by running:

```
sh run_all.sh
```

The results will be stored in a folder `simulations/ouput`. Simulating a systematic review produces files that are very large in size. GitHub does not allow for repositories of this size, therefore the raw datafiles of the simulation are stored on [OSF in another project](https://osf.io/kcqhz/), instead.

## Licence

The content in this repository is published under the MIT license.

## Contact

For any questions or remarks, please send an email to asreview@uu.nl.
",2022-08-12
https://github.com/asreview/paper-simulation-smid-2020-mbr,"This repository contains scripts to reproduce the results from the study:

Gerbrich Ferdinands (2020) AI-Assisted Systematic Reviewing: Selecting Studies to Compare Bayesian Versus Frequentist SEM for Small Sample Sizes, Multivariate Behavioral Research, DOI: [10.1080/00273171.2020.1853501](https://doi.org/10.1080/00273171.2020.1853501)

The persistent version of these scripts, the data, and results can be found at the Open Science Framework:

Ferdinands, G. (2021). Supplementary material for publication in Multivariate Behavioural Research (Ferdinands, 2020). Retrieved from [osf.io/re3cd](https://osf.io/re3cd/)

# Simulation study
To reproduce the results presented in this study, run the following in your
terminal:

```
# install required software (such as ASReview)
pip install requirements.txt

# run all simulations
./simulation_scripts.sh

# get statistics
asreview stat simoutput/nb
asreview stat simoutput/logistic

# create figure
python plots.py
```

# License
This repository has an MIT license.
",2022-08-12
https://github.com/asreview/semantic-clusters,"# ASReview Semantic Clustering
This repository contains the Semantic Clustering plugin for
[ASReview](https://github.com/asreview/asreview). It applies multiple techniques
(SciBert, PCA, T-SNE, KMeans, a custom Cluster Optimizer) to an [ASReview data
object](https://asreview.readthedocs.io/en/latest/API/generated/asreview.data.ASReviewData.html#asreview.data.ASReviewData),
in order to cluster records based on semantic differences. The end result is an
interactive dashboard:

![Alt Text](/docs/cord19_semantic_clusters.gif)


## Installation

The packaged is called `semantic_clustering` and can be installed from the
download folder with:

```shell
pip install .
```
or from the command line directly with:

```shell
python -m pip install git+https://github.com/asreview/semantic-clusters.git
```

### Commands

For help use:

```shell
asreview semantic_clustering -h
asreview semantic_clustering --help
```

Other options are:

```shell
asreview semantic_clustering -f <input> -o <output.csv>
asreview semantic_clustering --filepath <input> --output <output.csv>
```

```shell
asreview semantic_clustering -a <output.csv>
asreview semantic_clustering --app <output.csv>
```

```shell
asreview semantic_clustering -v
asreview semantic_clustering --version
```

```shell
asreview semantic_clustering --transformer
```


## Usage
The functionality of the semantic clustering extension is implemented in a
[subcommand
extension](https://asreview.readthedocs.io/en/latest/API/extension_dev.html#subcommand-extensions).
The following commands can be run:

### Processing
In the processing phase, a dataset is processed and clustered for use in the
interactive interface. The following options are available:

```shell
asreview semantic_clustering -f <input.csv or url> -o <output_file.csv>
```

Using `-f` will process a file and store the results in the file specified in
`-o`. 

Semantic_clustering uses an [`ASReviewData`
object](https://asreview.readthedocs.io/en/latest/API/generated/asreview.data.ASReviewData.html#asreview.data.ASReviewData),
and can handle files, urls and benchmark sets:

```shell
asreview semantic_clustering -f benchmark:van_de_schoot_2017 -o output.csv
asreview semantic_clustering -f van_de_Schoot_2017.csv -o output.csv
```

If an output file is not specified, `output.csv` is used as output file name.

### Transformer
Semantic Clustering uses the
[`allenai/scibert_scivocab_uncased`](https://github.com/allenai/scibert)
transformer model as default setting. Using the `--transformer <model>` option,
another model can be selected for use instead:

```shell
asreview semantic_clustering -f benchmark:van_de_schoot_2017 -o <output_file.csv> --transformer bert-base-uncased
```

Any pretrained model will work.
[Here](https://huggingface.co/transformers/pretrained_models.html) is an example
of models, but more exist.

### Dashboard
Running the dashboard server is also done from the command line. This command
will start a [Dash](https://plotly.com/dash/) server in the console and
visualize the processed file.

```shell
asreview semantic_clustering -a output.csv
asreview semantic_clustering --app output.csv
```

When the server has been started with the command above, it can be found at
[`http://127.0.0.1:8050/`](http://127.0.0.1:8050/) in your browser.

## License

MIT license

## Contact
Got ideas for improvement? For any questions or remarks, please send an email to
[asreview@uu.nl](mailto:asreview@uu.nl).

",2022-08-12
https://github.com/asreview/systematic-review-datasets,"# Systematic Review Datasets

This repository provides an overview of labeled datasets used for Systematic Reviews. The
datasets are available under an open licence and can be used for text mining and machine
learning purposes. This repository contains scripts to collect, preprocess and clean
the systematic review datasets.

## Datasets

The datasets are alphabetically ordered. See [index.csv](index.csv) for all available properties.

<!-- DO NOT EDIT TABLE BELOW, EDIT METADATA INSTEAD -->

<!-- BEGIN TABLE -->

| id                                                                                                                                                                               | topic                      |   n_papers |   n_included | license                                                                                      |
|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------|-----------:|-------------:|:---------------------------------------------------------------------------------------------|
| [Appenzeller-Herzog_2020](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Appenzeller-Herzog_2020/output/Appenzeller-Herzog_2020.csv)      | Wilson disease             |       3453 |           29 | [CC-BY Attribution 4.0 International](http://doi.org/10.5281/zenodo.3625931)                 |
| [Bannach-Brown_2019](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Bannach-Brown_2019/output/Bannach-Brown_2019.csv)                     | Animal Model of Depression |       1993 |          280 | [CC-BY Attribution 4.0 International](http://doi.org/10.5281/zenodo.151190)                  |
| [Bos_2018](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Bos_2018/output/Bos_2018.csv)                                                   | Dementia                   |       5746 |           11 | [CC-BY Attribution 4.0 International](https://osf.io/w3kbq/)                                 |
| [Cohen_2006_ACEInhibitors](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/ACEInhibitors.csv)                     | ACEInhibitors              |       2544 |           41 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Cohen_2006_ADHD](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/ADHD.csv)                                       | ADHD                       |        851 |           20 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Cohen_2006_Antihistamines](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/Antihistamines.csv)                   | Antihistamines             |        310 |           16 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Cohen_2006_AtypicalAntipsychotics](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/AtypicalAntipsychotics.csv)   | Atypical Antipsychotics    |       1120 |          146 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Cohen_2006_BetaBlockers](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/BetaBlockers.csv)                       | Beta Blockers              |       2072 |           42 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Cohen_2006_CalciumChannelBlockers](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/CalciumChannelBlockers.csv)   | Calcium Channel Blockers   |       1218 |          100 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Cohen_2006_Estrogens](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/Estrogens.csv)                             | Estrogens                  |        368 |           80 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Cohen_2006_NSAIDS](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/NSAIDS.csv)                                   | NSAIDS                     |        393 |           41 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Cohen_2006_Opiods](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/Opiods.csv)                                   | Opiods                     |       1915 |           15 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Cohen_2006_OralHypoglycemics](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/OralHypoglycemics.csv)             | Oral Hypoglycemics         |        503 |          136 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Cohen_2006_ProtonPumpInhibitors](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/ProtonPumpInhibitors.csv)       | Proton Pump Inhibitors     |       1333 |           51 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Cohen_2006_SkeletalMuscleRelaxants](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/SkeletalMuscleRelaxants.csv) | Skeletal Muscle Relaxants  |       1643 |            9 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Cohen_2006_Statins](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/Statins.csv)                                 | Statins                    |       3465 |           85 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Cohen_2006_Triptans](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/Triptans.csv)                               | Triptans                   |        671 |           24 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Cohen_2006_UrinaryIncontinence](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Cohen_2006/output/online/UrinaryIncontinence.csv)         | Urinary Incontinence       |        327 |           40 | [custom open license](https://dmice.ohsu.edu/cohenaa/systematic-drug-class-review-data.html) |
| [Hall_2012](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Hall_Wahono_Radjenovic_Kitchenham/output/Hall_2012.csv)                        | Software Fault Prediction  |       8911 |          104 | [CC-BY Attribution 4.0 International](http://doi.org/10.5281/zenodo.1162952)                 |
| [Kitchenham_2010](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Hall_Wahono_Radjenovic_Kitchenham/output/Kitchenham_2010.csv)            | Software Engineering       |       1704 |           45 | [CC-BY Attribution 4.0 International](http://doi.org/10.5281/zenodo.1162952)                 |
| [Kwok_2020](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Kwok_2020/output/Kwok_2020.csv)                                                | Virus Metagenomics         |       2481 |          120 | [CC-BY Attribution 4.0 International](https://doi.org/10.17605/OSF.IO/5S27M)                 |
| [Nagtegaal_2019](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Nagtegaal_2019/output/Nagtegaal_2019.csv)                                 | Nudging                    |       2019 |          101 | [CC0](https://doi.org/10.7910/DVN/WMGPGZ/HY6N2S)                                             |
| [Radjenovic_2013](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Hall_Wahono_Radjenovic_Kitchenham/output/Radjenovic_2013.csv)            | Software Fault Prediction  |       6000 |           48 | [CC-BY Attribution 4.0 International](http://doi.org/10.5281/zenodo.1162952)                 |
| [Wahono_2015](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Hall_Wahono_Radjenovic_Kitchenham/output/Wahono_2015.csv)                    | Software Defect Detection  |       7002 |           62 | [CC-BY Attribution 4.0 International](http://doi.org/10.5281/zenodo.1162952)                 |
| [Wolters_2018](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/Wolters_2018/output/Wolters_2018.csv)                                       | Dementia                   |       5019 |           19 | [CC-BY Attribution 4.0 International](https://osf.io/sxzjg/)                                 |
| [van_Dis_2020](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/van_Dis_2020/output/van_Dis_2020.csv)                                       | Anxiety-Related Disorders  |      10953 |           73 | [CC-BY Attribution 4.0 International](https://osf.io/4d9tu/)                                 |
| [van_de_Schoot_2017](https://raw.githubusercontent.com/asreview/systematic-review-datasets/master/datasets/van_de_Schoot_2017/output/van_de_Schoot_2017.csv)                     | PTSD Trajectories          |       6189 |           43 | [CC-BY Attribution 4.0 International](https://doi.org/10.17605/OSF.IO/VK4BE)                 |

<!-- END TABLE -->


## Publishing your data
For publishing either your data, we recommend using the Open Science frame (OSF). OSF is part of the Center for Open Science (COS), which aims at increasing openness, integrity, and reproducibility of research ([OSF](https://www.cos.io/our-products/osf), 2020). How to share your data using OSF: A [step-by-step guide](https://journals.sagepub.com/doi/pdf/10.1177/2515245918757689).

Another platform to publish your data open access is provided by Zenodo. Zenodo is a platform which encourages scientists to share all materials (including data) that are necessary to understand the scholarly process ([Zenodo](https://about.zenodo.org/), 2020).

When uploading your dataset to OSF or Zenodo, make sure to provide all relevant information about the dataset, by filling out all available fields. The data to be put on Zenodo or OSF can be documented as extensively as you would like (flowcharts, explanation of certain decisions, etc.). This can include a link to the systematic review itself, if it has been published elsewhere.

### License

When sharing your dataset or a link to your already published systematic review, we recommend using a CC-BY or CC0 license for both Zenodo and OSF. By adding a Creative Commons license, everybody from individual creators to large institutions are given a standardized way to allow use of their creative work under copyright law ([Creative Commons](https://creativecommons.org/about/cclicenses/), 2020).

In short, the CC-BY license means that reusers are allowed to distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator. The license allows for commercial use. The CC0 license releases data in the public domain, allowing reuse in any form without any conditions. This can be appropriate when sharing (meta)data only. With both OSF (see step-by-step guide) and Zenodo you can easily add the license to your project after creating a project in either platform.


### File format

The folder `datasets/` has subfolders for the different systematic reviews
datasets. In each of these subfolders, the `.ipynb` script retrieves a dataset from OSF or Zenodo, and preprocesses it by adding customized labels and marking duplicates. The script also reports the inclusion rate, and missing patterns and word clouds of titles and abstracts. After preprocessing, an ASReview-compatible dataset in `.csv` format is generated in the `output/` folder. Extensions .csv, .xlsx, and .xls. CSV files should be comma-separated and UTF-8 encoded. To indicate labeling decisions, one can use ""included"" or ""label_included"". This label should be filled with all 0’s and 1’s, where 0 means that the record is not included and 1 means included.

## License
The scripts in the current project are MIT licensed. The datasets (should) have a permissive license.

## Contact

Contact details can be found at the [ASReview](https://github.com/asreview/asreview#contact)
project page.
",2022-08-12
https://github.com/asreview/template-extension-new-dataset,"# Template for extending ASReview with a new dataset

ASReview has support for extensions, which enable you to seemlessly integrate
your own programs with the ASReview framework. This template can extent ASReview
with new data.

See the section [Extensions](https://asreview.readthedocs.io/en/latest/extensions_dev.html) 
on ReadTheDocs for more information on writing extensions.

## Getting started

Click the `Use this template` button and add/modify the algorithms. Install 
your new dataset with

```bash
pip install .
```

or

```bash
pip install git+https://github.com/{USER_NAME}/{REPO_NAME}.git
```

and replace `{USER_NAME}` and `{REPO_NAME}` by your own details. 


## Usage

The new dataset is defined in
[`asreviewcontrib/dataset_name/your_dataset.py`](asreviewcontrib\dataset_name\your_dataset.py)
and can be used as a new dataset. 

By supplying the `from_config()` method with a
config object, a new DataSet object is created, and integrated to ASReview. See
[asreview.datasets.BaseDataSet.from_config](https://asreview.readthedocs.io/en/latest/API/generated/asreview.datasets.BaseDataSet.html#asreview.datasets.BaseDataSet.from_config)
for more information on this function.

[`setup.py`](setup.py) contains the code needed for integration into ASReview.

[`asreviewcontrib/dataset_name/__init__.py`](asreviewcontrib/dataset_name/__init__.py) 
contains directions for loading the dataset module.

## License

MIT license
",2022-08-12
https://github.com/asreview/template-extension-new-model,"# Template for extending ASReview with new model

ASReview has support for extensions, which enable you to seemlessly integrate
your own programs with the ASReview framework. These extensions can extend the
software with new classifiers, qurey strategies, balance strategies, and feature
extraction techniques. This template can be used to write such an extension
(add a new SKLearn naive Bayes classifier with default settings in this case).

See the section [Extensions](https://asreview.readthedocs.io/en/latest/extensions_dev.html) 
on ReadTheDocs for more information on writing extensions.

## Getting started

Click the `Use this template` button and add/modify the algorithms. Install 
your new classifier with

```bash
pip install .
```

or

```bash
pip install git@github.com:{USER_NAME}/{REPO_NAME}.git
```

and replace `{USER_NAME}` and `{REPO_NAME}` by your own details. 


## Usage

The new classifier `nb_example` is defined in
[`asreviewcontrib/models/nb_default_param.py`](asreviewcontrib/models/nb_default_param.py) 
and can be used in a simulation.

```bash
asreview simulate example_data_file.csv -m nb_example
```

## License

MIT license
",2022-08-12
https://github.com/basm92/applied_microeconometrics,"# applied_microeconometrics

A repository for Applied Microeconometrics (TI2021-2022) assignments. 
",2022-08-12
https://github.com/basm92/basm92,"## Bas Machielsen 👋

Welcome to my [Github Profile](http://www.github.com/basm92)! My name is Bas Machielsen, and I am a PhD student specializing in economic history at Utrecht University (the Netherlands). My favorite (and most-used) programming languages are R, in which I have experience developing some low-key Shiny apps and packages, as well as Python. I also have some elementary knowledge of Stata, HTML, CSS, and bash. In my spare time, I also like to dabble in web design, and I love learning to integrate my web tools using the aforementioned programming languages. 

## I’m currently working on..

Several economics and economic history-related projects, among which is my (future) job market paper! Feel free to take a look at [my website](http://bas-m.netlify.app) if you want to know more.

",2022-08-12
https://github.com/basm92/Big-Data-Big-Questions,"## Big Data, Big Questions

Repository for my course materials used in the working groups for the course Big Data, Big Questions at Utrecht University. 

- Includes data sources, lecture slides, and other resources
- The lecture slides can be found [here](https://github.com/basm92/Big-Data-Big-Questions/tree/master/lecture_slides/lecture_slides.pdf)

",2022-08-12
https://github.com/basm92/Clio,"Clio
================

## Introduction

Clio is an R package that serves as a bridge between the user and the
website [Clio Infra](www.clio-infra.eu), a repository containing
publicly available data on various aspects of economic history at the
country level. This package is designed to quickly and efficiently
extract data, allowing the user to make large queries. This way, the
user is not confined to manually downloading Excel files, and the
associated filtering and merging process. The package also facilitates
transparent and reproducible data collection, and it is ‘typo robust’ to
a certain extent, to prevent annoyance.

## Functions

The package is designed to be very easy to use and contains only four
(key) functions:

  - clio\_overview()

This function provides an overview of all available variables on
clio-infra. It has no arguments, and returns a dataframe with all
variables, their title, and availability (from and to). The function’s
goal is to aid the user in browsing the variables, without having to go
back and forth to the website.

  - clio\_overview\_cat()

This function allows the user to browse through various categories of
data available. It is meant to be used in the following way: first, call
it without argument. That gives you a vector of currently available
categories of data. Secondly, call it again with a character vector of
one of the categories as an argument:

``` r
clio_overview_cat(""production"")
```

This will give the user an overview of variables *within* a certain
category. Note that the function does not support looking at two
categories at the same time. The `_get` equivalent of this function, the
function that actually provides the data to the user, does, however.

  - clio\_get()

`clio_get` returns a data frame on the basis of a couple of arguments: -
variables: You can select variables with the help of clio\_overview() or
clio\_overview\_cat(). You can enter multiple variables using
`c(var1,var2)`. - countries: You can enter one, or multiple countries,
also using `c(country1, country2)`. Defaults to all countries available
in the query. - from, to: condition the dataset on a certain time
period. If ineffective, no warning will be given. - list = FALSE: create
a list instead of a merged data.frame. - mergetype = inner\_join. Select
a merge type, `inner_join, outer_join, left_join, right_join`. Ignored
in case of `list = TRUE`.

  - clio\_get\_cat()

`clio_get_cat` accepts *categories* as inputs, as well as combinations
of categories using `c(cat1, cat2)`. All other arguments are passed on
to `clio_get`.

## Demonstration

``` r
head(clio_overview(),10)
```

    ##            variable_name from   to  obs
    ## 1      Cattle per Capita 1500 2010 7456
    ## 2    Cropland per Capita 1500 2010 6226
    ## 3       Goats per Capita 1500 2010 7037
    ## 4     Pasture per Capita 1500 2010 5963
    ## 5        Pigs per Capita 1500 2010 6841
    ## 6       Sheep per Capita 1500 2010 6835
    ## 7           Total Cattle 1500 2010 7457
    ## 8         Total Cropland 1500 2010 6191
    ## 9  Total Number of Goats 1500 2010 7037
    ## 10  Total Number of Pigs 1500 2010 6841

All categories of data are shown below:

``` r
clio_overview_cat()
```

    ##  [1] ""Agriculture""       ""Demography""        ""Environment""      
    ##  [4] ""Finance""           ""Gender Equality""   ""Human Capital""    
    ##  [7] ""Institutions""      ""Labour Relations""  ""National Accounts""
    ## [10] ""Prices and Wages""  ""Production""

Browse through the database by feeding arguments to
`clio_overview_cat()`. It is
    typo-robust.

``` r
clio_overview_cat(""Finanze"")
```

    ##                                                 variable_name from   to   obs
    ## 1                                  Exchange Rates to UK Pound 1500 2013 15572
    ## 2                                 Exchange Rates to US Dollar 1500 2013 11765
    ## 3                                               Gold Standard 1800 2010 14359
    ## 4                            Long-Term Government  Bond Yield 1727 2011  2849
    ## 5 Total Gross Central Government  Debt as a Percentage of GDP 1692 2010  7134

``` r
clio_overview_cat(""prices end weeges"")
```

    ##         variable_name from   to   obs
    ## 1   Income Inequality 1820 2000   866
    ## 2           Inflation 1500 2010 16676
    ## 3 Labourers Real Wage 1820 2008  5053

``` r
clio_get(c(""income inequality"", ""labouresrs real wage""))
```

    ## Joining, by = c(""ccode"", ""country.name"", ""year"")

    ## # A tibble: 5,702 x 5
    ##    ccode country.name  year `Income Inequality` `Labourers Real Wage`
    ##    <dbl> <chr>        <dbl>               <dbl>                 <dbl>
    ##  1    24 Angola        1820                48.9                 NA   
    ##  2    32 Argentina     1820                47.1                 NA   
    ##  3    40 Austria       1820                53.4                 NA   
    ##  4    56 Belgium       1820                62.4                 16.6 
    ##  5   204 Benin         1820                48.0                 NA   
    ##  6    76 Brazil        1820                47.1                 NA   
    ##  7   120 Cameroon      1820                56.2                 NA   
    ##  8   124 Canada        1820                45.1                 NA   
    ##  9   152 Chile         1820                47.1                 NA   
    ## 10   156 China         1820                44.9                  3.71
    ## # … with 5,692 more rows

``` r
clio_get(c(""infant mortality"", ""zinc production""))
```

    ## Joining, by = c(""ccode"", ""country.name"", ""year"")

    ## # A tibble: 14,570 x 5
    ##    ccode country.name    year `Infant Mortality` `Zinc Production`
    ##    <dbl> <chr>          <dbl>              <dbl>             <dbl>
    ##  1   191 Croatia         1810               175                 NA
    ##  2   246 Finland         1810               200.                 0
    ##  3   826 United Kingdom  1810               141                  0
    ##  4    40 Austria         1820               188.                 0
    ##  5   191 Croatia         1820               150                 NA
    ##  6   246 Finland         1820               198.                 0
    ##  7   250 France          1820               182                  0
    ##  8   528 Netherlands     1820               179                 NA
    ##  9   826 United Kingdom  1820               153                  0
    ## 10    40 Austria         1830               251.                 0
    ## # … with 14,560 more rows

``` r
clio_get(c(""biodiversity - naturalness"", ""xecutive Constraints  (XCONST)""), 
         from = 1850, to = 1900, 
         countries = c(""Armenia"", ""Azerbaijan""))
```

    ## Joining, by = c(""ccode"", ""country.name"", ""year"")

    ## # A tibble: 12 x 5
    ##    ccode country.name  year `Biodiversity - natural… `Executive Constraints  (X…
    ##    <dbl> <chr>        <dbl>                    <dbl>                       <dbl>
    ##  1    51 Armenia       1850                    0.903                          NA
    ##  2    31 Azerbaijan    1850                    0.908                          NA
    ##  3    51 Armenia       1860                    0.899                          NA
    ##  4    31 Azerbaijan    1860                    0.900                          NA
    ##  5    51 Armenia       1870                    0.896                          NA
    ##  6    31 Azerbaijan    1870                    0.892                          NA
    ##  7    51 Armenia       1880                    0.892                          NA
    ##  8    31 Azerbaijan    1880                    0.883                          NA
    ##  9    51 Armenia       1890                    0.888                          NA
    ## 10    31 Azerbaijan    1890                    0.873                          NA
    ## 11    51 Armenia       1900                    0.884                          NA
    ## 12    31 Azerbaijan    1900                    0.863                          NA

``` r
clio_get(c(""Zinc production"", ""Gold production""), 
         from = 1800, to = 1920, 
         countries = c(""Botswana"", ""Zimbabwe"", 
                       mergetype = inner_join))
```

    ## Joining, by = c(""ccode"", ""country.name"", ""year"")

    ## # A tibble: 242 x 5
    ##    ccode country.name  year `Zinc Production` `Gold Production`
    ##    <dbl> <chr>        <dbl>             <dbl>             <dbl>
    ##  1    72 Botswana      1800                NA                 0
    ##  2   716 Zimbabwe      1800                NA                 0
    ##  3    72 Botswana      1801                NA                 0
    ##  4   716 Zimbabwe      1801                NA                 0
    ##  5    72 Botswana      1802                NA                 0
    ##  6   716 Zimbabwe      1802                NA                 0
    ##  7    72 Botswana      1803                NA                 0
    ##  8   716 Zimbabwe      1803                NA                 0
    ##  9    72 Botswana      1804                NA                 0
    ## 10   716 Zimbabwe      1804                NA                 0
    ## # … with 232 more rows

``` r
clio_get(c(""Armed conflicts internal"", ""Gold production"", ""Armed conflicts international""), 
         mergetype = inner_join)
```

    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")

    ## # A tibble: 34,559 x 6
    ##    ccode country.name  year `Armed conflicts… `Gold Productio… `Armed Conflicts…
    ##    <dbl> <chr>        <dbl>             <dbl>            <dbl>             <dbl>
    ##  1    12 Algeria       1681                 0              0                   1
    ##  2    24 Angola        1681                 0              0                   1
    ##  3    32 Argentina     1681                 0              0                   0
    ##  4    51 Armenia       1681                 0              0                   0
    ##  5    36 Australia     1681                 0              0                   0
    ##  6    40 Austria       1681                 0              0                   0
    ##  7    31 Azerbaijan    1681                 0              0                   0
    ##  8    68 Bolivia       1681                 0              0                   0
    ##  9    72 Botswana      1681                 0              0                   0
    ## 10    76 Brazil        1681                 0              1.5                 0
    ## # … with 34,549 more rows

The kind of merge is customizable . The argument name is \(mergetype\).
And it takes the values full\_join (default), left\_join, inner\_join,
outer\_join, etc. if you have loaded `dplyr`.

``` r
clio_get_cat(""finanz"", list = F, from = 1800, to = 1900)
```

    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")

    ## # A tibble: 7,074 x 8
    ##    ccode country.name  year `Exchange Rates… `Exchange Rates… `Gold Standard`
    ##    <dbl> <chr>        <dbl>            <dbl>            <dbl>           <dbl>
    ##  1    40 Austria       1800            10.1             NA                  0
    ##  2   124 Canada        1800             1.08            NA                  0
    ##  3   156 China         1800             3.64            NA                  0
    ##  4   208 Denmark       1800             5.09            NA                  0
    ##  5   276 Germany       1800            11.9              3.00               0
    ##  6   372 Ireland       1800             1.11            NA                  0
    ##  7   380 Italy         1800             5.24            NA                  0
    ##  8   428 Latvia        1800             3.78            NA                 NA
    ##  9   528 Netherlands   1800            11.3              2.61               0
    ## 10   616 Poland        1800            21.3             NA                  0
    ## # … with 7,064 more rows, and 2 more variables: `Long-Term Government Bond
    ## #   Yield` <dbl>, `Total Gross Central Government Debt as a Percentage of
    ## #   GDP` <dbl>

``` r
clio_overview_cat()
```

    ##  [1] ""Agriculture""       ""Demography""        ""Environment""      
    ##  [4] ""Finance""           ""Gender Equality""   ""Human Capital""    
    ##  [7] ""Institutions""      ""Labour Relations""  ""National Accounts""
    ## [10] ""Prices and Wages""  ""Production""

``` r
clio_get_cat(c(""agriculture"", ""environment""),
             countries = ""Netherlands"",
             mergetype = inner_join, from = 1850, to = 1900)
```

    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")

    ## # A tibble: 6 x 20
    ##   ccode country.name  year `Cattle per Cap… `Cropland per C… `Goats per Capi…
    ##   <dbl> <chr>        <dbl>            <dbl>            <dbl>            <dbl>
    ## 1   528 Netherlands   1850            0.397            0.235          0.00352
    ## 2   528 Netherlands   1860            0.386            0.229          0.00352
    ## 3   528 Netherlands   1870            0.389            0.223          0.00352
    ## 4   528 Netherlands   1880            0.362            0.217          0.00352
    ## 5   528 Netherlands   1890            0.335            0.211          0.00352
    ## 6   528 Netherlands   1900            0.320            0.204          0.00352
    ## # … with 14 more variables: `Pasture per Capita` <dbl>, `Pigs per
    ## #   Capita` <dbl>, `Sheep per Capita` <dbl>, `Total Cattle` <dbl>, `Total
    ## #   Cropland` <dbl>, `Total Number of Goats` <dbl>, `Total Number of
    ## #   Pigs` <dbl>, `Total Number of Sheep` <dbl>, `Total Pasture` <dbl>,
    ## #   `Biodiversity - naturalness` <dbl>, `CO2 Emissions per Capita` <dbl>, `SO2
    ## #   Emissions per Capita` <dbl>, `Total CO2 Emissions` <dbl>, `Total SO2
    ## #   Emissions` <dbl>

``` r
clio_get_cat(""Produzioni"", from = 1700, list = F, mergetype = inner_join)
```

    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")

    ## # A tibble: 1,197 x 15
    ##    ccode country.name  year `Aluminium Prod… `Bauxite Produc… `Copper Product…
    ##    <dbl> <chr>        <dbl>            <dbl>            <dbl>            <dbl>
    ##  1    36 Australia     1880                0                0              0  
    ##  2    76 Brazil        1880                0                0              0  
    ##  3   156 China         1880                0                0              0  
    ##  4   356 India         1880                0                0              0  
    ##  5   364 Iran          1880                0                0              0  
    ##  6   398 Kazakhstan    1880                0                0              0  
    ##  7   643 Russia        1880                0                0              3.2
    ##  8   724 Spain         1880                0                0             36.6
    ##  9   840 United Stat…  1880                0                0             27  
    ## 10    36 Australia     1881                0                0              0  
    ## # … with 1,187 more rows, and 9 more variables: `Gold Production` <dbl>, `Iron
    ## #   Ore Production` <dbl>, `Lead Production` <dbl>, `Manganese
    ## #   Production` <dbl>, `Nickel Production` <dbl>, `Silver Production` <dbl>,
    ## #   `Tin Production` <dbl>, `Tungsten Production` <dbl>, `Zinc
    ## #   Production` <dbl>

``` r
clio_get(c(""Tin Production"", ""income inequality""), from = 1800, countries = c(""Netherlands"", ""Russia""))
```

    ## Joining, by = c(""ccode"", ""country.name"", ""year"")

    ## # A tibble: 225 x 5
    ##    ccode country.name  year `Tin Production` `Income Inequality`
    ##    <dbl> <chr>        <dbl>            <dbl>               <dbl>
    ##  1   643 Russia        1800                0                  NA
    ##  2   643 Russia        1801                0                  NA
    ##  3   643 Russia        1802                0                  NA
    ##  4   643 Russia        1803                0                  NA
    ##  5   643 Russia        1804                0                  NA
    ##  6   643 Russia        1805                0                  NA
    ##  7   643 Russia        1806                0                  NA
    ##  8   643 Russia        1807                0                  NA
    ##  9   643 Russia        1808                0                  NA
    ## 10   643 Russia        1809                0                  NA
    ## # … with 215 more rows

``` r
clio_get_cat(""labor relation"")
```

    ## Joining, by = c(""ccode"", ""country.name"", ""year"")
    ## Joining, by = c(""ccode"", ""country.name"", ""year"")

    ## # A tibble: 4,951 x 6
    ##    ccode country.name  year `Number of Days … `Number of Labou… `Number of Work…
    ##    <dbl> <chr>        <dbl>             <dbl>             <dbl>            <dbl>
    ##  1    32 Argentina     1927            363492                56            26888
    ##  2    36 Australia     1927           1713581               441           200757
    ##  3    40 Austria       1927            686560               216            35300
    ##  4    56 Belgium       1927           1658836               186            45071
    ##  5   100 Bulgaria      1927             57196                23             2919
    ##  6   124 Canada        1927            152570                74            22299
    ##  7   156 China         1927           7622029               117           881289
    ##  8   208 Denmark       1927            119000                17             2851
    ##  9   233 Estonia       1927              3067                 5              218
    ## 10   246 Finland       1927           1528182                79            13368
    ## # … with 4,941 more rows

Thank you for reading\! Questions and remarks: Github or
[e-mail](mailto:a.h.machielsen@uu.nl). If you have any improvements,
feel free to submit a PR. In case of issues: Feel free to [start a
thread](https://github.com/basm92/Clio/issues) or point them out
otherwise.
",2022-08-12
https://github.com/basm92/deep_learning,"## Deep Learning

Summer School TI - BDS (July 2022)",2022-08-12
https://github.com/basm92/econometrics2,"# Econometrics II 

This is a repository for all assignments for the Econometrics II Course (TI, 2020-2021). ",2022-08-12
https://github.com/basm92/econometrics_iii,"## Econometrics III

This is a repository for Econometrics III (TI 2020-2021), containing the assignments + code. ",2022-08-12
https://github.com/basm92/Getting-and-Cleaning-Data,"## Course Project

This is a readme file for my course project for 'Getting and Cleaning Data' on Coursera. This repository contains all unzipped files in the appropriate folders, and the scripts necessary to do the analysis and extract the tidy data file. 

### Downloading the data and executing the script

First, we will download the necessary data on your local pc. 

Run the following chunk of code:

```{r}
download.file(""https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"", destfile = ""data.zip"")
``` 

And unzip the appropriate file. Then, set your working directory to:

```{r}
setwd(""./UCI HAR Dataset"")
```

Download the `run_analysis.R` file and paste it into the working directory folder. 
Load and execute the script by the following chunk of code: 

```{r}
source(""run_analysis.R"")
``` 


### Downloading the tidy data set

Use data <- read.table(""data_set_with_the_averages.txt"") to read the data. It is 180x68 because there are 30 subjects and 6 activities, thus ""for each activity and each subject"" means 30 * 6 = 180 rows. Note that the provided R script has no assumptions on numbers of records, only on locations of files.

### Codebook
For a detailed description and explanation of the code, see the file CodeBook.md.

Thank you for reading my project
",2022-08-12
https://github.com/basm92/hdng,"---
title: ""README""
author: ""Bas Machielsen""
date: ""5/3/2020""
output: 
  html_document: 
    keep_md: yes
---



## Introduction

`hdng` is a package facilitating the use of the [HDNG Database](https://datasets.iisg.amsterdam/dataverse/HDNG?q=&types=files&sort=dateSort&order=desc&page=1), a database containing quantative information in various aspects of Dutch municipalities from about 1800 to the 1980's. This package aids the user in finding their way through the database, allowing them to search for wanted variables, browse within categories, and finally, make a query to extract the wanted data. Below is a short demonstration of the package's functionality. The package consists of three (basic) functions:

  - `hdng_names_cats`: allows the user to browse through categories. An empty function call gives the basic categories, and a function call containing one or more specific categories returns a dataframe with all variables, and times at which they are available. Fully customizable in various aspects (see function documentation).
  
  - `closest_matches`: gives the user the closest matches (according to string distance) of one or more particular searches. This allows the user to quickly find variables of interest without having to browse through categories. Also supports 'within-category' search. The particular kind of string distance can be customized. 
  
  - `hdng_data_get`: function allowing the user to extract the data. It takes as input variable codes, and returns a data.frame as output containing the variables. Many options customizable. 

## Demonstration

The package can be installed and loaded via:


```r
devtools::install_github(""basm92/hdng"")
```


```r
library(hdng)
```

Depending on your machine, the installation might take a while because the data is lazy loading (it is a lot of data).

## Search for your variables

Generally, you can use `hdng_names_cats()` to look for specific data. An empty call gives you all available categories, wheras a call with one of the categories gives you a data.frame containing the availability of all variables within the time frame you specified:


```r
hdng_names_cats()
```

```
##  [1] ""Beroepen""      ""Bedrijvigheid"" ""Godsdienst""    ""District""     
##  [5] ""Bevolking""     ""Politiek""      ""Onderwijs""     ""Welvaart""     
##  [9] ""Voorzieningen"" ""Woningen""      ""Openheid""
```


```r
hdng_names_cats(""Beroepen"") %>%
  head()
```

```
## # A tibble: 6 x 12
##   descr main.cat meaning var   `1889` `1899` `1930` `1933` `1935` `1947` `1960`
##   <chr> <chr>    <chr>   <chr>  <int>  <int>  <int>  <int>  <int>  <int>  <int>
## 1 Aard… a        Beroep… aaa1       1      1      1      0      0      0      0
## 2 Aard… a        Beroep… aab1       1      1      1      0      0      0      0
## 3 Aard… a        Beroep… aac1       1      1      1      0      0      0      0
## 4 Aard… a        Beroep… aad3       1      0      0      0      0      0      0
## 5 Aard… a        Beroep… aax1       1      1      1      0      0      0      0
## 6 Aard… a        Beroep… aax2       1      1      1      0      0      0      0
## # … with 1 more variable: `1971` <int>
```

```r
hdng_names_cats(""Beroepen"", from = 1870 ,to = 1899) %>%
  head()
```

```
## # A tibble: 6 x 6
##   descr                   main.cat meaning  var   `1889` `1899`
##   <chr>                   <chr>    <chr>    <chr>  <int>  <int>
## 1 Aardewerk enz. Pos.A, M a        Beroepen aaa1       1      1
## 2 Aardewerk enz. Pos.B, M a        Beroepen aab1       1      1
## 3 Aardewerk enz. Pos.C, M a        Beroepen aac1       1      1
## 4 Aardewerk enz. Pos.D    a        Beroepen aad3       1      0
## 5 Aardewerk enz., M       a        Beroepen aax1       1      1
## 6 Aardewerk enz., V       a        Beroepen aax2       1      1
```

You can also apply a 'less strict' filter to the data. By default, the query filters the data to variables that are available at least once in the indicated period. You can override this option by specifying `show.only.available = FALSE`.


```r
hdng_names_cats(""Welvaart"", from = 1880, to = 1940) 
```

```
## # A tibble: 12 x 8
##    descr                      main.cat meaning var   `1889` `1933` `1934` `1935`
##    <chr>                      <chr>    <chr>   <chr>  <int>  <int>  <int>  <int>
##  1 Aantal aansluitingen elec… h        Welvaa… aael       0      0      1      0
##  2 Aantal aansluitingen gas   h        Welvaa… aaga       0      0      1      0
##  3 Aantal aansluitingen tele… h        Welvaa… aate       0      0      0      1
##  4 Aantal aangemelde radio o… h        Welvaa… aaro       0      0      0      1
##  5 Aantal op distributiecent… h        Welvaa… adar       0      0      0      1
##  6 Aantal personenautos       h        Welvaa… aper       0      0      0      1
##  7 Aantal vrachtautos         h        Welvaa… avra       0      0      0      1
##  8 Aantal motorbussen         h        Welvaa… amot       0      0      0      1
##  9 Aantal motorrijtuigen op … h        Welvaa… amtm       0      0      0      1
## 10 Aandeel der gemeente in s… h        Welvaa… agpb       1      0      0      0
## 11 % aangeslagenen Rijksinko… h        Welvaa… prbd       0      1      0      0
## 12 % aangesl. Vermogensbel. … h        Welvaa… pvar       0      0      1      0
```

```r
hdng_names_cats(""Welvaart"", from = 1880, to = 1940, show.only.available = F) 
```

```
## # A tibble: 23 x 65
##    descr main.cat meaning var   `1880` `1881` `1882` `1883` `1884` `1885` `1886`
##    <chr> <chr>    <chr>   <chr>  <int>  <int>  <int>  <int>  <int>  <int>  <int>
##  1 Aant… h        Welvaa… aael       0      0      0      0      0      0      0
##  2 Aant… h        Welvaa… aaga       0      0      0      0      0      0      0
##  3 Aant… h        Welvaa… aate       0      0      0      0      0      0      0
##  4 Aant… h        Welvaa… aaro       0      0      0      0      0      0      0
##  5 Aant… h        Welvaa… adar       0      0      0      0      0      0      0
##  6 Aant… h        Welvaa… aper       0      0      0      0      0      0      0
##  7 Aant… h        Welvaa… avra       0      0      0      0      0      0      0
##  8 Aant… h        Welvaa… amot       0      0      0      0      0      0      0
##  9 Aant… h        Welvaa… amtm       0      0      0      0      0      0      0
## 10 Aand… h        Welvaa… agpb       0      0      0      0      0      0      0
## # … with 13 more rows, and 54 more variables: `1887` <int>, `1888` <int>,
## #   `1889` <int>, `1890` <int>, `1891` <int>, `1892` <int>, `1893` <int>,
## #   `1894` <int>, `1895` <int>, `1896` <int>, `1897` <int>, `1898` <int>,
## #   `1899` <int>, `1900` <int>, `1901` <int>, `1902` <int>, `1903` <int>,
## #   `1904` <int>, `1905` <int>, `1906` <int>, `1907` <int>, `1908` <int>,
## #   `1909` <int>, `1910` <int>, `1911` <int>, `1912` <int>, `1913` <int>,
## #   `1914` <int>, `1915` <int>, `1916` <int>, `1917` <int>, `1918` <int>,
## #   `1919` <int>, `1920` <int>, `1921` <int>, `1922` <int>, `1923` <int>,
## #   `1924` <int>, `1925` <int>, `1926` <int>, `1927` <int>, `1928` <int>,
## #   `1929` <int>, `1930` <int>, `1931` <int>, `1932` <int>, `1933` <int>,
## #   `1934` <int>, `1935` <int>, `1936` <int>, `1937` <int>, `1938` <int>,
## #   `1939` <int>, `1940` <int>
```


You can also search for a specific term using the function `closest_matches`. By default, it returns variable names. If you want to see names, set `variable.names` to `TRUE`. 


```r
closest_matches(""aantal autos"", ""Welvaart"", n = 20)
```

```
##  [1] ""avra"" ""aper"" ""amot"" ""ahcg"" ""aaga"" ""ahbk"" ""ahwg"" ""aate"" ""aael"" ""tahe""
## [11] ""zofk"" ""aaro"" ""amtm"" ""ahvg"" ""adar"" ""gisg"" ""falw"" ""agpb"" ""agpb"" ""hamt""
```

```r
closest_matches(""aantal autos"", ""Welvaart"", variable.names = TRUE)
```

```
##  [1] ""Aantal vrachtautos""                 ""Aantal personenautos""              
##  [3] ""Aantal motorbussen""                 ""Aantal ha. cultuurgrond""           
##  [5] ""Aantal aansluitingen gas""           ""Aantal ha. bebouwde kom""           
##  [7] ""Aantal ha. woeste grond""            ""Aantal aansluitingen telefoon""     
##  [9] ""Aantal aansluitingen electriciteit"" ""Totaal aantal ha. (excl. kwelders)""
```
## Execute the query

Finally, you enter a number of variable names into `hdng_get_data` to retrieve a data.frame consisting of the actual variables. Please note that variable names are case-sensitive. 



```r
#Example 1
variables <- closest_matches(""aantal autos"", ""Welvaart"", n = 3)

hdng_get_data(variables, gemeenten = c(""Amsterdam"", ""Rotterdam"",
                                       ""Utrecht"", ""'s Gravenhage""),
              col.names = TRUE)
```

```
## # A tibble: 4 x 8
##   cbsnr naam  acode main.cat  year `Aantal vrachta… `Aantal persone…
##   <dbl> <chr> <dbl> <chr>    <dbl>            <dbl>            <dbl>
## 1   344 UTRE… 10722 h         1935             1108             1892
## 2   363 AMST… 11150 h         1935             3885             9709
## 3   518 'S G… 11434 h         1935             2563             6973
## 4   599 ROTT… 10345 h         1935             2770             4315
## # … with 1 more variable: `Aantal motorbussen` <dbl>
```

```r
#Example 2
hdng_names_cats(""Beroepen"") -> query

query$var[1:10] -> variables2       #Select only 10 variables

hdng_get_data(variables2, gemeenten = c(""Groningen"", ""Leeuwarden"", ""Assen"", ""Zwolle""),
                col.names = TRUE) 
```

```
## # A tibble: 12 x 15
##    cbsnr naam  acode main.cat  year `Aardewerk enz.… `Aardewerk enz.…
##    <dbl> <chr> <dbl> <chr>    <dbl>            <dbl>            <dbl>
##  1    14 GRON… 10426 a         1889               30                1
##  2    14 GRON… 10426 a         1899               11                2
##  3    14 GRON… 10426 a         1930                7                7
##  4    80 LEEU… 11228 a         1889               14                0
##  5    80 LEEU… 11228 a         1899                8                0
##  6    80 LEEU… 11228 a         1930                3                2
##  7   106 ASSEN 10522 a         1889                3                0
##  8   106 ASSEN 10522 a         1899                0                0
##  9   106 ASSEN 10522 a         1930                0                1
## 10   193 ZWOL… 10093 a         1889                7                0
## 11   193 ZWOL… 10093 a         1899                4                1
## 12   193 ZWOL… 10093 a         1930                0                3
## # … with 8 more variables: `Aardewerk enz. Pos.C, M` <dbl>, `Aardewerk enz.
## #   Pos.D` <dbl>, `Aardewerk enz., M` <dbl>, `Aardewerk enz., V` <dbl>,
## #   `Drukkersbedrijven Pos.A, M` <dbl>, `Drukkersbedrijven Pos.B, M` <dbl>,
## #   `Drukkersbedrijven Pos.C, M` <dbl>, `Drukkersbedrijven Pos.D, M` <dbl>
```

## Future work

I plan to include a function that takes as input variable names (or closest matches to them), and as output variable codes, so as not to frustrate the user to always look for variable codes to enter as arguments to `hdng_data_get`. Alternatively, I might start to work on an extract function that takes names, not codes as input. I also want to integrate searches on the basis of province. Let me know if you have any suggestions. Thank you for reading!
",2022-08-12
https://github.com/basm92/macroeconomics_i,"## Macroeconomics I 

Repository containing code + pdf. 

Acronym we use for submission: BBE",2022-08-12
https://github.com/basm92/micro2_gametheory,"# Microeconomics II: Game Theory

In this repository, there are some programming assignments and maybe some pdfs and tex documents.
",2022-08-12
https://github.com/basm92/Miscprojects,"Miscprojects
============

A repository for miscellaneous projects

What projects are in here?
==========================

-   Data tidying in R
-   Regression tables in R
-   Getting Data from CRSP Guide
-   BSc Data Guide - Political Connections
-   Some data treatment for Paul’s paper
-   A manual event study guide in R [Complete]
-   Excel course slides
-   Working in Markdown demonstration

What to do with these data?
===========================

-   Free for use and distribution

<html>
<a rel=""license"" href=""http://creativecommons.org/licenses/by-nc-sa/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"" /></a><br />This work is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by-nc-sa/4.0/"">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
</html>

",2022-08-12
https://github.com/basm92/Polcon_Indonesia,"# Political Connections in Indonesia

## Effects of company location on Economic Development

- Data

  - Government Data - Economic Development
  - World Values Survey - Trust etc. 
  - 1850 (or whatever) data location characteristics (economic, geographical)
  - Location of Dutch companies (treatment)
  - Financial info Dutch companies

  ​",2022-08-12
https://github.com/basm92/Polrents,"## Welcome

This is a repository for my paper 'Political Rents Under A Changing Electoral Regime', and features Code (R/Python), Raw Data, and some Administration files.",2022-08-12
https://github.com/basm92/replication_packages,"## Replication Packages for the Dissertation ""The Political Elite, Self-Interest and Democratization""

### By Bas Machielsen, Utrecht University

- This repository contains three replication packages for Chapter 2, 3 and 4 of the dissertation, respectively. In each of the Chapters, an additional `README.md` file can be found, explaining in detail how to deal with reproducing tables and figures in each chapter.
	- Since the workflow of each Chapter differs slightly. 
	
- In each repository, aside from the Code used to create the Tables and Figures, there are also several auxiliary files used to shorten the Code.

- To use this, either click on the Green ""Code"" button (upper right), and then on ""Download ZIP"", or alternatively, download and install git, and fork this repository. 

- Run the `replication_packages.Rproj` file to start up the R project. 

- Then, if you want to focus on a specific chapter, run `set_working_directory.R`, which will ask for a prompt and set the working directory appropriately for any of the chapters.
	- Make sure to stay in the same R session. 
	
- Below is a .gif that demonstrates this for one particular table:

![Demonstration of Replication](https://raw.githubusercontent.com/basm92/replication_packages/master/replication_package_demonstration.gif)
	
- If you find any mistakes or typos, please contact me at:
	- a.h.machielsen at uu dot nl
	- basmachielsen at live dot nl
	
- Thank you very much for reading!
	

",2022-08-12
https://github.com/basm92/R_Introduction,"---
title: ""Syllabus""
author: ""Bas Machielsen""
date: ""3/2/2020""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Welcome to the three-part course that will (hopefully) convince all of you to start using R and tilt you over the threshold. This course will introduce you to the basic tenets of tidying and analyzing data in R, and more specifically, will (hopefully) convince you that everything that you previously did in Excel can be done **easier and more efficiently** in R. The syntax of R takes a while to get used to, but after an initial period of frustration, you will quickly find yourself making less and less syntactical mistakes, and programming in R will go smoother and smoother. This course is an attempt of making that period of frustration as short as possible. 

First, we will focus on the fundamentals: installing R/Rstudio, and importing and reading data in various formats in R. This is a relatively simple task, but there are several dangers/nuances which are useful to know beforehand. Afterwards, we focus on argueably the most important task: tidying our data. Data comes in many formats, and it is not always straightforward to get the data in a format which can be recognized by programs/functions that perform statistical or graphical analysis. The process of doing this is called **data tidying**, and R programmers have developed a fantastic set of packages called the **tidyverse** to do this. In this course, we will familiarize ourselves with the most common and most useful tidyverse functions to learn how to tidy just about every format of untidy data. 

There are two specific kinds of variables which might be especially hard to 'tidy': **strings** and **dates**. Fortunately, R has several excellent packages which manage to handle this information nicely (unlike Excel). I will pay special attention to these packages, __lubridate__ and __stringr__. 

After you've mastered how to tidy data in R, we can use several plotting environments to create graphs and other visualizations of our data. In the past, many people used Excel or other programs to generate graphs, which harmed reproducibility (there is no code leading up to a graph), and the graphs are ..not aesthetically pleasing. We will learn how to efficiently program visualizations, and make our **workflow** more efficient by learning how to easily update our figures when, for example, we use additional data, or we want to add an additional variable. 

Finally, we will learn how to perform basic statistical analyses in R, and efficiently report them, for example, in your next presentation at a conference, or in your current paper. 

## Prerequisites

This course requires that you be familiar with the basic tenets of data treatment, and more particularly, data treatment in Microsoft Excel. In particular, I will often use Excel as a benchmark against which I contrast the approach using R and various packages. 

## Course objectives

The objectives of this course are: 

- Learn how to use R for basic data treatment and discard Excel entirely. 

- Learn how to effectively visualize data that meets quality standards required for journal submissions

- Learn how to perform basic statistical analyses in R

- Learn to effectively report all previously mentioned aspects using RMarkdown and/or basic LaTeX 

- Develop an effective workflow, and ensure reproducibility of your work

## Course contents

- Lecture 1a: Installing R, RStudio, Basic R Syntax, Object Classes

- Lecture 1b: Importing & Tidying Data using the __tidyverse__: `pivot_longer` and `pivot_wider`

- Lecture 1c: String Manipulation using __stringr__, dates using __lubridate__, and the **swirl** package for practicing. 

- Lecture 1d: Functions and the apply-functions family using __purrr__

- Lecture 2a: __ggplot2__: the basics

- Lecture 2b: __ggplot2__: customizing your visualizations

- Lecture 3a: Introduction to statistical analyses using the __stargazer__ package

- Lecture 3b: Integrating everything in a document or presentation: __xtable__, __kable__, __kableExtra__, RMarkdown, TeX

- Lecture 4: Finalizing & Contrasting R approaches to Excel approaches

## Reading

None! At the beginning of each session, you can follow along by downloading the RMarkdown files which I have created for each session. This way, you can (i) reproduce everything I do, and (ii) add spontaneous pieces of code to explore variations. Additionally, it will (hopefully) spontaneously teach you some of the syntax of Markdown and/or LaTeX, and encourage you to use those to make your own workflow as efficient as possible. 

## Cheat sheets

Many packages in R have so-called cheat sheets available. As it takes a lot of time to memorize all commands and arguements, it is more logical to have a cheat sheet at arms length that can (i) briefly describe the most common functions of a particular function and (ii) explain which arguments can and should be specified in a function call. In this section, I provide links to a few cheat sheets for the most often-used R packages:

### Languages/Programs

- [RStudio](https://github.com/rstudio/cheatsheets/raw/master/rstudio-ide.pdf)

- [Markdown](https://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf)

- [LaTeX](https://wch.github.io/latexsheet/)

### Data treatment

- [Forcats](https://github.com/rstudio/cheatsheets/raw/master/factors.pdf)

- [Data Import](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf)

- [Dplyr](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf)

- [Stringr](https://github.com/rstudio/cheatsheets/raw/master/strings.pdf)

- [purr](https://github.com/rstudio/cheatsheets/raw/master/purrr.pdf)

- [lubridate](https://github.com/rstudio/cheatsheets/raw/master/lubridate.pdf)

### Data analysis

- [ggplot2](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf)

- [stargazer](https://www.jakeruss.com/cheatsheets/stargazer/)

- [estimatr](https://github.com/rstudio/cheatsheets/raw/master/estimatr.pdf)

## Developments & questions

I plan to extend this course to cover various more advanced aspects of R, most importantly, string-matching to match data from various sources, and web scraping (both client-side, and by using API's) and time series and panel analyses using various estimators. 

Please mail any questions and suggestions to a.h.machielsen@uu.nl! 
",2022-08-12
https://github.com/Bastiaansen/Climate_Feedback_Projections,"# Projections of the Transient State-Dependency of Climate Feedbacks

Codes used to obtain climate feedback contributions and compute climate feedback strengths over time. This repository contains the MATLAB (2018a), Python (3.0) and Jupyter Notebook files used for the research in the paper 'Projections of the Transient State-Dependency of Climate Feedbacks'.

These codes can be used to compute feedback contributions for the CESM2 experiments in CMIP6. Below follow a description of the various folders and codes, seperated by the folder in which they can be found. In general, the Python and Jupyter notebook files are used for computations of the climate feedback contributions and plotting of 2D spatial maps, and MATLAB files are used for regressions and computations of feedback strengths.

To compute climate feedback contributions from GCMs radiative kernels are needed, which are not provided here. For our study, we have used CAM5 kernels by Pendergrass et al (2018) (see also https://doi.org/10.5065/D6F47MT6 and https://doi.org/10.5281/zenodo.997899). Some codes might rely on the presence of additional folders to write the data and figures to; you might have to create these yourself beforehand (as Github does not allow for empty folders).


## Instructions and Explanations of codes used

Explanation is per map.



### 1. Python

For computations of observables from GCM data (2D fields and globally averaged), which gets saved in nc files. These require radiative kernels. For this study, we have used CAM5 kernels by Pendergrass et al (2018) [https://doi.org/10.5065/D6F47MT6
329 and https://doi.org/10.5281/zenodo.997899.] The Jupyter notebooks should be run to create these data files. Most of the actual code is contained in seperate python files.

There are two submaps for the different forcing scenarios:
	* CESM2 abrupt4xCO2
	* CESM2 1pctCO2
	
	
### 2. MATLAB - Fitting

Used to fit previously obtained observable data on feedback contributions, net radiative imbalance and temperature from the abrupt4xCO2 experiment to the response function (a sum of exponential decaying functions) -- see the paper for more details.

Submaps:
1. **global observables fit**

	Used for fits of the globally averaged observables.

	To create the fits, use the script 'compute_feedbacks.m'. This script automatically finds the computed observable files if left in their original folder. The code computes cloud feedback contributions and tries values M=1 to M=5 for the amount of (Koopman) modes, create some statistics of these tests and saves some fits (in the folder 'fit-data'). It also creates plots of the best found fits for different values for M (i.e. the amount of modes). Finally, M = 3 is taken to create the plots in the paper and to compute feedback strengths per mode.

	This script relies on the other MATLAB files in the folder:
	* exponent_fit_function.m: contains the function to be fitted to the data
	* exponent_fit_function_der.m: contains the derivative of the function to be fitted (used for computations of instantaneous feedback strengths)
	* compute_feedbacks_for_fit.m: computes feedback strengths per mode and equilibrium feedback strengths from fitted parameters (contained in the variable x). Also propogates uncertainties.
	* propogate_confidenceintervals_sum: propogation of uncertainties for a sum x_1 + ... + x_n.
	* propogate_confidenceintervals_quotient.m: propogation of uncertainties for fractions p/q.
	* append_plot_with_fits.m: add fitted function to plots of observables (cloud feedback included masking of forcing).
	* append_plot_with_fit_clouds.m: add fitted function to plots of observables (but cloud feedback is seperated from masking of forcing).
		
2. **spatial observables fit**

	For fits of the spatial modes. The requires timescales tau_m obtained from nonlinear regression on globally averaged observables.

	To obtain the spatial fits, the script 'perform_feedback_eigendecomposition.m' should be run. It automatically loads the 2D fields if left in original folder, prepares those files and computes 2D modes from linear regression (coded here via multiplication with Moore-Penrose pseudoinverse matrices). Computed 2D modes are saved in the folder 'Eigenmodes'.

	This script depends on other MATLAB files in the folder:
	* loadData.m: load the 2D fields and prepare them for analysis.
	* combine_Planck_surface_and_atmosphere_fields.m: combines the fields that constitute the Planck feedback.
	* compute_feedback_eigenmodes.m: compute and save 2D modes for generic feedback.
	* compute_IMB_feedback_eigenmodes.m: compute and save 2D modes for radiative imbalance (and initial radiative forcing).
	* compute_cloud_feedback_eigenmodes.m: compute and save 2D modes for cloud feedback (and cloud masking of forcing).
	* save_eigenmodes_var.m: save the 2D modes found to a nc file.
	* save_cloud_masking.m: save the cloud masking found to a nc file.
		
3. **Analysis**

	For computation of (instantaneous) feedback strengths over time in the abrupt4xCO2 and the 1pctCO2 experiments, via the script 'Feedback_dissected.m'. It depends on some scripts in the folder '1. global observables fit' and should be run while in that folder.
		
### 3. Python Visualisations
For visualisations of the 2D modes, estimated equilibrium 2D spatial paterning in the abrupt4xCO2 epxeriment (both via the Jupyter notebook 'make_mode_plots.ipynb') and 2D projections compared to actual data for the 1pctCO2 experiment (via the Jupyter notebook 'compare 1pct_projections.ipynb').

### 4. MATLAB - 1pct projection
For creation of projections for the 1pctCO2 experiment, made from the fits of the abrupt4xCO2 experiment.

This contains two subfolders
1. **global**

	For projections of the globally averaged observables. The script 'global_observables_projection.m' takes care of (1) loading in the actual data for the 1pctCO2 experiment to compare projections with (files should be loaded automatically if left in place), (2) loading the fitted parameters, (3) creation of projections and (4) making some plots.

	The script relies on other MATLAB files in the folder:
	* exponent_fit_function_1pct.m: the response function for the 1pctCO2 experiment.
	* plot_data_plus_projection.m: plots data and projections.
	* add_LRT_projection.m: add linear response theory proections made with numeric (i.e. non-fitted) data Green functions (not shown in paper)
	* plot_projection_error.m: plot the mismatch of the projections (not shown in paper).

2. **spatial**

	For projections of the 2D fields of the observables. To run, use the script 'spatial_observables_projection.m' which loads in the data from the actual GCM and the fitted spatial modes. Also constructs spatial projections for the observables, which are saved to nc files in the folder 'spatial projection'.

	The script relies on other MATLAB files in the folder:
	* loadData.m: to load data derived from teh actual GCM 1pctCO2 experiment.
	* loadModes.m: to load the fitted spatial modes.
	* loadInitialForcing.m: to load the fitted effective initial radiative forcing.
	* loadCloudMasking.m: to load the fitted cloud masking of radiative forcing.
	* combine_Planck_surface_and_atmosphere_fields.m: combines the fields that constitute the Planck feedback.
	* construct_projection_field.m: construct 2D projection for generic climate feedbacks.
	* construct_projection_field_CLOUD.m: construct 2D projection for cloud feedback field, and compute (derived) actual cloud feedback.
	* construct_projection_field_IMB.m: construct projection for the net radiative imbalance.
	* save_projection.m: saves the projection to a nc file.
	* save_projection_cloud.m: saves the cloud feedback to a nc file.
		
### Python Packages used
The following python packages have been used for the feedback computations:
* numpy
* xarray
* intake
* os
* pandas
* warnings
* sys

In addition, for the spatial plots the following packages have also been used:
* cartopy
* matplotlib
		
",2022-08-12
https://github.com/Bastiaansen/MCLR-ECS-estimation,"<h1> MCLR-ECS-estimation </h1>
Multi-Component Linear Regression method to obtain equilibrium climate sensitivity from short transient warming model simulations

This repository contains the MATLAB (2018a), Jupyter (3.0) and Jupyter Notebook files used for the research in the paper 'Improved Equilibrium Climate Sensitivity Estimations using Short Transient Warming Model Simulations' (submitted)

These codes can be used to perform and compare Equilibrium Climate Sensitivity (ECS) estimation methods. In the paper, these tests have been performed on a conceptual 0D Global Energy Budget Model (GEBM) as well as on model simulations from LongRunMIP (longrunmip.org).

Below follows a short description and instruction for the various codes.

<h2> MATLAB - 3 component model </h2>
This folder contains the MATLAB code to use a 0D GEBM that models global mean surface temperature, albedo and emissivity using a stochastic differential equations (SDE). The files 'doSimulationGEM_ensemble.m' and 'doSimulationGEM_once.m' should be run for respectively an ensemble run or a single run of the model, the parameters can be specified in that file. The file 'ThreeComponentGEM.m' contains the code for the actual SDE. Finally, the file 'perform_estimatoins.m' contains code for equilibrium estimation methods that are compared.

<h2> MATLAB - fig1 - DTDR </h2>
This folder contains MATLAB code to perform various estimation techniques on data. To use these, first the code 'makeDTDRfig1_v2.m' should be run and then subsequently the other scripts can be run as desired.

It is necessary to provide your own files for the globally averaged and yearly averaged observables 'tas' (near surface atmospheric temperature) , 'rsdt' (top-of-atmsophere downwelling shortwave radiation), 'rsut' (top-of-atmsophere upwelling shortwave radiation) and 'rlut' (top-of-atmosphere upwelling longwave radiation). Paths to these files should be put into the relevant variables in the 'make_fig1_v2.m' file (in the code section 'Paths'). These data files are available directly from the LongRunMIP database (see longrunmip.org; [Rugenstein et al, 2010, https://doi.org/10.1175/BAMS-D-19-0068.1]).

Methods provided here are:
* 3EXP: fit a sum of three exponentials to data on global mean temperature and radiative imbalance [Proistosecu and Huybers, 2017, https://doi.org/10.1126/sciadv.1602821]
* EBMeps: fit a two-component GEBM with ocean heat uptake [Geoffroy et al, 2013, https://doi.org/10.1175/JCLI-D-12-00195.1]
* Gregory: fits using Gregory-like linear regressions [Gregory et al, 2004, https://doi.org/10.1029/2003GL018747]
* sysFit: the here newly introduced Multi-Component Linear (system) Regression (MC-LR) [Bastiaansen et al, 2020, submitted]

<h2> MATLAB - rationale images </h2>
Straightforward code to produce one of the illustrative example figures for the rationale section in the Supporting Information

<h2> Python - LongRunMIP </h2>
This folder contains all codes necessary to (re)produce the tests made on the models participating in LongRunMIP. The Jupyter Notebook 'LongRunMIP_ECS_estimations.ipynb' should be run to perform the computations and produce some of the figures. This also automatically creates the folder 'longrunmip_figs'; here, subfolders are created for each model that contain png files of the result figures, as well as a folder for tikz code for the figures and another folder for the results in nc and json file formats. Also some overview figures are created along with relevant data. The notebook does not contain the code for the compuations; the actual python files that are used for the computations are collected in the subfolder 'python_codes' ('bestEstimate' contains code to find true equilibrium values, 'errorComputations' contains code to compute errors of the estimations, 'estimations' contains code to produce the estimations and 'loadData' contains code that loads-in the relevant data files; all files are commented and should be self-explanatory.)

The Jupyter notebook 'remaining_error_plot.ipynb' can be used to quickly perform additional visualisations of the overview results comparing methods and models at fixed times using the automatically created files from the 'LongRunMIP_ECS_estimations.ipynb' notebook.

Finally, 'list_abrupt4x.json' is a json file that contains information on the input data the codes should use (formatted as an array of dictionaries, each array element being a new dictionary for a different model). Specifically, the entry 'model' should use the string that should be used in the figures, 'name' should be the internal name for the model (as used in the input file formats and map structure), 'control' should be the name suffix for the control run and 'experiment' the name suffix for the experiment run that one wants to use. In this repositry this file is formatted for use with models in LongRunMIP that have a long enough 'abrupt-4xCO2' run.

<h3> Instructions -- How to use </h3>

* Create a new folder 'longrunmip_data' with another folder 'global'
* For each model one wants to consider create another folder with the internal name of the model
* In each model, put the nc files (for rlut, rsut, rsdt and tas) for both the control and experiment runs (for LongRunMIP data see longrunmip.org)
* Files should be named like this: 'variablename_modelinternalname_experimentname.nc'; for example 'rlut_CESM104_abrupt4x_5900.nc'
* For each model, create a new array entry in the 'list_abrupt4x.json' file with the required information
* Once this has been done, set-up for the input data is complete

<h4> Python Packages </h4>
The notebooks and python files more contain detailed information on their package dependecies. Specifically, the following packes are used

* numpy
* xarray
* os
* warnings
* json
* sys
* matplotlib
* sklear.linear_model
* scipy.optimize
* tikzplotlib

The package 'tikzplotlib' is probably the least-used and least-common package. This package is used to obtain tikz code (https://github.com/pgf-tikz/pgf) that can be used along with LaTeX to produce (vector) graphics. The code has been set-up in such a way that codes should still work if this package is not installed, provided that one comments out all 'import tikzplotlib' lines in all notebooks and python files (all code referencing the package is put into try except blocks).
",2022-08-12
https://github.com/berg0138/NumericalMethods_Python,"# NM2018-python
Python introduction
",2022-08-12
https://github.com/billjee/bsmd,"# Reference
López, David, and Farooq, Bilal (2020) A multi-layered blockchain framework for smart mobility data-markets. Transportation Research Part C: Emerging Technologies 111: 588-615. https://arxiv.org/abs/1906.06435

López, David, and Farooq, Bilal (2018) A blockchain framework for smart mobility.	In the proceedings of IEEE International Smart Cities Conference 2018. https://arxiv.org/abs/1809.05785

# Blockchain for smart mobility
Blockchain framework for Smart Mobility Data-market (BSMD) is designed to solve the privacy, security and management issues related to the sharing of passively as well as actively solicited large-scale data. Data from the individuals, governments, universities and companies are distributed on the network and stored in a decentralized manner, the data transactions are recorded and must have the authorization of the owners.

For building the simulations and examples of the BSMD we use the [Hyperledger indy-sdk](https://github.com/hyperledger/indy-sdk) and [Hyperledger indy-node](https://github.com/hyperledger/indy-node).

## Contents
1. In the [examples](/examples) folder are samples of cybersecurity, privacy impacts and transactions. In each subfolder are instructions for running the examples 
2. In the [experiments](/experiments) folder are the simulations. In each subfolder are instructions for running the simulations 

## Built With

* [Hyperledger indy-sdk](https://github.com/hyperledger/indy-sdk) - The blockchain sdk
* [Hyperledger indy-node](https://github.com/hyperledger/indy-node) - The blockchain of the physical network
* [Python3](https://www.python.org/download/releases/3.0/) - source code

## Authors

* **David Lopez** [mitrailer](https://github.com/mitrailer)
* **Bilal Farooq** [billjee](https://github.com/billjee/)

## License

* Hyperledger indy-sdk and indy-node are licensed under the Apache License 2.0 - see the [LICENSE](https://github.com/hyperledger/indy-node/blob/master/LICENSE) file for details
* This project is licensed under the Apache License 2.0 - see the [LICENSE.md](LICENSE.md) file for details
",2022-08-12
https://github.com/billjee/simpsynz,"Simulation based Population Synthesizer (SimPSynz) is distributed free of charge. We ask the user to please explicitly mention the use of the package when publishing results, using the following reference:

Anderson, P., Farooq, B., Efthymiou, D., and Bierlaire, M. (2014) Association Generation in Synthetic Population for Transportation Applications: Graph-Theoretic Solution. Transportation Research Record 2429: 38–50.

Farooq, B., Bierlaire, M., Hurtubia, R., and Floetteroed, G. (2013). Simulation based Synthesis of Population, Transportation Research Part B: Methodological (Available online 22 October 2013).

Note 1: The agents association generation is currently written in java only. This code has been tested by Paul Anderson.

Please see LICENSE.md for details on usage.
",2022-08-12
https://github.com/bjcs39/Import-Data,"# Import-Data
Data import
The 3 docs show the progress of the code. In the first one (1850) I only chunk it, then in the second and third, I imported, chunked, appended and to_csv.

",2022-08-12
https://github.com/boer0107/PlanS_compliancy,"# PlanS_compliancy
establishing Plan S compliancy for articles based on DOI

## Introduction
We would like to establish to which extent publications (journal articles) have been published in Plan S compliant venues and to which extent these articles are in fact Plan S compliant. 

### Plan S compliant venues must meet the following conditions

**Journal is**

1. Full OA journal (DOAJ registered) OR

2. Journal part of transformative deal OR

3. Green Journal with 0-months embargo for AAM or VOR

**And journal allows for Creative Commons License:**

4. CC-BY OR

5. CC0 OR

6. CC-BY-SA 4.0 OR

7. CC-BY-ND (on request)

### Plan S compliant articles must meet the following conditions

8. Have been published in a Plan S compliant venue, as established above

9. Are Open Access available

10. Have  a CC-BY, CC0, CC-BY-SA 4.0 OR CC-BY-ND license.

We do not/cannot establish if an article was open access available immediately in a repository.
",2022-08-12
https://github.com/bokertof/alignment,"# Alignment 
(Inspired by ClayFlannigan)
This script was created to calculate RMSD between two molecule structures including proteins (in general case - point clouds).
I've used Kabsch algorithm (DOI: 10.1107/S0567739476001873) to find the rotation martix via SVD to match one structure on another. This algorithm claims both matrix have equal size.
Note, that this algorithm demands the same order of atoms in both matrixes of molecules which one intends to align each other. I.e. first atom of first structure will align to first atom of second structure. Otherwise algorithm will not be able to find the proper rotation. Then also one can calculate root-mean-square-deviation (RMSD) between structures to estimate diversity. 
![alt text](https://github.com/bokertof/alignment/blob/master/20190606_141030.jpg?raw=true)
",2022-08-12
https://github.com/bokertof/ChemTokenizer,"# ChemTokenizer

ChemTokenizer is intended to tokenize molecules in SMILES form 

It can tokenize SMILES into characters (atoms and auxiliary characters), randomize SMILES in order to get a handful SMILES for the same molecule and standardize SMILES using MolVS library.
",2022-08-12
https://github.com/bokertof/cif_reader,# cif_reader,2022-08-12
https://github.com/bokertof/deep_learning_projects,"# The Toy Deep Learning Projects

1) Word2Vec - one of the possible ways to get words embeddings. Both CBOW and SkipGram approaches are implemented in pure NumPy
2) Pre-trained RoBERTa model (from DeepPavlov, http://deeppavlov.ai/) with CRF-head - solution of punctuation restoration task for the russian language (>90% accuracy)
3) Seq2Seq model with Attention (Bahdanau and Luong) for translation from German into English (BLEu score ~ 26)
4) Simple deep-CNN Autoencoder on the face dataset. Smile vector was computed that can be used to add smiles to images of sad people
",2022-08-12
https://github.com/bokertof/vdw,"# vdw
calculate Van-der-Waals surface of molecule 
",2022-08-12
https://github.com/chauncyzhu/analysis,"# analysis
some analysis code of experiments for papers
",2022-08-12
https://github.com/chauncyzhu/csdnSMP,CSDN用户画像,2022-08-12
https://github.com/chauncyzhu/deep_learning_class,"# deep_learning_class
深度学习课的作业
",2022-08-12
https://github.com/chauncyzhu/deep_learning_classthree,"# deep_learning_classthree
基于AdaBoost算法的人脸分类
",2022-08-12
https://github.com/chauncyzhu/deep_learning_classtwo,"# deep_learning_classtwo
深度学习实验2
",2022-08-12
https://github.com/chauncyzhu/homework_fall2020,"Assignments for [Berkeley CS 285: Deep Reinforcement Learning, Decision Making, and Control](http://rail.eecs.berkeley.edu/deeprlcourse/).
",2022-08-12
https://github.com/chauncyzhu/machinelearning_class_four,"# machinelearning_class_four
机器学习课程作业，基于矩阵分解的推荐系统
",2022-08-12
https://github.com/chauncyzhu/neuralSentence,,2022-08-12
https://github.com/chauncyzhu/sentimentanalysis,"情感分析
python3.5

注意：
1.comment的voca_dict.csv中第43123行出现了NULL，可能是数据清理过程中出现了错误
把它替换为“保留”，防止pandas dataframe转换错误",2022-08-12
https://github.com/chauncyzhu/squad,"1.读取train-v1.1.json和dev-v1.1.json，转换成pandas dataframe并写入csv中
dataframe中包含：
passage：每篇文章
passage_words：每篇文章的分词，仅仅使用了word_tokenize，没有去停用词，list(string)
passage_length：文章的长度
sentences：通过split(""."")，分成了一个个句子
question：问题，string
question_words：使用word_tokenize分词
question_id：问题的id，这个应该是唯一的
answer：正确的答案，dict{answer_text:answer_start}
answer_words：使用word_tokenize分词

2.关于dev.json数据
(1)当generate_candidate读到第275个时，'≠'无法和正确解析出来
(2)当generate_candidate读到第340个时，

3.slide window效果非常差，用了一千多条数据做测试，如果限制答案长度，则准确率仅为1%，如果不限制答案长度，则准确率为25%~30%

",2022-08-12
https://github.com/chauncyzhu/textclassification,"注意的事项：
1.对语料库进行分词、去停用词等数据清理
2.统一将语料库转为pandas dataframe格式，有词典和多分类和二分类文档向量

数据格式：
1.词典向量voca
[word_appear_set,class_word_appear_set,word_doc_set,doc_class_set,bdc,df_bdc]

2.文档向量（下面为每一行）
class:类别，格式如[1,0,0,0,0,0,0,0]，1出现的序号表示属于第几个类
content:内容，格式为[x,x,x,x,x...]，x表示word
mark:标记，表示属于训练集还是测试集，0是训练集，1为测试集
bdc:文档向量，格式为[x,x,x,x,x...]，x表示bdc值

注意的错误1：
未知原因，reuters_train.csv第4192行，即编号为4190处，在运行textclassification\classification\knn\knn.py时出现了错误
原来在这一行的vector出现了：...1.0, nan, 0.0042338727427472245...
而在前面的content中也可以发现：u'nan'
可能这是一个pandas dataframe的坑，会自动将nan转成NaN，因为reuters_train.csv只出现了一次，因此手动修改为0
",2022-08-12
https://github.com/cjvanlissa/bain,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

[![lifecycle](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html#stable)
[![CRAN
status](https://www.r-pkg.org/badges/version/bain)](https://cran.r-project.org/package=bain)
[![R-CMD-check](https://github.com/cjvanlissa/bain/workflows/R-CMD-check/badge.svg)](https://github.com/cjvanlissa/bain/actions)
[![](https://cranlogs.r-pkg.org/badges/bain)](https://cran.r-project.org/package=bain)
[![test-coverage](https://github.com/cjvanlissa/bain/workflows/test-coverage/badge.svg)](https://github.com/cjvanlissa/bain/actions)
[![Contributor
Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://www.contributor-covenant.org/version/2/0/code_of_conduct.html)
[![CII Best
Practices](https://bestpractices.coreinfrastructure.org/projects/3871/badge)](https://bestpractices.coreinfrastructure.org/projects/3871)

# bain

Bain stands for Bayesian informative hypothesis evaluation. It computes
Bayes factors for informative hypotheses in a wide variety of
statistical models. Just run your analysis as usual, and then apply bain
to the output. A tutorial is available at
[DOI:10.31234/osf.io/v3shc](https://psyarxiv.com/v3shc/). A sequel with
the focus on Structural Equation Models is available at
<https://doi.org/10.1080/10705511.2020.1745644>.

## Installation

Install the latest release version of `bain` from CRAN:

``` r
install.packages(""bain"")
```

You can also install the latest development version of `bain` from
GitHub. This requires a working toolchain, to compile the Fortran source
code. [Step 3 in this
tutorial](https://cjvanlissa.github.io/worcs/articles/setup.html)
explains how to set up the toolchain. Then, run:

``` r
install.packages(""devtools"")
devtools::install_github(""cjvanlissa/bain"")
```

## Workflow

Add bain to your existing R workflow, and obtain Bayes factors for your
familiar R analyses\! Bain is compatible with the pipe operator. Here is
an example for testing an informative hypothesis about mean differences
in an ANOVA:

``` r
# Load bain
library(bain)
# dplyr to access the %>% operator
library(dplyr)
#> Warning: package 'dplyr' was built under R version 4.0.5
# Iris as example data
iris %>%
  # Select outcome and predictor variables
  select(Sepal.Length, Species) %>%      
  # Add -1 to the formula to estimate group means, as in ANOVA
  lm(Sepal.Length ~ -1 + Species, .) %>% 
  bain(""Speciessetosa < Speciesversicolor = Speciesvirginica;
       Speciessetosa < Speciesversicolor < Speciesvirginica"")
#> Bayesian informative hypothesis testing for an object of class lm (ANOVA):
#> 
#>    Fit   Com   BF.u  BF.c             PMPa  PMPb  PMPc 
#> H1 0.000 0.224 0.000 0.000            0.000 0.000 0.000
#> H2 1.000 0.171 5.863 118577263118.841 1.000 0.854 1.000
#> Hu                                          0.146      
#> Hc 0.000 0.829 0.000                              0.000
#> 
#> Hypotheses:
#>   H1: Speciessetosa<Speciesversicolor=Speciesvirginica
#>   H2: Speciessetosa<Speciesversicolor<Speciesvirginica
#> 
#> Note: BF.u denotes the Bayes factor of the hypothesis at hand versus the unconstrained hypothesis Hu. BF.c denotes the Bayes factor of the hypothesis at hand versus its complement. PMPa contains the posterior model probabilities of the hypotheses specified. PMPb adds Hu, the unconstrained hypothesis. PMPc adds Hc, the complement of the union of the hypotheses specified.
```

## Documentation

Every user-facing function in the package is documented, and the
documentation can be accessed by running `?function_name` in the R
console, e.g., `?bain`.

Moreover, you can read the *Introduction to bain* vignette by running
`vignette(""Introduction_to_bain"", package = ""bain"")`

## Citing bain

You can cite the R-package with the following citation:

> Gu, X., Hoijtink, H., Mulder, J., & van Lissa, C. (2021). bain: Bayes
> factors for informative hypotheses. (Version 0.2.8) \[R package\].
> <https://CRAN.R-project.org/package=bain>

## Contributing and Contact Information

If you have ideas, please get involved. You can contribute by opening an
issue on GitHub, or sending a pull request with proposed features.
Contributions in code must adhere to the [tidyverse style
guide](https://style.tidyverse.org/).

  - File a GitHub issue [here](https://github.com/cjvanlissa/bain)
  - Make a pull request [here](https://github.com/cjvanlissa/bain/pulls)

By participating in this project, you agree to abide by the [Contributor
Code of Conduct
v2.0](https://www.contributor-covenant.org/version/2/0/code_of_conduct.html).
",2022-08-12
https://github.com/cjvanlissa/bayesynth,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'bayesynth.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File            | Description                | Usage         
--------------- | -------------------------- | --------------
README.md       | Description of project     | Human editable
bayesynth.Rproj | Project file               | Loads project 
LICENSE         | User permissions           | Read only     
.worcs          | WORCS metadata YAML        | Read only     
prepare_data.R  | Script to process raw data | Human editable
renv.lock       | Reproducible R environment | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles,
read the preprint at https://osf.io/zcvbs/

## WORCS: Advice for authors

* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)

## WORCS: Advice for readers

Please refer to the vignette on [reproducing a WORCS project]() for step by step advice.
<!-- If your project deviates from the steps outlined in the vignette on     -->
<!-- reproducing a WORCS project, please provide your own advice for         -->
<!-- readers here.                                                           -->
",2022-08-12
https://github.com/cjvanlissa/coping_covid,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>



[![WORCS](https://img.shields.io/badge/WORCS-limited-orange)](https://osf.io/zcvbs/)


<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in Rstudio by opening the file called 'coping_covid.Rproj'.
Then, run run_me.R.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File           | Description                | Usage         
-------------- | -------------------------- | --------------
README.md      | Description of project     | Human editable
coping_covid.Rproj | Project file               | Loads project 
LICENSE        | User permissions           | Read only     
.worcs         | WORCS metadata YAML        | Read only     
prepare_data.R | Script to process raw data | Human editable
renv.lock      | Reproducible R environment | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
## Citation

To cite this project, please use:

van Mulukom, V., Muzzulini, B., Rutjens, B. T., van Lissa, C. J., & Farias, M. (2020). *Coping with COVID-19*. Retrieved from [osf.io/tsjnb](https://osf.io/tsjnb).

<!--  * Contact information for questions/comments                           -->
## Contact information

Please see the [Open Science Framework project page](https://osf.io/tsjnb).
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

## Preregistration

This project was preregistered, and the preregistration is available [here](https://osf.io/7bcmk).

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

* To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles, read the preprint at https://osf.io/zcvbs/
* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)
* For a brief overview of the steps of the WORCS workflow, see below.

## WORCS: Steps to follow for a project

## Phase 1: Study design

1. Create a (Public or Private) remote repository on a 'Git' hosting service
2. When using R, initialize a new RStudio project using the WORCS template. Otherwise, clone the remote repository to your local project folder.
3. Add a README.md file, explaining how users should interact with the project, and a LICENSE to explain users' rights and limit your liability. The `worcs` project template does this automatically.
3. Optional: Preregister your analysis by committing a plain-text preregistration and tagging the commit as ""preregistration"".
4. Optional: Upload the preregistration to a dedicated preregistration server
5. Optional: Add study Materials to the repository

## Phase 2: Writing and analysis

6. Create an executable script documenting the code required to load the raw data into a tabular format, and de-identify human subjects if applicable
7. Save the data into a plain-text tabular format like `.csv`. When using open data, commit this file to 'Git'. When using closed data, commit a checksum of the file, and a synthetic copy of the data.
8. Write the manuscript using a dynamic document generation format, with code chunks to perform the analyses.
9. Commit every small change to the 'Git' repository
10. Cite essential references with `@`, and non-essential references with `@@`

## Phase 3: Submission and publication

11. Use dependency management to make the computational environment fully reproducible
12. Optional: Add a WORCS-badge to your project's README file
13. Make a Private 'Git' remote repository Public
14. Optional: [Create a project page on the Open Science Framework](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project)
15. [Connect your 'OSF' project page to the 'Git' remote repository](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an open science statement to the Abstract or Author notes, which links to the 'Git' remote repository or 'OSF' page
17. Render the dynamic document to PDF
18. Optional: [Publish the PDF as a preprint, and add it to the OSF project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
19. Submit the paper, and tag the release of the submitted paper, as in Step 3.

## Notes for cautious researchers

Some researchers might want to share their work only once the paper is accepted for publication. In this case, we recommend creating a ""Private"" repository in Step 1, and completing Steps 13-18 upon acceptance.


## Access to data

Some of the data used in this project are not publically available. Instead, synthetic data have been provided. Using the function load_data() will load these synthetic data if the original data are unavailable. Note that this synthetic data cannot be used to reproduce the original results. However, it does allow users to run the code and, optionally, generate valid code that can be evaluated using the original data by the project authors. To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--For worcs users: Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->",2022-08-12
https://github.com/cjvanlissa/COVID19_metadata,"README
================
Caspar J. van Lissa
4/1/2020

# COVID-19 Metadata

A collection of relevant country/city level metadata about the COVID-19
pandemic, made interoperable for secondary analysis. Curated by [Data
scientists Against Corona](https://dataversuscorona.com/),
collaborators: *Caspar van Lissa*, *Tim Draws*, *Andrii Grygoryshyn*,
*Konstantin Tomić*, and *Malte Lüken*.

## Available data sets

The following data sets have been processed:

|Category    |Information                                        |Source                                  |URL                                                                                            |Progress|Folder          |License                           |Reference                                                                              |
|------------|---------------------------------------------------|----------------------------------------|-----------------------------------------------------------------------------------------------|--------|----------------|----------------------------------|---------------------------------------------------------------------------------------|
|Mobility    |Google mobility data                               |Google                                  |https://www.google.com/covid19/mobility/                                                       |Done    |google_mobility |                                  |                                                                                       |
|Risk level  |Hospital data per country                          |WHO Health workforce/facilities database|https://apps.who.int/gho/data/node.main.HWF                                                    |Done    |WHO_OECD        |                                  |                                                                                       |
|Risk level  |Health infrastructure per country data             |OECD Health care resources database     |https://stats.oecd.org/index.aspx?queryid=30183                                                |Done    |WHO_OECD        |                                  |                                                                                       |
|Policies    |Government effectiveness                           |Worldwide Governance Indicators         |www.govindicators.org                                                                          |Done    |WB_GOV          |CC-BY 3.0                         |                                                                                       |
|Policies    |COVID-19 specific regulation policies              |Oxford Tracker for Regulation Policies  |https://www.bsg.ox.ac.uk/research/research-projects/oxford-covid-19-government-response-tracker|Done    |Ox_CGRT         |CC-BY 4.0                         |Hale, Thomas and Samuel Webster (2020)                                                 |
|Preparedness|Global Health Security Index                       |Nuclear Threat Initiative               |https://www.ghsindex.org/                                                                      |Done    |GHS             |CC BY-NC-ND�4.0                   |                                                                                       |
|COVID19     |Number of cases and fatalities                     |CSSE Global Cases                       |https://systems.jhu.edu/                                                                       |Done    |CSSE            |Copyright (academic use permitted)|<a href = ""https://doi.org/10.1016/S1473-3099(20)30120-1"">Dong, Du, & Gardner, 2020</a>|
|Economic    |World Development Indicators                       |World Bank                              |https://datacatalog.worldbank.org/dataset/world-development-indicators                         |Done    |WB_WDI          |CC-BY 4.0                         |                                                                                       |
|Response    |Number of tests                                    |Our world in data                       |                                                                                               |        |OWID_Tests      |                                  |                                                                                       |
|Economic    |Doing Business                                     |World Bank                              |https://s3.amazonaws.com/datascope-ast-datasets-nov29/datasets/435/data.csv                    |        |WB_BUSINESS     |CC-BY 4.0                         |                                                                                       |
|Mobility    |Logistics Performance Index                        |World Bank                              |https://s3.amazonaws.com/datascope-ast-datasets-nov29/datasets/50/data.csv                     |        |WB_LOGISTICS    |CC-BY 4.0                         |                                                                                       |
|            |Failed States Index                                |World Bank                              |https://s3.amazonaws.com/datascope-ast-datasets-nov29/datasets/97/data.csv                     |        |WB_FAILED       |CC-BY 4.0                         |                                                                                       |
|            |Freedom House                                      |World Bank                              |https://s3.amazonaws.com/datascope-ast-datasets-nov29/datasets/997/data.csv                    |        |WB_FREEDOM      |CC-BY 4.0                         |                                                                                       |
|            |Global Indicators of Regulatory Governance         |World Bank                              |https://s3.amazonaws.com/datascope-ast-datasets-nov29/datasets/50/data.csv                     |        |WB_GOVERNANCE   |CC-BY 4.0                         |                                                                                       |
|            |Institutional Profiles Database                    |World Bank                              |https://s3.amazonaws.com/datascope-ast-datasets-nov29/datasets/999/data.csv                    |        |WB_INSTITUTIONAL|CC-BY 4.0                         |                                                                                       |
|            |Worldwide Buresucracy Indicators                   |World Bank                              |https://s3.amazonaws.com/datascope-ast-datasets-nov29/datasets/4127/data.csv                   |        |WB_BUREAUCRACY  |CC-BY 4.0                         |                                                                                       |
|            |United Nations Conference on Trade and Development |World Bank                              |https://s3.amazonaws.com/datascope-ast-datasets-nov29/datasets/513/data.csv                    |        |WB_TRADE_DEV    |CC-BY 4.0                         |                                                                                       |
|            |Press Freedom Index by Reporters without Borders   |World Bank                              |https://s3.amazonaws.com/datascope-ast-datasets-nov29/datasets/1000/data.csv                   |        |WB_PRESS_FREE   |CC-BY 4.0                         |                                                                                       |
|            |Education Statistics                               |World Bank                              |https://s3.amazonaws.com/datascope-ast-datasets-nov29/datasets/748/data.csv                    |        |WB_EDUCATION    |CC-BY 4.0                         |                                                                                       |
|            |Gender Statistics                                  |World Bank                              |https://s3.amazonaws.com/datascope-ast-datasets-nov29/datasets/747/data.csv                    |        |WB_GENDER       |CC-BY 4.0                         |                                                                                       |
|            |Travel & Tourism Competitiveness                   |World Bank                              |https://s3.amazonaws.com/datascope-ast-datasets-nov29/datasets/78/data.csv                     |        |WB_TOURISM      |CC-BY 4.0                         |                                                                                       |
|            |World Travel & Tourism Counsil                     |World Bank                              |https://s3.amazonaws.com/datascope-ast-datasets-nov29/datasets/79/data.csv                     |        |WB_WTTC         |CC-BY 4.0                         |                                                                                       |
|            |Poverty ans Equity Data                            |World Bank                              |https://s3.amazonaws.com/datascope-ast-datasets-nov29/datasets/3755/data.csv                   |        |WB_POV_EQUITY   |CC-BY 4.0                         |                                                                                       |




## Folder structure:

| Folder  | Description                                                                                                                                                           | Permissions    |
| :------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------- |
| data    | Metadata sources in .csv format (intermediate formats are acceptable until they can be made tidy).                                                                    | Do not edit    |
| scripts | (R)-scripts                                                                                                                                                           | Human editable |
| doc     | Documentation for your contribution, ideally in Rmarkdown format. Rmarkdown can contain code chunks. Elaborate functions should be relegated to the ‘scripts’ folder. | Human editable |

## How to use

Fork or clone this repository (for GitHub beginners: You can also click
the green button that says “Clone or download”, and download a .zip).
All data are in the `/data` folder. Some data are rarely updated (e.g.,
annual data), and some are updated daily. To ensure that you have access
to the latest data for frequently updated sources, run the R-script in
the `run_me.R` file, in the main folder.

## Standards for data

Every source is condensed into one data file in `.csv` format, according
to these specifications:

  - Data should be available on the country- or community-and-country
    level.
  - Recent data are the focus; if multi-year data is available, older
    years *can* be dropped
  - All variable names should be lower case
  - Mandatory variables are `country` (plain text country), and
    `countryiso3` (ISO3 country code)
  - Optionally, a `region` variable can be added
  - Data should be in wide format: One row per country, one column per
    variable

## Standards for data dictionary

A `data_dictionary.csv` is available for each data set, *unless the file
contents are immediately clear from the file*. This data dictionary
includes:

  - `variable`: The name of the variable in the data file
  - `description`: The description of this variable

Any other important information per variable can be included in this
dictionary, such as sources, weights, etc.

## News

The following issues are ongoing:

  - Adding more databases; feel free to make a suggest or request a
    database
    [here](https://github.com/cjvanlissa/COVID19_metadata/issues)
  - Added time-since first occurrence for Oxford policy / incidence
    trackers
  - Added last observation carried forward for WHO data

## License

This project is under a GNU GPL v3 open source license (see the LICENSE
file). Individual data sources have different licenses; always check the
license before publishing based on these data.

## Contributing and Contact Information

This project is open for collaborators with valuable expertise.
Contribute by:

  - Filing a GitHub issue
    [here](https://github.com/cjvanlissa/COVID19_metadata/issues)
  - Making a pull request
    [here](https://github.com/cjvanlissa/COVID19_metadata/pulls)

By participating in this project, you agree to abide by the [Contributor
Code of Conduct v2.0](https://www.contributor-covenant.org/).

# A WORCS Project

This project is based on the Workflow for Open Reproducible Code in
Science (WORCS). For more details, please read the preprint at
<https://osf.io/zcvbs/>.

# WORCS - steps to follow for each project

## Study design phase

1.  Create a new Private repository on github, copy the <https://> link
    to clipboard  
    The link should look something like
    <https://github.com/yourname/yourrepo.git>
2.  In Rstudio, click File \> New Project \> New directory \> WORCS
    Project Template
    1.  Paste the GitHub Repository address in the textbox
    2.  Keep the checkbox for `renv` checked if you want to document all
        dependencies (recommended)
    3.  Select a preregistration template
3.  Write the preregistration `.Rmd`
4.  In the top-right corner of Rstudio, select the Git tab, select the
    checkboxes next to all files, and click the Commit button. Write an
    informative message for the commit, e.g., “Preregistration”, again
    click Commit, and then click the green Push arrow to send your
    commit to GitHub
5.  Go to the GitHub repository for this project, and tag the Commit as
    a preregistration
6.  Optional: Render the preregistration to PDF, and upload it to
    AsPredicted.org or OSF.io as an attachment
7.  Optional: Add study Materials (to which you own the rights) to the
    repository. It is possible to solicit feedback (by opening a GitHub
    Issue) and acknowledge outside contributions (by accepting Pull
    requests)

## Data analysis phase

8.  Read the data into R, and document this procedure in
    `prepare_data.R`
9.  Use `open_data()` or `closed_data()` to store the data
10. Write the manuscript in `Manuscript.Rmd`, using code chunks to
    perform the analyses.
11. Regularly commit your progress to the Git repository; ideally, after
    completing each small and clearly defined task. Use informative
    commit messages. Push the commits to GitHub.
12. Cite essential references with one at-symbol
    (`[@essentialref2020]`), and non-essential references with a double
    at-symbol (`[@@nonessential2020]`).

## Submission phase

13. To save the state of the project library (all packages used), call
    `renv::snapshot()`. This updates the lockfile, `renv.lock`.
14. To render the paper with essential citations only for submission,
    change the line `knit: worcs::cite_all` to `knit:
    worcs::cite_essential`. Then, press the Knit button to generate a
    PDF

## Publication phase

13. Make the GitHub repository public
14. [Create an OSF
    project](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project);
    although you may have already done this in Step 6.
15. [Connect your GitHub repository to the OSF
    project](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an Open Science statement to the manuscript, with a link to the
    OSF project
17. Optional: [Publish preprint in a not-for-profit preprint repository
    such as PsyArchiv, and connect it to your existing OSF
    project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
      - Check [Sherpa Romeo](http://sherpa.ac.uk/romeo/index.php) to be
        sure that your intended outlet allows the publication of
        preprints; many journals do, nowadays - and if they do not, it
        is worth considering other outlets.
",2022-08-12
https://github.com/cjvanlissa/gitbook-demo,"This is an example of a GitBook for teaching, generated using 
R Markdown and **bookdown** (https://github.com/rstudio/bookdown). 

The example
is based on the minimal bookdown example by Yihui Xie, available at
https://bookdown.org/yihui/bookdown-demo.
Please see the page ""[Get Started](https://bookdown.org/yihui/bookdown/get-started.html)"" at https://bookdown.org/yihui/bookdown/ for how to compile this example into HTML. You may generate a copy of the book in `bookdown::pdf_book` format by calling `bookdown::render_book('index.Rmd', 'bookdown::pdf_book')`. More detailed instructions are available here https://bookdown.org/yihui/bookdown/build-the-book.html.

You can find the preview of this example at https://cjvanlissa.github.io/gitbook-demo/.
",2022-08-12
https://github.com/cjvanlissa/gorica,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

[![lifecycle](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://lifecycle.r-lib.org/articles/stages.html)
[![CRAN
status](https://www.r-pkg.org/badges/version/gorica)](https://cran.r-project.org/package=gorica)
[![](https://cranlogs.r-pkg.org/badges/gorica)](https://cran.r-project.org/package=gorica)

<!-- [![DOI](http://joss.theoj.org/papers/10.21105/joss.00978/status.svg)](10.1111/bmsp.12110)-->

# GORICA: Evaluation of Inequality Constrained Hypotheses Using Generalized AIC

Implements the generalized order-restricted information criterion
approximation (GORICA). The GORICA can be utilized to evaluate
(in)equality constrained hypotheses. The GORICA is applicable not only
to normal linear models, but also to generalized linear models (GLMs),
generalized linear mixed models (GLMMs), and structural equation models
(SEMs). In addition, the GORICA can be utilized in the context of
contingency tables for which (in)equality constrained hypotheses do not
necessarily contain linear restrictions on cell probabilities, but
instead often contain non-linear restrictions on cell probabilities.

## Installation

You can install gorica from GitHub with:

``` r
# install.packages(""devtools"")
devtools::install_github(""cjvanlissa/gorica"")
```

## Workflow

Add gorica to your existing R workflow, and evaluate informative
hypotheses for your familiar R analyses! Here is an example for testing
an informative hypothesis about mean differences in an ANOVA:

``` r
res <- lm(Sepal.Length ~ -1 + Species, data = iris)
gorica(res, ""Speciessetosa < Speciesversicolor = Speciesvirginica; Speciessetosa < Speciesversicolor < Speciesvirginica"")
#> Informative hypothesis test for an object of class lm:
#> 
#>    loglik  penalty gorica gorica_weights
#> H1 -14.948 1.501   32.898 0.000         
#> H2 5.103   1.836   -6.534 0.762         
#> Hu 5.103   3.000   -4.206 0.238         
#> 
#> Hypotheses:
#>   H1: Speciessetosa<Speciesversicolor=Speciesvirginica
#>   H2: Speciessetosa<Speciesversicolor<Speciesvirginica 
#>   Hu: Unconstrained hypothesis
#> 
```
",2022-08-12
https://github.com/cjvanlissa/hackaton_sciencefunding,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'hackaton_sciencefunding.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                          | Description                      | Usage         
----------------------------- | -------------------------------- | --------------
README.md                     | Description of project           | Human editable
hackaton_sciencefunding.Rproj | Project file                     | Loads project 
LICENSE                       | User permissions                 | Read only     
.worcs                        | WORCS metadata YAML              | Read only     
prepare_data.R                | Script to process raw data       | Human editable
manuscript/manuscript.rmd     | Source code for paper            | Human editable
manuscript/references.bib     | BibTex references for manuscript | Human editable
renv.lock                     | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles,
read the preprint at https://osf.io/zcvbs/

## WORCS: Advice for authors

* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)

## WORCS: Advice for readers

Please refer to the vignette on [reproducing a WORCS project]() for step by step advice.
<!-- If your project deviates from the steps outlined in the vignette on     -->
<!-- reproducing a WORCS project, please provide your own advice for         -->
<!-- readers here.                                                           -->
",2022-08-12
https://github.com/cjvanlissa/hroberts_natural,"This work is in press. To reference this repository pending publication of the paper, please refer to the OSF project: https://doi.org/10.17605/OSF.IO/S2JV4.

When using data or syntax from this repository, you must provide a reference, in concordance with the [CC-By Attribution 4.0 International license](http://creativecommons.org/licenses/by/4.0/).",2022-08-12
https://github.com/cjvanlissa/ic_mixture,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'ic_mixture.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File             | Description                | Usage         
---------------- | -------------------------- | --------------
README.md        | Description of project     | Human editable
ic_mixture.Rproj | Project file               | Loads project 
LICENSE          | User permissions           | Read only     
.worcs           | WORCS metadata YAML        | Read only     
prepare_data.R   | Script to process raw data | Human editable
renv.lock        | Reproducible R environment | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles,
read the preprint at https://osf.io/zcvbs/

## WORCS: Advice for authors

* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)

## WORCS: Advice for readers

Please refer to the vignette on [reproducing a WORCS project]() for step by step advice.
<!-- If your project deviates from the steps outlined in the vignette on     -->
<!-- reproducing a WORCS project, please provide your own advice for         -->
<!-- readers here.                                                           -->
",2022-08-12
https://github.com/cjvanlissa/IHBT-SEM,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'IHBT-SEM.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                      | Description                      | Usage         
------------------------- | -------------------------------- | --------------
README.md                 | Description of project           | Human editable
IHBT-SEM.Rproj            | Project file                     | Loads project 
LICENSE                   | User permissions                 | Read only     
.worcs                    | WORCS metadata YAML              | Read only     
prepare_data.R            | Script to process raw data       | Human editable
manuscript/manuscript.rmd | Source code for paper            | Human editable
manuscript/references.bib | BibTex references for manuscript | Human editable
renv.lock                 | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

* To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles, read the preprint at https://osf.io/zcvbs/
* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)
* For a brief overview of the steps of the WORCS workflow, see below.

## WORCS: Steps to follow for a project

## Phase 1: Study design

1. Create a (Public or Private) remote repository on a 'Git' hosting service
2. When using R, initialize a new RStudio project using the WORCS template. Otherwise, clone the remote repository to your local project folder.
3. Add a README.md file, explaining how users should interact with the project, and a LICENSE to explain users' rights and limit your liability. The `worcs` project template does this automatically.
3. Optional: Preregister your analysis by committing a plain-text preregistration and tagging the commit as ""preregistration"".
4. Optional: Upload the preregistration to a dedicated preregistration server
5. Optional: Add study Materials to the repository

## Phase 2: Writing and analysis

6. Create an executable script documenting the code required to load the raw data into a tabular format, and de-identify human subjects if applicable
7. Save the data into a plain-text tabular format like `.csv`. When using open data, commit this file to 'Git'. When using closed data, commit a checksum of the file, and a synthetic copy of the data.
8. Write the manuscript using a dynamic document generation format, with code chunks to perform the analyses.
9. Commit every small change to the 'Git' repository
10. Cite essential references with `@`, and non-essential references with `@@`

## Phase 3: Submission and publication

11. Use dependency management to make the computational environment fully reproducible
12. Optional: Add a WORCS-badge to your project's README file
13. Make a Private 'Git' remote repository Public
14. Optional: [Create a project page on the Open Science Framework](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project)
15. [Connect your 'OSF' project page to the 'Git' remote repository](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an open science statement to the Abstract or Author notes, which links to the 'Git' remote repository or 'OSF' page
17. Render the dynamic document to PDF
18. Optional: [Publish the PDF as a preprint, and add it to the OSF project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
19. Submit the paper, and tag the release of the submitted paper, as in Step 3.

## Notes for cautious researchers

Some researchers might want to share their work only once the paper is accepted for publication. In this case, we recommend creating a ""Private"" repository in Step 1, and completing Steps 13-18 upon acceptance.
",2022-08-12
https://github.com/cjvanlissa/kindness_meta-analysis,"# kindness_meta-analysis
Meta-analysis of experimental studies examining whether acts of kindness increase well-being. Feel free to use the data and/or syntax in this repository, but please cite our work.
",2022-08-12
https://github.com/cjvanlissa/LazySEM,"# LazySEM
Hotkeys to automate procedures in MPlus (uses AutoHotkey)
",2022-08-12
https://github.com/cjvanlissa/MAC,"Research related to the theory of Morality As Cooperation. If you use any of the syntax in this repository, please cite the relevant paper or this repository.",2022-08-12
https://github.com/cjvanlissa/manyanalysts_religion,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_badge.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in Rstudio by opening the file called 

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                        | Description                      | Usage         
--------------------------- | -------------------------------- | --------------
README.md                   | Description of project           | Human editable
manyanalysts_religion.Rproj | Project file                     | Loads project 
LICENSE                     | User permissions                 | Read only     
.worcs                      | WORCS metadata YAML              | Read only     
prepare_data.R              | Script to process raw data       | Human editable
manuscript/manuscript.rmd   | Source code for paper            | Human editable
manuscript/references.bib   | BibTex references for manuscript | Human editable
renv.lock                   | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

* To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles, read the preprint at https://osf.io/zcvbs/
* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)
* For a brief overview of the steps of the WORCS workflow, see below.

## WORCS: Steps to follow for a project

## Phase 1: Study design

1. Create a new (Public or Private) repository on 'GitHub'
2. Create a new RStudio project using the WORCS template
3. Optional: Preregister your analysis
4. Optional: Upload preregistration to another repository
5. Optional: Add study Materials to the repository

## Phase 2: Data analysis

6. Load the raw data
7. Save the data using `open_data()` or `closed_data()`. Never commit data to 'Git' that you do not intend to share
8. Write the manuscript in `manuscript.Rmd`, using code chunks to perform the analyses.
9. Commit every small change
10. Cite essential references with `@`, and non-essential references with `@@`

## Phase 3: Submission/publication

11. Store the R environment by calling `renv::snapshot()`
12. Optional: Add a WORCS-badge to your README file and complete the optional elements of the WORCS checklist
13. Make the Private 'GitHub' repository Public
14. [Create a project page on the Open Science Framework](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project)
15. Connect your 'OSF' project page to the 'GitHub' repository](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an open science statement to the Abstract or Author notes, which links to the 'GitHub' repository or 'OSF' page
17. Knit the paper to PDF
18. Optional: Publish a preprint
19. Submit the paper, and tag the release of the submitted paper as in Step 3.

## Notes for cautious researchers

Some researchers might want to share their work only once the paper is accepted for publication. In this case, we recommend creating a ""Private"" repository in Step 1, and completing Steps 13-18 upon acceptance.


## Access to data

Some of the data used in this project are not publically available.
Synthetic data with similar characteristics to the original data have been provided. Using the function load_data() will load these synthetic data when the original data are unavailable. Note that these synthetic data cannot be used to reproduce the original results. However, it does allow users to run the code and, optionally, generate valid code that can be evaluated using the original data by the project authors.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->
",2022-08-12
https://github.com/cjvanlissa/metaforest,"<!-- README.md is generated from README.Rmd. Please edit that file -->

MetaForest <a href='https://cjvanlissa.github.io/metaforest/'><img src='https://github.com/cjvanlissa/metaforest/raw/master/docs/metaforest_icon.png' align=""right"" height=""139"" /></a>
=======================================================================================================================================================================================

[![CRAN
status](https://www.r-pkg.org/badges/version/metaforest)](https://cran.r-project.org/package=metaforest)
[![lifecycle](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://www.tidyverse.org/lifecycle/#maturing)
[![R-CMD-check](https://github.com/cjvanlissa/metaforest/workflows/R-CMD-check/badge.svg)](https://github.com/cjvanlissa/metaforest/actions)
[![codecov](https://codecov.io/gh/cjvanlissa/metaforest/branch/master/graph/badge.svg?token=KuBBTF3CjU)](https://codecov.io/gh/cjvanlissa/metaforest)

Background
==========

The goal of MetaForest is to explore heterogeneity in meta-analytic
data, identify important moderators, and explore the functional form of
the relationship between moderators and effect size. To do so,
MetaForest conducts a weighted random forest analysis, using
random-effects or fixed-effects weights, as in classic meta-analysis, or
uniform weights (unweighted random forest). Simulation studies have
demonstrated that this technique has substantial power to detect
relevant moderators, even in datasets as small as 20 cases (based on
cross-validated *R*<sup>2</sup>). Using a variable importance plot,
important moderators can be identified, and using partial prediction
plots, the shape of the marginal relationship between moderators and
effect size can be visualized. MetaForest can be readily integrated in
classical meta-analytic approaches: If MetaForest is conducted as a
primary analysis, classic meta-analysis can be used to quantify
heterogeneity (in fact, MetaForest by default reports a random-effects
meta-analysis on the raw data, and the residuals of the random forests
analysis), or to provide a simplified representation of the linear
effects of important predictors. Conversely, a theory-driven classical
meta-analysis could be complemented by an exploratory MetaForest
analysis, as a final check to ensure that important moderators have not
been overlooked. We hope that this approach will be of use to
researchers, and that the availability of user-friendly R functions will
facilitate its adoption.

Installation
============

You can install `metaforest` from CRAN with:

``` r
install.packages(""metaforest"")
```

Documentation
=============

Every user-facing function in the package is documented, and the
documentation can be accessed by running `?function_name` in the R
console, e.g., `?graph`, or by checking the [project
website](https://cjvanlissa.github.io/metaforest/reference/index.html)

Citing metaforest
=================

You can cite the method by referencing this open access book chapter:

Van Lissa, C. J. (2020). Small sample meta-analyses: Exploring
heterogeneity using MetaForest. In R. Van De Schoot & M. Miočević
(Eds.), *Small Sample Size Solutions (Open Access): A Guide for Applied
Researchers and Practitioners.* CRC Press.
<a href=""https://www.crcpress.com/Small-Sample-Size-Solutions-Open-Access-A-Guide-for-Applied-Researchers/Schoot-Miocevic/p/book/9780367222222"" class=""uri"">https://www.crcpress.com/Small-Sample-Size-Solutions-Open-Access-A-Guide-for-Applied-Researchers/Schoot-Miocevic/p/book/9780367222222</a>

Contributing and Contact Information
====================================

If you have ideas, please get involved. You can contribute by opening an
issue on GitHub, or sending a pull request with proposed features.

-   File a GitHub issue [here](https://github.com/cjvanlissa/metaforest)
-   Make a pull request
    [here](https://github.com/cjvanlissa/metaforest/pulls)

By participating in this project, you agree to abide by the [Contributor
Code of Conduct v2.0](https://www.contributor-covenant.org/).
Contributions to the package must adhere to the [tidyverse style
guide](https://style.tidyverse.org/). When contributing code, please add
tests for that contribution to the `tests/testthat` folder, and ensure
that these tests pass in the [GitHub Actions
panel](https://github.com/cjvanlissa/worcs/actions/workflows/R-CMD-check).

Example analysis
================

This example demonstrates how one might go about conducting a
meta-analysis using MetaForest. For more information, check the [package
vignette](https://cjvanlissa.github.io/metaforest/articles/Introduction_to_metaforest.html).

``` r
#Load metaforest package
library(metaforest)

#Simulate a meta-analysis dataset with 20 studies, 1 relevant moderator, and 4 irrelevant moderators
set.seed(42)
data <- SimulateSMD()$training

#Conduct an unweighted MetaForest analysis, to estimate the residual tau2
mf.unif <- MetaForest(formula = yi ~ ., data = data,
                      whichweights = ""unif"", method = ""DL"", num.trees = 2000)

#Extract the result of this analysis and print them
results <- summary(mf.unif)
results
#> MetaForest results
#>                                          
#> Type of analysis:              MetaForest
#> Number of studies:             20        
#> Number of moderators:          5         
#> Number of trees in forest:     2000      
#> Candidate variables per split: 2         
#> Minimum terminal node size:    5         
#> OOB prediction error (MSE):    0.1012    
#> R squared (OOB):               0.2970    
#> 
#> Tests for Heterogeneity: 
#>                                tau2   tau2_SE I^2     H^2    Q-test  df Q_p   
#> Raw effect sizes:              0.0553 0.0486  37.2642 1.5940 30.2857 19 0.0483
#> Residuals (after MetaForest):  0.0099 0.0334  9.6420  1.1067 21.0275 19 0.3353
#> 
#> 
#> Random intercept meta-analyses:
#>                                Intercept se     ci.lb   ci.ub   p     
#> Raw effect sizes:              -0.2136   0.0875 -0.3851 -0.0421 0.0147
#> Residuals (after MetaForest):  0.0357    0.0720 -0.1053 0.1768  0.6197

#Conduct a weighted MetaForest analysis, using the residual tau2 from the
#unweighted analysis above
mf.random <- MetaForest(formula = yi ~ ., data = data,
                      whichweights = ""random"", method = ""DL"", 
                      tau2 = results$rma[2,1],
                      num.trees = 2000)

#Print the result of this analysis
summary(mf.random)
#> MetaForest results
#>                                          
#> Type of analysis:              MetaForest
#> Number of studies:             20        
#> Number of moderators:          5         
#> Number of trees in forest:     2000      
#> Candidate variables per split: 2         
#> Minimum terminal node size:    5         
#> OOB prediction error (MSE):    0.0945    
#> R squared (OOB):               0.3438    
#> 
#> Tests for Heterogeneity: 
#>                                tau2   tau2_SE I^2     H^2    Q-test  df Q_p   
#> Raw effect sizes:              0.0553 0.0486  37.2642 1.5940 30.2857 19 0.0483
#> Residuals (after MetaForest):  0.0031 0.0312  3.2094  1.0332 19.6300 19 0.4171
#> 
#> 
#> Random intercept meta-analyses:
#>                                Intercept se     ci.lb   ci.ub   p     
#> Raw effect sizes:              -0.2136   0.0875 -0.3851 -0.0421 0.0147
#> Residuals (after MetaForest):  0.0298    0.0693 -0.1059 0.1656  0.6666
```
",2022-08-12
https://github.com/cjvanlissa/MetaForest_paper,"﻿# MetaForest_paper
Syntax and supplementary materials for the paper at DOI:10.17605/OSF.IO/MYG6S. If you use any of the syntax in this repository, please cite the validation paper: DOI:10.17605/OSF.IO/MYG6S.
",2022-08-12
https://github.com/cjvanlissa/meta_fathers,"To reference this work pending publication of the paper, please refer to the OSF project: https://doi.org/10.17605/OSF.IO/DZ39Y.

When using data or syntax from this repository, you must provide a reference, in concordance with the CC-By Attribution 4.0 International license.
",2022-08-12
https://github.com/cjvanlissa/missingnessTools,"# missingnessTools
Tools to deal with missing data problems in R
",2022-08-12
https://github.com/cjvanlissa/moral_politics,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'moral_politics.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                      | Description                      | Usage         
------------------------- | -------------------------------- | --------------
README.md                 | Description of project           | Human editable
moral_politics.Rproj      | Project file                     | Loads project 
LICENSE                   | User permissions                 | Read only     
.worcs                    | WORCS metadata YAML              | Read only     
prepare_data.R            | Script to process raw data       | Human editable
manuscript/manuscript.rmd | Source code for paper            | Human editable
manuscript/references.bib | BibTex references for manuscript | Human editable
renv.lock                 | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles,
read the preprint at https://osf.io/zcvbs/

## WORCS: Advice for authors

* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)

## WORCS: Advice for readers

Please refer to the vignette on [reproducing a WORCS project]() for step by step advice.
<!-- If your project deviates from the steps outlined in the vignette on     -->
<!-- reproducing a WORCS project, please provide your own advice for         -->
<!-- readers here.                                                           -->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->
",2022-08-12
https://github.com/cjvanlissa/mpib,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>


[![WORCS](https://img.shields.io/badge/WORCS-limited-orange)](https://osf.io/zcvbs/)

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'mpib.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                      | Description                      | Usage         
------------------------- | -------------------------------- | --------------
README.md                 | Description of project           | Human editable
mpib.Rproj                | Project file                     | Loads project 
LICENSE                   | User permissions                 | Read only     
.worcs                    | WORCS metadata YAML              | Read only     
preregistration.rmd       | Preregistered hypotheses         | Human editable
prepare_data.R            | Script to process raw data       | Human editable
manuscript/manuscript.rmd | Source code for paper            | Human editable
manuscript/references.bib | BibTex references for manuscript | Human editable
renv.lock                 | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

* To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles, read the preprint at https://osf.io/zcvbs/
* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)
* For a brief overview of the steps of the WORCS workflow, see below.

## WORCS: Steps to follow for a project

## Phase 1: Study design

1. Create a (Public or Private) remote repository on a 'Git' hosting service
2. When using R, initialize a new RStudio project using the WORCS template. Otherwise, clone the remote repository to your local project folder.
3. Add a README.md file, explaining how users should interact with the project, and a LICENSE to explain users' rights and limit your liability. The `worcs` project template does this automatically.
3. Optional: Preregister your analysis by committing a plain-text preregistration and tagging the commit as ""preregistration"".
4. Optional: Upload the preregistration to a dedicated preregistration server
5. Optional: Add study Materials to the repository

## Phase 2: Writing and analysis

6. Create an executable script documenting the code required to load the raw data into a tabular format, and de-identify human subjects if applicable
7. Save the data into a plain-text tabular format like `.csv`. When using open data, commit this file to 'Git'. When using closed data, commit a checksum of the file, and a synthetic copy of the data.
8. Write the manuscript using a dynamic document generation format, with code chunks to perform the analyses.
9. Commit every small change to the 'Git' repository
10. Cite essential references with `@`, and non-essential references with `@@`

## Phase 3: Submission and publication

11. Use dependency management to make the computational environment fully reproducible
12. Optional: Add a WORCS-badge to your project's README file
13. Make a Private 'Git' remote repository Public
14. Optional: [Create a project page on the Open Science Framework](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project)
15. [Connect your 'OSF' project page to the 'Git' remote repository](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an open science statement to the Abstract or Author notes, which links to the 'Git' remote repository or 'OSF' page
17. Render the dynamic document to PDF
18. Optional: [Publish the PDF as a preprint, and add it to the OSF project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
19. Submit the paper, and tag the release of the submitted paper, as in Step 3.

## Notes for cautious researchers

Some researchers might want to share their work only once the paper is accepted for publication. In this case, we recommend creating a ""Private"" repository in Step 1, and completing Steps 13-18 upon acceptance.
",2022-08-12
https://github.com/cjvanlissa/MSRC_LPA,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'MSRC_LPA.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                      | Description                      | Usage         
------------------------- | -------------------------------- | --------------
README.md                 | Description of project           | Human editable
MSRC_LPA.Rproj            | Project file                     | Loads project 
LICENSE                   | User permissions                 | Read only     
.worcs                    | WORCS metadata YAML              | Read only     
prepare_data.R            | Script to process raw data       | Human editable
manuscript/manuscript.rmd | Source code for paper            | Human editable
manuscript/references.bib | BibTex references for manuscript | Human editable
renv.lock                 | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles,
read the preprint at https://osf.io/zcvbs/

## WORCS: Advice for authors

* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)

## WORCS: Advice for readers

Please refer to the vignette on [reproducing a WORCS project]() for step by step advice.
<!-- If your project deviates from the steps outlined in the vignette on     -->
<!-- reproducing a WORCS project, please provide your own advice for         -->
<!-- readers here.                                                           -->


## Access to data

Some of the data used in this project are not publically available.
Synthetic data with similar characteristics to the original data have been provided. Using the function load_data() will load these synthetic data when the original data are unavailable. Note that these synthetic data cannot be used to reproduce the original results. However, it does allow users to run the code and, optionally, generate valid code that can be evaluated using the original data by the project authors.
Synthetic data with similar characteristics to the original data have been provided. Using the function load_data() will load these synthetic data when the original data are unavailable. Note that these synthetic data cannot be used to reproduce the original results. However, it does allow users to run the code and, optionally, generate valid code that can be evaluated using the original data by the project authors.
Synthetic data with similar characteristics to the original data have been provided. Using the function load_data() will load these synthetic data when the original data are unavailable. Note that these synthetic data cannot be used to reproduce the original results. However, it does allow users to run the code and, optionally, generate valid code that can be evaluated using the original data by the project authors.
Synthetic data with similar characteristics to the original data have been provided. Using the function load_data() will load these synthetic data when the original data are unavailable. Note that these synthetic data cannot be used to reproduce the original results. However, it does allow users to run the code and, optionally, generate valid code that can be evaluated using the original data by the project authors.
Synthetic data with similar characteristics to the original data have been provided. Using the function load_data() will load these synthetic data when the original data are unavailable. Note that these synthetic data cannot be used to reproduce the original results. However, it does allow users to run the code and, optionally, generate valid code that can be evaluated using the original data by the project authors.
Synthetic data with similar characteristics to the original data have been provided. Using the function load_data() will load these synthetic data when the original data are unavailable. Note that these synthetic data cannot be used to reproduce the original results. However, it does allow users to run the code and, optionally, generate valid code that can be evaluated using the original data by the project authors.
Synthetic data with similar characteristics to the original data have been provided. Using the function load_data() will load these synthetic data when the original data are unavailable. Note that these synthetic data cannot be used to reproduce the original results. However, it does allow users to run the code and, optionally, generate valid code that can be evaluated using the original data by the project authors.
Synthetic data with similar characteristics to the original data have been provided. Using the function load_data() will load these synthetic data when the original data are unavailable. Note that these synthetic data cannot be used to reproduce the original results. However, it does allow users to run the code and, optionally, generate valid code that can be evaluated using the original data by the project authors.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->
",2022-08-12
https://github.com/cjvanlissa/pema,"
# pema: Penalized Meta-Analysis <!--a href='https://osf.io/zcvbs/'><img src='https://github.com/cjvanlissa/pema/raw/master/docs/pema_icon.png' align=""right"" height=""139"" /></a-->

[![CRAN
status](https://www.r-pkg.org/badges/version/pema)](https://CRAN.R-project.org/package=pema)
[![CRAN RStudio mirror
downloads](https://cranlogs.r-pkg.org/badges/grand-total/pema?color=blue)](https://r-pkg.org/pkg/pema)
[![Lifecycle:
experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![R-CMD-check](https://github.com/cjvanlissa/pema/workflows/R-CMD-check/badge.svg)](https://github.com/cjvanlissa/pema/actions)
<!-- [![R-CMD-check](https://github.com/cjvanlissa/pema/workflows/R-CMD-check/badge.svg)](https://github.com/cjvanlissa/pema/actions) -->
<!-- [![codecov](https://codecov.io/gh/cjvanlissa/pema/branch/master/graph/badge.svg?token=7S9XKDRT4M)](https://codecov.io/gh/cjvanlissa/pema) -->
[![Contributor
Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://www.contributor-covenant.org/version/2/0/code_of_conduct.html)
<!-- [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3969/badge)](https://bestpractices.coreinfrastructure.org/projects/3969) -->
<!--[![DOI](http://joss.theoj.org/papers/10.21105/joss.00978/status.svg)](https://doi.org/10.21105/joss.00978)-->

Conduct *pe*nalized *m*eta-*a*nalysis (*“pema”*) In meta-analysis, there
are often between-study differences. These can be coded as moderator
variables, and controlled for using meta-regression. However, if the
number of moderators is large relative to the number of studies, such an
analysis may be overfitted. Penalized meta-regression is useful in these
cases, because it shrinks the regression slopes of irrelevant moderators
towards zero.

<!-- ## Where do I start? -->
<!-- For most users, the recommended starting point is to [read the paper](https://osf.io/zcvbs/), currently in press in [Data Science](https://content.iospress.com/journals/data-science/Pre-press/Pre-press), -->
<!-- which introduces the pema workflow, explains the underlying tools, and illustrates how the `pema` package can be used to create a new project that follows the workflow. -->

## Installing the package

Use [CRAN](https://CRAN.R-project.org/package=pema) to install the
latest release of `pema`:

    install.packages(""pema"")

Alternatively, use [R-universe](https://cjvanlissa.r-universe.dev) to
install the development version of `pema` by running the following code:

``` r
options(repos = c(
    cjvanlissa = 'https://cjvanlissa.r-universe.dev',
    CRAN = 'https://cloud.r-project.org'))

install.packages('pema')
```

## Citing pema

The `pema` validation paper is currently in preprint stage. The code and
results of the validation study are publicly available. You can cite
`pema` using the following citation (please use the same citation for
either the package, or the paper); consider updating that citation once
the preprint is published:

> Van Lissa, C. J., & van Erp, S. (2021, December 9). Select relevant
> moderators using Bayesian regularized meta-regression. Retrieved from
> psyarxiv.com/6phs5

## About this repository

This repository contains the source code for the R-package called
`pema`.

## Contributing and Contact Information

We are always eager to receive user feedback and contributions to help
us improve both the workflow and the software. Major contributions
warrant coauthorship to the package. Please contact the lead author at
<c.j.vanlissa@uu.nl>, or:

-   [File a GitHub issue](https://github.com/cjvanlissa/pema) for
    feedback, bug reports or feature requests
-   [Make a pull request](https://github.com/cjvanlissa/pema/pulls) to
    contribute your code or prose

By participating in this project, you agree to abide by the [Contributor
Code of Conduct v2.0](https://www.contributor-covenant.org/).
Contributions to the package must adhere to the [tidyverse style
guide](https://style.tidyverse.org/). When contributing code, please add
tests for that contribution to the `tests/testthat` folder, and ensure
that these tests pass in the [GitHub Actions
panel](https://github.com/cjvanlissa/pema/actions/workflows/R-CMD-check).
",2022-08-12
https://github.com/cjvanlissa/placebo_adhd,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'placebo_adhd.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                      | Description                      | Usage         
------------------------- | -------------------------------- | --------------
README.md                 | Description of project           | Human editable
placebo_adhd.Rproj        | Project file                     | Loads project 
LICENSE                   | User permissions                 | Read only     
.worcs                    | WORCS metadata YAML              | Read only     
prepare_data.R            | Script to process raw data       | Human editable
manuscript/manuscript.rmd | Source code for paper            | Human editable
manuscript/references.bib | BibTex references for manuscript | Human editable
renv.lock                 | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

* To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles, read the preprint at https://osf.io/zcvbs/
* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)
* For a brief overview of the steps of the WORCS workflow, see below.

## WORCS: Steps to follow for a project

## Phase 1: Study design

1. Create a (Public or Private) remote repository on a 'Git' hosting service
2. When using R, initialize a new RStudio project using the WORCS template. Otherwise, clone the remote repository to your local project folder.
3. Add a README.md file, explaining how users should interact with the project, and a LICENSE to explain users' rights and limit your liability. The `worcs` project template does this automatically.
3. Optional: Preregister your analysis by committing a plain-text preregistration and tagging the commit as ""preregistration"".
4. Optional: Upload the preregistration to a dedicated preregistration server
5. Optional: Add study Materials to the repository

## Phase 2: Writing and analysis

6. Create an executable script documenting the code required to load the raw data into a tabular format, and de-identify human subjects if applicable
7. Save the data into a plain-text tabular format like `.csv`. When using open data, commit this file to 'Git'. When using closed data, commit a checksum of the file, and a synthetic copy of the data.
8. Write the manuscript using a dynamic document generation format, with code chunks to perform the analyses.
9. Commit every small change to the 'Git' repository
10. Cite essential references with `@`, and non-essential references with `@@`

## Phase 3: Submission and publication

11. Use dependency management to make the computational environment fully reproducible
12. Optional: Add a WORCS-badge to your project's README file
13. Make a Private 'Git' remote repository Public
14. Optional: [Create a project page on the Open Science Framework](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project)
15. [Connect your 'OSF' project page to the 'Git' remote repository](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an open science statement to the Abstract or Author notes, which links to the 'Git' remote repository or 'OSF' page
17. Render the dynamic document to PDF
18. Optional: [Publish the PDF as a preprint, and add it to the OSF project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
19. Submit the paper, and tag the release of the submitted paper, as in Step 3.

## Notes for cautious researchers

Some researchers might want to share their work only once the paper is accepted for publication. In this case, we recommend creating a ""Private"" repository in Step 1, and completing Steps 13-18 upon acceptance.
",2022-08-12
https://github.com/cjvanlissa/psycorona_trust,"# A WORCS Project

This project is based on the Workflow for Open Reproducible Code in Science (WORCS). For more details, please read...

# WORCS - steps to follow for each project

## Study design phase

1. Create a new Private repository on github, copy the https:// link to clipboard  
  The link should look something like https://github.com/yourname/yourrepo.git
2. In Rstudio, click File > New Project > New directory > WORCS Project Template
    a. Paste the GitHub Repository address in the textbox
    b. Keep the checkbox for `renv` checked if you want to document all dependencies (recommended)
    c. Select a preregistration template
3. Write the preregistration `.Rmd`
4. In the top-right corner of Rstudio, select the Git tab, select the checkboxes next to all files, and click the Commit button. Write an informative message for the commit, e.g., ""Preregistration"", again click Commit, and then click the green Push arrow to send your commit to GitHub
5. Go to the GitHub repository for this project, and tag the Commit as a preregistration
6. Optional: Render the preregistration to PDF, and upload it to AsPredicted.org or OSF.io as an attachment
7. Optional: Add study Materials (to which you own the rights) to the repository. It is possible to solicit feedback (by opening a GitHub Issue) and acknowledge outside contributions (by accepting Pull requests)

## Data analysis phase

8. Read the data into R, and document this procedure in `prepare_data.R`
9. Use `open_data()` or `closed_data()` to store the data
10. Write the manuscript in `Manuscript.Rmd`, using code chunks to perform the analyses.
11. Regularly commit your progress to the Git repository; ideally, after completing each small and clearly defined task. Use informative commit messages. Push the commits to GitHub.
12. Cite essential references with one at-symbol (`[@essentialref2020]`), and non-essential references with a double at-symbol (`[@@nonessential2020]`).

## Submission phase

13. To save the state of the project library (all packages used), call `renv::snapshot()`. This updates the lockfile, `renv.lock`.
14. To render the paper with essential citations only for submission, change the line `knit: worcs::cite_all` to `knit: worcs::cite_essential`. Then, press the Knit button to generate a PDF

## Publication phase

13. Make the GitHub repository public
14. [Create an OSF project](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project); although you may have already done this in Step 6.
15. [Connect your GitHub repository to the OSF project](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an Open Science statement to the manuscript, with a link to the OSF project
17. Optional: [Publish preprint in a not-for-profit preprint repository such as PsyArchiv, and connect it to your existing OSF project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
    + Check [Sherpa Romeo](http://sherpa.ac.uk/romeo/index.php) to be sure that your intended outlet allows the publication of preprints; many journals do, nowadays - and if they do not, it is worth considering other outlets.

",2022-08-12
https://github.com/cjvanlissa/S20,"# Utrecht University Summer School S20 and S23 
This repository contains exercises for day 4 of the Summer School S20 (Introduction to Structural Equation Modeling using Mplus), and day 4 of the Summer School S23 (Advanced Course on using Mplus). These summer school courses are organized by Utrecht University (the Netherlands). You can find these exercises in a GitBook at jeroendmulder.github.io/MplusSummerSchool. 
",2022-08-12
https://github.com/cjvanlissa/schumpe,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'schumpe.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                      | Description                      | Usage         
------------------------- | -------------------------------- | --------------
README.md                 | Description of project           | Human editable
schumpe.Rproj             | Project file                     | Loads project 
LICENSE                   | User permissions                 | Read only     
.worcs                    | WORCS metadata YAML              | Read only     
prepare_data.R            | Script to process raw data       | Human editable
manuscript/manuscript.rmd | Source code for paper            | Human editable
manuscript/references.bib | BibTex references for manuscript | Human editable
renv.lock                 | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

* To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles, read the preprint at https://osf.io/zcvbs/
* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)
* For a brief overview of the steps of the WORCS workflow, see below.

## WORCS: Steps to follow for a project

## Phase 1: Study design

1. Create a (Public or Private) remote repository on a 'Git' hosting service
2. When using R, initialize a new RStudio project using the WORCS template. Otherwise, clone the remote repository to your local project folder.
3. Add a README.md file, explaining how users should interact with the project, and a LICENSE to explain users' rights and limit your liability. The `worcs` project template does this automatically.
3. Optional: Preregister your analysis by committing a plain-text preregistration and tagging the commit as ""preregistration"".
4. Optional: Upload the preregistration to a dedicated preregistration server
5. Optional: Add study Materials to the repository

## Phase 2: Writing and analysis

6. Create an executable script documenting the code required to load the raw data into a tabular format, and de-identify human subjects if applicable
7. Save the data into a plain-text tabular format like `.csv`. When using open data, commit this file to 'Git'. When using closed data, commit a checksum of the file, and a synthetic copy of the data.
8. Write the manuscript using a dynamic document generation format, with code chunks to perform the analyses.
9. Commit every small change to the 'Git' repository
10. Cite essential references with `@`, and non-essential references with `@@`

## Phase 3: Submission and publication

11. Use dependency management to make the computational environment fully reproducible
12. Optional: Add a WORCS-badge to your project's README file
13. Make a Private 'Git' remote repository Public
14. Optional: [Create a project page on the Open Science Framework](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project)
15. [Connect your 'OSF' project page to the 'Git' remote repository](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an open science statement to the Abstract or Author notes, which links to the 'Git' remote repository or 'OSF' page
17. Render the dynamic document to PDF
18. Optional: [Publish the PDF as a preprint, and add it to the OSF project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
19. Submit the paper, and tag the release of the submitted paper, as in Step 3.

## Notes for cautious researchers

Some researchers might want to share their work only once the paper is accepted for publication. In this case, we recommend creating a ""Private"" repository in Step 1, and completing Steps 13-18 upon acceptance.
",2022-08-12
https://github.com/cjvanlissa/shiny_errata,"# Emotion Regulation Risk Assessment Tool for Adolescents (ERRATA)

This application, including its constituent parts (e.g., model objects), is provided under an MIT License with a Commons Clause to restrict commercial use.

Copyright (c) 2022 Caspar J. Van Lissa

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

“Commons Clause” License Condition v1.0

The Software is provided to you by the Licensor under the License, as defined below, subject to the following condition.

Without limiting other conditions in the License, the grant of rights under the License will not include, and the License does not grant to you, the right to Sell the Software.

For purposes of the foregoing, “Sell” means practicing any or all of the rights granted to you under the License to provide to third parties, for a fee or other consideration (including without limitation fees for hosting or consulting/ support services related to the Software), a product or service whose value derives, entirely or substantially, from the functionality of the Software. Any license notice or attribution required by the License must also include this Commons Clause License Condition notice.

Software: Emotion Regulation Risk Assessment Tool for Adolescents (ERRATA)

License: MIT

Licensor: Caspar J. Van Lissa
",2022-08-12
https://github.com/cjvanlissa/sra_demo,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'sra_demo.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                      | Description                      | Usage         
------------------------- | -------------------------------- | --------------
README.md                 | Description of project           | Human editable
sra_demo.Rproj            | Project file                     | Loads project 
LICENSE                   | User permissions                 | Read only     
.worcs                    | WORCS metadata YAML              | Read only     
preregistration.rmd       | Preregistered hypotheses         | Human editable
prepare_data.R            | Script to process raw data       | Human editable
manuscript/manuscript.rmd | Source code for paper            | Human editable
manuscript/references.bib | BibTex references for manuscript | Human editable
renv.lock                 | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles,
read the preprint at https://osf.io/zcvbs/

## WORCS: Advice for authors

* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)

## WORCS: Advice for readers

Please refer to the vignette on [reproducing a WORCS project]() for step by step advice.
<!-- If your project deviates from the steps outlined in the vignette on     -->
<!-- reproducing a WORCS project, please provide your own advice for         -->
<!-- readers here.                                                           -->


## Access to data

Some of the data used in this project are not publically available.
Synthetic data with similar characteristics to the original data have been provided. Using the function load_data() will load these synthetic data when the original data are unavailable. Note that these synthetic data cannot be used to reproduce the original results. However, it does allow users to run the code and, optionally, generate valid code that can be evaluated using the original data by the project authors.
Synthetic data with similar characteristics to the original data have been provided. Using the function load_data() will load these synthetic data when the original data are unavailable. Note that these synthetic data cannot be used to reproduce the original results. However, it does allow users to run the code and, optionally, generate valid code that can be evaluated using the original data by the project authors.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->
",2022-08-12
https://github.com/cjvanlissa/storm_meta_father_mother,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_badge.png' align=""right"" height=""139"" /></a>

[![WORCS](https://img.shields.io/badge/WORCS-open%20science-brightgreen)](https://osf.io/zcvbs/)

This repository contains the data and source code for the manuscript *""A meta-analysis structural equation model of Mothers’ and Fathers’ unique contributions Parenting Behavior in Association to Children’s Prosocial Behavior: A Meta-Analysis""*, by Van der Storm, Van Lissa, Lucassen, Helmerhorst, and Keizer, [DOI:10.17605/OSF.IO/MHX8E](https://osf.io/MHX8E).

## Where do I start?

You can load this project in Rstudio by opening the file called storm.Rproj

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                      | Description                      | Usage         
------------------------- | -------------------------------- | --------------
README.md                 | Description of project           | Human editable
storm.Rproj               | Project file                     | Loads project 
LICENSE                   | User permissions                 | Read only     
.worcs                    | WORCS metadata YAML              | Read only     
preregistration.rmd       | Preregistered hypotheses         | Human editable
prepare_data.R            | Script to process raw data       | Human editable
manuscript/manuscript.rmd | Source code for paper            | Human editable
manuscript/references.bib | BibTex references for manuscript | Human editable
renv.lock                 | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

* To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles, read the preprint at https://osf.io/zcvbs/
* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)
* For a brief overview of the steps of the WORCS workflow, see below.

## WORCS: Steps to follow for a project

## Phase 1: Study design

1. Create a new (Public or Private) repository on 'GitHub'
2. Create a new RStudio project using the WORCS template
3. Optional: Preregister your analysis
4. Optional: Upload preregistration to another repository
5. Optional: Add study Materials to the repository

## Phase 2: Data analysis

6. Load the raw data
7. Save the data using `open_data()` or `closed_data()`. Never commit data to 'Git' that you do not intend to share
8. Write the manuscript in `manuscript.Rmd`, using code chunks to perform the analyses.
9. Commit every small change
10. Cite essential references with `@`, and non-essential references with `@@`

## Phase 3: Submission/publication

11. Store the R environment by calling `renv::snapshot()`
12. Optional: Add a WORCS-badge to your README file and complete the optional elements of the WORCS checklist
13. Make the Private 'GitHub' repository Public
14. [Create a project page on the Open Science Framework](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project)
15. Connect your 'OSF' project page to the 'GitHub' repository](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an open science statement to the Abstract or Author notes, which links to the 'GitHub' repository or 'OSF' page
17. Knit the paper to PDF
18. Optional: Publish a preprint
19. Submit the paper, and tag the release of the submitted paper as in Step 3.

## Notes for cautious researchers

Some researchers might want to share their work only once the paper is accepted for publication. In this case, we recommend creating a ""Private"" repository in Step 1, and completing Steps 13-18 upon acceptance.
",2022-08-12
https://github.com/cjvanlissa/teaching,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# tidySEM <a href='https://github.com/cjvanlissa/tidySEM'><img src='https://github.com/cjvanlissa/tidySEM/raw/master/docs/badge.png' align=""right"" height=""139"" /></a>

<!--[![CRAN status](https://www.r-pkg.org/badges/version/tidySEM)](https://cran.r-project.org/package=tidySEM)
[![](https://cranlogs.r-pkg.org/badges/tidySEM)](https://cran.r-project.org/package=tidySEM)-->

[![lifecycle](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://www.tidyverse.org/lifecycle/#maturing)
[![Travis build
status](https://travis-ci.org/cjvanlissa/tidySEM.svg?branch=master)](https://travis-ci.org/cjvanlissa/tidySEM)
<!--[![Codecov test coverage](https://codecov.io/gh/cjvanlissa/tidySEM/branch/master/graph/badge.svg)](https://codecov.io/gh/cjvanlissa/tidySEM?branch=master)
<!--[![DOI](http://joss.theoj.org/papers/10.21105/joss.00978/status.svg)](https://doi.org/10.21105/joss.00978)-->

The package `tidySEM` provides a ‘tidy’ workflow for conducting,
reporting, and plotting structural equation modeling analyses. It does
not perform model estimation, but instead allows users to estimate
models in a software-agnostic way, using either the R package `lavaan`,
or the commercial stand-alone program `Mplus` (through
`MplusAutomation`). The aim of `tidySEM` is to provide three specific
functions:

1.  Generate model syntax in a top-down, tidy way,
2.  Tabulate model output in a publication-ready, uniform manner,
3.  Make easily customizable graphs for SEM-models.

These functions are designed with the [*tidy tools manifesto* (Wickham,
last updated
23-11-2019)](https://tidyverse.tidyverse.org/articles/manifesto.html) in
mind, and interface with the existing suite of packages in the
[`tidyverse`](https://tidyverse.tidyverse.org/).

## Installation

<!--You can install tidySEM from CRAN with:


```r
install.packages(""tidySEM"")
```
-->

You can install the development version of `tidySEM` from ‘GitHub’ with:

``` r
install.packages(""devtools"")
devtools::install_github(""cjvanlissa/tidySEM"")
```

## Documentation

Every user-facing function in the package is documented, and the
documentation can be accessed by running `?function_name` in the R
console, e.g., `?graph_sem`.

Furthermore, there are three main vignettes, describing the three main
tracks of `tidySEM` functions:

1.  A [vignette about generating syntax and estimating
    models](https://cjvanlissa.github.io/tidySEM/articles/Generating_syntax.html)
2.  A [vignette about tabulating
    results](https://cjvanlissa.github.io/tidySEM/articles/Tabulating_results.html)
3.  A [vignette about making
    graphs](https://cjvanlissa.github.io/tidySEM/articles/Plotting_graphs.html)
    1)  An additional vignette describes the [graphing
        conventions](https://cjvanlissa.github.io/tidySEM/articles/sem_graph.html)
        for structural equation models.

## Citing tidySEM

You can cite the R-package with the following citation:

Van Lissa, C. J., (2019). *tidySEM: A tidy workflow for running,
reporting, and plotting structural equation models in lavaan or Mplus.*
\[R package\]. <https://github.com/cjvanlissa/tidySEM/>

## Contributing and Contact Information

If you have ideas, please get involved. I am currently looking for a
major contributor on this project, and smaller contributions are also
welcome. You can contribute by opening an issue on ‘GitHub’, or sending
a pull request with proposed features.

  - File a ‘GitHub’ issue [here](https://github.com/cjvanlissa/tidySEM)
  - Make a pull request
    [here](https://github.com/cjvanlissa/tidySEM/pulls)

By participating in this project, you agree to abide by the [Contributor
Code of Conduct v2.0](https://www.contributor-covenant.org/).
",2022-08-12
https://github.com/cjvanlissa/test-gitbook,"This is an example of a GitBook for teaching, generated using 
R Markdown and **bookdown** (https://github.com/rstudio/bookdown). 

The example
is based on the minimal bookdown example by Yihui Xie, available at
https://bookdown.org/yihui/bookdown-demo.
Please see the page ""[Get Started](https://bookdown.org/yihui/bookdown/get-started.html)"" at https://bookdown.org/yihui/bookdown/ for how to compile this example into HTML. You may generate a copy of the book in `bookdown::pdf_book` format by calling `bookdown::render_book('index.Rmd', 'bookdown::pdf_book')`. More detailed instructions are available here https://bookdown.org/yihui/bookdown/build-the-book.html.

You can find the preview of this example at https://cjvanlissa.github.io/gitbook-demo/.
",2022-08-12
https://github.com/cjvanlissa/test_gert,# test_gert,2022-08-12
https://github.com/cjvanlissa/test_https,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'tmp_https.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File            | Description                | Usage         
--------------- | -------------------------- | --------------
README.md       | Description of project     | Human editable
tmp_https.Rproj | Project file               | Loads project 
LICENSE         | User permissions           | Read only     
.worcs          | WORCS metadata YAML        | Read only     
prepare_data.R  | Script to process raw data | Human editable

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

* To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles, read the preprint at https://osf.io/zcvbs/
* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)
* For a brief overview of the steps of the WORCS workflow, see below.

## WORCS: Steps to follow for a project

## Phase 1: Study design

1. Create a (Public or Private) remote repository on a 'Git' hosting service
2. When using R, initialize a new RStudio project using the WORCS template. Otherwise, clone the remote repository to your local project folder.
3. Add a README.md file, explaining how users should interact with the project, and a LICENSE to explain users' rights and limit your liability. The `worcs` project template does this automatically.
3. Optional: Preregister your analysis by committing a plain-text preregistration and tagging the commit as ""preregistration"".
4. Optional: Upload the preregistration to a dedicated preregistration server
5. Optional: Add study Materials to the repository

## Phase 2: Writing and analysis

6. Create an executable script documenting the code required to load the raw data into a tabular format, and de-identify human subjects if applicable
7. Save the data into a plain-text tabular format like `.csv`. When using open data, commit this file to 'Git'. When using closed data, commit a checksum of the file, and a synthetic copy of the data.
8. Write the manuscript using a dynamic document generation format, with code chunks to perform the analyses.
9. Commit every small change to the 'Git' repository
10. Cite essential references with `@`, and non-essential references with `@@`

## Phase 3: Submission and publication

11. Use dependency management to make the computational environment fully reproducible
12. Optional: Add a WORCS-badge to your project's README file
13. Make a Private 'Git' remote repository Public
14. Optional: [Create a project page on the Open Science Framework](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project)
15. [Connect your 'OSF' project page to the 'Git' remote repository](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an open science statement to the Abstract or Author notes, which links to the 'Git' remote repository or 'OSF' page
17. Render the dynamic document to PDF
18. Optional: [Publish the PDF as a preprint, and add it to the OSF project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
19. Submit the paper, and tag the release of the submitted paper, as in Step 3.

## Notes for cautious researchers

Some researchers might want to share their work only once the paper is accepted for publication. In this case, we recommend creating a ""Private"" repository in Step 1, and completing Steps 13-18 upon acceptance.
",2022-08-12
https://github.com/cjvanlissa/test_ssh,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'test_ssh.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File           | Description                | Usage         
-------------- | -------------------------- | --------------
README.md      | Description of project     | Human editable
test_ssh.Rproj | Project file               | Loads project 
LICENSE        | User permissions           | Read only     
.worcs         | WORCS metadata YAML        | Read only     
prepare_data.R | Script to process raw data | Human editable

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

* To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles, read the preprint at https://osf.io/zcvbs/
* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)
* For a brief overview of the steps of the WORCS workflow, see below.

## WORCS: Steps to follow for a project

## Phase 1: Study design

1. Create a (Public or Private) remote repository on a 'Git' hosting service
2. When using R, initialize a new RStudio project using the WORCS template. Otherwise, clone the remote repository to your local project folder.
3. Add a README.md file, explaining how users should interact with the project, and a LICENSE to explain users' rights and limit your liability. The `worcs` project template does this automatically.
3. Optional: Preregister your analysis by committing a plain-text preregistration and tagging the commit as ""preregistration"".
4. Optional: Upload the preregistration to a dedicated preregistration server
5. Optional: Add study Materials to the repository

## Phase 2: Writing and analysis

6. Create an executable script documenting the code required to load the raw data into a tabular format, and de-identify human subjects if applicable
7. Save the data into a plain-text tabular format like `.csv`. When using open data, commit this file to 'Git'. When using closed data, commit a checksum of the file, and a synthetic copy of the data.
8. Write the manuscript using a dynamic document generation format, with code chunks to perform the analyses.
9. Commit every small change to the 'Git' repository
10. Cite essential references with `@`, and non-essential references with `@@`

## Phase 3: Submission and publication

11. Use dependency management to make the computational environment fully reproducible
12. Optional: Add a WORCS-badge to your project's README file
13. Make a Private 'Git' remote repository Public
14. Optional: [Create a project page on the Open Science Framework](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project)
15. [Connect your 'OSF' project page to the 'Git' remote repository](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an open science statement to the Abstract or Author notes, which links to the 'Git' remote repository or 'OSF' page
17. Render the dynamic document to PDF
18. Optional: [Publish the PDF as a preprint, and add it to the OSF project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
19. Submit the paper, and tag the release of the submitted paper, as in Step 3.

## Notes for cautious researchers

Some researchers might want to share their work only once the paper is accepted for publication. In this case, we recommend creating a ""Private"" repository in Step 1, and completing Steps 13-18 upon acceptance.
",2022-08-12
https://github.com/cjvanlissa/tidySEM,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# tidySEM <a href='https://github.com/cjvanlissa/tidySEM'><img src='https://github.com/cjvanlissa/tidySEM/raw/master/docs/badge.png' align=""right"" height=""139"" /></a>

[![CRAN
status](https://www.r-pkg.org/badges/version/tidySEM)](https://cran.r-project.org/package=tidySEM)
[![CRAN RStudio mirror
downloads](https://cranlogs.r-pkg.org/badges/grand-total/tidySEM?color=blue)](https://r-pkg.org/pkg/tidySEM)
[![lifecycle](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://lifecycle.r-lib.org/articles/stages.html#maturing)
[![R-CMD-check](https://github.com/cjvanlissa/tidySEM/workflows/R-CMD-check/badge.svg)](https://github.com/cjvanlissa/tidySEM/actions)
[![codecov](https://codecov.io/gh/cjvanlissa/tidySEM/branch/master/graph/badge.svg?token=0GfxUZIC9r)](https://app.codecov.io/gh/cjvanlissa/tidySEM)
[![Contributor
Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](https://www.contributor-covenant.org/version/2/0/code_of_conduct.html)
<!--[![Code of Merit](https://img.shields.io/badge/Code%20%20of%20Merit-adopted-ff69b4.svg)](https://codeofmerit.org/CODE_OF_CONDUCT.md)-->

The package `tidySEM` provides a ‘tidy’ workflow for conducting,
reporting, and plotting structural equation modeling analyses. It does
not perform model estimation, but instead allows users to estimate
models in a software-agnostic way, using either the free open source R
packages `lavaan` or `OpenMx`, or the commercial closed-source program
`Mplus` (controlled through the R package `MplusAutomation`). The aim of
`tidySEM` is to provide three specific functions:

1.  Generate model syntax in a top-down, tidy way,
    -   With particular attention to specifying mixture models in
        `OpenMx`
2.  Tabulate model output in a publication-ready, uniform manner,
3.  Make easily customizable graphs for SEM-models.

These functions are designed with the [*tidy tools manifesto* (Wickham,
last updated
23-11-2019)](https://tidyverse.tidyverse.org/articles/manifesto.html) in
mind, and interface with the existing suite of packages in the
[`tidyverse`](https://tidyverse.tidyverse.org/).

## Installation

<!--You can install tidySEM from CRAN with:


```r
install.packages(""tidySEM"")
```
-->

You can install the development version of `tidySEM` from ‘GitHub’ with:

``` r
install.packages(""devtools"")
devtools::install_github(""cjvanlissa/tidySEM"")
```

## Documentation

Every user-facing function in the package is documented, and the
documentation can be accessed by running `?function_name` in the R
console, e.g., `?graph_sem`.

Furthermore, there are three main vignettes, describing the three main
tracks of `tidySEM` functions:

1.  A [vignette about generating syntax and estimating
    models](https://cjvanlissa.github.io/tidySEM/articles/Generating_syntax.html)
2.  A [vignette about tabulating
    results](https://cjvanlissa.github.io/tidySEM/articles/Tabulating_results.html)
3.  A [vignette about making
    graphs](https://cjvanlissa.github.io/tidySEM/articles/Plotting_graphs.html)
    -   An additional vignette describes the [graphing
        conventions](https://cjvanlissa.github.io/tidySEM/articles/sem_graph.html)
        for structural equation models.

## Citing tidySEM

You can cite the R-package with the following citation:

> Van Lissa, C. J., (2019). *tidySEM: Tidy structural equation
> modeling.* R package version 0.2.1.
> <https://github.com/cjvanlissa/tidySEM/>

## Contributing and Contact Information

If you have ideas, please get involved. You can contribute by opening an
issue on ‘GitHub’, or sending a pull request with proposed features (see
further instructions below).

-   File a ‘GitHub’ issue [here](https://github.com/cjvanlissa/tidySEM)
-   Make a pull request
    [here](https://github.com/cjvanlissa/tidySEM/pulls)

By participating in this project, you agree to abide by the [Contributor
Covenant](https://www.contributor-covenant.org/version/2/0/code_of_conduct.html).

### Pull requests

-   Please always contribute via pull request instead of committing to
    master, so I can review the contribution.
    -   You can either fork the repository to your own account and then
        submit a pull request, or make minor changes directly on this
        repository, and choose to commit them to a different branch and
        send a pull request when you save those changes
-   If you fix an issue, in your Commit message, please write “closes
    #issuenumber” so we can trace which commits fixed which issues
-   Please add a line to NEWS.md for each bug fix or feature
    contribution
-   Please increment the minor version number in DESCRIPTION, e.g.:
    0.2.3.1 -> 0.2.3.2
",2022-08-12
https://github.com/cjvanlissa/tmp,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_badge.png' align=""right"" height=""139"" /></a>



[![WORCS](https://img.shields.io/badge/WORCS-limited-orange)](https://osf.io/zcvbs/)


<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in Rstudio by opening the file called 'test_new.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                      | Description                      | Usage         
------------------------- | -------------------------------- | --------------
README.md                 | Description of project           | Human editable
test2.Rproj               | Project file                     | Loads project 
LICENSE                   | User permissions                 | Read only     
.worcs                    | WORCS metadata YAML              | Read only     
preregistration.rmd       | Preregistered hypotheses         | Human editable
prepare_data.R            | Script to process raw data       | Human editable
manuscript/manuscript.rmd | Source code for paper            | Human editable
manuscript/references.bib | BibTex references for manuscript | Human editable
renv.lock                 | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. For more details,
please read the preprint at https://osf.io/zcvbs/

## WORCS: Steps to follow for a project

### Study design phase

1. Create a new Private repository on github, copy the https:// link to clipboard  
  The link should look something like https://github.com/yourname/yourrepo.git
2. In Rstudio, click File > New Project > New directory > WORCS Project Template
    a. Paste the GitHub Repository address in the textbox
    b. Keep the checkbox for `renv` checked if you want to document all dependencies (recommended)
    c. Select a preregistration template
3. Write the preregistration `.Rmd`
4. In the top-right corner of Rstudio, select the Git tab, select the checkboxes next to all files, and click the Commit button. Write an informative message for the commit, e.g., ""Preregistration"", again click Commit, and then click the green Push arrow to send your commit to GitHub
5. Go to the GitHub repository for this project, and tag the Commit as a preregistration
6. Optional: Render the preregistration to PDF, and upload it to AsPredicted.org or OSF.io as an attachment
7. Optional: Add study Materials (to which you own the rights) to the repository. It is possible to solicit feedback (by opening a GitHub Issue) and acknowledge outside contributions (by accepting Pull requests)

### Data analysis phase

8. Read the data into R, and document this procedure in `prepare_data.R`
9. Use `open_data()` or `closed_data()` to store the data
10. Write the manuscript in `Manuscript.Rmd`, using code chunks to perform the analyses.
11. Regularly commit your progress to the Git repository; ideally, after completing each small and clearly defined task. Use informative commit messages. Push the commits to GitHub.
12. Cite essential references with one at-symbol (`[@essentialref2020]`), and non-essential references with a double at-symbol (`[@@nonessential2020]`).

### Submission phase

13. To save the state of the project library (all packages used), call `renv::snapshot()`. This updates the lockfile, `renv.lock`.
14. To render the paper with essential citations only for submission, change the line `knit: worcs::cite_all` to `knit: worcs::cite_essential`. Then, press the Knit button to generate a PDF

### Publication phase

13. Make the GitHub repository public
14. [Create an OSF project](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project); although you may have already done this in Step 6.
15. [Connect your GitHub repository to the OSF project](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an Open Science statement to the manuscript, with a link to the OSF project
17. Optional: [Publish preprint in a not-for-profit preprint repository such as PsyArchiv, and connect it to your existing OSF project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
    + Check [Sherpa Romeo](http://sherpa.ac.uk/romeo/index.php) to be sure that your intended outlet allows the publication of preprints; many journals do, nowadays - and if they do not, it is worth considering other outlets.
",2022-08-12
https://github.com/cjvanlissa/tmp_lakens,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in Rstudio by opening the file called 

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                      | Description                      | Usage         
------------------------- | -------------------------------- | --------------
README.md                 | Description of project           | Human editable
tmp_lakens.Rproj          | Project file                     | Loads project 
LICENSE                   | User permissions                 | Read only     
.worcs                    | WORCS metadata YAML              | Read only     
preregistration.rmd       | Preregistered hypotheses         | Human editable
prepare_data.R            | Script to process raw data       | Human editable
manuscript/manuscript.rmd | Source code for paper            | Human editable
manuscript/references.bib | BibTex references for manuscript | Human editable
renv.lock                 | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

* To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles, read the preprint at https://osf.io/zcvbs/
* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)
* For a brief overview of the steps of the WORCS workflow, see below.

## WORCS: Steps to follow for a project

## Phase 1: Study design

1. Create a (Public or Private) remote repository on a 'Git' hosting service
2. When using R, initialize a new RStudio project using the WORCS template. Otherwise, clone the remote repository to your local project folder.
3. Add a README.md file, explaining how users should interact with the project, and a LICENSE to explain users' rights and limit your liability. The `worcs` project template does this automatically.
3. Optional: Preregister your analysis by committing a plain-text preregistration and tagging the commit as ""preregistration"".
4. Optional: Upload the preregistration to a dedicated preregistration server
5. Optional: Add study Materials to the repository

## Phase 2: Writing and analysis

6. Create an executable script documenting the code required to load the raw data into a tabular format, and de-identify human subjects if applicable
7. Save the data into a plain-text tabular format like `.csv`. When using open data, commit this file to 'Git'. When using closed data, commit a checksum of the file, and a synthetic copy of the data.
8. Write the manuscript using a dynamic document generation format, with code chunks to perform the analyses.
9. Commit every small change to the 'Git' repository
10. Cite essential references with `@`, and non-essential references with `@@`

## Phase 3: Submission and publication

11. Use dependency management to make the computational environment fully reproducible
12. Optional: Add a WORCS-badge to your project's README file
13. Make a Private 'Git' remote repository Public
14. Optional: [Create a project page on the Open Science Framework](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project)
15. [Connect your 'OSF' project page to the 'Git' remote repository](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an open science statement to the Abstract or Author notes, which links to the 'Git' remote repository or 'OSF' page
17. Render the dynamic document to PDF
18. Optional: [Publish the PDF as a preprint, and add it to the OSF project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
19. Submit the paper, and tag the release of the submitted paper, as in Step 3.

## Notes for cautious researchers

Some researchers might want to share their work only once the paper is accepted for publication. In this case, we recommend creating a ""Private"" repository in Step 1, and completing Steps 13-18 upon acceptance.
",2022-08-12
https://github.com/cjvanlissa/veni_sysrev,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

This repository contains the source code for the paper *""Mapping Correlates of Adolescent Emotion Dysregulation: A Text-mining Systematic Review""*.

## Where do I start?

You can load this project in RStudio by opening the file called `veni_sysrev.Rproj`.
**NOTE:** To reproduce the published analyses, use the command `rmarkdown::render(""manuscript.Rmd"")`.
One of the dependencies causes the code to break when trying to Knit the document using the ""Knit"" button in RStudio.
Raw data are contained in the `recs/` folder.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                 | Description                | Usage         
-------------------- | -------------------------- | --------------
README.md            | Description of project     | Human editable
veni_sysrev.Rproj    | Project file               | Loads project 
LICENSE              | User permissions           | Read only     
manuscript.Rmd       | Fully reproducible manuscript | Human editable
.worcs               | WORCS metadata YAML        | Read only     
preregistration.rmd  | Preregistered hypotheses   | Human editable
prepare_data.R       | Script to process raw data | Human editable
renv.lock            | Reproducible R environment | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

To reproduce the published analyses, use the command `rmarkdown::render(""manuscript.Rmd"")`.
One of the dependencies causes the code to break when trying to Knit the document using the ""Knit"" button in RStudio.

To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles,
read the preprint at https://osf.io/zcvbs/

## WORCS: Advice for authors

* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)

## WORCS: Advice for readers

Please refer to the vignette on [reproducing a WORCS project]() for step by step advice.
<!-- If your project deviates from the steps outlined in the vignette on     -->
<!-- reproducing a WORCS project, please provide your own advice for         -->
<!-- readers here.                                                           -->
",2022-08-12
https://github.com/cjvanlissa/worcs,"
# WORCS <a href='https://osf.io/zcvbs/'><img src='https://github.com/cjvanlissa/worcs/raw/master/docs/worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- README.md is generated from README.Rmd. Please edit that file -->

[![CRAN
status](https://www.r-pkg.org/badges/version/worcs)](https://cran.r-project.org/package=worcs)
[![CRAN RStudio mirror
downloads](https://cranlogs.r-pkg.org/badges/grand-total/worcs?color=blue)](https://r-pkg.org/pkg/worcs)
[![lifecycle](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://www.tidyverse.org/lifecycle/#maturing)
[![R-CMD-check](https://github.com/cjvanlissa/worcs/workflows/R-CMD-check/badge.svg)](https://github.com/cjvanlissa/worcs/actions)
[![codecov](https://codecov.io/gh/cjvanlissa/worcs/branch/master/graph/badge.svg?token=7S9XKDRT4M)](https://codecov.io/gh/cjvanlissa/worcs)
[![Contributor
Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg)](code_of_conduct.md)
[![CII Best
Practices](https://bestpractices.coreinfrastructure.org/projects/3969/badge)](https://bestpractices.coreinfrastructure.org/projects/3969)
<!--[![DOI](http://joss.theoj.org/papers/10.21105/joss.00978/status.svg)](https://doi.org/10.21105/joss.00978)-->

The Workflow for Open Reproducible Code in Science (WORCS) is an easy to
adopt approach to ensuring a research project meets the requirements of
Open Science from the start. It is based on a “good enough” philosophy,
prioritizing user-friendliness over exhaustiveness. It can be used
either in absence of, or in parallel to, existing requirements for Open
workflows. It can also be enhanced with more elaborate solutions for
specific issues.

## Where do I start?

For most users, the recommended starting point is to [read the paper,
published in Data
Science](https://content.iospress.com/articles/data-science/ds210031),
which introduces the WORCS workflow, explains the underlying tools, and
illustrates how the `worcs` package can be used to create a new project
that follows the workflow.

The workflow is illustrated below; the [workflow
vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)
describes each step in detail.

![](https://github.com/cjvanlissa/worcs/raw/master/paper/workflow_graph/workflow.png)<!-- -->

## Installing the package

Before installing the package, please read [this
vignette](https://cjvanlissa.github.io/worcs/articles/setup.html), which
explains how to set up your computer for `worcs` (using the CRAN
version).

If you know what you’re doing and you wish to install the development
version of the `worcs` package from GitHub instead, you can use:

``` r
if(!requireNamespace(""remotes"")) install.packages(""remotes"")
remotes::install_github(""cjvanlissa/worcs"", dependencies = TRUE, update = ""never"")
```

## Citing WORCS

You can cite WORCS using the following citation (please use the same
citation for either the package, or the paper):

> Van Lissa, C. J., Brandmaier, A. M., Brinkman, L., Lamprecht, A.,
> Peikert, A., , Struiksma, M. E., & Vreede, B. (2021). WORCS: A
> Workflow for Open Reproducible Code in Science. Data Science. Data
> Science, vol. 4, no. 1, pp. 29-49. DOI: 10.3233/DS-210031.

## About this repository

This repository contains the following:

1.  An R-package called `worcs`, with convenience functions to
    facilitate the WORCS workflow.
2.  In the subfolder `./paper`, the source files for the paper
    describing the WORCS workflow.

The repository serves two functions: To allow users to install the
`worcs` package, and to allow collaborators access to the source code
for the package and paper.

## Repository structure

| File          | Description                     | Usage           |
|:--------------|:--------------------------------|:----------------|
| \_pkgdown.yml | YAML for package website        | do not edit     |
| DESCRIPTION   | R-package DESCRIPTION           | do not edit     |
| LICENSE.md    | Project license                 | do not edit     |
| NAMESPACE     | R-package namespace             | machine-written |
| README.md     | Read this file to get started!  | do not edit     |
| README.Rmd    | R-markdown source for readme.md | human editable  |
| worcs.Rproj   | RStudio project file            | do not edit     |
| docs/         | Package website                 | machine-written |
| inst/         | RStudio project template files  | human editable  |
| man/          | R-package documentation         | do not edit     |
| paper/        | WORCS paper source files        | human editable  |
| R/            | R-package source code           | human editable  |
| vignettes/    | R-package vignettes             | human editable  |

<!-- ## Adoption of WORCS by users -->
<!-- As of 2022-06-25, these are indicators of the adoption of `worcs` by users: -->
<!-- 1. The preprint has been downloaded 1372 times, since being published on 31-05-2020 -->
<!-- 1. The paper in [Data Science](https://content.iospress.com/articles/data-science/ds210031) has been cited 13 times -->
<!-- 1. The `worcs` R-package has been downloaded r dl times from CRAN, since being published on 18-05-2020 -->
<!-- 1. The GitHub project has been forked r worcs_repo$network_count times, watched r worcs_repo$subscribers_count` times, and starred r worcs_repo$watchers times -->
<!-- 1. The lead author has given invited lecturegs on WORCS at: r invited -->
<!-- 1. WORCS is currently used in the following public itHub repositories (sorted by user): r tmp -->

## Contributing and Contact Information

We are always eager to receive user feedback and contributions to help
us improve both the workflow and the software. Major contributions
warrant coauthorship to the package. Please contact the lead author at
<c.j.vanlissa@uu.nl>, or:

-   [File a GitHub issue](https://github.com/cjvanlissa/worcs) for
    feedback, bug reports or feature requests
-   [Make a pull request](https://github.com/cjvanlissa/worcs/pulls) to
    contribute your code or prose

By participating in this project, you agree to abide by the [Contributor
Code of Conduct v2.0](https://www.contributor-covenant.org/).
Contributions to the package must adhere to the [tidyverse style
guide](https://style.tidyverse.org/). When contributing code, please add
tests for that contribution to the `tests/testthat` folder, and ensure
that these tests pass in the [GitHub Actions
panel](https://github.com/cjvanlissa/worcs/actions/workflows/R-CMD-check).

## Acknowledgements

The worcs logo is inspired by the Open Science Badges by the Center for
Open Science (CC-BY-4.0), and makes use of the gear, services, gears,
preferences, settings icon, made by MD Badsha Meah from www.freeicons.io
(CC-BY-3.0).
",2022-08-12
https://github.com/cjvanlissa/worcshop_demo,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'penguins.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                      | Description                      | Usage         
------------------------- | -------------------------------- | --------------
README.md                 | Description of project           | Human editable
penguins.Rproj            | Project file                     | Loads project 
LICENSE                   | User permissions                 | Read only     
.worcs                    | WORCS metadata YAML              | Read only     
preregistration.rmd       | Preregistered hypotheses         | Human editable
prepare_data.R            | Script to process raw data       | Human editable
manuscript/manuscript.rmd | Source code for paper            | Human editable
manuscript/references.bib | BibTex references for manuscript | Human editable
renv.lock                 | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

* To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles, read the preprint at https://osf.io/zcvbs/
* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)
* For a brief overview of the steps of the WORCS workflow, see below.

## WORCS: Steps to follow for a project

## Phase 1: Study design

1. Create a (Public or Private) remote repository on a 'Git' hosting service
2. When using R, initialize a new RStudio project using the WORCS template. Otherwise, clone the remote repository to your local project folder.
3. Add a README.md file, explaining how users should interact with the project, and a LICENSE to explain users' rights and limit your liability. The `worcs` project template does this automatically.
3. Optional: Preregister your analysis by committing a plain-text preregistration and tagging the commit as ""preregistration"".
4. Optional: Upload the preregistration to a dedicated preregistration server
5. Optional: Add study Materials to the repository

## Phase 2: Writing and analysis

6. Create an executable script documenting the code required to load the raw data into a tabular format, and de-identify human subjects if applicable
7. Save the data into a plain-text tabular format like `.csv`. When using open data, commit this file to 'Git'. When using closed data, commit a checksum of the file, and a synthetic copy of the data.
8. Write the manuscript using a dynamic document generation format, with code chunks to perform the analyses.
9. Commit every small change to the 'Git' repository
10. Cite essential references with `@`, and non-essential references with `@@`

## Phase 3: Submission and publication

11. Use dependency management to make the computational environment fully reproducible
12. Optional: Add a WORCS-badge to your project's README file
13. Make a Private 'Git' remote repository Public
14. Optional: [Create a project page on the Open Science Framework](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project)
15. [Connect your 'OSF' project page to the 'Git' remote repository](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an open science statement to the Abstract or Author notes, which links to the 'Git' remote repository or 'OSF' page
17. Render the dynamic document to PDF
18. Optional: [Publish the PDF as a preprint, and add it to the OSF project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
19. Submit the paper, and tag the release of the submitted paper, as in Step 3.

## Notes for cautious researchers

Some researchers might want to share their work only once the paper is accepted for publication. In this case, we recommend creating a ""Private"" repository in Step 1, and completing Steps 13-18 upon acceptance.
",2022-08-12
https://github.com/cjvanlissa/worcs_demo,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>


[![WORCS](https://img.shields.io/badge/WORCS-limited-orange)](https://osf.io/zcvbs/)

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'worcs_demo.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                      | Description                      | Usage         
------------------------- | -------------------------------- | --------------
README.md                 | Description of project           | Human editable
worcs_demo.Rproj          | Project file                     | Loads project 
LICENSE                   | User permissions                 | Read only     
.worcs                    | WORCS metadata YAML              | Read only     
preregistration.rmd       | Preregistered hypotheses         | Human editable
prepare_data.R            | Script to process raw data       | Human editable
manuscript/manuscript.rmd | Source code for paper            | Human editable
manuscript/references.bib | BibTex references for manuscript | Human editable
renv.lock                 | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

* To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles, read the preprint at https://osf.io/zcvbs/
* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)
* For a brief overview of the steps of the WORCS workflow, see below.

## WORCS: Steps to follow for a project

## Phase 1: Study design

1. Create a (Public or Private) remote repository on a 'Git' hosting service
2. When using R, initialize a new RStudio project using the WORCS template. Otherwise, clone the remote repository to your local project folder.
3. Add a README.md file, explaining how users should interact with the project, and a LICENSE to explain users' rights and limit your liability. The `worcs` project template does this automatically.
3. Optional: Preregister your analysis by committing a plain-text preregistration and tagging the commit as ""preregistration"".
4. Optional: Upload the preregistration to a dedicated preregistration server
5. Optional: Add study Materials to the repository

## Phase 2: Writing and analysis

6. Create an executable script documenting the code required to load the raw data into a tabular format, and de-identify human subjects if applicable
7. Save the data into a plain-text tabular format like `.csv`. When using open data, commit this file to 'Git'. When using closed data, commit a checksum of the file, and a synthetic copy of the data.
8. Write the manuscript using a dynamic document generation format, with code chunks to perform the analyses.
9. Commit every small change to the 'Git' repository
10. Cite essential references with `@`, and non-essential references with `@@`

## Phase 3: Submission and publication

11. Use dependency management to make the computational environment fully reproducible
12. Optional: Add a WORCS-badge to your project's README file
13. Make a Private 'Git' remote repository Public
14. Optional: [Create a project page on the Open Science Framework](https://help.osf.io/hc/en-us/articles/360019737594-Create-a-Project)
15. [Connect your 'OSF' project page to the 'Git' remote repository](https://help.osf.io/hc/en-us/articles/360019929813-Connect-GitHub-to-a-Project)
16. Add an open science statement to the Abstract or Author notes, which links to the 'Git' remote repository or 'OSF' page
17. Render the dynamic document to PDF
18. Optional: [Publish the PDF as a preprint, and add it to the OSF project](https://help.osf.io/hc/en-us/articles/360019930533-Upload-a-Preprint)
19. Submit the paper, and tag the release of the submitted paper, as in Step 3.

## Notes for cautious researchers

Some researchers might want to share their work only once the paper is accepted for publication. In this case, we recommend creating a ""Private"" repository in Step 1, and completing Steps 13-18 upon acceptance.
",2022-08-12
https://github.com/cjvanlissa/worcs_oscoffee,"# Readme <a href='https://osf.io/zcvbs/'><img src='worcs_icon.png' align=""right"" height=""139"" /></a>

<!-- Please add a brief introduction to explain what the project is about    -->

## Where do I start?

You can load this project in RStudio by opening the file called 'worcs_oscoffee.Rproj'.

## Project structure

<!--  You can add rows to this table, using ""|"" to separate columns.         -->
File                      | Description                      | Usage         
------------------------- | -------------------------------- | --------------
README.md                 | Description of project           | Human editable
worcs_oscoffee.Rproj      | Project file                     | Loads project 
LICENSE                   | User permissions                 | Read only     
.worcs                    | WORCS metadata YAML              | Read only     
preregistration.rmd       | Preregistered hypotheses         | Human editable
prepare_data.R            | Script to process raw data       | Human editable
manuscript/manuscript.rmd | Source code for paper            | Human editable
manuscript/references.bib | BibTex references for manuscript | Human editable
renv.lock                 | Reproducible R environment       | Read only     

<!--  You can consider adding the following to this file:                    -->
<!--  * A citation reference for your project                                -->
<!--  * Contact information for questions/comments                           -->
<!--  * How people can offer to contribute to the project                    -->
<!--  * A contributor code of conduct, https://www.contributor-covenant.org/ -->

# Reproducibility

This project uses the Workflow for Open Reproducible Code in Science (WORCS) to
ensure transparency and reproducibility. The workflow is designed to meet the
principles of Open Science throughout a research project. 

To learn how WORCS helps researchers meet the TOP-guidelines and FAIR principles,
read the preprint at https://osf.io/zcvbs/

## WORCS: Advice for authors

* To get started with `worcs`, see the [setup vignette](https://cjvanlissa.github.io/worcs/articles/setup.html)
* For detailed information about the steps of the WORCS workflow, see the [workflow vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html)

## WORCS: Advice for readers

Please refer to the vignette on [reproducing a WORCS project]() for step by step advice.
<!-- If your project deviates from the steps outlined in the vignette on     -->
<!-- reproducing a WORCS project, please provide your own advice for         -->
<!-- readers here.                                                           -->


## Access to data

Some of the data used in this project are not publically available.
Synthetic data with similar characteristics to the original data have been provided. Using the function load_data() will load these synthetic data when the original data are unavailable. Note that these synthetic data cannot be used to reproduce the original results. However, it does allow users to run the code and, optionally, generate valid code that can be evaluated using the original data by the project authors.
Synthetic data with similar characteristics to the original data have been provided. Using the function load_data() will load these synthetic data when the original data are unavailable. Note that these synthetic data cannot be used to reproduce the original results. However, it does allow users to run the code and, optionally, generate valid code that can be evaluated using the original data by the project authors.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->


## Access to data

Some of the data used in this project are not publically available.
To request access to the original data, [open a GitHub issue](https://docs.github.com/en/free-pro-team@latest/github/managing-your-work-on-github/creating-an-issue).

<!--Clarify here how users should contact you to gain access to the data, or to submit syntax for evaluation on the original data.-->
",2022-08-12
https://github.com/CLARIAH/ATM,"# Amsterdam Time Machine

This repository contains materials regarding the build up of an Amsterdam Time Machine, a hub for historical Linked (Open) Data (LOD) on Amsterdam. We focus on combining materials derived from textual sources, multimedia sources (images, video) and so called structured data (tables). The data currently reside as Linked Data in a SPARQL endpoint at https://druid.datalegend.net/ATM-DEMO/ATM-CLARIAH-DEMO. For the original data please check the directories in this repository. To read more on the ATM itself, please visit http://www.create.humanities.uva.nl/uncategorized/amsterdam-time-machine/ .
",2022-08-12
https://github.com/CLARIAH/awesome-humanities-ontologies,"# Awesome Ontologies for Digital Humanities [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

> A curated list of ontologies for Digital Humanities

*What is an [awesome](https://github.com/sindresorhus/awesome) list?*

This list collects useful ontologies, vocabularies, terminologies, and taxonomies for modelling and publishing datasets from Humanities domains as [Linked Data](http://linkeddatabook.com/editions/1.0/). If you wish to contribute, [send a PR](https://github.com/albertmeronyo/awesome-humanities-ontologies/pull/new/master) or get in touch with [Albert Meroño](mailto:albert.meronyo@gmail.com) or [Melodee Beals](mailto:m.h.beals@lboro.ac.uk).

## Contents

- [General](#general)
- [Cultural heritage](#cultural-heritage)
- [Periodicals](#periodicals)
- [Musicology](#musicology)
- [Language](#language)
- [Fiction studies](#fiction-studies)

## General
*Persons, places, events, time, documents*

- [CIDOC Conceptual Reference Model (CRM)](http://www.cidoc-crm.org/) - provides definitions and a formal structure for describing the implicit and explicit concepts and relationships used in cultural heritage documentation
- [CRMinf Argumentation Model](http://www.cidoc-crm.org/crminf/ModelVersion/version-0.7) - an Extension of CIDOC-CRM to support argumentation
- [FOAF vocabulary](http://xmlns.com/foaf/spec/) - linking people and information
- [Functional Requirements for Bibliographic Record (FRBR)](http://www.sparontologies.net/ontologies/frbr) - for describing documents and their evolution
- [Geonames](https://www.geonames.org/) - covers all countries and contains over eleven million placenames that are available for download free of charge
- [GeoSPARQL](http://www.opengis.net/ont/geosparql#) - the richest and most common spatial ontologies, plus an optional SPARQL extension (hence the name) that supports spatial reasoning over geodata that are annotated with its ontology. Supported by various triples stores, such as TriplyDB (Druid)
triple stores
- [PROV ontology](https://www.w3.org/TR/prov-o/) - set of classes, properties, and restrictions that can be used to represent and interchange provenance information
- [NIE-INE ontologies](https://github.com/nie-ine/Ontologies/) - infrastructure developed to ensure long-term storage of data of scientific edition projects in the Humanities at the Swiss Universities of Basel, Bern, Zürich, and Geneva
- [LIO](https://lov.linkeddata.es/dataset/lov/vocabs/lio) - A Lightweight Ontology for Describing Images
- [OWL Time](https://www.w3.org/TR/owl-time/) - Time ontology in OWL
- [Perio.do](https://perio.do/en/) - a gazetteer of period definitions for linking and visualizing data
- [Simple Event Model (SEM)](https://semanticweb.cs.vu.nl/2009/11/sem/) - an ontology for modelling events
- [Simple Knowledge Organization System (SKOS)](https://www.w3.org/2004/02/skos/) - standards to support the use of knowledge organization systems (KOS) such as thesauri, classification schemes, subject heading lists and taxonomies within the framework of the Semantic Web
- [schema.org](https://schema.org/) - schemas for structured data on the Internet, on web pages, in email messages, and beyond
- [VIAF](https://viaf.org/) - combines multiple name authority files into a single OCLC-hosted name authority service

## Cultural Heritage / History
*Cultural heritage objects, museums, archaeology, etc.*

- [Linked Art Data Model](https://linked.art/model/) - The Linked Art Data Model is a (CIDOC-CRM) application profile that can be used to describe cultural heritage resources, with a focus on artworks and museum-oriented activities. It defines common patterns and terms to ensure that the resulting data can be easily used and is based on real-world data and use cases.
- [CRMArcheo Excavation Model](https://www.ics.forth.gr/isl/index_main.php?l=e&c=711) - an ontology and RDF Schema to encode metadata about the archaeological excavation process (CIDOC-CRM extensnion)
- [Europeana Data Model (EDM)](https://pro.europeana.eu/resources/standardization-tools/edm-documentation) - proposal for structuring the data that Europeana will be ingesting, managing and publishing on museums, digital archives and digital libraries
- [Getty AAT Art and Architecture Thesaurus](https://www.getty.edu/research/tools/vocabularies/aat/) - an evolving vocabulary, growing and changing on art and architecture
- [JUSO](http://rdfs.co/juso/latest/html#term-doc/) - Juso Ontology is a Web vocabulary for describing geographical addresses and features
- [PNV](https://www.lodewijkpetram.nl/vocab/pnv/doc/) - The Person Name Vocabulary (PNV) is an rdf vocabulary and data model for persons' names
- [ROAR](https://leonvanwissen.nl/vocab/roar/docs/) - Ontology to describe person, location etc. observations in archival resources
- [ICONCLASS](http://iconclass.org/help/lod) - a multilingual classification system for cultural content, the de facto global standard for subject classification

## Periodicals
*Newspapers, magazines, etc.*

- [Enumeration and Chronology of Periodicals Ontology (ECPO)](http://cklee.github.io/ecpo/ecpo.html#introduction)

## Musicology
*Scores, music metadata, symbolic notations*

- [DOing REusable MUSical data (DOREMUS) datamodel](https://drive.google.com/file/d/0B_nxZpGQv9GKSlhhN2tEUGxDbVU/view) - Events, works, expressions around classic music data
- [MIDI ontology](https://www.albertmeronyo.org/wp-content/uploads/2017/07/ISWC2017_paper_343.pdf) - for publishing MIDI pieces, tracks, events, and their attributes
- [MusicOWL](http://linkeddata.uni-muenster.de/ontology/musicscore/) - ontology for encoding music scores of western music
- [Music ontology (MO)](http://musicontology.com/specification/) - provides main concepts and properties for describing music (i.e. artists, albums and tracks) on the Semantic Web
- [Music theory ontology (MTO)](https://dl-acm-org.vu-nl.idm.oclc.org/citation.cfm?id=3243913)

## Language
*Lexicology, named entity recognition, etc.*
- [Lemon](http://lemon-model.net/) - Lemon is a proposed model for modeling lexicon and machine-readable dictionaries and linked to the Semantic Web and the Linked Data cloud
- [Lexvo](http://www.lexvo.org/) - brings information about languages, words, characters, and other human language-related entities to the Linked Data Web and Semantic Web

## Fiction Studies
*Film, literature, etc.*
- [Drammar](https://www.cirma.unito.it/drammar/drammarlode/) - a comprehensive ontology of drama
- [Literary Theme Ontology (LTO)](https://github.com/theme-ontology/theming) - a taxonomy of defined literary themes for thematically annotating works of fiction
- [Taxonomy of Themes and Motifs (TTM2)](https://github.com/anasfkhan81/MemorataPoetis) - a taxonomy of approximately 1,250 Latin labeled themes/motifs, categorized into six different areas: Animalia [Animals], Arbores et virentia [Trees and Plants], Homines [Men], Dei et heroes [Gods and Heroes], Loca [Places], and finally Res [Things]

## License

<a href=""http://creativecommons.org/publicdomain/zero/1.0/""><img src=""https://i.creativecommons.org/p/zero/1.0/88x31.png""></a>
",2022-08-12
https://github.com/CLARIAH/BdVteaching,"# BdVteaching
teaching materials for a replication study using Linked Data

This is the coolest project ever!


",2022-08-12
https://github.com/CLARIAH/BdVteaching-queries,"# BdVteach-queries
This repo contains SPARQL queries related to data in github.com/CLARIAH/BdVteach .
If you're unsure how to run SPARQL queries, load these queries in [grlc](http://grlc.io/api/CLARIAH/BdVteaching-queries), click on a query, and press the 'try it out' button.
",2022-08-12
https://github.com/CLARIAH/benchmark,"# benchmark
Benchmarks SPARQL query performance across triple stores
",2022-08-12
https://github.com/CLARIAH/burgerLinker,"## **burgerLinker -** Civil Registries Linking Tool

Further details regarding the data standardisation and the data model are available in the [burgerLinker Wiki](https://github.com/CLARIAH/burgerLinker/wiki) or via the [burgerLinker lecture](https://vimeo.com/573950112).


### Purpose
This tool is being developed to improve and replace the current [LINKS](https://iisg.amsterdam/en/hsn/projects/links) software. Points of improvement are:
- extremely fast and scalable matching procedure (using Levenshtein automaton and HDT);
- searches candidate matches based on main individuals and relations, or if need be, allows for matching of the main individual only;
- when matching two individuals with multiple first names, at least two names need to be similar in order to find a candidate match; when matching individuals with multiple first names to individuals with only one first name any first name that is identical results in a match(!);
- blocking is not required (i.e. all candidate records can be considered for matching, with no restrictions on their registration date or location, and no requirements on blocking parts of their individual names);
- candidate matches contain detailed metadata on why they are suggested, and can be saved in different formats (CSV and RDF are covered in the current version);
- allows family and life course reconstruction (by computing the transitive closure over all detected links);
- open software.

To download the latest version of the tool click [releases](https://github.com/CLARIAH/burgerLinker/releases) on the right of the screen.

### Use case
Historians use archival records to describe persons' lives. Each record (e.g. a marriage record) just describes a point in time. Hence historians try to link multiple records on the same person to describe a life course. This tool focuses on ""just"" the linkage of civil records. By doing so, pedigrees of humans can be created over multiple generations for research on social inequality, especially in the part of health sciences where the focus is on gene-social contact interactions.

### User profile
The software is designed for the so called ""digital historians"" (e.g. humanities scholars with basic command line skills) who are interested in using the Dutch civil registries for their studies, or for linking their data to it.

### Data
In its current version, the tool cannot be used to match entities from just any source. The current tool is solely focused on the linkage of civil records, relying on the sanguineous relations on the civil record, modelled according to our [Civil Registries schema](assets/CIV.ttl). An overview of the Civil Registries schema is available as a [PNG file](/assets/CIV.png), and you can browse it on [Druid](https://druid.datalegend.net/LINKS/civ).

### Previous work
So far, (Dutch) civil records have been linked by bespoke programming by researchers, sometimes supported by engineers. Specifically the IISG-LINKS program has a pipeline to link these records and provide them to the Central Bureau of Genealogy (CBG). Because the number of records has grown over time and the IISG-LINKS takes an enormous amount of time (weeks) to LINK all records currently present, *burgerLinker* is designed to do this much faster (full sample takes less than 48 hours).

The Golden Agents project has brought about [Lenticular Lenses](https://www.goldenagents.org/tools/lenticular-lenses/) a tool designed to link persons across sources of various nature. We have engaged with the Lenticular Lenses team on multiple occasions (a demo-presentation, two person-vocabulary workshops, and a specific between-teams-workshop). From those meetings we have adopted the [ROAR vocabulary](https://leonvanwissen.nl/vocab/roar/docs/) for work in CLARIAH-WP4. On the specific *burgerLinker* and lenticular lenses tool, however we found that the prerequisite in Lenticular Lenses to allow for heterogenous sources, conflicted with the *burgerLinker* prerequisite to be fast: one reason for it to be fast is the limited set of sources that *burgerLinker* allows for.

The only other set of initiatives that we are aware of are bespoke programming initiatives by domain specific researchers, with country and time specific rules for linking in for example R. These linkage tools are on the whole slow. What we did do is make our own rule set for linking modular, to allow in the future for country and time specific rule sets to be incorporated in *burgerLinker*.

---

### Operating Systems
- This tool is tested on Linux and Mac OS.
- Windows users are advised to use the Docker image.


### Installation requirements
- Only the [JAVA Runtime Environment (JRE)](https://www.oracle.com/java/technologies/javase-jre8-downloads.html), which is free and installed on almost every computer these days.

### Input requirements
- Only one RDF dataset, describing the civil registries that are modelled according to our simple [Civil Registries schema](assets/CIV.png) (see [Wiki](https://github.com/CLARIAH/burgerLinker/wiki/Adapting-the-data-model) for more details regarding the data model and the conversion to RDF).

- For efficient querying (i.e. lower memory usage with fast search), the matching tool requires the dataset to be compressed and given as an [HDT](http://www.rdfhdt.org/what-is-hdt/) file with its index. The tool allows the conversion of any valid RDF file to HDT using the `--function convertToHDT` (see Example 2 below).

### Output format
Two possible output formats to represent the detected links:
- CSV file (default if no output format is specified by the user)
- N-QUADS file (it can be specified in the parameters of the tool using `--format RDF`)

### Main dependencies
This tool mainly rely on two open-source libraries:
- [Levenshtein automata](https://github.com/universal-automata/liblevenshtein-java) (MIT License)
- [RDF-HDT](https://github.com/rdfhdt/hdt-java) (LGPL License)

### Tool functionalities

Functionalities that are supported in the current version: (case insensitive)

- `ConvertToHDT`: compress an RDF dataset given as input to an HDT file that will be generated in the same directory. This function can also be used for merging two HDT files into one (see Example 3 below)

- `ShowDatasetStats`: display some general stats about the HDT dataset, given as input.

- `Within_B_M`: link *newborns* in Birth Certificates to *brides/grooms* in Marriage Certificates (reconstructs life course)

- `Within_B_D`: link *newborns* in Birth Certificates to *deceased* individuals in Death Certificates (reconstructs life course)

- `Between_B_M`: link *parents of newborns* in Birth Certificates to *brides & grooms* in Marriage Certificates (reconstructs family ties)

- `Between_B_D`: link *parents of newborns* in Birth Certificates to *deceased & partner* in Death Certificates (reconstructs family ties)

- `Between_M_M`: link *parents of brides/grooms* in Marriage Certificates to *brides & grooms* in Marriage Certificates (reconstructs family ties)

- `Between_D_M`: link *parents of deceased* in Death Certificates to *brides & grooms* in Marriage Certificates (reconstructs family ties)

- `Closure`: compute the transitive closure of all detected links to get a unique identifier per individual. The output of this function is a new RDF dataset, where linked individuals are replaced by the same identifier in the civil registries dataset.


### Tool parameters
Parameters that can be provided as input to the linking tool:
- `--function`:        *(required)* one of the functionalities listed below

- `--inputData`:       *(required)* path of the HDT dataset
- `--outputDir`:       *(required)* path of the directory for saving the indices and the detected links
- `--maxLev`:          *(optional, default = 4)* integer between 0 and 4, indicating the maximum Levenshtein distance per first or last name allowed for accepting a link
- `--fixedLev`:        *(optional, default = False)* add this flag without a value (i.e. True) for applying the same maximum Levenshtein distance independently from the string lengths
- `--ignoreDate`:        *(optional, default = False)* add this flag without a value (i.e. True) for ignoring the date consistency check before saving a link. By default, the tool only saves links that are  temporally consistent (e.g. when linking newborns to deceased individuals, the tool checks whether the date of death is later than the individual's date of birth)
- `--ignoreBlock`:        *(optional, default = False)* add this flag without a value (i.e. True) for not requiring the equality of the last names' first letter of the matched individuals. By default, the tool only saves links between individuals that at least have the same first letter of their last names
- `--singleInd`:        *(optional, default = False)* add this flag without a value (i.e. True) for allowing the match of the main individual, without the requirement of matching their parents as well
- `--format`:          *(optional, default = CSV)* one of the two Strings: 'RDF' or 'CSV', indicating the desired format for saving the detected links between certificates
- `--debug`:           *(optional, default = error)* one of the two Strings: 'error' (only display error messages in console) or 'all' (show all warning in console)

---

### Examples

- Example 1. Run the help command of the software:

`java -jar burgerLinker.jar --help`

---

- Example 2. Generate an HDT file and its index from an RDF dataset:

`java -jar burgerLinker.jar --function ConvertToHDT --inputData dataDirectory/myCivilRegistries.nq --outputDir .`

This will generate the HDT file 'myCivilRegistries.hdt' and its index 'myCivilRegistries.hdt.index' in the same directory.
The index should be kept in the same directory of the HDT file to speed up all queries.

:warning:

This is the most memory-intensive step of the tool. Therefore, for avoiding running out of memory for larger datasets, we recommend (i) running this step on a machine with enough memory, and (ii) changing the initial lower bound and upper bound of the JAVA heap memory size, by adding the `-Xms` and `-Xmx` flags.

As an example, here are the flags used for generating the HDT file of all Dutch birth and marriage certificates:

`java -Xms64g -Xmx96g -jar burgerLinker.jar --function ConvertToHDT --inputData dataDirectory/myCivilRegistries.nq --outputDir .`

---

- Example 3. Merge two HDT files into one:

`java -jar burgerLinker.jar --function ConvertToHDT --inputData dataDirectory/hdt1.hdt,dataDirectory/hdt2.hdt --outputDir . `

This will generate a third HDT file 'merged-dataset.hdt' and its index 'merged-dataset.hdt.index' in the same directory.

:warning:

The two HDT files given as input are only separated by `,` (without empty space)

---

- Example 4. Link *parents of newborns* to *brides & grooms*:

`java -jar burgerLinker.jar --function Between_B_M --inputData dataDirectory/myCivilRegistries.hdt --outputDir . --format CSV  --maxLev 3 --fixedLev`

These arguments indicate that the user wants to:

    [Between_B_M] link parents of newborns in Birth Certificates to brides and grooms in Marriage Certificates,
    [dataDirectory/myCivilRegistries.hdt] in the civil registries dataset myCivilRegistries.hdt modelled according to our civil registries RDF schema,
    [.] save the detected links in the current directory,
    [CSV] as a CSV file,
    [3] allowing a maximum Levenshtein of 3 per name (first name or last name),
    [fixedLev] independently from the length of the name.

---

- Example 5. Family Reconstruction

`java -jar burgerLinker.jar --function closure --inputData dataDirectory/myCivilRegistries.hdt --outputDir myResultsDirectory `

This command computes the transitive closure of all links existing in the directory `myResultsDirectory`, and generates a new `finalDataset.nt.gz` dataset in this directory by replacing all matched individuals' identifiers from the `myCivilRegistries.hdt` input dataset with the same unique identifier.

**How?**

The directory `myResultsDirectory` must contain the CSV files that resulted from the linking functions described above, without changing the file names (the tool finds these files using a regular expression search in this directory). It can contain one, or all of the following CSV files, with X being any integer from 0 to 4:
- within-B-M-maxLev-X.csv
- within-B-D-maxLev-X.csv
- between-B-M-maxLev-X.csv
- between-B-D-maxLev-X.csv
- between-M-M-maxLev-X.csv
- between-D-M-maxLev-X.csv

The function will first transform the links in these CSV files, that are asserted between identifiers of certificates, into links between individuals. Since identity links are transitive and symmetric, this function computes the transitive closure of all these transformed individual links, and generates new identifiers for each resulted equivalence class.

Example:
- :newborn1 owl:sameAs :bride1
- :bride1 owl:sameAs :mother1

This means that all these identifiers (:newborn1, :bride1, and :mother1) refer to the same individual, appearing in different roles in different civil certificates. This function generates a new dataset, replacing all occurrences of these three identifiers with a single unique identifier (e.g. :i-1). This process allows the reconstruction of historical families, without the need of writing complex queries or following a large number of identity links across the dataset.

- Example 6. Link individuals without the requirement of linking one of the parents

Convert a file hkh-maids.nt to HDT
`java -jar burgerLinker.jar --function convertToHDT --inputData maids/maids-dataset/maids.nt --outputDir maids/maids-dataset/`

Merge the resulting HDT dataset of hkh-maids to the HDT file of the marriages:
`nohup java -Xms128g -Xmx192g -jar burgerLinker.jar --function convertToHDT --inputData maids/maids-dataset/maids.hdt,civ-reg-2021/HDT/marriages.hdt --outputDir maids/maids-and-marriages-dataset/ &`

Run Within_B_M with the singleInd flag on the resulted mergedDataset:
`nohup java -Xms128g -Xmx192g -jar burgerLinker.jar --function within_B_M --inputData maids/maids-and-marriages-dataset/merged-dataset.hdt --outputDir maids/results/ --maxLev 1 --ignoreDate --singleInd &`

Links are saved in the following CSV file (around 100K links detected with the above parameters):
`maids/results/within_b_m-maxLev-1-singleInd-ignoreDate/results/within-B-M-maxLev-1-singleInd-ignoreDate.csv`

NB: when running burgerLinker with nohup, the progress of the linking is saved in the nohup.out file. You can track the progress using `tail -f :
tail -f nohup.out`.

---
## Post-processing rules

### Date filtering assumptions
- Persons will not become older than 110 years of age
- Persons can marry at age 13
- Children are born to: 
    1. married parents, 
    2. up to 9 months after a married father perished,
    3. up to 5 years before the parents married IF acknowledged by the father, or
    4. up to 10 years before the parents married IF acknowledged by the father from birth
- Women can give birth to children between age 14 and 50 years 
- Men can become father at age 14, and stop reproducing after their wife turns 50

---

## Possible direct extensions
It would be possible to add more general matching functionalities that are not dependent on the Civil Registries schema.
One possible way would be to provide a JSON Schema as an additional input to any given dataset, specifying the (i) Classes that the user wish to match their instances (e.g. sourceClass: iisg:Newborn ; targetClass: iisg:Groom), and the (ii) Properties that should be considered in the matching (e.g. schema:givenName; schema:familyName).

Subsequently, the fast matching algorithm could be used for many other linkage purposes (in Digital Humanities), e.g. places, occupations and products.
",2022-08-12
https://github.com/CLARIAH/cattle,"NOTICE: CATTLE used to be a service to run COW on a server. This service has been discontinued, to focus on the development of CoW itself. Please find CoW at https://github.com/clariah/cow/ or install via pip (```pip install cow-csvw```). 


# cattle

[cattle](http://cattle.datalegend.net/) is a [COW](https://github.com/CLARIAH/COW) Web service

## Introduction

cattle is a simple web service to convert CSV files to RDF, using the CSVW compatible library [COW](https://github.com/clariah/cow)

To understand this service please read the [COW](https://github.com/clariah/cow) documentation first. To convert CSVs to Linked Data using this webservice and ruminator, goto [http://cattle.datalegend.net](http://cattle.datalegend.net)

The cattle web service provides the following options (in steps matching the COW 'logic'). You will need to run these commands from a term/shell (in Unix) or the command prompt in Windows.

<!-- ## API command line examples

### Step 1: build a metadata json file
Build a metadata.json file containing the conversion script and save it as a file

`curl -F ""csv=@/home/amp/src/cattle/data/imf.csv"" http://cattle.datalegend.net/build > imf.csv-metadata.json`

WARNING!: Unlike using COW locally, this will actually OVERWRITE a previous build of your file!

### Step 2: change your metadata file
This is something you do locally, so manually edit the `*-metadata.json` file you just created.

### Step 3: convert your csv file using the metadata.json script you created

`curl -F ""csv=@/home/amp/src/cattle/data/imf.csv"" -F ""json=@imf.csv-metadata.json"" http://cattle.datalegend.net/convert -H'Accept: application/n-quads' > imf.csv.nq`


### Other examples
If you just want to print something on your screen and not write them, simply omit the `> ...` part. E.g.:

`curl -i -F ""csv=@/home/amp/src/cattle/data/imf.csv"" http://cattle.datalegend.net/build`

`curl -i -F ""csv=@/home/amp/src/cattle/data/imf.csv"" -F ""json=@imf.csv-metadata.json"" http://cattle.datalegend.net/convert`

`curl -i -F ""csv=@/home/amp/src/cattle/data/imf.csv"" -F ""json=@imf.csv-metadata.json"" http://cattle.datalegend.net/convert -H'Accept: application/n-quads'`

Please note, that the webservice also allows you to save Linked Data as turtle (contrary to COW):

`curl -i -F ""csv=@/home/amp/src/cattle/data/imf.csv"" -F ""json=@imf.csv-metadata.json"" http://cattle.datalegend.net/convert -H'Accept: text/turtle'`

`curl -F ""csv=@/home/amp/src/cattle/data/imf.csv"" -F ""json=@imf.csv-metadata.json"" http://cattle.datalegend.net/convert -H'Accept: text/turtle' > imf.csv.ttl`
 -->
",2022-08-12
https://github.com/CLARIAH/cattle-druid,"# cattle

[cattle](http://cattle.datalegend.net/) is a [COW](https://github.com/CLARIAH/COW) Web service

## Introduction

cattle is a simple web service to convert CSV files to RDF, using the CSVW compatible library [COW](https://github.com/clariah/cow)

To understand this service please read the [COW](https://github.com/clariah/cow) documentation first. To convert CSVs to Linked Data using this webservice and ruminator, goto [http://cattle.datalegend.net](http://cattle.datalegend.net)

The cattle web service provides the following options (in steps matching the COW 'logic'). You will need to run these commands from a term/shell (in Unix) or the command prompt in Windows.

## API command line examples

### Step 1: build a metadata json file
Build a metadata.json file containing the conversion script and save it as a file

`curl -F ""csv=@/home/amp/src/cattle/data/imf.csv"" http://cattle.datalegend.net/build > imf.csv-metadata.json`

WARNING!: Unlike using COW locally, this will actually OVERWRITE a previous build of your file!

### Step 2: change your metadata file
This is something you do locally, so manually edit the `*-metadata.json` file you just created.

### Step 3: convert your csv file using the metadata.json script you created

`curl -F ""csv=@/home/amp/src/cattle/data/imf.csv"" -F ""json=@imf.csv-metadata.json"" http://cattle.datalegend.net/convert -H'Accept: application/n-quads' > imf.csv.nq`


### Other examples
If you just want to print something on your screen and not write them, simply omit the `> ...` part. E.g.:

`curl -i -F ""csv=@/home/amp/src/cattle/data/imf.csv"" http://cattle.datalegend.net/build`

`curl -i -F ""csv=@/home/amp/src/cattle/data/imf.csv"" -F ""json=@imf.csv-metadata.json"" http://cattle.datalegend.net/convert`

`curl -i -F ""csv=@/home/amp/src/cattle/data/imf.csv"" -F ""json=@imf.csv-metadata.json"" http://cattle.datalegend.net/convert -H'Accept: application/n-quads'`

Please note, that the webservice also allows you to save Linked Data as turtle (contrary to COW):

`curl -i -F ""csv=@/home/amp/src/cattle/data/imf.csv"" -F ""json=@imf.csv-metadata.json"" http://cattle.datalegend.net/convert -H'Accept: text/turtle'`

`curl -F ""csv=@/home/amp/src/cattle/data/imf.csv"" -F ""json=@imf.csv-metadata.json"" http://cattle.datalegend.net/convert -H'Accept: text/turtle' > imf.csv.ttl`
",2022-08-12
https://github.com/CLARIAH/claas-authentication,# claas-authentication,2022-08-12
https://github.com/CLARIAH/clariah-plus,"# CLARIAH-PLUS Project

This is the project planning repository for the [CLARIAH-PLUS
project](https://clariah.nl).  It groups all technical documents and
discussions pertaining to CLARIAH-PLUS in a central place and should facilitate
findability, transparency and project planning, for the project as a whole.

This repository will hold important deliverables such as specifications and technical
requirements we agree upon for the project as a whole.

The [issue tracker](https://github.com/CLARIAH/clariah-plus/issues) associated with this github repository is the main place to
hold tickets regarding CLARIAH-PLUS project planning. On top of this we use
[GitHub Projects](https://github.com/orgs/CLARIAH/projects?type=beta) to organize tickets in various kanban boards to guide our
shared development process. The composition of our various [teams](https://github.com/orgs/CLARIAH/teams) can also be
consulted via GitHub.

## Table of Contents

* [Interest Groups](interest-groups/) - Each Interest Group has a directory where to store its information and documents.
* [Requirements](https://github.com/CLARIAH/clariah-plus/tree/main/requirements) - Software & Infrastructure Requirements (initially discussed in [#4](https://github.com/CLARIAH/clariah-plus/issues/4) and [#5](https://github.com/CLARIAH/clariah-plus/pull/5))
* [Shared Development Roadmap](shared-development-roadmap) - phase 2, contains extensive description and state of the
    various core shared epics:
    * [FAIR Datasets](shared-development-roadmap/epics/fair-datasets.md)
    * [FAIR Tool Discovery](shared-development-roadmap/epics/fair-tool-discovery.md)
    * [FAIR Distribution & Deployment](shared-development-roadmap/epics/fair-distribution-and-deployment.md)
* [Technical Committee](technical-committee/) -  Output of the Technical Advisory Committee as a whole (including proceedings).
* [Task Descriptions](task-descriptions/) - Task descriptions of tasks that are being conducted within Work Packages (primarily used by WP3 only)
* Documentation:
    * [CLARIAH Authentication](interest-groups/devops/authentication/authentication_clariah_nl.md)
    * [Connect service to CLARIAH/CLARIN authentication](interest-groups/devops/authentication/CLARIAH_service_authentication_flow.md)
* [Use cases](use-cases/) - Use cases that have been collected CLARIAH-wide

## Motivation

We came from a situation of various separate repositories, but this led to too
much fragmentation, and activity on the individual repositories was often
minimal and boundaries between for instance instance groups could be vague.  We
hope that consolidating our combined efforts in a single repository leads to
increased visibility and participation and collaboration amongst each-other.

Note that this repository does not contain any software projects, those are
always in separate repositories.

## Contribution Guidelines

Please see the [contribution guidelines](CONTRIBUTING.md) on how to work with
this repository. Subsections may have additional instructions in
``CONTRIBUTING.md``.

## Additional Resources

Sometimes we have additional resources that are not in this git repository
(such as documents on Google Docs or elsewhere), they should be linked from
this repository in all cases except for privacy/security-sensitive documents. A
simple link from a README.md in an appropriate place in this repository
suffices.

Some notable resources for the project as a whole:

* [CLARIAH Workplan](https://docs.google.com/spreadsheets/d/1WTbtA20vpKz5Oo_EnDYe1xNhRpR24mr0eESPa49jALg/edit#gid=151792289)
* [CLARIAH Projects page at GitHub](https://github.com/orgs/CLARIAH/projects/?type=beta) - Project planning (kanban boards etc)
* [CLARIAH Issue Tracker](https://github.com/CLARIAH/clariah-plus/issues) - Issue tracker for meta-issues regarding the project
* [CLARIAH Teams](https://github.com/orgs/CLARIAH/teams) - Shows participants of Work Packages, Interest Groups, Working
    Groups
* [CLARIAH Shared Development Roadmap - phase 1](https://docs.google.com/document/d/1dCTK5w9jJRKIQuQ9t_xl7YbTtFljLoLTNT3C2EEIPtg/edit)
* [CLARIAH Website](https://clariah.nl)





",2022-08-12
https://github.com/CLARIAH/clariah-plus-tasks,"# Note: This repo is deprecated and moved to the [central CLARIAH PLUS repository](https://github.com/CLARIAH/clariah-plus)!


# CLARIAH PLUS Tasks

This repository contains tasks descriptions of all the tasks for CLARIAH-PLUS. The task descriptions here
should be kept up to date and are the authoritative source.

Please see the [Contribution Guidelines](CONTRIBUTING.md) on how to add your tasks to this repository.

All tasks for all work packages, and their current status, are listed in the [CLARIAH+ work plan (Google Docs, read-only)](https://docs.google.com/spreadsheets/d/e/2PACX-1vTXKu7TKL_ow2y-d5yV9u0y_WaSUp9iLP884MCwqHXNfkw8p4RxP30Lo0EBbtG4ARFsUpnzyRy00M2W/pubhtml). Use the tabs on the top to switch between the work packages.

Links to the task descriptions per work package:

* [WP2](wp2/)
* [WP3](wp3/)
* [WP4](wp4/)
* [WP5](wp5/)
* [WP6](wp6/)


",2022-08-12
https://github.com/CLARIAH/CLARIAH-use-cases,"# CLARIAH-use-cases
This repository contains the use cases for CLARIAH.",2022-08-12
https://github.com/CLARIAH/COW,"## CoW: Integrated CSV to RDF Converter

> CoW (Csv on the Web) is an integrated CSV to RDF converter that uses the W3C standard [CSVW](https://www.w3.org/TR/tabular-data-primer/) for rich semantic table specificatons, and [nanopublications](http://nanopub.org/) as an output RDF model



### What is CoW

CoW is a command-line utility to convert any CSV file into an RDF dataset. Its distinctive features are:

- Expressive CSVW-compatible schemas based on the [Jinja](https://github.com/pallets/jinja) template enginge
- Highly efficient implementation leveraging multithreaded and multicore architectures
- Available as a pythonic [CLI tool](#cli), [library](#library), and [web service](#web-service)
- Supports Python 3

### Documentation and support
For user documentation see the basic introduction video https://t.co/SDWC3NhWZf and [wiki](https://github.com/clariah/cow/wiki/). Technical details are provided below. If you encounter an issue then please [report](https://github.com/CLARIAH/COW/issues/new/choose) it. Also feel free to create pull requests!

### Install (requires Python to be installed)

`pip3` is the recommended method of installing COW in your system:

```
pip3 install cow-csvw
```

You can upgrade your currently installed version with:

```
pip3 install cow-csvw --upgrade
```

Possible issues:

- Permission issues. You can get around them by installing CoW in user space: `pip3 install cow-csvw --user`. Make sure your binary user directory (typically something like `/Users/user/Library/Python/3.7/bin` in MacOS or `/home/user/.local/bin` in Linux) is in your PATH (in MacOS: `/etc/paths`. For Windows/MacOS we recommend to install Python via the [official distribution page](https://www.python.org/downloads/). You can also use [virtualenv](https://virtualenv.pypa.io/en/latest/) to avoid conflicts with your system libraries
- Please [report your unlisted issue](https://github.com/CLARIAH/CoW/issues/new)

If you can't/don't want to deal with installing CoW, you can use the [cattle](http://cattle.datalegend.net/) [web service version](#web-service) (deprecated).

### Usage

#### CLI

The CLI (command line interface) is the recommended way of using CoW for most users. The straightforward CSV to RDF conversion is done in two steps. First:

```
cow_tool build myfile.csv
```

This will create a file named `myfile.csv-metadata.json` (from now on: JSON schema file or just JSF). You don't need to worry about this file if you only want a syntactic conversion. Then:

```
cow_tool convert myfile.csv
```

Will output a `myfile.csv.nq` RDF file (nquads by default; you can control the output RDF serialization with e.g. ``--format turtle``). That's it!

If you want to control the base URI namespace, URIs used in predicates, virtual columns, and the many other features of CoW, you'll need to edit the `myfile.csv-metadata.json` JSF and/or use CoW arguments. Have a look at the [CLI options](#options) below, the examples in the [wiki](https://github.com/CLARIAH/CoW/wiki), and the [technical documentation](http://csvw-converter.readthedocs.io/en/latest/).

##### Options

Check the ``--help`` for a complete list of options:

```
usage: cow_tool [-h] [--dataset DATASET] [--delimiter DELIMITER]
                [--quotechar QUOTECHAR] [--encoding ENCODING] [--processes PROCESSES]
                [--chunksize CHUNKSIZE] [--base BASE]
                [--format [{xml,n3,turtle,nt,pretty-xml,trix,trig,nquads}]]
				[--gzip] [--version]
                {convert,build} file [file ...]

Not nearly CSVW compliant schema builder and RDF converter

positional arguments:
  {convert,build}       Use the schema of the `file` specified to convert it
                        to RDF, or build a schema from scratch.
  file                  Path(s) of the file(s) that should be used for
                        building or converting. Must be a CSV file.

optional arguments:
  -h, --help            show this help message and exit
  --dataset DATASET     A short name (slug) for the name of the dataset (will
                        use input file name if not specified)
  --delimiter DELIMITER
                        The delimiter used in the CSV file(s)
  --quotechar QUOTECHAR
                        The character used as quotation character in the CSV
                        file(s)
  --encoding ENCODING   The character encoding used in the CSV file(s)

  --processes PROCESSES
                        The number of processes the converter should use
  --chunksize CHUNKSIZE
                        The number of rows processed at each time
  --base BASE           The base for URIs generated with the schema (only
                        relevant when `build`ing a schema)
  --gzip 				Compress the output file using gzip
  --format [{xml,n3,turtle,nt,pretty-xml,trix,trig,nquads}], -f [{xml,n3,turtle,nt,pretty-xml,trix,trig,nquads}]
                        RDF serialization format
  --version             show program's version number and exit
```

#### Web service

There is web service and interface running CoW, called [cattle](http://cattle.datalegend.net/). Two public instances are running at:

- http://cattle.datalegend.net/ - runs CoW in Python3
- http://legacy.cattle.datalegend.net/ - runs CoW in Python2 for legacy reasons

Beware of the web service limitations:

- There's a limit to the size of the CSVs you can upload
- It's a public instance, so your conversion could take longer
- Cattle is no longer being maintained and these public instances will eventually be taken offline

#### Library

Once installed, CoW can be used as a library as follows:

```
from cow_csvw.csvw_tool import COW
import os

COW(mode='build', files=[os.path.join(path, filename)], dataset='My dataset', delimiter=';', quotechar='\""')

COW(mode='convert', files=[os.path.join(path, filename)], dataset='My dataset', delimiter=';', quotechar='\""', processes=4, chunksize=100, base='http://example.org/my-dataset', format='turtle', gzipped=False)
```

### Technical documentation

Technical documentation for CoW are maintained in this GitHub repository (under <docs>), and published through [Read the Docs](http://readthedocs.org) at <http://csvw-converter.readthedocs.io/en/latest/>.

To build the documentation from source, change into the `docs` directory, and run `make html`. This should produce an HTML version of the documentation in the `_build/html` directory.

### Examples

The [wiki](https://github.com/CLARIAH/COW/wiki) provides more hands-on examples of transposing CSVs into Linked Data

### License

MIT License (see [license.txt](license.txt))

### Acknowledgements

**Authors:**    Albert Meroño-Peñuela, Roderick van der Weerdt, Rinke Hoekstra, Kathrin Dentler, Auke Rijpma, Richard Zijdeman, Melvin Roest, Xander Wilcke

**Copyright:**  Vrije Universiteit Amsterdam, Utrecht University, International Institute of Social History


CoW is developed and maintained by the CLARIAH project and funded by NWO.
",2022-08-12
https://github.com/CLARIAH/cow_tutorial,"# cow_tutorial
[CoW](https://github.com/CLARIAH/COW) :cow: is a csv to Linked Data converter with [wiki](https://github.com/clariah/cow/wiki) user documentation and developer documentation in [readthedocs](https://csvw-converter.readthedocs.io/en/latest/) and [pypi](https://pypi.org/project/cow-csvw/). 
CoW is intended for anyone who has Python installed, can use the command line and can edit a text file. Examples of CoW conversions can be found [here](https://github.com/CLARIAH/wp4-cow-conversions). CoW is part of the [CLARIAH datalegend ecosystem](https://www.sciencedirect.com/science/article/abs/pii/S1570826818300106?via%3Dihub) and amongst others being used to transpose part of the [Dutch civil registry](https://repository.ubn.ru.nl/bitstream/handle/2066/225728/225728pub.pdf?sequence=1) to Linked Data.

# getting started
Open a terminal and follow the commands below. [Call for help](https://github.com/CLARIAH/COW/issues/new), if you get stuck.


_in a terminal clone this repository_
```
git clone https://github.com/rlzijdeman/cow_tutorial.git
cd cow_tutorial
```

_next install cow using [pypi](https://pypi.org/project/cow-csvw/)_
```
pip3 install cow-csvw
```

_create metadata file (the recipe)_
```
cow_tool build db_harderwijk_1888_1909.csv
```

_inspect the metadata-json or edit it, see [here](https://github.com/clariah/cow/wiki) how to_
```
less db_harderwijk_1888_1909.csv-metadata.json
```

_create triples from the csv using the metadata.json file as recipe_
```
cow_tool convert db_harderwijk_1888_1909.csv
```

_inspect triples_
```
less db_harderwijk_1888_1909.csv.nq
```
",2022-08-12
https://github.com/CLARIAH/DANE,"# DANE
The Distributed Annotation 'n' Enrichment (DANE) system handles compute task assignment and file storage for the automatic annotation of content.

This repository contains contains the building blocks for with DANE, such as creating custom analysis workers or submitting new task.

## Installation

This package can be installed through pip:

    pip install dane

### Configuration

DANE components are configured through the dane.config module, which is described here: https://dane.readthedocs.io/en/latest/intro.html#configuration 
It is however noteable that, because all DANE components are expected to rely on it, some of the DANE-server, ElasticSearch and RabbitMQ configuration 
are included in the default config. As such it is recommended that you create a `$HOME/.dane/config.yml` or `$DANE_HOME/config.yml` which contain machine-wide settings for how to connect to these services, which involves specifying the following settings:

```
DANE:
    API_URL: 'http://localhost:5500/DANE/'
    MANAGE_URL: 'http://localhost:5500/manage/'
RABBITMQ:
    HOST: 'localhost'
    PORT: 5672
    EXCHANGE: 'DANE-exchange'
    RESPONSE_QUEUE: 'DANE-response-queue'
    USER: 'guest'
    PASSWORD: 'guest'
ELASTICSEARCH:
    HOST: ['localhost']
    PORT: 9200
    USER: 'elastic'
    PASSWORD: 'changeme'
    SCHEME: 'http'
    INDEX: 'your_dane_index'
```

The values given here are the default values.

### Usage

Examples of how to use DANE can be found in the `examples/` directory.

## Local Development

We moved from `setup.py` & `requirements.txt` to a single `pyproject.toml`. For local builds and publishing we use [poetry](https://python-poetry.org/).

For local installation:

```bash
poetry install
poetry shell
```

After installation the following unit test should succeed:

```bash
python -m test.test_dane
```

To build a wheel + source package (will end up in `dist` directory):

```bash
poetry build
```

The wheel can be conveniently tested in e.g. your own DANE worker by installing it e.g. using `pip`:

```bash
pip install path_to_dane_wheel_file
```

or with poetry

```bash
poetry add path_to_dane_wheel_file
```

### Breaking changes after 0.3.1 

Since version 0.3.1 DANE must be imported in lowercase letters:

```python
import dane
```

Before version 0.3.1 you should import using uppercase letters:

```python
import DANE
```",2022-08-12
https://github.com/CLARIAH/DANE-server,"# DANE-server
DANE-server is the back-end component of [DANE](https://github.com/CLARIAH/DANE) and takes care of task routing as well as the (meta)data storage. A task submitted to 
DANE-server is registered in a database, and then its `.run()` function is called. Running a task involves assigning it to a worker via a message queue.

A specific task is run by publishing the task to a [RabbitMQ Topic Exchange](https://www.rabbitmq.com/tutorials/tutorial-five-python.html),
on this exchange the task is routed based on its Task Key. The task key corresponds to the `binding_key` of a worker,
and each worker with this binding_key listens to a shared queue. Once a worker is available it will take the next task from the queue and process it.

DANE-server depends on the [DANE](https://github.com/CLARIAH/DANE) package for the logic of how to iterate over tasks, and how to interpret a task
in general.

# Local Installation

DANE-server has been tested with Python 3 and is installable through pip:

    pip install dane-server

Besides the python base, the DANE-server also relies on an [Elasticsearch](https://www.elastic.co/elasticsearch/) server (version 7.9) for storage, 
and [RabbitMQ](https://www.rabbitmq.com/) (tested with version 3.7) for messaging.

After installing all dependencies it is necessary to configure the DANE server, how to do this is described here: https://dane.readthedocs.io/en/latest/intro.html#configuration

The base config for DANE-server consists of the following parameters, which you might want to overwrite:

```
LOGGING: 
    DIR: ""./dane-server-logs/""
    LEVEL: ""DEBUG""
DANE_SERVER:
    TEMP_FOLDER: ""/home/DANE/DANE-data/TEMP/""
    OUT_FOLDER: ""/home/DANE/DANE-data/OUT/""
```

# Usage

*NOTE: DANE-server is still in development, as such authorisation (amongst other featueres) has not yet been added. Use at your own peril.*

Run the server component (which listens to the RabbitMQ) as follows:

    dane-server

Besides the server component we also need the API, which we can start with:

    dane-api

If no errors occur then this should start a webserver (at port 5500) which will handle API requests, 
while in the background the server will handle interaction with the DB and RabbitMQ.

## API

The DANE api is documented with a swagger UI, available at: http://localhost:5500/DANE/

## Examples

Examples of how to work with DANE can be found at: https://dane.readthedocs.io/en/latest/examples.html

# Docker

To run DANE-server, using Docker make sure to install a Docker Engine, e.g. Docker Desktop for OSX.

## Build the Docker images

As the DANE-server has two separate processes. Two images need to be created:

- One for running the Task Scheduler
- One for running the API

Run the following from the main directory of this repo:

```
docker build -t dane-server -f Dockerfile.ts .
docker build -t dane-server-api -f Dockerfile.api .
```

**Note**: currently the build relies on the `es-index-cfg` branch of DANE (see `requirements.txt`)

After the images have been successfully built, it is possible to run DANE-server via Kubernetes as well

# Kubernetes

These instructions are optimized for `minikube`, which is for local development only. For deployment to a proper k8s cluster, you're on your own for now...

Note that the provided Kubernetes config only provisions your k8s cluster with:

- Endpoint to external Elasticsearch (make sure you got one running)
- RabbitMQ
- DANE server (task scheduler)
- DANE server API

In order to get a bunch of workers setup, you can check the k8s config files in [DANE-asr-worker](https://github.com/beeldengeluid/DANE-asr-worker) repository (later on more examples should follow).

## Create a configmap for config.yml

First make sure to create the config.yml from the config-k8s.yml:

```
cp config-k8s.yml config.yml
```

Now before applying the Kubernetes file `dane-server-k8s.yaml` to your cluster, first create a ConfigMap for config.yml

```
kubectl create configmap dane-server-cfg --from-file config.yml
```

Now the ConfigMap is there, make sure to check that dane-server-k8s.yml points to a existing Elasticsearch host. After that you can go ahead and run:

```
kubectl apply -f dane-server-k8s.yaml
```

## Configure your local DNS to access the API (and RabbitMQ dashboard)

Check the ip assigned to the `dane-server-ingress` (and `dane-rabbitmq-ingress`) by running:

```
kubectl get ingress
```

grab the IP from the `ADDRESS` column and put this in your `/etc/hosts` file:

```
{IP}    api.dane.nl rabbitmq.dane.nl
```

**Note**: you can assign different domain names by editing the Ingresses in `dane-server-k8s.yaml`
",2022-08-12
https://github.com/CLARIAH/data-stories,"# data-stories
 As a scholar, I want to tell a story illustrated by live queries on CLARIAH data sources and attractive visualisations of the answers in order to present the results/value of my data work.
",2022-08-12
https://github.com/CLARIAH/geoconnect,"# geoconnectables or mapping the priests: connecting 8 major LOD suppliers

In this data story, we are going to show how you can combine tools from 8 major institutes and networks into a single *pipeline to visualize* data from a CSV onto a map image, whilst offering it as Linked Open Data. For interactive versions of this data story visit:
- [Kadaster labs](https://data.labs.kadaster.nl/kadaster-dev/-/stories/hack-a-little)
- [Druid](https://druid.datalegend.net/dataLegend/-/stories/hack-a-little)

## Starting in the 16th century...
A long time ago in a galax..., well actually in this galaxy and in a place that we call 'The Netherlands', at least nowadays... What we now from that era may not be a lot, but for an important group of people, those who would rise in the ranks of religious orders we know quite a lot. For example, we know, where they were born... and that might be a little more interesting than you'd expect... because was faith random? Did the divine call spread evenly across the country? Or were those of the cloth born, in the vicinity of churces, where the influence of religious orders was strong.

## Step 1: transpose CSV to Linked Data
The first thing we did was to create Linked Data from a [CSV](https://github.com/CLARIAH/geoconnect/blob/main/reference_priest_data.txt) file using the [LDWizard](https://ldwizard.netwerkdigitaalerfgoed.nl/1) a tool brought to you by the [Dutch Digital Heritage Network](https://www.netwerkdigitaalerfgoed.nl). Please see this [demo](https://www.youtube.com/watch?v=VO61pqKWw7A) on how you can do it yourself.

One really cool feature of the LDWizard is that through its design everyone can create their own 'flavour' of LDWizard. For this [Hack-a-LOD](https://hackalod.com) Jorrit from Kadaster created a geo-flavoured LDWizard, that allows you to directly transpose geo coordinates into properly defined Linked Data. Make sure you [watch his demo](https://youtu.be/6V7ejBSCpH8?t=24). This allows us to easily visualize data and to perform geo related sparql queries, for example whether something is close to something else. 

This feature proved to be crucial: our 16th century place names, did not at all resemble contemporary place names: e.g. ""Oculo"" back then, is ""Schiermonnikoog"" now. Now your average regex excercise...

## Step 2: match via geographic proximity
So in order to relate anything contemporary to our 16th century data, we decided to match the historical places based on their geographic location to contemporary places. For the contemporary places we used Kadaster's [BRT](https://www.kadaster.nl/zakelijk/registraties/basisregistraties/brt) which is already available via this [endpoint](https://data.labs.kadaster.nl/kadaster-dev/-/queries/). We uploaded the geo-LDWizard RDF representation of the birthplaces csv file to an instance of [TriplyDB](https://triplydb.com). As a result we could write a federated query, retrieving both contemporary and historical information on the birth places of priests. 

__Query__: See this [example query](https://data.labs.kadaster.nl/kadaster-dev/-/queries/Find-a-Dutch-place-for-a-given-point/9), for Schiermonnikoog.

## Step 3: eye candy or not
When presenting historic map visualizations an often heard complaint at conference is that contemporary maps are ugly (or even 'evil') as layer for the actual visualization. And to be fair, especially in the case of the Netherlands, it is awkward to read place names, see bridges and highways, where 500 years ago, there was nothing but sea. It cast a shadow on the accuratess of the academic work.

Now there are ton of tools that dealt with this problem, and actually a very good one is QGIS. But even QGIS requires, well.. QGIS yet another tool in the pipeline. So instead, [Triply](https://triply.cc) brought a new feature to their triple store, that allows you to use maps as underlays, as long as these maps are provided as a WMS service from a secure website (https://). This even works, when you don't have access to the triplestore, via a federated query. So how do we get a map?

Well, to my knowledge orginating from the New York Public Library, mapwarper is a very decent piece of tooling that allows you to host maps, georeference them and provde them as .kml and WMS (amongst others). For our use case, we decided to pick a map that probably qualifies for the golden raspberry amongst maps: https://mapwarper.net/maps/40981. That said, many kudos to mapwarper.net for providing this excellent service and don't forget to [donate](https://paypal.me/timdevelops) to this good cause. So brace your eyes, here goes map overlaying via SPARQL:

__Query__: See this [example query](https://stories.triply.cc/wms-playground/#query=prefix%20geo%3A%20%3Chttp%3A%2F%2Fwww.opengis.net%2Font%2Fgeosparql%23%3E%0Aselect%20%3FmapName%20%3FmapEndpoint%20%3Fwkt%20%3Fwkt2%7B%0A%20%20bind(%22Polygon((3.37087%2050.7539%2C3.37087%2053.4658%2C7.21097%2053.4658%2C7.21097%2050.7539%2C3.37087%2050.7539))%22%5E%5Egeo%3AwktLiteral%20as%20%3Fwkt).%0A%20%20bind(%22https%3A%2F%2Fmapwarper.net%2Fmaps%2Fwms%2F40981%3Frequest%3DGetCapabilities%26service%3DWMS%26version%3D1.1.1%22%20as%20%3FmapEndpoint)%0A%7D%0A&endpoint=https%3A%2F%2Fapi.labs.kadaster.nl%2Fdatasets%2FEirikKultorp%2Fmetadata-replaced-11-11-2020%2Fservices%2Fmetadata%2Fsparql&requestMethod=POST&tabTitle=Query%203&headers=%7B%7D&contentTypeConstruct=text%2Fturtle%2C*%2F*%3Bq%3D0.9&contentTypeSelect=application%2Fsparql-results%2Bjson%2C*%2F*%3Bq%3D0.9&outputFormat=geo&outputSettings=%7B%22map%22%3A%22nlmaps%22%2C%22visualization%22%3A%22vanilla%22%2C%22activeLayers%22%3A%5B%22base%22%2C%22aardgas_buurt_bedrijven_2014%22%2C%22MapWarper%22%5D%7D)


## Warping time by 502 years: 1518-2020 what's left of the Monestaries?

So, thus far we have recovered the places that priests are coming from and how we call those places now. But what about their religious institutions? What has become of those? To answer that question we use the 16th century coordinaates of the locations of the monestaries and retrieve from the BRT what currently lies at that point: fastforwarding 500+ years in one federated query.

__Query__: See this [example query](https://druid.datalegend.net/dataLegend/-/queries/kloosters-toen-kadaster-nu/1)


## Wrapping the mapping
So far, in over four years of hack-a-LODS the focus was always on a telling an important data story. The Hack-a-LOD has been of crucial importance to the (Duthc) LOD community and as a result, this year, we felt things have matured enough to tell a story about the networks themselves. There are now so many components in place, that we are able to exchange tools and information from a variety of networks and suppliers, without the need of adding much ourselves. To sum up: source data are provided by the IISG, VU; the Linked Data by Kadaster, the newly LDWizard derived LOD is hosted by CLARIAH and PLDN, while the maplayers are provided by Mapwarper and PDOK, whilst Triply provided the WMS to SPARQL demonstrator.

<img src=""hack-a-little-arch.png"">
",2022-08-12
https://github.com/CLARIAH/grlc,"<p algin=""center""><img src=""https://raw.githubusercontent.com/CLARIAH/grlc/master/src/static/grlc_logo_01.png"" width=""250px""></p>

[![PyPI version](https://badge.fury.io/py/grlc.svg)](https://badge.fury.io/py/grlc)
[![DOI](https://zenodo.org/badge/46131212.svg)](https://zenodo.org/badge/latestdoi/46131212)
[![Build Status](https://travis-ci.org/CLARIAH/grlc.svg?branch=master)](https://travis-ci.org/CLARIAH/grlc)


grlc, the <b>g</b>it <b>r</b>epository <b>l</b>inked data API <b>c</b>onstructor, automatically builds Web APIs using shared SPARQL queries. http://grlc.io/

If you use grlc in your work, please cite it as:

```
@InProceedings{merono2016grlc,
 author = {Mero{\~{n}}o-Pe{\~{n}}uela, Albert and Hoekstra, Rinke},
 title = {{grlc Makes GitHub Taste Like Linked Data APIs}},
 booktitle = {The Semantic Web: ESWC 2016 Satellite Events, Heraklion, Crete, Greece, May 29 -- June 2,  2016},
 year = {2016},
 publisher = {Springer},
 pages = {342--353},
 isbn = {978-3-319-47602-5},
 doi = {10.1007/978-3-319-47602-5_48}
}
```

## What is grlc?
grlc is a lightweight server that takes SPARQL queries (stored in a GitHub repository, in your local filesystem, or listed in a URL), and translates them to Linked Data Web APIs. This enables universal access to Linked Data. Users are not required to know SPARQL to query their data, but instead can access a web API.

## Quick tutorial
For a quick usage tutorial check out our wiki [walkthrough](https://github.com/CLARIAH/grlc/wiki/Quick-tutorial) and [list of features](https://github.com/CLARIAH/grlc/wiki/Features).

## Usage
grlc assumes that you have a collection of SPARQL queries as .rq files (like [this](https://github.com/CLARIAH/grlc-queries)). grlc will create one API operation for each SPARQL query/.rq file in the collection.

Your queries can add API parameters to each operation by using the [parameter mapping](https://github.com/CLARIAH/grlc/wiki/Parameter-Mapping) syntax. This allows your query to define query variables which will be mapped to API parameters for your API operation ([see here](https://github.com/CLARIAH/grlc-queries/blob/master/enumerate.rq) for an example).

Your queries can include special [decorators](#decorator-syntax) to add extra functionality to your API.

### Query location
grlc can load your query collection from different locations: from a GitHub repository (`api-git`), from local storage (`api-local`), and from a specification file (`api-url`). Each type of location has specific features and is accessible via different paths. However all location types produce the same beautiful APIs.

#### From a GitHub repository
> API path:
`http://grlc-server/api-git/<user>/<repo>`

grlc can build an API from any Github repository, specified by the GitHub user name of the owner (`<user>`) and repository name (`<repo>`).

For example, assuming your queries are stored on a Github repo: `https://github.com/CLARIAH/grlc-queries/`, point your browser to the following location
`http://grlc.io/api-git/CLARIAH/grlc-queries/`

grlc can make use of git's version control mechanism to generate an API based on a specific version of queries in the repository. This can be done by including the commit sha in the URL path (`http://grlc-server/api-git/<user>/<repo>/commit/<sha>`), for example: `http://grlc.io/api-git/CLARIAH/grlc-queries/commit/79ceef2ee814a12e2ec572ffaa2f8212a22bae23`

grlc can also use a subdirectory inside your Github repo. This can be done by including a subdirectory in the URL path (`http://grlc-server/api-git/<user>/<repo>/subdir/<subdir>`).

#### From local storage
> API path:
`http://grlc-server/api-local/`

grlc can generate an API from a local directory in the computer where your grlc server runs. You can configure the location of this directory in your [grlc server configuration file](#grlc-server-configuration). See also [how to install and run your own grlc instance](#install-and-run).

When the API is generated from a local directory, API information can be loaded from a configuration file in that folder. This file must be called `local-api-config.ini` and it has the following format:
```ini
[repo_info]
repo_title = Some title
api_description = Description of my API
contact_name = My name
contact_url = https://mypage/
licence_url = https://mylicence/
```

#### From a specification file
> API path:
`http://grlc-server/api-url/?specUrl=<specUrl>`

grlc can generate an API from a yaml specification file accessible on the web.

For example, assuming your queries are listed on spec file: `https://raw.githubusercontent.com/CLARIAH/grlc-queries/master/urls.yml`, point your browser to the following location
`http://grlc.io/api-url?specUrl=https://raw.githubusercontent.com/CLARIAH/grlc-queries/master/urls.yml`

##### Specification file syntax
A grlc API specification file is a YAML file which includes the necessary information to create a grlc API, most importantly a list of URLs to decorated and HTTP-dereferenceable SPARQL queries. This file should contain the following fields

 - `title`: Title of my API
 - `description`: API description
 - `contact`: Contact details of the API owner. This should include the `name` and `url` properties.
 - `licence`: A URL pointing to the licence file for the API.
 - `queries`: A list of URLs of SPARQL queries (with header decorators).

For example:
```YAML
title: Title of my API
description: Description of my API
contact:
  name: Contact Name
  url: https://www.mywebsite.org
licence: http://example.org/licence.html
queries:
  - https://www.mywebsite.org/query1.rq
  - https://www.mywebsite.org/query2.rq
  - https://www.otherwebsite.org/query3.rq
```

### grlc generated API

The API paths of all location types point to the generated [swagger-ui](https://swagger.io/) style API documentation. On the API documentation page, you can explore available API calls and execute individual API calls.

You can also view the swagger spec of your API, by visiting `<API-path>/swagger`, for example: `http://grlc.io/api-git/CLARIAH/grlc-queries/swagger`

### grlc query execution
When you call an API endpoint, grlc executes the SPARQL query for that endpoint by combining supplied parameters and decorators.

There are 4 options to specify your own endpoint:

* Add a `sparql_endpoint` on your [`config.ini`](#grlc-server-configuration)
* Add a `endpoint` parameter to your request: 'http://grlc.io/user/repo/query?endpoint=http://sparql-endpoint/'. You can add a `#+ endpoint_in_url: False` decorator if you DO NOT want to see the `endpoint` parameter in the swagger-ui of your API.
* Add the `#+ endpoint:` [decorator](#`endpoint`).
* Add the URL of the endpoint on a single line in an `endpoint.txt` file within the GitHub repository that contains the queries.

The endpoint call will return the result of executing the query as a json representation of rdflib.query.QueryResult (for other result formats, you can use content negotiation via HTTP `Accept` headers). For json responses, the schema of the response can be modified by using the `#+ transform:` [decorator](#`transform`).

## Decorator syntax
Special decorators are available to make your swagger-ui look nicer and to increase functionality. These are provided as comments at the start of your query file, making it still syntactically valid SPARQL. All decorators start with `#+ `, for example:

```SPARQL
#+ decorator_1: decorator value
#+ decorator_1: decorator value

SELECT * WHERE {
  ?s ?p ?o .
}
```
The following is a list of available decorators and their function:

### `summary`
Creates a summary of your query/operation. This is shown next to your operation name in the swagger-ui.

Syntax:
```
#+ summary: This is the summary of my query/operation
```

Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/summary.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_summary).

### `description`
Creates a description of your query/operation. This is shown as the description of your operation in the swagger-ui.

Syntax:
```
#+ description: Extended description of my query/operation.
```

Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/description.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_description).

### `endpoint`
Specifies a query-specific endpoint.

Syntax:
```
#+ endpoint: http://example.com/sparql
```

Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/endpoint.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_endpoint).

### `pagination`
Paginates the results in groups of (for example) 100. Links to previous, next, first, and last result pages are provided as HTTP response headers to avoid polluting the payload (see details [here](https://developer.github.com/v3/guides/traversing-with-pagination/))

Syntax:
```
#+ pagination: 100
```

Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/pagination.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_pagination).

### `method`
Indicates the HTTP request method (`GET` and `POST` are supported).

Syntax:
```
#+ method: GET
```

Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/method.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/post_method).

### `tags`
Assign tags to your query/operation. Query/operations with the same tag are grouped together in the swagger-ui.

Syntax:
```
#+ tags:
#+   - firstTag
#+   - secondTag
```

Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/tags.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/group1/get_tags).

### `defaults`
Set the default value in the swagger-ui for a specific parameter in the query.

Syntax:
```
#+ defaults:
#+   - param_name: default_value
```

Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/defaults.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_defaults).

### `enumerate`
Indicates which parameters of your query/operation should get enumerations (and get dropdown menus in the swagger-ui) using the given values from the SPARQL endpoint. The values for each enumeration variable can also be specified into the query decorators to save endpoint requests and speed up the API generation.

Syntax:
```
#+ enumerate:
#+   - var1:
#+     - value1
#+     - value2
```

Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/enumerate.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_enumerate).

Notice that these should be plain variable names without SPARQL/BASIL conventions (so `var1` instead of `?_var1_iri`)

###  `endpoint_in_url`
Allows/disallows the `endpoint` parameter from being provided as a URL parameter (allowed by default).

Syntax:
```
#+ endpoint_in_url: False
```

Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/endpoint_url.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_endpoint_url).

###  `transform`
Allows  query results to be converted to the specified JSON structure, by using [SPARQLTransformer](https://github.com/D2KLab/py-sparql-transformer) syntax. Notice that the response content type must be set to `application/json` for the transformation to take effect.

Syntax:
```
#+ transform: {
#+     ""key"": ""?p"",
#+     ""value"": ""?o"",
#+     ""$anchor"": ""key""
#+   }
```

Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/transform.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_transform).

### Example APIs

Check these out:
- http://grlc.io/api-git/CLARIAH/grlc-queries
- http://grlc.io/api-url?specUrl=https://raw.githubusercontent.com/CLARIAH/grlc-queries/master/urls.yml
- http://grlc.io/api-git/CLARIAH/wp4-queries-hisco
- http://grlc.io/api-git/albertmeronyo/lodapi
- http://grlc.io/api-git/albertmeronyo/lsq-api
- https://grlc.io/api-git/CEDAR-project/Queries

You'll find the sources of these and many more in [GitHub](https://github.com/search?o=desc&q=endpoint+summary+language%3ASPARQL&s=indexed&type=Code&utf8=%E2%9C%93)

Use [this GitHub search](https://github.com/search?q=endpoint+summary+language%3ASPARQL&type=Code&utf8=%E2%9C%93) to see examples from other grlc users.

## Install and run
You can use grlc in different ways:
 - [Via grlc.io](#grlc.io): you can use the [grlc.io service](https://grlc.io/)
 - [Via Docker](#Docker): you can use the [grlc docker image](https://hub.docker.com/r/clariah/grlc) and start your own grlc server.
 - [Via pip](#Pip): you can install the [grlc Python package](https://pypi.org/project/grlc/) and start your own grlc server or use grlc as a Python library.

More details for each of these options are given below.

### grlc.io
The easiest way to use grlc is by visiting [grlc.io](http://grlc.io/) and using this service to convert SPARQL queries into a RESTful API. Your queries can be [stored on a github repo](#from-a-github-repository) or can be [listed on a specification file](#from-a-specification-file).

### Docker
To run grlc via [docker](https://www.docker.com/), you'll need a working installation of docker. To deploy grlc, just pull the [latest image from Docker hub](https://hub.docker.com/r/clariah/grlc/). :
```bash
docker run -it --rm -p 8088:80 clariah/grlc
```

The docker image allows you to setup several environment variable such as `GRLC_SERVER_NAME` `GRLC_GITHUB_ACCESS_TOKEN` and `GRLC_SPARQL_ENDPOINT`:
```bash
docker run -it --rm -p 8088:80 -e GRLC_SERVER_NAME=grlc.io -e GRLC_GITHUB_ACCESS_TOKEN=xxx -e GRLC_SPARQL_ENDPOINT=http://dbpedia.org/sparql -e DEBUG=true clariah/grlc
```

### Pip
If you want to run grlc locally or use it as a library, you can install grlc on your machine. Grlc is [registered in PyPi](https://pypi.org/project/grlc/) so you can install it using pip.

#### Prerequisites
grlc has the following requirements:
- Python3
- development files (depending on your OS):
```bash
sudo apt-get install libevent-dev python-all-dev
```

#### pip install
Once the base requirements are satisfied, you can install grlc like this:
```bash
pip install grlc
```

Once grlc is installed, you have several options:
 - [Stand alone server](#Standalone-server)
 - [Using a WSGI server](#Using-a-WSGI-server)
 - [As a python library](#Grlc-library)

#### Standalone server
grlc includes a command line tool which you can use to start your own grlc server:
```bash
grlc-server
```

#### Using a WSGI server
You can run grlc using a WSGI server such as gunicorn as follows:
```bash
gunicorn grlc.server:app
```

If you want to use your own gunicorn configuration, for example `gunicorn_config.py`:
```python
workers = 5
worker_class = 'gevent'
bind = '0.0.0.0:8088'
```
Then you can run it as:
```bash
gunicorn -c gunicorn_config.py grlc.server:app
```

**Note:** Since `gunicorn` does not work under Windows, you can use `waitress` instead:
```bash
waitress-serve --port=8088 grlc.server:app
```

If you want to run grlc at system boot as a service, you can find example upstart scripts at [upstart/](upstart/grlc-docker.conf)

#### grlc library
You can use grlc as a library directly from your own python script. See the [usage example](https://github.com/CLARIAH/grlc/blob/master/doc/notebooks/GrlcFromNotebook.ipynb) to find out more.

#### grlc server configuration
Regardless of how you are running your grlc server, you will need to configure it using the `config.ini` file. Have a look at the [example config file](./config.default.ini) to see how it this file is structured.

The configuration file contains the following variables:
 - `github_access_token` [access token](#github-access-token) to communicate with Github API.
 - `local_sparql_dir` local storage directory where [local queries](#from-local-storage) are located.
 - `server_name` name of the server (e.g. grlc.io)
 - `sparql_endpoint` default SPARQL endpoint
 - `user` and `password` SPARQL endpoint default authentication (if required, specify `'none'` if not required)
 - `debug` enable debug level logging.

##### GitHub access token
In order for grlc to communicate with GitHub, you'll need to tell grlc what your access token is:

1. Get a GitHub personal access token. In your GitHub's profile page, go to _Settings_, then _Developer settings_, _Personal access tokens_, and _Generate new token_
2. You'll get an access token string, copy it and save it somewhere safe (GitHub won't let you see it again!)
3. Edit your `config.ini` or `docker-compose.yml` as value of the environment variable `GRLC_GITHUB_ACCESS_TOKEN`.

# Contribute!
grlc needs **you** to continue bringing Semantic Web content to developers, applications and users. No matter if you are just a curious user, a developer, or a researcher; there are many ways in which you can contribute:

- File in bug reports
- Request new features
- Set up your own environment and start hacking

Check our [contributing](CONTRIBUTING.md) guidelines for these and more, and join us today!

If you cannot code, that's no problem! There's still plenty you can contribute:

- Share your experience at using grlc in Twitter (mention the handle **@grlcldapi**)
- If you are good with HTML/CSS, [let us know](mailto:albert.meronyo@gmail.com)

## Related tools
- [SPARQL2Git](https://github.com/albertmeronyo/SPARQL2Git) is a Web interface for editing SPARQL queries and saving them in GitHub as grlc APIs.
- [grlcR](https://github.com/CLARIAH/grlcR) is a package for R that brings Linked Data into your R environment easily through grlc.
- [Hay's tools](https://tools.wmflabs.org/hay/directory/#/showall) lists grlc as a Wikimedia-related tool :-)

## This is what grlc users are saying
- [Flavour your Linked Data with grlc](https://blog.esciencecenter.nl/flavour-your-linked-data-with-garlic-98bfbb358e06), by Carlos Martinez
- [Converting any SPARQL endpoint to an OpenAPI](http://chem-bla-ics.blogspot.com/2018/07/converting-any-sparql-endpoint-to.html) by Egon Willighagen

Quotes from grlc users:
> A cool project that can convert a random SPARQL endpoint into an OpenAPI endpoint

> It enables us to quickly integrate any new API requirements in a matter of seconds, without having to worry about configuration or deployment of the system

> You can store your SPARQL queries on GitHub and then you can run your queries on your favourite programming language (Python, Javascript, etc.) using a Web API (including swagger documentation) just as easily as loading data from a web page

**Contributors:**	[Albert Meroño](https://github.com/albertmeronyo), [Rinke Hoekstra](https://github.com/RinkeHoekstra), [Carlos Martínez](https://github.com/c-martinez)

**Copyright:**	Albert Meroño, Rinke Hoekstra, Carlos Martínez  
**License:**	MIT License (see [LICENSE.txt](LICENSE.txt))

## Academic publications

- Albert Meroño-Peñuela, Rinke Hoekstra. “grlc Makes GitHub Taste Like Linked Data APIs”. The Semantic Web – ESWC 2016 Satellite Events, Heraklion, Crete, Greece, May 29 – June 2, 2016, Revised Selected Papers. LNCS 9989, pp. 342-353 (2016). ([PDF](https://link.springer.com/content/pdf/10.1007%2F978-3-319-47602-5_48.pdf))
- Albert Meroño-Peñuela, Rinke Hoekstra. “SPARQL2Git: Transparent SPARQL and Linked Data API Curation via Git”. In: Proceedings of the 14th Extended Semantic Web Conference (ESWC 2017), Poster and Demo Track. Portoroz, Slovenia, May 28th – June 1st, 2017 (2017). ([PDF](https://www.albertmeronyo.org/wp-content/uploads/2017/04/sparql2git-transparent-sparql-4.pdf))
- Albert Meroño-Peñuela, Rinke Hoekstra. “Automatic Query-centric API for Routine Access to Linked Data”. In: The Semantic Web – ISWC 2017, 16th International Semantic Web Conference. Lecture Notes in Computer Science, vol 10587, pp. 334-339 (2017). ([PDF](https://www.albertmeronyo.org/wp-content/uploads/2017/07/ISWC2017_paper_430.pdf))
- Pasquale Lisena, Albert Meroño-Peñuela, Tobias Kuhn, Raphaël Troncy. “Easy Web API Development with SPARQL Transformer”. In: The Semantic Web – ISWC 2019, 18th International Semantic Web Conference. Lecture Notes in Computer Science, vol 11779, pp. 454-470 (2019). ([PDF](https://www.albertmeronyo.org/wp-content/uploads/2019/06/ISWC2019_paper_237.pdf))
",2022-08-12
https://github.com/CLARIAH/grlc-queries,"# grlc-queries
Example queries to illustrate the functionalities of [grlc decorator syntax](https://github.com/CLARIAH/grlc/tree/dev#decorator-syntax)
",2022-08-12
https://github.com/CLARIAH/grlcR,"# grlcR
R package to retrieve sparql queries stored in grlc format

To install & load run:

```
# install.packages(""devtools"") # uncomment if not installed yet
devtools::install_github(""CLARIAH/grlcR"", subdir = ""grlcR"")
library(grlcR)
```

```
# Examples
repo_summary(""albertmeronyo/lodapi"")

# not run
# repo_summary(""CLARIAH/wp4-queries"") #takes forever

# retrieve results from sparql query
hisq <- repo_summary(""CLARIAH/wp4-queries-hisco"")
hisq
df <- repo_query(hisq, 2) # 2 being the second of the sparql queries in this repo
str(df) # notice that df exits of 2 data.frames

# extract values from a uri (e.g. retrieve 'value' from 'http(s)://.../..../value')
df$hisco$code <- sapply(df$hisco$value, uri_value)
df$hisco
```
",2022-08-12
https://github.com/CLARIAH/HDNG,"# HDNG

The HDNG is the Historische Database Nederlandse Gemeentes (Historical Database Dutch Municipalities) and contains a wide range of historical indicators for Dutch municipalities for the 19th and 20th centuries. The HDNG dates from the floppy disk era and has been made available online by Onno Boonstra. Hitherto, there have been 4 versions of the database:

## Version history

**Version 1 (HED, 1970-medio 1990):** 
- Data gathered under supervision of Hans Knippenberg.
- Contains demographic indicators stored in wide format. 
- Using municipality name and CBS-code as unique identifyers.
- Information was stored on 7 floppy disks in .POR (SPSS portable) files.
- Variable names were coded as 8 digits, which were interpretable via a look-up table.

**Version 2 (HDNG, 2003):** 
- Was published online by Onno Boonstra as 11 separate csv files.
- Added the Amsterdam-codes for all Dutch municipalities.
- Dropped 14 variables.

**Version 3 (HDNG, 2020):** 
- Published the HDNG in one csv file. 
- Transformed the HDNG from a wide format into long format, so that look-up tables are no longer necessary.
- Added variables from the Historisch-Ecologische Database (HED) on religion.
- Added provincial and national aggregates from the Historisch-Ecologische Database (HED).
- Returned the 14 dropped variables.

**Version 4 (HDNG, 2021):** 
- Dropped redundant columns from previous versions.
- Adds descriptive fields to the HDNG to make the database more easily queryable.
- Adds visualisation descriptions to the HDNG, to enable hookups with NLGIS or other shapefiles.
- Redundant rows have been removed from the database. 


## Script overview

**1. transform HDNG from separate files into long format.R** transforms the HDNG v2 into long format.

**2. transform HDNG_long to HDNG+HED.R** adds information from the HED to the HDNG.

**3. provincietotalen.R** puts the provincial and national aggregates from the HED into long format.

**4. add missings and provincietotalen to HDNG+.R** adds provincial and national aggregates from the HED, as well as the 15 variables dropped in HDNG v2.

**5. filter existing municipalities.R** removes entries for non-existing municipalities.


## Variable description
| Variable | Description |
| -------- | ----------- |
| amco | provides the _Amsterdamse code_, a 5-digit number to refer to municipalities |
| name | the name of the municipality as provided in the HDNG v2 |
| variable | the original 8-digit variable name from the HED. The variable is now _redundant_, but originally was used to note a variable's topic (A-H), year (809-997), variable (3 letters), and sex (1,2,3,X). |
| description | queryable summary of the available variables in the HDNG. |
| information | actual variables in the HDNG. |
| sex | male (M), female (F), both sexes (-T), or not applicable ( ). |
| year | the year for which data is available. |
| visualisation_year | the year for which municipal borders have been used. |
| value | score on description. |
| sources | reference to source in ... | 
| remark | reference to remarks in ... |
",2022-08-12
https://github.com/CLARIAH/hhucap,"# hhucap
Current historical studies of career mobility often focus on linkage of personal records such as baptism records. More qualitative sources, such as biographies contain vital information as well, but are labour intensive to process. We propose a combination of Robust Semantic Parsing and Linked Data conversion tools to automatically derive career patterns from 35,000 biographies in the Biography Portal in the period 1815-1940. Substantively, we answer the question what career patterns looked like and changed over the long Nineteenth century. Methodologically, we evaluate to what extent current CLARIAH tools are up to automate this process. We will progress the semantic parsing tools by improving the linguistic expression set related to HISCO, adding an OCR cleaning step to the pipeline and experimenting with alternative CLARIAH tools for Dutch. This will result in a detailed report on the performance of CLARIAH tools on this data.

Update 2020-03-20:
The code for the simple tagger tool is available via: https://github.com/cltl/SimpleTagger
",2022-08-12
https://github.com/CLARIAH/humigec,"# humigec
humigec project
",2022-08-12
https://github.com/CLARIAH/IG,"# Note: This repo is deprecated and moved to the [central CLARIAH PLUS repository](https://github.com/CLARIAH/clariah-plus)!

# IG
CLARIAH Interest Groups

The CLARIAH Interest Groups are groups of engineers that share a common ... well ... interest! In the first 4 years of CLARIAH ('CORE') researchers and engineers were united in Workpackages to garantuee perfect interaction among them, except for one work package. That particular work package was the connection between the engineers from all other WP's and tasked with the design of the overall infra-structure.

It shows however, that there is a demand for much interaction between the engineers and on a more granular level. Therefore, in the current tranch of CLARIAH ('PLUS'), we foster the cooperation of engineers and harmonization of the infrastructure through Interest Groups. We set out with a number of Interest Groups knowing that during the development of the project there may be need of alternative Interest Groups and some IG's might cease to exist. Below is the list of Interest Groups we will set out with, but this ['live' query](https://github.com/CLARIAH?q=ig&type=&language=) will always be more up to date.

The interest groups are:
- [Annotation](https://github.com/CLARIAH/IG-Annotation)
- [Audiovisual processing](https://github.com/CLARIAH/IG-AVProcessing)
- [Curation](https://github.com/CLARIAH/IG-Curation)
- Devops
- Linked Open Data
    - [Depracated](https://github.com/CLARIAH/IG-LOD), now split into ""Modeling and Vocabulary Alignment"", ""Publication, Use Findability"", ""Sustainability and Storage""
- [Modeling and Vocabulary Alignment](https://github.com/CLARIAH/IG-Vocabularies)
- Preservation
- [Publication and Use, Findability](https://github.com/CLARIAH/IG-LOD-Findability)
- Security & Monitoring
- [Sustainability & Storage](https://github.com/CLARIAH/IG-Sustainability)
- [Text processing](https://github.com/CLARIAH/IG-Text)
- UI/UX
- [Workflows](https://github.com/CLARIAH/IG-Workflows)



## References

* October 2020 - [CLARIAH-PLUS Interest Groups Work Plan](docs/workplan.pdf)
* July 2020 - [CLARIAH-PLUS: Roadmap and Call to Action](docs/roadmap.pdf)
* October 2020 - [Slides kick-off presentation](https://docs.google.com/presentation/d/1ywZY3b3eW_uIlFEtJb5l6QPlB50_KKeFsB-qn1rVUXc/edit#slide=id.p)
    (read-only)


",2022-08-12
https://github.com/CLARIAH/IG-Annotation,"# Note: This repo is deprecated and moved to the [central CLARIAH PLUS repository](https://github.com/CLARIAH/clariah-plus)!

# CLARIAH Interest Group on Annotation Infrastructure

Development in the [CLARIAH](https://clariah.nl) project is coordinated through themed [Interest Groups](https://github.com/CLARIAH/IG). This repository is intended to organize the work, output and documentation of the CLARIAH Interest Group (IG) on Annotation Infrastructure. Other [CLARIAH Interest Groups](#other-interets-groups) are listed below.

## Introduction

The enrichment of various types of data with all kinds of annotations is a shared
interest of several WPs, although requirements may differ greatly depending on the
material to be annotated. The further development of a network of distributed
annotation servers in WP2 calls for close collaboration with WP3, WP5 and WP6,
who have defined several annotation-related tasks.

## Aims of the Interest Group

The aims of the IG on Annotation Infrastructure are:

- foster discussion and knowledge sharing regarding annotation infrastructure
- develop and share best practices
- inform development of CLARIAH annotation tools and services

## Scope of the Interest Group

- [inventory of need for manual annotation support in CLARIAH](./docs/annotation-needs.md)
- support of the creation and use of annotations on any media type
- support of manual annotation processes
- support of manual correction of automatic annotation
- [inventory of and recommendations for annotation models, formats and converters](./docs/inventory.md)

Annotation aspects that are outside the scope of this Interest Group (because they are covered by other IGs):

- automatic annotation processes
- crowdsourcing
- ...


## Contributing and Communication

This group is open to anyone interested in annotation in the context of CLARIAH.

If you want to contribute, please the [Contributing guidelines](./CONTRIBUTING.md)

We use the following communication channel:

- [gitter](https://gitter.im/CLARIAH/chat)

## Tasks

Annotation infrastructure problems that we try to tackle are:

- Storage and distribution
- Querying
- Interoperability
    - Video Annotation Interoperability ([VAINT](https://github.com/CLARIAH/video-annotation-interoperability)) initiative
- Provenance and context of creation
- Shared libraries of basic annotation components
- Choosing standards

## Group Members

- Marijn Koolen (KNAW Humanities Cluster, group coordinator)
- Jaap Blom (Netherlands Institute for Sound and Vision)
- Maarten van Gompel (KNAW Humanities Cluster)
- Roeland Ordelman (Netherlands Institute for Sound and Vision)
- Hennie Brugman (KNAW Humanities Cluster)
- Dirk Roorda (DANS)
- Lodewijk Petram (Huygens ING)
- Peter Boot (Huygens ING)
- Han Sloetjes (Max Planck Institute Nijmegen)
- Ted van der Togt (KB)

The group is open to new members. Contact Marijn Koolen.


## Related Interest Groups

The CLARIAH IGs have a [shared work plan](https://github.com/CLARIAH/IG/blob/main/docs/workplan.pdf)

The following IGs are related in their scope to the Annotation IG:

- [Text Processing](https://github.com/CLARIAH/IG-Text)
- [Audiovisuaal Processing](https://github.com/CLARIAH/IG-AVProcessing)
- [Research Workflows](https://github.com/CLARIAH/IG-Workflows) (pipelines and toolchains)
- [Linked Open Data](https://github.com/CLARIAH/IG-LOD)
",2022-08-12
https://github.com/CLARIAH/IG-AVProcessing,"# Note: This repo is deprecated and moved to the [central CLARIAH PLUS repository](https://github.com/CLARIAH/clariah-plus)!

# IG-AVProcessing
This repository is intended to organize the work, output and documentation of the CLARIAH Interest Group (IG) on Audiovisual Processing infrastructure

## Aims of the Interest Group

The aims of the IG on Audiovisual Processing Infrastructure are:

- foster discussion and knowledge sharing regarding infrastructure for AV processing such as automatic speech recognition or computer vision
- develop and share best practices
- inform development of CLARIAH annotation tools and services

## Scope of the Interest Group

- support for the scholarly use of tools for automatic annalysis of audiovisual media
- support for the implementation of AV analysis services/pipelines in the CLARIAH infrastructure
- support for the maintenance, updates and upgrades of tools for automatic annalysis of audiovisual media
- primary focus tools: segmentation, text-audio alignment, speech/speaker recognition, computer vision.

Aspects of audiovisual processing that are outside the scope of this Interest Group (because they are covered by other IGs):

- annotation processes and/or correction of automatic annotation
- crowdsourcing
- ...


## Communication

We use the following communication channel:
- gitter?
- slack

## Tasks

AV processing infrastructure problems that we try to tackle are:

- pipelines for robust and efficient large scale AV processing
- services related to AV processing
- auxiliary tools needed

## Group Members

- Roeland Ordelman (Netherlands Institute for Sound and Vision, group coordinator)
- Nanne van Noord (University of Amsterdam)
- Henk van den Heuvel (Radboud University)
- Maarten van Gompel (KNAW Humanities Cluster)
- Arjan van Hessen (University of Twente)


The group is open to new members. Contact Roeland Ordelman.
",2022-08-12
https://github.com/CLARIAH/IG-Curation,"# Note: This repo is deprecated and moved to the [central CLARIAH PLUS repository](https://github.com/CLARIAH/clariah-plus)!

# CLARIAH Interest Group on Curation

CLARIAH is organized in Work Packages and Interest Groups (IG's). The main goal of each IG is to research, negotiate, propose, and implement technical choices that become a standard requirement in CLARIAH. This repository is intended to organize the work, output and documentation of the CLARIAH Interest Group (IG) on Curation. Please check out the overall [IG page](https://github.com/clariah/ig/) for more information.

## Aims of the Interest Group

The aims of the IG on Curation are:
- foster discussion and knowledge sharing regarding data curation, modeling and enrichment
- develop and share best practices
- inform development of CLARIAH curation tools and services

## Scope of the Interest Group

- stimulate coordination between the WPs on the use of conversion tools (data converters)
- support for the development of a CLARIAH library of curation and conversion tools
- support for the creation and maintenance of documentation on data standards and vocabularies
- support for the implementation of the FAIR guiding principles, with specific emphasis on data interoperability and reusability

This IG has a strong connection to IG 2: Preservation and [IG 7: Linked Open Data](https://github.com/CLARIAH/IG-LOD).

## Communication

We use the following communication channel:
- Slack

## Tasks
-

## Products

### Topics suggested
-  the development of a toolbox / library of conversion tools
-  inform development of a central forum for evaluation and training data
-  reuse and design of data models, vocabularies and ontologies
-  data validation and citizen science
-  data provenance
-  data ethics and privacy legislation

### Vocabularies
- Cf. the [awesome humanities ontologies](https://github.com/CLARIAH/awesome-humanities-ontologies) list.

## Group Members
- Mike Bryant (EHRI / King's College London)
- Steven Claeyssens (National Library of the Netherlands)
- Sebastiaan Derks (Huygens ING / KNAW Humanities Cluster, group coordinator)
- Ed de Heer (National Archives of the Netherlands)
- Lizzy Jongma (Netwerk Oorlogsbronnen)
- Willem Melder (Netherlands Institute for Sound and Vision)
- Harm Nijboer (University of Amsterdam)
- Dirk Roorda (DANS / KNAW)
- Ruben Schalk (Utrecht University)
- Ronald Sluijter (Huygens ING / KNAW Humanities Cluster)
- Frank Uiterwaal (EHRI / NIOD)
- Mari Wigham (Netherlands Institute for Sound and Vision)
- Leon van Wissen (University of Amsterdam)
- Douwe Zeldenrust (Meertens Institute / KNAW Humanities Cluster)

The group is open to new members.

",2022-08-12
https://github.com/CLARIAH/IG-LOD,"# Note: This repo is deprecated and moved to the [central CLARIAH PLUS repository](https://github.com/CLARIAH/clariah-plus)!

The IG LOD has evolved into three new IG's:
- [Modeling and Vocabularies](https://github.com/CLARIAH/IG-Vocabularies)
- [Publication, Use and Findability](https://github.com/CLARIAH/IG-LOD-Findability)
- [Sustainability and Storage](https://github.com/CLARIAH/IG-Sustainability)

Information below is kept for archival purposes, but is no longer up to date. Please visit the IG overview page for [a list of all current IG's](https://github.com/clariah/ig/).



# IG-LOD
CLARIAH is organized in Work Packages and Interest Groups (IG's). The key goal of each IG is to research, negotiate, propose, and implement technical choices that become a standard requirement in CLARIAH. This repository is intended to organize the work, output and documentation of the IG on Linked Open Data (LOD). Don't forget to checkout the other IG's at the overall [IG page](https://github.com/clariah/ig/).

## Newsflash!
Next meetup: December 14th, 12.30-14.00 hours CET.

## Aims of the Interest Group
In CLARIAH CORE infrastructure and tools were mainly developed on a by work package basis.
The IG-LOD aims to gather, asses and link (or reconstruct) these parts whilst making sure the WP's needs (requirements) are safeguarded.


## Scope of the Interest Group
The IG-LOD is currently organized in four subgroups focusing on:
- Modeling and Vocabulary alignment
- Publication and use, findability
- Sustainability and storage
- Annotation, uncertainty and provenance (reliability)

## Use Cases
- [SHCL] ""Hoe kunnen we als “perifere” erfgoedinstelling (SHCL, Maastricht) de benodigde kennis in huis halen om concrete stappen in de gewenste richting te maken?""
- [WP4/5] How can I augment my dataset with information from other datasets?
- [WP4] Link multiple observations of the same person to each other?
- [WP4] How can I analyze Linked Open Data with R / Python Pandas?
- [WP4] Computers can now co-write with authors. Can they also assist me to design my hypotheses?
- [WP5] How can I discover relevant materials beyond searching for names of the person/film that I'm interested in? (tbc)
- [WP5] How can I restrain the resources that provide me with Linked Data (especially from a single graph)? (tbc)

## Communication
- Slack (quick and dirty)
- Github (documentation)
Create an [issue](https://github.com/CLARIAH/IG-LOD/issues/new/choose) to join!

## Organization
- subgroups by scope components;
- engineers on top, supported by senior staff;
- subgroup meetings and IG-LOD meetings.


## Tasks
-

## Products

### LOD Browsers
- Anansi
- Brwsr
- Druid

### LOD Storage
- Anansi
- Druid
- DIVE (deprecated). Media Suite aims for a hybrid approach where LOD can provide an auxiliary mechanism for connecting information sources.

### Linked Data Converters
- CoW (CSV to RDF)
- Kabara
- CMDI2RDF (CMDI to RDF)
- LDWizard (CSV to RDF)

### Miscellaneous
- FoLiA Set Definitions (...)
- Codemeta in LaMachine (...)

### Outreach
- datastories (tool for creating webpages based on sparql queries)

### SPARQL API Generators
- [grlc](grlc.io)

### Topics suggested
- Service for permanant identifiers for subjects (e.g. Shall we bless w3id next to handle?)
- find compatibility between various solutions (e.g. Anansi/Druid, CoW/LD Wizard) within the ClaaS framework;
- improved out-of-the-box availability of existing tools (e.g. DIVE, brwsr) within the CLAAS framework;
- improve compatibility with inter- and national initiatives (e.g. within Pelagios, Odissei and NDE);
- to attract and interact with a larger less technical audience (e.g. via LD Wizard, Data Stories, Notebooks)
- what is the CLAAS way to store & retrieve triples (file storage, triple stores, media fragments)
- how/when/why can/should partners share LD services & data in the CLAAS the infrastructure (using Docker/Kubernetes)
- performance (of querying, updating data, etc)
- the use of grlc (and further development on it)
- link with NDE principles
- link with NDE termennetwerk
- distributed search functionality for vocab and ontology resources
- recommendation on metadata describing (documenting) a dataset within Clariah; ex: title (name), description, ownership, license, provenance, temporal and/or spatial coverage, keywords, etc. Which properties should be made mandatory? Which vocabulary (schema, dc, void, etc.)?

### Vocabularies
- See specifically IG-Curation and the [awesome humanities ontologies](https://github.com/CLARIAH/awesome-humanities-ontologies) list.
- The need for a good vocabulary to describe metadata as related to linksets datasets (VOID?)


## Group Members
- Richard Zijdeman (International Institute of Social History, group coordinator)
- Wouter Beek (VU University, Triply)
- Victor de Boer (VU University)
- Antske Fokkens (VU University)
- Willem van Hage (eScience Center)
- Chiara Latronico (University of Amsterdam)
- Carlos Martinez-Ortis (eScience Center)
- Enno Meijers (National Library of the Netherlands, Dutch Digital Heritage Network)
- Willem Melder (invited, Sound & Vision)
- Harmen Nijboer (Huygens ING)
- Jauco Noordzij (KNAW Humanities Cluster)
- Jacco van Ossenbrugge (CWI, VU University)
- Lodewijk Petram (Huygens ING)
- Joe Raad (VU University)
- Kathrin Dentler (Triply)
- Andrea Scharnhorst (DANS)
- Ronald Siebes (VU University)
- Sjors de Valk (Dutch Digital Heritage Network)
- Thomas Vermaut (KNAW Humanities Cluster)
- Jerry de Vries (DANS)
- Mari Wigham (Sound & Vision)
- Menzo Windhouwer (KNAW Humanities Cluster)
- Leon van Wissen (University of Amsterdam)
- Maarten van Gompel (KNAW Humanities Cluster)
- Pieterjan De Potter (Ghent Centre for Digital Humanities)

The group is open to new members. Create [an issue](https://github.com/clariah/ig-lod/issues/) to join.
",2022-08-12
https://github.com/CLARIAH/IG-LOD-Findability,"# IG Publication, Use and Findability
IG on finding the Linked Data for your goals and making your Linked Data findable



## Group Members
- Richard Zijdeman (International Institute of Social History, group coordinator)
- Enno Meijers (National Library of the Netherlands, Dutch Digital Heritage Network)
- Willem Melder (Sound & Vision)
- Jacco van Ossenbrugge (CWI, VU University)
- Lodewijk Petram (Huygens ING)
- Joe Raad (VU University)
- Kathrin Dentler (Triply)
- Andrea Scharnhorst (DANS)
- Ronald Siebes (VU University)
- Sjors de Valk (Dutch Digital Heritage Network)
- Thomas Vermaut (KNAW Humanities Cluster)
- Jerry de Vries (DANS)
- Menzo Windhouwer (KNAW Humanities Cluster)
- C. Martinez (E-Science Center)
- M. Wigham (Sound & Vision)
- Ricarda Braukmann (DANS)
- Gerard Coen (DANS)
- Jerry de Vries (DANS)


The group is open to new members. Please join the [group mailing list](https://groups.google.com/u/1/g/clariah-ig---publication-and-use-findability/) and fill out your details to join.
",2022-08-12
https://github.com/CLARIAH/IG-Sustainability,"# IG Sustainability & Storage

This repository is intended to organize the work, output and documentation of the CLARIAH Interest Group (IG) on Sustainability and Storage.

### Goals
- How to archive datasets?
   - Where to store them and in what format
   - How to make them queryable and resolvable
- How to exchange large datasets?
- How to keep endpoints alive and IRIs resolvable?

### Meetings
1. ~~December 7, 2020 at 11.00~~
2. ~~January 14, 2021 at 10:00~~
3. ~~February 11, 2021 at 10:00~~
4. ~~March 11, 2021 at 10:00~~
5. ~~April 19, 2021 at 11:00~~
6. ~~May 26, 2021 at 13:00~~
7. **June 29, 2021 at 13:00** 

### Members
- Harm Nijboer (Huygens ING, Golden Agents)
- Jerry de Vries (DANS, WP4)
- Joe Raad (VU, WP4)
- Menzo Windhouwer (KNAW, WP3)
- René Voorburg (KB)
- Slava Tykhonov (DANS)
- Willem Melder (NISV, WP5)
- Wouter Beek (Triply, WP4)

### Contact
[Joe Raad](mailto:j.raad@vu.nl)
",2022-08-12
https://github.com/CLARIAH/IG-Text,"# Note: This repo is deprecated and moved to the [central CLARIAH PLUS repository](https://github.com/CLARIAH/clariah-plus)!

# CLARIAH Interest Group on Text

This repository is intended to organize the work, output and documentation of the CLARIAH Interest Group (IG) on Text
Processing.

*(note: in the current stage, all of this should be interpreted as a proposal and open for discussion)*

## Introduction

There is a CLARIAH-wide need for robust text processing technologies that can handle historical as well as contemporary
Dutch texts. Partners like VU, INT and RU have contributed different components in WP3 and WP6.

## Aims of the Interest Group

The aims of the IG on Text are:

- foster discussion and knowledge sharing regarding automatic text processing
- enhance interoperability between various text processing solutions
- develop and share best practices
- inform development of CLARIAH text processing tools and services

## Scope of the Interest Group

Our scope is **automatic** text processing, and roughly encompasses the following fields:

- Natural Language Processing
    - automatic linguistic enrichment for multiple languages and multiple time periods
        - named entity extraction & linking
        - dependency parsing, syntactic parsing, morphological analysis
        - part-of-speech tagging
        - lemmatisation
        - sentiment analysis
        - tokenisation and sentence segmentation
    - text normalisation (including post-OCR/HTR correction)
    - optical character recognition & handwriting recognition
    - machine translation
    - language modelling
- Text Mining
- Text Search & Retrieval (raw text, querying of annotations is covered by the [annotation group](https://github.com/CLARIAH/IG-Annotation))

Though our scope is not limited to Dutch, it is probably fair to say that Dutch, Flemish and Frisian, merit most
attention, as we are a project in the Netherlands.

Aspects that are outside the scope of this Interest Group (because they are covered by other IGs):

- manual text annotation (covered by the [annotation group](https://github.com/CLARIAH/IG-Annotation))
- annotation models and formats (covered by the [annotation group](https://github.com/CLARIAH/IG-Annotation))
- speech recognition (covered by the AV group)

## Communication

We use the following communication channel:

- [slack](clariah-workspace.slack.com) (if you don't have access yet, please contact one of the coordinators)

## Tasks

1. Provide [an inventory](docs/inventory.md) of current text processing tools, services and models in CLARIAH,
   either developed in CLARIAH (WP3 or WP6), or third party projects that are adopted as solutions.
2. Identify connections that can be made between various tools (specific workflows/pipelines) to certain specific ends
   desired by the research community.
3. Specify what requirements we want text processing solutions to adhere to for CLARIAH, to facilitate interoperability
   between tools/services. Indicate to what extent the existing solutions adhere to these requirements.

## Group Members

- Maarten van Gompel (KNAW Humanities Cluster) (coordinator)
- Jesse de Does (INT)
- Hennie Brugman (KNAW Humanities Cluster)
- Roeland Ordelman (Netherlands Institute for Sound and Vision)
- Martin Reynaert (DCA - Tilburg University / ILLC - Universiteit van Amsterdam)
- Dirk Roorda (DANS)
- Eduard Drenth (Fryske Akademy)
- Piek Vossen (CLTL, Vrije Universiteit Amsterdam)
- Sophie Arnoult (CLTL, Vrije Universiteit Amsterdam)
- Jan Wijffels (Vrije Universiteit Brussel)
- Enno Meijers (Koninklijke Bibliotheek)
- Rana Klein (Netherlands Institute for Sound and Vision)
- Jan Niestadt (INT)

The group is open to new members.
",2022-08-12
https://github.com/CLARIAH/IG-Vocabularies,"# Note: This repo is deprecated and moved to the [central CLARIAH PLUS repository](https://github.com/CLARIAH/clariah-plus)!

# IG-Vocabularies
Modelling and Vocabulary alignment

### Goals
- Advice, best practices
- Shared vocabs/data models
  - geo
  - persons
  - events
  - time
  - media fragments
  - provenance
- Common thesauri, vocabs (terms and/or entities)
  - What to do if you have a new candidate term/entity/concept?
- Alignment/matching (reconciliation)
  - Tools
  - (data) scope/lenses
  - between data models or entities

### Meetings
- ~~[June 24, 2021](minutes/20210624.md)~~
- ~~[May 25, 2021](minutes/20210525.md)~~
- ~~[April 21, 2021](minutes/20210421.md)~~
- ~~[March 11, 2021](minutes/20210311.md)~~
- ~~[February 4, 2021](minutes/20210204.md)~~
- ~~[December 19, 2020](minutes/20201219.md)~~
- ~~[November 26, 2020](minutes/20201126.md)~~

### Contact
Please join the [mailing list](https://groups.google.com/u/1/g/clariah-ig-vocabularies) or use #ig-vocabularies in the [CLARIAH slack](https://clariah.slack.com/).

",2022-08-12
https://github.com/CLARIAH/IG-Workflows,"# Note: This repo is deprecated and moved to the [central CLARIAH PLUS repository](https://github.com/CLARIAH/clariah-plus)!

# CLARIAH Interest Group on Workflows

This repository is intended to organize the work, output and documentation of the CLARIAH Interest Group (IG) on
Workflows.

## Introduction

WP2 works to facilitate modular distributed workflows that bring together components
developed within and outside of CLARIAH to facilitate the different steps of research
processes. Workflows used in WP3, WP4, WP5 and WP6 contain potential
components at all levels. This Interest Group has a strong connection to IG 1
(DevOps), which focuses on toolchains that support workflows.

### Defining workflows

We can define a worfklow as connection of two or more sofware components (in a directed acyclic graph) for a specific
purpose.  We can make a distinction between fully automated workflows, which is the main focus of this group, and those
that require user-interaction.

Possible characteristics of workflow systems (not all need apply):

- **Scheduling**: Determining which sequence of tasks leads to the desired end result
- **Execution**: Execution of tasks, possibly with automatic parallellisation/abstraction
    - Abstraction of pipeline logic from execution layer: local execution, computing cluster, cloud platform
- **Monitoring**: Real-time monitoring of a running workflow and ability to inspect logs on failure
- **Correction**: Ability to resume the workflow after failure and manual intervention; and/or automated fallbacks
- **Reproducibility**: Given the same input, the workflow should ideally produce the same output every time
    - This requires solutions for software deployment and versioning and is mostly in the realm of the DevOps Interest Group.
    - Handling of provenance data
- **Discovery**: automated discovery of possible workflows
- **Interaction**: Human intervention (steering/inspection/visualisation) in at specific points in a workflow
- **Specification**: How are the workflow specified? (e.g. in what language(s)). The specification can be very specific (e.g a shell script on one extreme of the spectrum) or can be completely abstracted from the executioner (Common Workflow Language (CWL) on the other end of the spectrum).

There are various paradigms for workflow systems and very layers of complexity, a simple shell script or Makefile can already be
considered a basic form of a workflow.

- **goal oriented** (GNU Make, luigi, snakemake, Airflow)
- **procedural** (shell scripts)
- **data-flow oriented** (Nextflow, SciPipe)

We can also make a distinction on the nature of the software components that are connected; are those local components
(local processes) or remote networked components (webservices).

## Aims and scope of the Interest Group

The aims of the IG on Workflows are:

- Foster discussion and knowledge sharing regarding research workflows/pipelines
- Develop and share best practices for setting up workflows, recommend solutions
    - Different audiences may require different solutions
- Create an inventory of workflow systems used in CLARIAH
- Promote interoperability between CLARIAH tools
    - More attention for underlying standards and protocols; focus should first and foremost be on clear specifications of input and output data and communicatin protocols. Tools need a common ground to communicate, if this does not exist then higher-level workflow orchestration is pointless.
    - This details of these are mostly in the scope of the other interest groups on Annotation, Text, Audio/Video, Linked Data.

Determining exactly which CLARIAH tools can or should be connected in a workflow, or envisioning possible workflows, is
**NOT** in the scope of this group but in that of other interest groups such as Text, Audio/Video.

## Communication

We use the following communication channel:

- [slack](clariah-workspace.slack.com) (if you don't have access yet, please contact one of the coordinators)

## Tasks

Unclear, to be defined still!

## Group Members

(Group coordinator is not yet clear)

- Daan Broeder (KNAW Humanities Cluster)
- Maarten van Gompel (KNAW Humanities Cluster)
- Jauco Noordzij (KNAW Humanities Cluster)
- Jaap Blom (Netherlands Institute for Sound and Vision)
- Hennie Brugman (KNAW Humanities Cluster)
- Martijn van de Donk (Netherlands Institute for Sound and Vision)
- Roeland Ordelman (Netherlands Institute for Sound and Vision)
- Joe Raad (VU Amsterdam)
- Adam Belloum (eScience Center)
- Lucien van Wouw (KNAW Humanities Cluster)

The group is open to new members.
",2022-08-12
https://github.com/CLARIAH/iribaker,"## IRI Baker ##

Simple library for baking RFC3987 compliant IRIs from IRI-like strings.

#### Installation

Simply do `pip3 install iribaker`

Or, download this package and run `python setup.py install`

#### Usage

* Import the package (`import iribaker`)
* Call `iribaker.to_iri(string)` with the string you want to check (utf-8 and unicode supported)
* For example: `iri = iribaker.to_iri('http://example.com/€eéf')`
* The function returns:
  * The same (unicode) string, if it is a valid IRI
  * A string where each invalid character is replaced with an underscore (`_`). This means no roundtripping!
  * A quoted version of the string (using the standard `urllib.parse.quote`)

#### License

This software is made available under the MIT license (see LICENSE for details)
",2022-08-12
https://github.com/CLARIAH/jobHoard,"# jobHoard: an occupational 'gazetteer'

Many research projects in the humanities and social sciences uncover mentions of occupations from sources such as archived documents and surveys. In spirit of the [World Historical Gazetteer](http://whgazetteer.org), the worlds largest time aware gazetteer, jobHoard aims to be a finding place for those mentions as well as the projects that uncovered them. Moreover, jobHoard provides important characteristics of those occupations as various projects also take effort to standardize the occupational strings and code them into international classifications, such as HISCO, OCC1950, and ISCO, and provide them with rankings such as ISEI, HISCAM, SIOPS, SOCPO, EGP, HISCLASS. 


# Contribute and make your research FAIR
Please consider contributing your occupational titles to jobHoard, so other may benefit from your coding efforts, like your efforts are likely to have relied on theirs. Also make your project more findable as we will provide links to your project. Contributions are accepted in a lightweight or proper format.

# Light weight format (CSV or spreadsheet)
The light weight format **MUST** contain the following variables:
- at least one occupational title (preferably the 'raw' version mentioned in your original document).

In addition (as colomuns in the spreadsheet or delivered in textas metadata):
- language in which the occupational titles are written;
- region that is covered by the source;
- time period that is covered by the source;
- name of the project from which occupations are derived;
- url of the project from which occupation are derived;


# Proper format (CSV UTF-8)
As the light weight format, but also containg:
- the occupation's standard classification equivalent, such as HISCO or ISCO;
- peristent identifier directing to the project's url.

For any questions, please create an [issue](https://github.com/CLARIAH/jobHoard/issues/new).
",2022-08-12
https://github.com/CLARIAH/LDProxy,"CLARIAH LDProxy - keep you LD URI's resolvable
=======

1. Compile and run:
```sh
mvn package
mkdir cached
rm *.txt && java -jar target/ldproxy.jar
```

2. Configure your browser's proxy, e.g. Firefox:
![FF proxy settings](assets/Preferences.png)

3. http://purl.org/dc/terms/abstract will now return the stored [DC terms RDF XML](src/resources/dc.xml) based on this [example configuration](src/resources/ldproxy-config.xml). 

----
Based on https://github.com/stefano-lupo/Java-Proxy-Server",2022-08-12
https://github.com/CLARIAH/LINKS,"# LINKS - Linkage tool for civil registries
This github repo was made by accident. The actual code and documentation has moved to the original repo at: <a href=""https://github.com/CLARIAH/wp4-links/"">wp4-links</a>.
",2022-08-12
https://github.com/CLARIAH/mediasuite-blog,"# clariah.github.io/mediasuite-info
The website of the CLARIAH Media Suite Blog
Repo: https://github.com/CLARIAH/mediasuite-blog
URL: https://clariah.github.io/mediasuite-blog/",2022-08-12
https://github.com/CLARIAH/nlgis,"# nlgis
placeholder git for issues regarding the [Amsterdam Code](https://datasets.iisg.amsterdam/dataverse/nlgis) (municipalities) and the [Historical Database of Dutch Municipalities (HDNG)](https://datasets.iisg.amsterdam/dataverse/HDNG). Please [create an issue](https://github.com/CLARIAH/nlgis/issues/new/choose) if you have an update or request on these two datasets.
",2022-08-12
https://github.com/CLARIAH/oai-rs-client,"oai-rs-client
====

# Demonstrates syncing of remote resource using: 

 * a simple oai-rs python client
 * an sqlite database loader and jetty server

# Remains to be done:
 - [ ] write more documentation in this README
 - [ ] publish docker
 - [ ] integrate docker into playground.sh",2022-08-12
https://github.com/CLARIAH/pure3d,"![logo](images/pure3d.png)
[![Project Status: WIP – Initial development is in progress, but there has not yet been a stable, usable release suitable for the public.](https://www.repostatus.org/badges/latest/wip.svg)](https://www.repostatus.org/#wip)


# About
This repository contains software and documentation for Pure3D, an infrastructure and conceptual framework for publishing, depositing and exploring 3D worlds and objects.

The work is the product of the [Pure3D project](https://pure3d.eu). 

When finished, it will run on infrastructure of the [KNAW/Humanities Cluster](https://huc.knaw.nl), conforming to [CLARIAH](https://github.com/CLARIAH/clariah-plus) specifications.

[DANS](https://dans.knaw.nl/nl/) is overseeing that Pure3D supports the [FAIR principles](https://www.go-fair.org/fair-principles/).

# Documentation
Technical documentation is in the [docs](https://github.com/CLARIAH/pure3d/blob/main/docs/index.md).

# Team
At the University of Maastricht (see also [here](https://pure3d.eu/index.php/about/team/)):

* [Costas Papadopoulus](https://www.linkedin.com/in/susan-schreibman-818240155/) Principal Investigator.
* [Susan Schreibman](https://www.linkedin.com/in/susan-schreibman-818240155/) Co-principal Investigator.
* [Kelly Gillikin Schoueri]() PhD researcher.
* [Sohini Mallick](https://nl.linkedin.com/in/sohini-mallick) Developer.

At KNAW/HuC:
* [Mario Mieldijk](https://nl.linkedin.com/in/mario-mieldijk-8b640422) Lead engineer concern infrastructure KNAW/HuC.
* [Dirk Roorda](https://pure.knaw.nl/portal/en/persons/dirk-roorda) Lead developer, works at KNAW/HuC.

At DANS, (see also [here](https://dans.knaw.nl/en/collaborations/pure3d/)):
* [Hella Hollander](https://pure.knaw.nl/portal/en/persons/hella-hollander)
* [Jerry de Vries](https://pure.knaw.nl/portal/en/persons/jerry-de-vries)
* [Raul Tavares](https://dans.knaw.nl/en/about/team/)

# Authors
* Sohini Mallick (code)
* Dirk Roorda (documentation)


",2022-08-12
https://github.com/CLARIAH/qber,"## QBer
**Author:**	Rinke Hoekstra  
**Copyright:**	Rinke Hoekstra, VU University Amsterdam  
**License:**	MIT License (see [license.txt](license.txt))  

QBer is a tool for automatically converting CSV or SAV files that contain statistical data (currently tailored for census data) to the [RDF Data Cube vocabulary of the W3C](http://www.w3.org/TR/vocab-data-cube/).

### Installation

##### Prerequisites

* Make sure you have a working version of the [CSDH API](https://github.com/CLARIAH/wp4-csdh-api) running (or can connect to one hosted elsewhere).


#### Running Qber

###### Using docker
* Make sure you have docker and docker-compose installed
* Run `docker-compose build` (this step can be removed when this img is pushed to docker hub)
* Optionally update the `docker-compose.yml` file to change the `CSDH_API` URL.
* Run `docker-compose up` to start qber.
* Qber is now running at `http://localhost:8000`

###### Locally

* Make sure you have [Node.js](http://nodejs.org) installed, including its package manager `npm`. Test this by running e.g. `npm --version` in a terminal window.
* If you don't have it, follow the instructions at [Node.js](http://nodejs.org). For MacOS users, we recommend you use [Homebrew](http://brew.sh) to install `npm` and its dependencies: `brew install npm`
* Clone the `master` branch of this repository to a directory of your choice: `git clone https://github.com/CLARIAH/qber.git`
* Change into the folder you just created, and run: `npm install`
* Edit the `QBerAPI.js` file in the `src/js/utils` directory and set the `CSDH_API` variable to the HTTP address of the CSDH instance of your choice, e.g.: `var CSDH_API = ""http://localhost:5000""`
* To start qber:
  * In development mode: `npm run dev`
  * In production mode:
     * First build qber: `npm run build`
     * Then start qber: `npm run serve`
* Qber is now running at `http://localhost:8000`



#### Known issues

QBer has only been tested using the Google Chrome browser. Other browsers may not work as expected (e.g. Safari doesn't always show the login button)

If you experience any unexpected behavior, please report it using the GitHub issues tracker.
",2022-08-12
https://github.com/CLARIAH/ruminator,"# ruminator

Ruminator is a GUI in which to edit JSON schemas produced by [COW](http://csvw-converter.readthedocs.io/en/latest/#). On the right, paste a JSON schema and press the top COW-button to populate the Ruminator-form. Edit the form and press the second COW-button to write your edits to the JSON schema.

![ruminator screendump](img/screendump.png)

## installation

1. git clone or download this repository to a directory
2. open directory in a browser
",2022-08-12
https://github.com/CLARIAH/scholarly-web-annotation,"# CLARIAH Scholarly Web Annotation

This is the site of the CLARIAH Scholarly Web Annotation working group.

",2022-08-12
https://github.com/CLARIAH/scholarly-web-annotation-server,"# Python server for scholarly web annotations

An experimental Python Flask RESTplus server based on the W3C Web Annotation standard, that implements both the [WA data model](https://www.w3.org/TR/annotation-model/) and the [WA protocol](https://www.w3.org/TR/annotation-protocol/). It is developed in tandem with the [Scholarly Web Annotation Client](https://github.com/CLARIAH/scholarly-web-annotation-client) and will eventually be replaced by a proper annotation server.

The server uses [Elasticsearch](https://www.elastic.co/products/elasticsearch) for storage and retrieval of annotations. When running the SWA server, make sure you have a running Elasticsearch instance. Configuration of the Elasticsearch server is done in `settings.py`. This repository contains a file `settings-example.py` that shows how to configure the connection to Elasticsearch. Rename or copy this to `settings.py` to make sure the SWA server can read the configuration file.

## How to install

Clone the repository:
```
git clone https://github.com/CLARIAH/scholarly-web-annotation-server.git
```

Install the required python packages (for pipenv see https://docs.pipenv.org/):
```
pipenv install
```

## How to run

Make sure you have a `settings.py` file. This repository comes with an example settings file `setttings-exampl.py`. The simplest way to get started is to copy that file to `settings.py`:

```
cp settings-example.py settings.py
```

Start the server:
```
pipenv run python annotation_server.py
```

and point your browser to `localhost:3000`

## How to modify

Run all tests:
```
make test_server
```
",2022-08-12
https://github.com/CLARIAH/serpens,"# serpens
*SERPENS: SEaRch PEst and Nuisance Species (A CLARIAH Research Pilot Project)* 

_work in progress_

Historically, some animals have been perceived as threats by humans. These species were believed to carry diseases or harm crops and farm animals. SERPENS aims to study the historical impact of pest and nuisance species on human practices and changes in the public perception of these animals. The KB newspaper collection will be primary source of information to study this. The first impediment lies in accessing the KB collection: keyword search cannot smoothe spelling variations, vernacular vs. Latin names or the fact that many vernacular animal names can refer to other things (e.g. surnames). To remedy this, the WP2-3 diachronic lexicons will be used for query expansion in combination with topic modelling to filter out irrelevant results. Furthermore, instead of returning documents as search results, which are difficult to compare and analyze, we will create a database containing core information for studying human-fauna relationships by employing the WP3-Semantic-Parsing-Dutch tools.

Data
-----
Newspaper articles from the Dutch National Library's [Delpher](http://delpher.nl) portal are used in this project. The directory 'Annotations' contains spreadsheet files with information about individual newspaper articles such as their publication date, newspaper, article type and document identifier and whether the article reports on a nuisance species or the species name is used in a different context. We discern 8 categories: 

**Natural history** General articles about the animal, e.g. it subsists on birds or x number were stuffed and became part of a museum collection  
**Nuisance, material damages** The article mentions the animal as causing ma- terial damages, e.g. beetles damaging crops or lynxes killing chickens  
**Nuisance, immaterial damages** The article mentions the animal as a nui- sance without material damages e.g. polecats found to walk over someone’s face whilst they were in bed, or (possibly irrational) fear for a certain animal  
**Pest control** Organised hunt to bring down the number of pest species, e.g. ad for hunting dogs  
**Hunt for economic reasons** Hunting to use the fur, meat or other parts of the animal e.g. an article mentioning that the hunting season has started again and that polecat fur earns the hunter a good price  
**Prevention** Non-lethal actions against pest species, e.g. advice in the newspa- per on which plants keep away pest species  
**Accidents** The article mentions an unintentional encounter with the animal, e.g. roadkill  
**Figurative** Figurative language featuring the animal e.g. eyes like a lynx Other Articles not pertaining to the animal, e.g. a ship named ‘Lynx’ or a person whose last name is ‘Bunzing’  
",2022-08-12
https://github.com/CLARIAH/skosmos,"# SKOSMOS
SKOSMOS container setup

## Run with docker-compose
### Prepare
Put your vocabularies as `*.ttl` in the `data/load` directory accompanied by a `*.ttl.config` file. See [countries.ttl.example](./data/load/countries.ttl.example) and [countries.ttl.config.example](./data/load/countries.ttl.config.example) for an example. To try out the example: remove the `.example` suffix.

### To start
```bash
$ docker-compose up -d
```

### To stop:
```bash
$ docker-compose down
```
### Add vocabularies
Just add the `.ttl` and `.ttl.config` to `data/load` and restart the `skosmos-web` container.

## License
[MIT License](LICENSE.md)
",2022-08-12
https://github.com/CLARIAH/software-quality-guidelines,"# Software Quality Guidelines

This repository contains guidelines for software quality & sustainability for
[CLARIAH](http://www.clariah.nl). It has been composed as a part of WP2 task 54.100. Please access the
[PDF document](https://github.com/CLARIAH/software-quality-guidelines/blob/master/softwareguidelines.pdf) in this repository to read the latest version. The guidelines themselves are also available in the form of an [interactive web-survey](http://softwarequality.clariah.nl).

The document came about in response to the need for increasing attention to
software quality and sustainability in the academic world, as software is such
an essential component of our research, yet good software development practice
is often not adhered to in a satisfactory degree, and sustainability is often
problematic. The guidelines are an instrument for both for the developers of software, as
well as software adopters, to assess the quality and sustainability of software

The guidelines are currently in a proposal stage in which we would very much like
to get some input. Please see the Request for Comment section at the end of the PDF.

Use our [Github Issue Tracker](https://github.com/CLARIAH/software-quality-guidelines/issues) for
comments if possible.

",2022-08-12
https://github.com/CLARIAH/stan-client,"# STAN client

Stand-off annotations, you know: for scholars

## Typescript

The tsconfig.json was initially generated with `tsc --init` and then subsequently altered to make sure:

- A modern version of ECMA script is supported (e.g. for using Promises)
- The Javascript output files are written to the `dist` directory
- TODO


Compile with `tsc -p tsconfig.json`

## Learning sources

[How to integrate Promises in ts](https://stackoverflow.com/questions/27573365/how-to-use-typescript-with-native-es6-promises#27573589)",2022-08-12
https://github.com/CLARIAH/tool-discovery,"# CLARIAH Tool Discovery

This repository contains everything related to tool discovery and software metadata in CLARIAH.
* `Dockerfile`:  The docker container for the CLARIAH Tool Discovery pipeline, including both the
    [harvester](https://github.com/proycon/codemeta-harvester) and the [server and
    API](https://github.com/proycon/codemeta-server) powering the CLARIAH Tool Store.
* `source-registry/`: The tool source registry, contains the source repositories locations and service endpoints for all
    CLARIAH tools. **This is open for [contributions](CONTRIBUTING.md)**
* ``etc/``, ``static/``: supporting files for the deployment at
* ``legacy/cmdi/``: Contains legacy CMDI metadata as gathered in WP3 task MD4T at Utrecht University

## Service

The tool discovery service, consisting of a harvester that runs on regular intervals (each night) and a tool store,
is deployed at https://tools.clariah.nl (production) and https://tools.dev.clariah.nl (development).

All harvested data is also available as individual files via https://tools.dev.clariah.nl/files/
 
## Links

* [Tool Discovery kanban board](https://github.com/orgs/CLARIAH/projects/1) - Project planning

## Execution

docker build -t codemeta-server-tool .
docker run -itd -p 80:80 --env-file=my-env.env --name=cm-srv -v codemeta_volume:/tool-store-data --restart=unless-stopped codemeta-server-tool 

Local yamls for sources harvesting: add to run -v $PWD/source-registry/:/usr/src/source-registry/source-registry/ and set LOCAL_SOURCE_REGISTRY=true in my-env.env

Event-based collection is always On. POST your codemeta.json file with curl -u <nginx-user> -XPOST -H ""Content-Type: application/json"" -dcodemeta.json -u user <url>/rest/

For private git repo add to docker run -e  GIT_USER='youruser' -e GIT_PASSWORD='yourtoken'
To clean up remove the volume codemeta_volume",2022-08-12
https://github.com/CLARIAH/tool-metadata,"

* ``schemas/tool-metadata.jsonld`` - Defines some software metadata fields used by [CLAPOP](http://portal.clarin.nl/) to be used as a CLARIN-specific extension for software metadata specifications using [codemeta](https://codemeta.github.io).
",2022-08-12
https://github.com/CLARIAH/usecases,"# Note: This repo is deprecated and moved to the [central CLARIAH PLUS repository](https://github.com/CLARIAH/clariah-plus)!

# CLARIAH-PLUS use cases

## Introduction

This repository collects **all** use cases for CLARIAH-PLUS, across all work
packages and interest groups. Our aim is to have a central repository where
everybody can get insight into the wide variety of use cases we currently deal
with. With this transparency with intend to foster cooperation and provide a basis
for discussion and implementation, both in the CLARIAH interest groups and beyond.

At this stage, the structure of the use cases is still very much open-ended, we
intend to first gather use cases and in a later stage distill a more structured
format.

## How to contribute?

Everybody working in CLARIAH is welcome and encouraged to contribute his/her
use cases. Please see the [contribution guidelines](CONTRIBUTING.md) and the [template](TEMPLATE.md).

## Use cases

The below listing links to all use cases based on their state, but in no further particular order, better ordering will be applied at a later stage and is all subject to further discussion as we grow.


### Proposed

|   Name                                                          |      WP | IG  | Tech |
| :-------------------------------------------------------------- | :------ | :-- | :--- |
| [A Cross-Media Analysis of the Refugee Crisis](cases/Cross-Media_Analysis_of_the_Refugee_Crisis.md) | 5, 6 | WF,AV,TP | MediaSuite |
| [Annotatable collections](cases/annotatable-collections.md) | 6, 3, 5 | Ann, AV, LoD |
| [Corpus-based history of ideas](cases/black-phil-lab.md) | 6 | TP |
| [Deep-learning for Dutch text mining](cases/deepfrog.md) | 3 | TP | DeepFrog |
| [Exploiting UD treebanks for the extraction of word combination statistics](cases/treebank-combi.md) | 3 | TP |
| [Exploring Dialect2Keyword](cases/Usecase-CLARIAH-dialect2keyword.md) | 3 | Ann, Cur |
| [Historical enrichment: Proof of concept corpus exploitation](cases/poc-corpus-historical-enrichment.md) | 3,6 | TP |
| [Historical research across (audio)visual and textual archives and collections](cases/historical_research_across_archives_collections.md) | 5 | Ann, WF, Cur | MediaSuite |
| [Interoperability Integrated Text Annotation and Linked Open Data](cases/folia-lod.md) | 3,6 | Ann,LoD,TP | FoLiA |
| [Key point detection/pose analysis for Eye Jean Desmet collection through DANE](cases/key_point_detection-pose_analysis.md) | 5 | AV | MediaSuite |
| [Micro-frontends for manual scholarly annotations](cases/scenarios-manual-annotation.md) | 6, (2, 3, 5) | Ann, LoD, Prv |
| [Parallel corpus mining](cases/parallel-corpus-mining.md) | 3 | TP |
| [Parallel historical corpus mining](cases/hipaco.md) | 3 | TP |
| [Sex, Beer and RomComs: Studying the Debate on Dutch film](cases/debate_on_dutch_film.md) | 5, 2, 3, 4, 6 | AV, TP, Ann, LoD, WF, Cur, UI |
| [Stories in Motion: Integrating oral histories into the Media Suite](cases/stories_in_Motion.md) | 5 | AV | MediaSuite |
| [Triples-workbench: store, browse, query and visualize triples](cases/triples-workbench.md) | 4 | LoD, UI, WF |
| [Neural-Tscan]: adapt the existing tscan for stylistic analysis of text with neural models and linguistic features for interpretibility | 3, 6 | FoLiA, NAF |

### In progress

|   Name                                                          |      WP | IG  | Tech |
| :-------------------------------------------------------------- | :------ | :-- | :--- |
| [ASR for sensitive data](cases/Usecase-sensitiveASR.md) | 3 | AV |
| [Computer vision annotations 'n' enrichments of audiovisual data](cases/dane-av-enrichments.md) | 5 | AV,DO,WF | DANE |
| [Curation of transcribed historical newspaper corpus (Wp6 Use case 2)](cases/curation-historical-newspapers.md) | 6 | TP |
| [DANS CMDI use case for CLARIAH WP3](cases/dans-cmdi.md) | 3 | Prv | CMDI |
| [Digitization workflow for historical newspapers (WP6 use case 2)](cases/ocr-historical-newspapers.md) | 6,3 | TP |
| [Historical research on media-events across heterogenous broadcast datasets with linked and missing data](cases/historical_research_on_media-events_across_heterogenous_broadcast_data.md) | 5 | AV,TP,Ann,LoD | MediaSuite |
| [Linkage of Dutch Civil Records](cases/civil-records-linkage.md) | 4 | LoD,WF,Cur | burgerLinker |
| [Providing Language and Speech webservices at CLST (Radboud University, Nijmegen)](cases/clst-webservices.md) | 3, 2 | DO,TP,WF | CLAM, LaMachine |
| [Retrodigitization of Text-critical Editions](cases/max-weber.md) | 3 | TP,Ann | FoLiA, FLAT |
| [Speech transcription of audiovisual data](cases/mediasuite-speech-transcription.md) | 5 | AV,Do | Media Suite |
| [Store, share and search web annotations](cases/web-annotation-services.md) | 6, 3, 5 | AG, LoD |
| [Tokenising, lemmatising, tagging and dependency parsing annotation of Frisian text using UD Pipe Frysk](cases/FRYPOS.md) | 3 | Ann,TP | UDPipe |
| [Tracing Re-use](cases/tracing_re-use.md) | 5 | WF,AN,AVP  | Media Suite|
| [Vocab Recommender](cases/vocab-recommender.md) | 4 | LoD,CuR,WF |
| [WP6 Use Case 005: Lossless Text Data Exchange](cases/wp6-use-case-5-lossless.md) | 6 | TP |
| [WP6 Use case 3: Tools to data](cases/wp6-use-case-3-tools-to-data.md) | 6 | DevOps, TP |
| [WP6 use case VOC](cases/wp6-use-case-1-voc.md) | 6 | TP |

### Completed

|   Name                                                          |      WP | IG  | Tech |
| :-------------------------------------------------------------- | :------ | :-- | :--- |
| [Annotation of spelling correction for CLIN28 Shared Task](cases/clin28sharedtask.md) | 3 | Ann | FLAT |
| [Automatic linguistic enrichment for Dutch texts using Frog](cases/frog.md) | 3 | TP | Frog, FoLiA |
| [COW:Integrated CSV to RDF converter](https://github.com/CLARIAH/usecases/blob/master/cases/usecase-cow.md) | 4 | LoD,Cur,WF | COW |
| [Data format for linguistically-annotated corpora](cases/folia-corpora.md) | 3 | Ann,TP,LoD | FoLiA |
| [Extracting Information about Flood Disasters ](cases/flood-tags.md) | 3 | Ann | FLAT |
| [Nederlab: Automatic Linguistic Enrichment of Historical Dutch](cases/nederlab-enrichment.md) | 3, 6 | TP,Ann | Frog,FoliA |
| [Negation Annotation in Dutch dialogue](cases/negation-annotation-task.md) | 3 | Ann | FLAT |
| [PARSEME: Annotation of verbal multi-word expressions](cases/parseme.md) | 3 | Ann | FoLiA |
| [PICCL deployment at a CLARIN centre](cases/piccl-deployment.md) | 3 | DO | LaMachine |
| [Quickly building webservices with CLAM](cases/clam-webservice.md) | 3, 2 | DO,WF,TP | CLAM |
| [Research Environment for Workshop: Cataloguing of Textual Cultural Heritage Objects](cases/cataloguing-of-textual-cultural-heritage-objects.md) | 3 | DO,TP | LaMachine |
| [Syntactic Movement Annotation](cases/syntactic-movement-annotation.md) | 3 | Ann | FLAT |
| [Tools to the data: Text Mining for Health Inspection](cases/text-mining-for-health-inspection.md) | 3 | DO,TP | LaMachine, Frog |
| [grlc -> sparql queries as api and with metadata](cases/queries-api-fair.md) | 4 | LoD | grlc |

### Legend

[Interest groups](https://github.com/CLARIAH/ig):

* Ann = [Annotation](https://github.com/CLARIAH/IG-Annotation)
* AV = [Text Processing](https://github.com/CLARIAH/IG-AVProcessing)
* Cur = [Curation](https://github.com/CLARIAH/IG-Curation)
* DO = [DevOps](https://github.com/CLARIAH/IG-DevOPS)
* LoD = [Linked Open Data](https://github.com/CLARIAH/IG-LOD)
* TP = [Text Processing](https://github.com/CLARIAH/IG-Text)
* Prv = Preservating
* UI = UI/UX
* WF = [Workflows](https://github.com/CLARIAH/IG-Workflows)

The technology column refers to the most prominent CLARIAH products that feature in the use case (keep it short):

* [BurgerLinker](https://github.com/CLARIAH/burgerLinker) - Command line tool for linking civil registries
* [CLAM](https://proycon.github.io/clam) - A framework to quickly build RESTful webservices and have a generic web-UI
* [CMDI](https://www.clarin.eu/content/component-metadata) - CLARIN's Component Metadata Infrastructure
* [COW](https://github.com/clariah/cow/) - CSV to RDF converter (CSVW)
* [DANE](https://github.com/CLARIAH/DANE) - Handles compute task assignment and file storage for the automatic annotation of content.
* [DeepFrog](https://github.com/proycon/deepfrog) - Deep-learning NLP tool & models for Dutch
* [FLAT](https://github.com/proycon/flat) - A web-based annotation tool for (linguistic) annotation of text documents
* [FoLiA](https://proycon.github.io/folia) - An XML-based Format for Linguistic Annotation
* [Frog](https://languagemachines.github.io/frog) - An NLP-suite for Dutch
* [grlc](http://grlc.io/) - converts your SPARQL queries into RESTful APIs.
* [LaMachine](https://proycon.github.io/LaMachine) - A meta-distribution with various NLP/CLARIAH tools and services
* [MediaSuite](https://mediasuite.clariah.nl/) - A research environment to search, analyse, and annotate media
    collections.
",2022-08-12
https://github.com/CLARIAH/video-annotation-interoperability,"# video-annotation-interoperability
Proposal for crosswalks between a number of video annotation tools, including the CLARIAH Web Annotation tool, ELAN, FrameTrail, VIAN and Waldorf.js.

[Blog post on our first meeting (12-13 July 2018)](https://clariah.github.io/mediasuite-blog/blog/2018/07/11/Clariah-annotation-expert-meeting)

[Blog post on our second meeting (13 July 2019)](https://www.clariah.nl/en/new/blogs/vaint-video-interoperability-interest-group-second-meeting-july-13-2019)

## Introduction
Digital annotation is a common activity among scholars, and takes places across different data and media types and across different digital tools. Digital annotation is one of the most active research fields in the last years. The conditions in order to use annotations in computer aided research and for the design of new digital research tools have improved significantly. The recent approval of the W3C Web Annotation Data Model (formerly Open Annotation) as W3C recommendation marks a big step in this respect, allowing to gain interoperability of annotation data across annotation types, tools, research practices and domains.

This proposal describes the planning of a two-day face-to-face expert meeting and workshop that aim to improve annotation interoperability within the CLARIAH infrastructure as well as with external annotation tools. Improving the interoperability allows scholars to connect their work both across CLARIAH work packages, as well as with annotations created by external tools.

The Web Annotation (WA) Data Model is a very generic model that allows for multiple solutions in order to model the structure of certain types of annotations. Furthermore, the model permits to be extended by other models in order to tackle issues which are beyond the scope of granularity of the Web Annotation Data Model. This situation calls for the definition of so called application profiles as they are used in the context of other generic models, such as Dublin Core. An application profile defines a subset of a data model in order to model a phenomenon that is of smaller scope than the scope of generic WA model, and that is relevant to specific types of research. In this respect the annotation of video is a specific case of annotating that has its own set of issues in comparison with the annotation of other resources. Furthermore an application profile describes recommendations and best practices for how specific terms of a model should be applied and interpreted within such a specific domain of application.

In the context of video annotation, such an application profile is of particular interest for a variety of reasons. Video annotation is a complex task because video is a multimodal media resource. The landscape of digital annotation tools is heterogeneous and reaches from desktop application tools to web applications. Tools specialize in specific tasks and model their data around such tasks. Many tools that are commonly used have a very long history which reaches back to times in which no shared understanding existed how to model annotations in a standardized way. Consequently, today’s annotation data is still produced and represented in different data models as well as content models. The work on an application profile for video annotation data will enable the definition of so called alignments and crosswalks between such models and the data that is represented in them. In doing so, such alignments enable two important things. On the one hand they make it possible to export annotation data which was created with one tool, to use it in another technological environment. On the other hand they create better conditions for the long-term-availability and -preservation of annotation data. Corresponding infrastructure will be able to focus on the requirements of the application profile and the transformation of annotation data to the model of the application profile.

CLARIAH work package five has worked in implementing an annotation tool which uses the W3C annotation model for structuring the annotation data. This tool is of high importance for supporting scholars who work with audio-visual collections (e.g., film and television scholars, oral historians and, in the future, communication scholars working on conversation analysis as part of the ADVANT proposal). Recent discussions in relevant stakeholder groups such as the DARIAH WG Digital Annotation and the ADHO SIG AVinDH showed that significant interest exist in the realisation of the aforementioned goals. Video annotation and interoperability were also an important topic during the recent Lorentz Workshop on “Collecting, Annotating and Analyzing Video Data”.

## Goals

1. Drafting definitions of Crosswalks between schemas of commonly used annotation tools,  in CLARIAH and external, on the basis of an interoperability layer which is derived from the W3C Web Annotation standard.
2. Drafting of a W3C Web Annotation application profile for video annotation on the ground of the aforementioned interoperability layer.
3. Involve international scholars and developers in the discussion and development of interoperable annotation in CLARIAH. 

The draft crosswalks and application profile will then be distributed for broader input, and will be improved and finalised in subsequent meetings.

## Deliverables

The schema crosswalks and application profiles will be published on GitHub and will be actively promoted to encourage uptake and reuse.

+ the [draft implementation of the data model](data/example_annotation.json) was written by John P. Bell (see [novomancy/webannotation](https://github.com/novomancy/webannotation) for the [original document](https://github.com/novomancy/webannotation/blob/master/unified_v0.1.json))

",2022-08-12
https://github.com/CLARIAH/virtuoso-quad-log,"# Virtuoso Quad Logger

- The components in this repository are intended to be used by developers and system administrators.
- In case of questions [contact](https://github.com/CLARIAH/virtuoso-quad-log/issues/new) the CLARIAH team.
 
____

The RDF data model can be used to share and distribute information from a wide variety of sources 
and has [distinguished advantages](https://www.w3.org/RDF/advantages.html) over other data models.
Keeping track of state and changes in dispersed data stores and propagating state and changes
to other data stores or a central hub
is out of scope of the data model it self. Reporting the initial state of a data store, keeping
track of changes during the life of the store and publishing this state and these changes to the
outside world in accordance with a well-described protocol is the subject of this repository.


This *virtuoso-quad-log* repository harbors a chain of tools for propagating and publishing 
changes in RDF-data
that are kept in a [Virtuoso triple store](http://virtuoso.openlinksw.com/). Essential components 
in this chain are:

1. **quad-logger** generates logs of all initial, added, mutated or 
deleted [N-Quads](https://www.w3.org/TR/n-quads/) in a
[Virtuoso quad store](http://virtuoso.openlinksw.com/rdf-quad-store/) in the
[RDF-patch](https://afs.github.io/rdf-patch/) format, grouped in files per graph.
2. **graph-splitter** will subdivide these rdf-patch files into 
directories per graph. If a subdivision along graph iri is not nescesary 
or not wanted, the graph-splitter can be left out of the chain.
3. **resourcesync-generator** enables the synchronization of the produced resources over the 
internet by means
of the [Resource Sync Framework](http://www.openarchives.org/rs/1.0/resourcesync).

The **example-virtuoso-server** is there for reference and demonstration purposes. The tools can be deployed as
services under [Docker-compose](https://docs.docker.com/compose/).

## Overview

![Overview](/img/environment2.png)

<i><small>The above image shows the quad-logger, the graph-splitter and the resourcesync-generator 
in their environment.
The Virtuoso server is instructed to log its transactions in log files.  
The `quad-logger` interacts
with the Virtuoso server by means of the Interactive SQL interface. It reads the 
transaction logs and transforms them to rdf-patch formatted files.  
The `graph-splitter`
will subdivide the rdf-patch files and group them
in folders per graph iri. Folder names are the base64 translation of the graph iri.
If a subdivision along graph iri is not nescesary or not wanted, 
the graph-splitter can be left out of the chain.  
The `resourcesync-generator`
bundles the rdf-patch files in zip-files and publishes them in accordance with the
Resource Sync Framework. In case N-Quads are subdivided along graph iri in separate folders, each folder will
be represented as a distinct set of resources.  
The quad-logger, the graph-splitter and the resourcesync-generator 
can be deployed as
Docker containers. Here they are deployed as docker-compose services. 
The Http server and the Virtuoso server can also be included in the docker-compose services.</small></i>

## Documentation
Documentation of the software in this repository is split over several files.
- **README.md** (this file) contains a general introduction.
- **[MOTIVATION.md](/MOTIVATION.md)** documents the background of this repository and 
motivates choices made.
- **[DEPLOY.md](/DEPLOY.md)** contains detailed instructions on the usage of the tools in this repository.
- **[VIRTUOSO_CONFIG.md](/VIRTUOSO_CONFIG.md)** communicates critical issues in your 
Virtuoso configuration.

## Quickstart

To launch a self-contained sandbox you can use the docker-compose-example-setup.yml

	docker-compose -f docker-compose-example-setup.yml build
	docker-compose -f docker-compose-example-setup.yml up

To connect the logger to a production virtuoso server, you can edit the environment variables in 
the docker-compose.yml and launch using

	docker-compose build
	docker-compose up

This also launches a local nginx which you might, or might not, want to do.

To advertise the logs you should provide either a robots.txt or a Source Description at the location 
that you submit to Work Package 2.
See http://www.openarchives.org/rs/1.0/resourcesync for more information, 
or [contact us!](https://github.com/CLARIAH/virtuoso-quad-log/issues/new?Title=How+do+I+submit+my+data)
(The playground advertises the logs using the hidden folder .well-known)
",2022-08-12
https://github.com/CLARIAH/vocabs,"# vocabs
Vocabularies, concept schemes, taxonomies, etc.
",2022-08-12
https://github.com/CLARIAH/webannotation-examples,"# webannotation-examples
Examples of web annotations
",2022-08-12
https://github.com/CLARIAH/wp2-GISLOD,"# wp2-GISLOD
Exchange for issues and code related to transposition of gemeentegeschiedenis.nl-data to LOD.

Data created during the 'Knutseldag' March 1st, 2017: are available from: http://hdl.handle.net/10622/Y00UJQ

![Map Age Guide](gfx/map_age_guide.png)

In the grand scheme of things, we'll eventually get a Dutch map age guide, similar to the [XKCD Map Guide](https://xkcd.com/1688/).
",2022-08-12
https://github.com/CLARIAH/wp2-interaction,"# Documentation on interactions of WP2

CLARIAH consists of 5 working packages as described in the Table 1.

*Table 1. Clariah working packages and domains of responsibility*

| Working package | Domain                                         |
|-----------------|------------------------------------------------|
| WP1             | Management and coordination                    |
| WP2             | Overarching Structure                          |
| WP3             | RI Linguistics / Textual data                  |
| WP4             | RI Social & Economic History / Structured data |
| WP5             | RI Media Studies / Adiovisual data             |

WP2 provides the overarching structure in Clariah and plays a central role in the communication amongst the working packages as visualized in Figure 1. 

![figure1](https://github.com/CLARIAH/wp2_interaction/blob/master/wp2_schematic.png?raw=true ""WP2 Interactions Schematic"")

Within CLARIAH, WP2 is connected to the other working packages through API's. The information exchange between the working packages goes in both directions as all working packages will provide data and vocabularies (ontologies). This goes for WP2 as well. While responsible for common concepts, such as '*where*' (locations) and '*what*' (thesauri, for example on occupations), WP2 will also provide content based on the individuals from the [Biografisch Portaal van Nederland](http://www.biografischportaal.nl) and the [Historische Steekproef Nederland](http://www.iisg.nl/hsn/index.html), the latter through authentication and registration procedures, also part of WP2.

Beyond CLARIAH, WP2 is connected to datahubs such as [Europeana](http://www.europeana.eu/portal/) and [DBpedia](http://wiki.dbpedia.org). In addition to the API's users will be able to select, extract and visualize data through a GUI connected to WP2.


> Figure 1 was made using [yEd](http://www.yworks.com/en/products/yfiles/yed/) freely available for Windows/Linux/Mac. The raw file that can be edited is [wp2_schematic.graphml](https://github.com/CLARIAH/wp2_interaction/blob/master/wp2_schematic.graphml).

",2022-08-12
https://github.com/CLARIAH/wp3-head-words,"# wp3-head-words
Contains scripts to derive, and lists of, head words

## Directory structure
- data
  + source: contains original data
  + derived: contains data derived through scripts
- img: contains images, graphs, etc
- scripts: contains scripts, for example .R files
",2022-08-12
https://github.com/CLARIAH/wp3-multilingual-finegrained-entity-typing,"# wp3-multilingual-finegrained-entity-typing
Code and experiments for multilingual fine-grained entity typing module developed in WP3: linguistics  

This module was developed at CLTL, where you will find the main code repository: https://github.com/cltl/multilingual-finegrained-entity-typing 

",2022-08-12
https://github.com/CLARIAH/wp3-queries,# wp3-queries,2022-08-12
https://github.com/CLARIAH/wp3-semantic-parsing-Dutch,"# wp3-semantic-parsing-Dutch
Robust semantic parsing Dutch (state-of-the-art natural language processing pipeline) 

- [Word Sense Disambiguation](https://github.com/cltl/svm_wsd): system based on Support Vector Machines to assign senses and a system confidence score to words. 
- [Entity recognition, classification](https://github.com/ixa-ehu/ixa-pipe-nerc) & [linking](https://github.com/ixa-ehu/ixa-pipe-ned): identifies names in text, assigns a type such as person, location or organisation and tries to anchor it to its DBpedia resource ([DBpedia] (http://wiki.dbpedia.org) is a graph database that contains the structured information from Wikipedia) 
- [Ontotagger](https://github.com/cltl/OntoTagger): module that inserts ontological labels to Wordnet synsets associated with terms or directly to the lemmas of the term based on the external resources provided.
- Semantic Role Labelling (event extraction): identifies and classifies the semantic arguments in a sentence, for example who was the perpetrator of an action and who was the subject, as well as locations which can be used to generate event descriptions 
- [Factuality/Attribution](https://github.com/cltl/multilingual_factuality): qualifies the certainty (certain/probable/possible) of an event, whether the event is confirmed or denied (pos/neg) and whether it is in the future (future/non-future)
- [Opinion extraction](https://github.com/rubenIzquierdo/opinion_miner_deluxePP): detects opinion entities (holder, target and expression)
- [Simple tagger](https://github.com/cltl/SimpleTagger): generic tagger that identifies concepts in text and links this to external references. It currently comes with a basic version to tag Historical Occupations (from [Hisco](https://socialhistory.org/en/projects/hisco-history-work)) and identify family relations for Dutch
- [NLP2RDF crystallisation strategies](https://github.com/cltl/EventCoreference): resolves coreference of entities and events within and across documents to generate event descriptions based on the [Simple Event Model](http://semanticweb.cs.vu.nl/2009/11/sem/) and source perspectives according to [GRaSP](http://dx.doi.org/10.1016/j.knosys.2016.07.013). 

We have other modules available, contact marieke.van.erp@vu.nl for more information. 

",2022-08-12
https://github.com/CLARIAH/wp3-vocab-conversion,"# wp3-vocab-conversion
Code to convert lexicons and vocabularies to RDF within the CLARIAH project. The code for this repo lives and is updated at: https://github.com/cltl/clariah-vocab-conversion 

The lexicon conversion is described in Isa Maks, Marieke van Erp, Piek Vossen, Rinke Hoekstra, Nicoline van der Sijs (2016) [Integrating Diachronous Conceptual Lexicons through Linked Open Data](http://2016.dhbenelux.org/wp-content/uploads/sites/4/2016/05/79_Maks-etal_FinalAbstract_DHBenelux2016_demo.pdf). In DHBenelux 2016. 9-10 June, 2016. Luxembourg. 
",2022-08-12
https://github.com/CLARIAH/wp4-civreg,"# wp4-civreg

Data pipeline for Dutch historical civil registry",2022-08-12
https://github.com/CLARIAH/wp4-clioinfra,"# wp4-clioinfra
Server files (in www folder) and R scripts + html templates (in rebuild folder) used to create the clio infra website just from the data deposited on the clioinfra dataverse

See in the folder Manual for detailed instructions on how to use the files provided here and the R scripts to rebuild the site almost from scratch. 
",2022-08-12
https://github.com/CLARIAH/wp4-cow-conversions,"# wp4-cow-conversions
This repository contains the converter files used to convert .csv's into RDF using COW.


URI's used by IISG

# code
- https://iisg.amsterdam/<dataset\>/code/<codelist\>/<code\>
- https://iisg.amsterdam/hisco/code/status/22
- https://iisg.amsterdam/hisco/code/hisco/12345

# dimension
- https://iisg.amsterdam/<dataset\>/dimension/<dimension\>/<value\>
- https://iisg.amsterdam/opgaafrollen/dimension/land/nederland

# vocab
- https://iisg.amsterdam/<dataset\>/vocab/\<term\>
- https://iisg.amsterdam/hisco/vocab/minorGroup
",2022-08-12
https://github.com/CLARIAH/wp4-datalegend-api,"## CLARIAH DataLegend API
**Author:**	Rinke Hoekstra  
**Copyright:**	Rinke Hoekstra, VU University Amsterdam  
**License:**	MIT License (see [license.txt](license.txt))  
**Website:**  <http://datalegend.net>  

### Installation

First, open a terminal or console and clone this git repository to a directory of your choice:

`git clone --recursive https://github.com/CLARIAH/wp4-datalegend-api.git`
(see e.g. `--branch 1.0.0` to download a stable version is currently the latest version)

Alternatively, you can  download a release from Github and pull the submodule:

`git submodule init`
`git submodule update`

Change directory to wp4-datalegend-api directory and then (to keep things nice and tidy) use [virtualenv](https://virtualenv.pypa.io/en/latest/installation.html) to create a virtual environment, and activate it:

`virtualenv .`

and

`source bin/activate` (on unix/linux-style systems)

Then install the required Python packages using [pip](https://pip.readthedocs.org):

`pip install -r requirements.txt`

This should also install all necessary packages for the `wp4-converters` package, but if this is not the case, you can `pip install -r src/app/wp4-converters/requirements.txt` as well

Copy the `src/app/config_template.py` file to `src/app/config.py` and make necessary changes (see documentation in the file).

Then, in directory `src` run:

`python run.py`

the API will be running on <http://localhost:5000> (click this link, or copy it to your browser).

Go to <http://localhost:5000/api-docs> to view the API specs in the Swagger UI

Make sure to always activate the `virtualenv` before running the API again.
",2022-08-12
https://github.com/CLARIAH/wp4-dataset-management,"# wp4-dataset-management
### Simple web app for generating SPARQL queries for removing older versions of datasets

### What it does

* The app queries the datalegend SPARQL endpoint to retrieve all dataset definitions (nanopublications).
* It renders a web page, where users can toggle candidate dataset versions for deletion
* At the bottom of the page, clicking the button `Generate SPARQL query` will generate the SPARQL query/queries needed to delete the corresponding graphs from the triplestore
* This query can only be executed through an endpoint with the proper access rights (copy & paste)
* **NB:** the web app does not allow the direct execution of the query.

### Starting

* Initiate and activate a `virtualenv` if you so desire
* Run `pip install requirements.txt`
* **Optional**: Set the `ENDPOINT` variable in `src/app/config.py` to point to the proper SPARQL endpoint
* Change dir into `src`
* Run `python run.py`
* The application will be running at <http://localhost:5000>
",2022-08-12
https://github.com/CLARIAH/wp4-docs,"# WP4-Documentation

Documentation for the Structured Data Hub on:
- installation procedures
- data models and workflows
- architecture
- project updates
",2022-08-12
https://github.com/CLARIAH/wp4-druid,"# wp4-druid
Druid is the 'triplestore' of dataLegend (work package 4 in CLARIAH). The triplestore hosts structured datasets from social and economic history. Druid shares it's generic datasets with Anansi (the central datastore in CLARIAH). Druid was created by Laurens Rietveld of [Triply](https://triply.cc).

This package holds the documentation for Druid in the [Wiki](https://github.com/CLARIAH/wp4-druid/wiki).
",2022-08-12
https://github.com/CLARIAH/wp4-inspector,"# CLARIAH-SDH Inspector

Shows datasets, dimensions and users as an interactive graph
",2022-08-12
https://github.com/computationalgeography/campo,"# Campo

Campo modelling framework for fields and agents

Campo is an environmental modelling framework for the construction of spatio-temporal models with support for field-based and agent-based phenomena.
Campo provides the elementary building blocks for the construction and execution of field-agent models:
operations accepting both fields and agents as arguments, and operations that are dedicated either to fields or to agents.
Campo resembles and extends the map algebra approach to field-agent modelling and allows for the construction of static or dynamic models.


You can find more information on the Campo [project website](http://campo.computationalgeography.org/).
Documentation can be found [here](https://campo.computationalgeography.org/documentation/index.html).
Campo is developed by the [Computational Geography](https://www.computationalgeography.org) group.


## Installation

[![Anaconda-Server Badge](https://anaconda.org/conda-forge/campo/badges/version.svg)](https://anaconda.org/conda-forge/campo)
[![Anaconda-Server Badge](https://anaconda.org/conda-forge/campo/badges/platforms.svg)](https://anaconda.org/conda-forge/campo)
[![Anaconda-Server Badge](https://anaconda.org/conda-forge/campo/badges/installer/conda.svg)](https://conda.anaconda.org/conda-forge)

Packages are available for Linux, macOS and Windows via [conda-forge](https://github.com/conda-forge/campo-feedstock).
Install Campo with:
```bash
conda install -c conda-forge campo
```

## Build status

[![Build and deploy](https://github.com/computationalgeography/campo/workflows/Build%20and%20deploy/badge.svg)](https://github.com/computationalgeography/campo/actions)

",2022-08-12
https://github.com/computationalgeography/hello_eejit,"# hello_eejit

Project with scripts and code to test the software build environment
on eejit. In our research software projects we use various 3rd party
software. We use CMake to generate build scripts for us.

Here, we let CMake find the 3rd party software we rely on, and generate
build scripts for compiling very simple test programs. If all this
succeeds, we can assume that the software build environment on eejit
is good.

These steps can be performed to configure and build `hello_eejit`:

- Load software modules
- Generate build scripts
- Build software
- Install software
- Test software

For convenience, a bash script is included that performs these steps in one go:

```
bash ./say_hello_to_eejit.sh
```
",2022-08-12
https://github.com/computationalgeography/land_surface_data_generator,"#

High resolution land surface data with a global coverage is a key requirement as input to global simulation models of, for instance, water resources, air pollution, and land use change.
Processing, reproducing and sharing inputs and derived products is a reoccurring challenge due to the amount of intermediate and final data, often implied by a hyper-resolution modelling requirement.

This Python based processing workflow provides a straightforward creation of input data for raster-based global simulation models.

##

A few steps are required to run the scripts:

 1. You will need a working Python environment.
    We recommend to install Miniconda, or use Anaconda if you have installed that already.
    Follow the Miniconda instructions given at [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html).
    The user guide and short reference on Conda can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/cheatsheet.html).

 2. Open a terminal (Linux/macOS) or Miniconda command prompt (Windows) and browse to a location where you want to store the repository contents.

 3. Clone this repository, or download and uncompress the zip file. Afterwards change to the `land_surface_data_generator` folder.

 4. Create the required Python environment:

    Linux/macOS:

    `conda env create -f environment/environment.yaml`

    Windows:

    `conda env create -f environment\course_environment.yaml`


The environment file will create a environment named *lsdg* using a recent Python version.
In case you prefer a different name or Python version you need to edit the environment file.

",2022-08-12
https://github.com/computationalgeography/lue,"# LUE
LUE scientific database and environmental modelling framework

LUE is software for storing and manipulating large amounts of information
for large amounts of objects. This information can optionally have a
reference in time and space. For example, LUE can represent collections of
wandering animals and their properties, some of which are changing through
time, or the elevation of the surface of one or multiple areas. The software
is useful, and currently mainly used, in the context of environmental
modelling of biological and physical systems, represented by agents and
fields, but we make sure that the software is as generic as possible,
allowing it to be useful in other contexts as well.

Currently, LUE contains two main parts: *LUE data model* and *LUE
framework*. LUE data model is an implementation of the LUE physical data
model, which allows users to perform I/O to the ""LUE Scientific Database"".
It allows for the storage of large amounts of objects and their location
in time and space, and their properties.

LUE framework is a collection of data types and algorithms that can be
combined to translate large amounts of information. It allows computations
to be performed on hardware ranging from laptops to compute clusters,
without the user having to know about high-performance computing and
the related technology.

- [Homepage](https://lue.computationalgeography.org)
    - [Documentation](https://lue.computationalgeography.org/doc)
    - [Publications](https://lue.computationalgeography.org/publication)
- [R&D team](https://www.computationalgeography.org)
- [![Chat with us on Matrix](https://img.shields.io/badge/chat-on%20Matrix-%230098D4)](https://matrix.to/#/#lue:matrix.org) (users)
- [![Chat with us on Matrix](https://img.shields.io/badge/chat-on%20Matrix-%230098D4)](https://matrix.to/#/#lue-dev:matrix.org) (developers)


## CI builds
[![Linux build status](https://github.com/computationalgeography/lue/workflows/Linux%20CI/badge.svg)](https://github.com/computationalgeography/lue/actions/workflows/linux.yml)
[![Linux Conda package](https://github.com/computationalgeography/lue/actions/workflows/linux-conda.yml/badge.svg)](https://github.com/computationalgeography/lue/actions/workflows/linux-conda.yml)

[![macOS build status](https://github.com/computationalgeography/lue/workflows/macOS%20CI/badge.svg)](https://github.com/computationalgeography/lue/actions/workflows/macos.yml)
[![macOS Conda package](https://github.com/computationalgeography/lue/actions/workflows/macos-conda.yml/badge.svg)](https://github.com/computationalgeography/lue/actions/workflows/macos-conda.yml)

[![Windows build status](https://github.com/computationalgeography/lue/workflows/Windows%20CI/badge.svg)](https://github.com/computationalgeography/lue/actions/workflows/windows.yml)
[![Windows Conda package](https://github.com/computationalgeography/lue/actions/workflows/windows-conda.yml/badge.svg)](https://github.com/computationalgeography/lue/actions/workflows/windows-conda.yml)

[![Codacy Badge](https://app.codacy.com/project/badge/Grade/2c02fc1c5b13424abfc414b82104801d)](https://www.codacy.com/gh/computationalgeography/lue/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=computationalgeography/lue&amp;utm_campaign=Badge_Grade)




## Installation
[![Anaconda-Server Badge](https://anaconda.org/conda-forge/lue/badges/version.svg)](https://anaconda.org/conda-forge/lue)
[![Anaconda-Server Badge](https://anaconda.org/conda-forge/lue/badges/platforms.svg)](https://anaconda.org/conda-forge/lue)
[![Anaconda-Server Badge](https://anaconda.org/conda-forge/lue/badges/installer/conda.svg)](https://conda.anaconda.org/conda-forge)

The easiest way to install LUE is using Conda:
```bash
conda install -c conda-forge lue
```

More information about how to install LUE can be found in the [LUE
documentation](https://lue.computationalgeography.org/doc).
",2022-08-12
https://github.com/computationalgeography/lue_qa,"# LUE-QA
LUE quality assurance

The code in this repository helps us with testing the performance and scalabilility of models
implemented with the [LUE](https://lue.computationalgeography.org) data model and framework.
",2022-08-12
https://github.com/computationalgeography/minimal_biomass_soil_model,"# minimal_biomass_soil_model
Non-spatial model to simulate semi-arid vegetation-soil systems

This is a lumped (non-spatial) model simulating plant growth coupled to erosion processes
in semi-arid vegetation-soil systems. The equations of the model are described in
Karssenberg, D., Bierkens, M.F.P., Rietkerk, M. (2017), Catastrophic Shifts in Semiarid
Vegetation-Soil Systems May Unfold Rapidly or Slowly, https://doi.org/10.1086/694413.  

The model uses components from the PCRaster Python framework and you need to install
PCRaster to run the model, https://www.pcraster.eu

main.py
Contains the model. It is written in the PCRaster Python framework. See
http://www.pcraster.eu.
The model writes two files with output timeseries for biomass and
regolith thickness. It also writes the grazing pressure.

parameters.py
Contains the parameters. These need to be varied to get different
realizations of the 'reality'. See the article for the explanation
of the parameters. The parameter values in the file correspond to those in the article
and settings for some of the scenarios are provided.

plot.py
Script to plot the timeseries.
Output is timeseries.pdf
",2022-08-12
https://github.com/computationalgeography/obia,"# obia
This repo contains python scripts and eCognition rule sets used for the RWS OBIA project. 

The folder [rule_sets](rule_sets) contains the eCognition rule set file.   

The folder [scripts](scripts) contains the python scripts that are needed for pre-processing (i.e. making tiles) and post-processing (i.e. merging tiles back) steps, before and after an eCognition process.   

All files were firstly created by Harke Douma and further developed by Lars Groeneveld and Edwin H. Sutanudjaja.   
",2022-08-12
https://github.com/computationalgeography/opengeohub2021,"# Geosimulation using fields and agents


This repository holds a Jupyter notebook demonstrating the Daisyworld model implementation in Campo,
a YAML file to create the Python environment required to run the model,
and necessary scripts for pre- and postprocessing.

More information will be given in the [OpenGeoHub Summer School 2021](https://www.opengeohub.org/summer_school_2021) lecture on September 2nd at 2pm CEST.

## How to install

A few steps are required to run the Jupyter notebook.
General information on Jupyter notebooks and manuals can be found [here](https://jupyter.readthedocs.io/en/latest/).
The user guide and short reference on Conda can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/cheatsheet.html).

 1. You will need a working Python environment, we recommend to install Miniconda. Follow their instructions given at:

    [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)

 2. Open a terminal (Linux/macOS) or Miniconda command prompt (Windows) and browse to a location where you want to store the course contents.

 3. Clone this repository, or download and uncompress the zip file. Afterwards change to the `opengeohub2021` folder.

 4. Create the required Python environment:

    Linux/macOS:

    `conda env create -f environment/course_environment.yaml`

    Windows:

    `conda env create -f environment\course_environment.yaml`


The environment file will create a environment named *fieldagents* using Python 3.9. In case you prefer a different name or Python version you need to edit the environment file.


## How to run

Activate the environment in the command prompt:

`conda activate fieldagents`

Then change to the `notebook` folder.
You can now start the Jupyter notebook from the command prompt. The notebook will open in your browser:

`jupyter-notebook course.ipynb`


## Further reading

Background on DaisyWorld:

  [https://en.wikipedia.org/wiki/Daisyworld](https://en.wikipedia.org/wiki/Daisyworld)
	
Scientific literature about Campo and LUE:

  M.P. de Bakker, K. de Jong, O. Schmitz, D. Karssenberg (2017). Design and demonstration of a data model to integrate agent-based and field-based modelling. Environmental Modelling & Software, 89, 172-189, DOI: [10.1016/j.envsoft.2016.11.016](https://doi.org/10.1016/j.envsoft.2016.11.016). 
	
  K. de Jong, D. Karssenberg (2019). A physical data model for spatio-temporal objects. Environmental Modelling & Software, 122, 104553, DOI: [10.1016/j.envsoft.2019.104553](https://doi.org/10.1016/j.envsoft.2019.104553).
	
  K. de Jong, D. Panja, M. van Kreveld, D. Karssenberg (2021). An environmental modelling framework based on asynchronous many-tasks: Scalability and usability. Environmental Modelling & Software, 139, 104998, DOI: [10.1016/j.envsoft.2021.104998](https://doi.org/10.1016/j.envsoft.2021.104998).
	
",2022-08-12
https://github.com/computationalgeography/paper_2020_scalable_algorithms,"# 2020_scalable_algorithms
This repository contains a version of the LUE environmental modelling
framework as presented in our 2020 manuscript,
as well as example scripts and other files used in the preparation of
that manuscript.

- De Jong, K., Panja, D., Van Kreveld, M., Karssenberg, D.,
  An environmental modelling framework based on asynchronous many-tasks:
  scalability and usability,
  Environmental modelling & Software (2021), [10.1016/j.envsoft.2021.104998](https://doi.org/10.1016/j.envsoft.2021.104998)

| directory | contents |
| --------- | -------- |
| `lue` | Version of LUE described in manuscript |
| `lue/benchmark` | Settings and scripts related to experiments performed |
| `lue/source/framework` | Modelling framework source code |
| `output` | Outputs of scaling experiments performed |

The most recent LUE source code can be found in LUE's [own
repository](https://github.com/computationalgeography/lue).

![Domain decomposition](figure/domain_decomposition.png)


## Build LUE modelling framework
LUE is currently developed and tested on Linux using GCC-7/9/10. All code
should compile and run fine on other platforms too, but this is not
regularly tested.

Here is an example session of building the version of LUE used for our
manuscript:

```bash
cd /tmp
# Recursive is used to also checkout submodules
git clone --recursive https://github.com/computationalgeography/paper_2020_scalable_algorithms
cd paper_2020_scalable_algorithms
mkdir build
cd build
cmake CMAKE_BUILD_TYPE=Release ..
cmake --build .
```

The LUE framework source code depends on 3rd party libraries and tools,
that may or may not be installed already on your system. The following
dependencies can usually be installed using your system's package manager,
using [Conan](https://conan.io), or using [Conda](https://conda.io):

| package | version used |
| ------- | ------------ |
| Boost | 1.72.0 |
| Docopt.cpp | 0.6.3 |
| GDAL | 2.2.3 |
| GooglePerfTools (TCMalloc) | 2.7 |
| HDF5 | 1.10.6 |
| hwloc | 1.11.8 |
| {fmt} | 7.0.3 |
| Microsoft GSL | 3.1.0 |
| MPI (OpenMPI) | 3.1 (4.0.4) |
| JSON for Modern C++ | 3.9.1 |

Other versions of these packages might also work. HPX is built during
the LUE build.

More information about building (the latest version of) LUE can be found
in [the LUE documentation](https://lue.computationalgeography.org/doc).


## Experiments
After building LUE, a set of command line utilities is available in
the build directory (`build/bin`). These accept a set of command line arguments that
determine the size of the problem that will be processed. Scripts have
been created that manage the execution of the experiments for various
kinds of experiments (partition shape, strong scaling, weak scaling),
for different platforms (cluster, desktop, laptop). Settings for these
can be found in [lue/benchmark/configuration](lue/benchmark/configuration).

Scripts for managing the experiments can be found in
[lue/benchmark/script/algorithm](lue/benchmark/script/algorithm).
The
[partition_shape_experiments.sh](lue/benchmark/script/algorithm/partition_shape_experiments.sh),
[strong_scaling_experiments.sh](lue/benchmark/script/algorithm/strong_scaling_experiments.sh),
and
[weak_scaling_experiments.sh](lue/benchmark/script/algorithm/weak_scaling_experiments.sh)
scripts perform the experiments for the operations and models
requested. Experiment results are stored in JSON files. Scripts for
importing these results in LUE datasets and generating plots can also
be found in
[lue/benchmark/script/algorithm](lue/benchmark/script/algorithm).

| experiment | executable | main source file |
| ---------- | ---------- | ---------------- |
| local operation | `lue_algorithm_sqrt_benchmark`       | [binary_local_operation.hpp](lue/source/framework/algorithm/include/lue/framework/algorithm/binary_local_operation.hpp) |
| focal operation | `lue_algorithm_focal_mean_benchmark` | [focal_operation.hpp](lue/source/framework/algorithm/include/lue/framework/algorithm/focal_operation.hpp) |
| zonal operation | `lue_algorithm_zonal_sum_benchmark`  | [zonal_operation.hpp](lue/source/framework/algorithm/include/lue/framework/algorithm/zonal_operation.hpp) |
| example model   | `lue_case_study_wildfire_benchmark`  | [wildfire_model_base.cpp](lue/source/framework/case_study/wildfire/src/wildfire_model_base.cpp) |

The source code of the experiments for the modelling operations can be
found in
[lue/source/framework/algorithm/benchmark](lue/source/framework/algorithm/benchmark).
The code implementing the wildfire model can be found in
[lue/source/framework/case_study/wildfire](lue/source/framework/case_study/wildfire).
The alternative implementation, using the
[PCRaster](https://pcraster.computationalgeography.org) environmental
modelling environment can be found in
[model/wildfire-pcraster.py](model/wildfire-pcraster.py).

Some outputs of the experiments performed can be found in the output
directory. For each experiment the output JSON files are stored, as well
as the scaling plots generated by the postprocessing scripts.

<img src=""figure/scaling.png"" alt=""Scaling plots"" width=""500""/>
",2022-08-12
https://github.com/computationalgeography/paper_2021_routing,"# paper_2021_routing

This repository contains a version of the LUE environmental modelling framework as used
in our 2021 manuscript, as well as example scripts and other files used in the preparation of
that manuscript. We try to provide all information needed for others to be able to rerun
experiments and reproduce results on similar platforms, but it is unlikely that the scripts in
this repository will work unchanged. Feel free to contact the corresponding author [Kor de
Jong](mailto:k.dejong1@uu.nl) in case you have questions.

- De Jong, K., Panja, D., Karssenberg, D., Van Kreveld, M., Scalability and composability
  of flow accumulation algorithms based on asynchronous many-tasks (submitted for review)

| directory                                    | contents                                                                       |
| ---------                                    | --------                                                                       |
| `lue`                                        | Version of LUE described in manuscript                                         |
| `lue/benchmark`                              | Settings and scripts related to scalability experiments performed              |
| `lue/source/framework`                       | Modelling framework source code                                                |
| `output`                                     | Outputs of scaling experiments performed                                       |
| `source`                                     | Various scripts related to performance and composability experiments performed |
| [document/figure](document/figure/README.md) | Supplementary figures                                                          |

The most recent LUE source code can be found in LUE's
[own repository](https://github.com/computationalgeography/lue).

In the sources, flow accumulation algorithms are named as in the manuscript, but with a postfix
of '3'. So `accu_threshold` is called `accu_threshold3`. The algorithms we describe are the 3rd
version of several trials.

[<img src=""document/figure/runoff.png"" width=""400""/>](document/figure/README.md)


## Create paper environment and build LUE modelling framework
LUE is currently developed and tested on Linux using GCC-9/10. Whenever LUE is changed, it is
built for various platforms (combinations of operating systems, versions of 3rd party libraries
and tools). For this Github workflows are used that use Github actions. Inspecting
[the Github workflow scripts](lue/.github/workflows)
can be useful to configure a platform on which LUE can be built. LUE potentially compiles and
runs fine on other platforms too, but this is not regularly tested.

Here is an example session of building the version of LUE used for our manuscript:

```bash
cd /tmp
# Recursive is used to also checkout submodules
git clone --recurse-submodules https://github.com/computationalgeography/paper_2021_routing
cd paper_2021_routing
conda env create -f environment/configuration/conda_environment.yml
conda activate paper_2021_routing
mkdir build
cd build
cmake \
    -DCMAKE_BUILD_TYPE=Release \
    -DCMAKE_TOOLCHAIN_FILE=$PROJECTS/my_devenv/configuration/platform/cmake/login01/Release.cmake \
    -DLUE_BUILD_DATA_MODEL:BOOL=TRUE \
    -DLUE_DATA_MODEL_WITH_PYTHON_API:BOOL=TRUE \
    -DLUE_DATA_MODEL_WITH_UTILITIES:BOOL=TRUE \
    -DLUE_BUILD_HPX:BOOL=TRUE \
    -DLUE_BUILD_FRAMEWORK:BOOL=TRUE \
    -DLUE_DATA_MODEL_WITH_PYTHON_API:BOOL=TRUE \
    -DLUE_FRAMEWORK_WITH_PYTHON_API:BOOL=TRUE \
    -DLUE_HAVE_DOCOPT:BOOL=FALSE \
    -DLUE_HAVE_FMT:BOOL=FALSE \
    -DLUE_HAVE_NLOHMANN_JSON:BOOL=FALSE \
    -DLUE_HAVE_PYBIND11:BOOL=FALSE \
    ../lue
cmake --build . --parallel 10
```

The mentioned CMake toolchain file contains additional settings specific for the
platform we used for running the experiments. It is part of the author's [my_devenv
repository](https://github.com/kordejong/my_devenv/tree/9eb8896d24389f5ae9090d368dd2fac88259c633)
containing files that are useful in multiple research and development projects.

The LUE framework source code depends on 3rd party libraries and tools, that may or may not
be installed already on your system. Dependencies can usually be installed using
your system's package manager, using [Conan](https://conan.io), or using
[Conda](https://conda.io). See the Github workflow scripts for examples of platforms that are
guaranteed to work.

Other versions of these packages than the ones mentioned in the Github workflow scripts might
also work. HPX is built during the LUE build.

More information about building (the latest version of) LUE can be found in [the LUE
documentation](https://lue.computationalgeography.org/doc).

Once LUE is built, executables can be found in `build/bin` and shared libraries and Python
packages in `build/lib`. On Linux, the software can be used like this:

```bash
cd build

# Use a LUE executable
bin/lue_translate --help

# Use the LUE Python package
PYTHONPATH=`pwd`/lib:$PYTHONPATH python -c ""import lue; print(lue.__version__)""
```

Note that LUE is still experimental software. The above has been tested on the platform we
performed our experiments on. This does not guarantee that it will work on other platforms,
but we expact that LUE can be made to work on other platforms without much effort.


## Running experiments
Example sessions. Update as appropriate.

### Performance
```bash
paper_prefix=""<prefix_to>/paper_2021_routing""
routing_data=""<prefix_to>/data""

export LUE_ROUTING_DATA=""$routing_data/routing""
export LUE_OBJECTS=""$paper_prefix/build""
export PYTHONPATH=""$paper_prefix/build/lib:$PYTHONPATH""

bash ""$paper_prefix/source/performance/flow_accumulation.sh""
```


### Determine shapes of arrays for experiments
```bash
paper_prefix=""<prefix_to>/paper_2021_routing""
routing_data=""<prefix_to>/data""

export LUE=""$paper_prefix/lue""
export PYTHONPATH=""$paper_prefix/build/lib:$PYTHONPATH""

# See usage info of script for meaning of arguments
python $paper_prefix/source/benchmark/array_shapes.py <platform> 0.0 13 150 2 $routing_data/africa/factor2.vrt 23 13 ~/tmp/bounding_boxes
```


### Composability
```bash
paper_prefix=""<prefix_to>/paper_2021_routing""
routing_data=""<prefix_to>/data""

export LUE_ROUTING_DATA=""$routing_data/routing""
export LUE_BENCHMARK_DATA=""$routing_data/benchmark""
export PYTHONPATH=""$paper_prefix/build/lib:$PYTHONPATH""

bash ""$paper_prefix/source/composability/composability.sh""
```
",2022-08-12
https://github.com/computationalgeography/pycatch,"# pycatch

Copyright Derek Karssenberg & Noemí Lana-Renault Monreal

For short how to and info, refer to readme.txt
",2022-08-12
https://github.com/D-score/childdevdata,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# childdevdata

<!-- badges: start -->

[![Lifecycle:
stable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html#stable)
[![CRAN
status](https://www.r-pkg.org/badges/version/childdevdata)](https://CRAN.R-project.org/package=childdevdata)
[![DOI](https://zenodo.org/badge/353137452.svg)](https://zenodo.org/badge/latestdoi/353137452)
<!-- badges: end -->

The goal of `childdevdata` is to support innovation in child
development. The package

1.  Makes anonymous microdata available to the research community;
2.  Adopts a simple naming schema for developmental milestones;
3.  Supports multiple measurement instruments;
4.  Eases joint analyses of the data.

The current version bundles milestone data from ten studies, containing
1,116,061 assessments made on 10,831 unique children during 28,465
visits, covering 21 different instruments.

## Installation

You can install the released version of childdevdata from
[CRAN](https://CRAN.R-project.org) with

``` r
install.packages(""childdevdata"")
```

You can install the development version of `childdevdata` from
[GitHub](https://github.com) with

``` r
install.packages(""remotes"")
remotes::install_github(""d-score/childdevdata"")
```

## Example

The following example visualises how the proportion of toddlers that are
able to walk increases with age.

``` r
library(childdevdata)
library(ggplot2)

# we use the Dutch SMOCC data
data <- with(gcdg_nld_smocc, 
             data.frame(age = round(agedays/365.25, 4),
                        walk = ddigmd068))
ggplot(na.omit(data), aes(age, walk)) +
  geom_point(cex = 0.5) +
  geom_smooth(method = ""gam"", formula = y ~ s(x, bs = ""cs""), 
              se = FALSE, lwd = 0.5) +
  theme_bw()
```

<img src=""man/figures/README-example-1.png"" width=""100%"" />

## Overview of available dataset and documentation

The package contains multiple datasets. Obtain the list of datasets by

``` r
data(package = ""childdevdata"")$results[, ""Item""]
#>  [1] ""gcdg_chl_1""       ""gcdg_chn""         ""gcdg_col_lt42m""   ""gcdg_col_lt45m""  
#>  [5] ""gcdg_ecu""         ""gcdg_jam_lbw""     ""gcdg_jam_stunted"" ""gcdg_mdg""        
#>  [9] ""gcdg_nld_smocc""   ""gcdg_zaf""
```

The documentation of the data can be found by typing into the console:

``` r
?gcdg_col_lt42m
```

The size of the data is

``` r
dim(gcdg_col_lt42m)
#> [1] 1311  627
```

The first six rows and first nine columns are

``` r
head(gcdg_col_lt42m[, 1:9])
#> # A tibble: 6 x 9
#>   ctrycd cohort       cohortn  subjid agedays sex   gagebrth aqicmc010 aqicmc013
#>   <chr>  <chr>          <int>   <int>   <int> <chr>    <int>     <int>     <int>
#> 1 COL    GCDG-COL-LT…      50 5000001     660 Fema…      224        NA        NA
#> 2 COL    GCDG-COL-LT…      50 5000002    1166 Fema…      280        NA        NA
#> 3 COL    GCDG-COL-LT…      50 5000003     314 Fema…      273        NA        NA
#> 4 COL    GCDG-COL-LT…      50 5000004    1239 Fema…      259        NA        NA
#> 5 COL    GCDG-COL-LT…      50 5000005     679 Fema…      224        NA        NA
#> 6 COL    GCDG-COL-LT…      50 5000006    1074 Fema…      252        NA        NA
```

The first seven columns are administrative and background variables.
Column numbers eight and up hold the milestone scores.

## Combining data

Concatenating two or more data is straightforward using `dplyr`. The
following code concatenates all publicly available GCDG datasets.

``` r
library(dplyr)
#> 
#> Attaching package: 'dplyr'
#> The following objects are masked from 'package:stats':
#> 
#>     filter, lag
#> The following objects are masked from 'package:base':
#> 
#>     intersect, setdiff, setequal, union
alldata <- bind_rows(gcdg_chl_1, gcdg_chn, gcdg_col_lt42m, gcdg_col_lt45m, gcdg_ecu, 
                     gcdg_jam_lbw, gcdg_jam_stunted, gcdg_mdg, gcdg_nld_smocc, gcdg_zaf)
dim(alldata)
#> [1] 28465  1306
```

Both the number of rows and the number of columns have increased.
Milestones not appearing in a particular data obtain all missing (`NA`)
scores.

The number of records per cohort by sex is

``` r
table(alldata$cohort, alldata$sex)
#>                   
#>                    Female Male
#>   GCDG-CHL-1          970 1169
#>   GCDG-CHN            509  481
#>   GCDG-COL-LT42M      646  665
#>   GCDG-COL-LT45M      651  684
#>   GCDG-ECU            337  330
#>   GCDG-JAM-LBW        242  201
#>   GCDG-JAM-STUNTED    207  270
#>   GCDG-MDG            113   92
#>   GCDG-NLD-SMOCC     8499 8223
#>   GCDG-ZAF           2154 2018
```

## Calculating D-score and DAZ

The [`dscore` package](https://d-score.org/dscore/) calculates the
*D-score* ([van Buuren 2014](#ref-vanbuuren2014)) and the *D-score
adjusted for age Z-score* (DAZ) for all cases:

``` r
library(dscore)
alldata$age <- round(alldata$agedays/365.25, 4)
d <- dscore(alldata)
head(d)
#>       a  n     p    d   sem    daz
#> 1 1.024 29 0.690 50.4 0.666  0.286
#> 2 1.509 22 0.955 57.8 1.445  0.269
#> 3 0.975 29 0.724 50.8 0.682  0.742
#> 4 1.016 29 0.759 51.3 0.700  0.649
#> 5 1.016 22 0.682 49.1 0.677 -0.099
#> 6 1.517 25 0.840 56.9 1.058 -0.070
dim(d)
#> [1] 28465     6
```

We visualise the D-score distribution by age per cohort as

``` r
alldata <- bind_cols(alldata, d)
ggplot(alldata, aes(age, d, group = cohort)) +
  geom_point(cex = 0.3) +
  facet_wrap(~ cohort) +
  ylab(""D-score"") + xlab(""Age (years)"") +
  theme_bw()
#> Warning: Removed 380 rows containing missing values (geom_point).
```

<img src=""man/figures/README-unnamed-chunk-9-1.png"" width=""100%"" />

## Why this package?

We all want our children to grow and prosper. While there is no shortage
of apps and instruments to track child development, it is often unclear
which data went into the construction of these tools. In order to
improve measurement and norm setting of child development, we need
child-level response data per milestone and age. However, no such public
dataset seem to exist. The `childdevdata` package fills that void.

The package grew out of a project in which we collected milestone data
from 16 cohorts. See [Weber et al.](#ref-weber2019)
([2019](#ref-weber2019)) and <http://d-score.org/dbook2/> for results.
Ten cohort owners graciously decided to make their data available for
third parties. We are grateful to them.

## How to use the data?

Tremendous effort has gone into the collection and harmonisation of the
data. You can use the data in this package under the [CC BY
4.0](https://creativecommons.org/licenses/by/4.0/) license. Basically,
this means that you may share and adapt the data, on the condition that
you give appropriate credit and clearly indicate any changes you’ve
made. See the license text for details.

We expect that you will properly cite the source data when you use the
data in your own product or publication, as follows:

-   If you use one dataset, please cite the publication(s) given in the
    documentation of that dataset.
-   If you use two or more datasets, cite the publication(s) for each
    dataset *and* cite the `childdevdata` package.

The citation of the `childevdata` data package is

    @software{stef_van_buuren_2021_4700229,
      author       = {Stef van Buuren and
                      Iris Eekhout and
                      Marta Rubio Codina and
                      Orazio Attanasio and
                      Costas Meghir and
                      Emla Fitzsimons and
                      Sally Grantham-McGregor and
                      Maria Caridad Araujo and
                      Susan Walker and
                      Susan Chang and
                      Christine Powell and
                      Ann Weber and
                      Lia Fernald and
                      Paul Verkerk and
                      Linda Richter and
                      Betsy Lozoff},
      title        = {D-score/childdevdata: childdevdata 1.1.0},
      month        = apr,
      year         = 2021,
      publisher    = {Zenodo},
      version      = {v1.1.0},
      doi          = {10.5281/zenodo.4700229},
      url          = {https://doi.org/10.5281/zenodo.4700229}
    }

## Want to contribute?

Do you have similar data and want to help others to advance the field?
Please let us know. We hope that the `childdevdata` package may continue
to grow into a valuable resource for developers and researchers
worldwide.

## References

<div id=""refs"" class=""references csl-bib-body hanging-indent"">

<div id=""ref-vanbuuren2014"" class=""csl-entry"">

van Buuren, S. 2014. “Growth Charts of Human Development.” *Statistical
Methods in Medical Research* 23 (4): 346–68.
<https://stefvanbuuren.name/publication/van-buuren-2014-gc/>.

</div>

<div id=""ref-weber2019"" class=""csl-entry"">

Weber, A. M., M. Rubio-Codina, S. P. Walker, S. van Buuren, I. Eekhout,
S. Grantham-McGregor, M. C. Araujo, et al. 2019. “The D-Score: A Metric
for Interpreting the Early Development of Infants and Toddlers Across
Global Settings.” *BMJ Global Health* 4: e001724.

</div>

</div>
",2022-08-12
https://github.com/D-score/dbook1,"# D-score booklet I: Turning milestones into measurement

Children learn to walk, speak, and think at an astonishing pace. The D-score captures this process as a one-number summary. The D-score booklets explain why we need the D-score, how we construct it, and how we calculate it. Application of the D-score enables comparisons in child development across populations, groups and individuals.

We are preparing four D-score booklets under the following titles:

|     |                                      |
| ---:|:------------------------------------ |
I.    |	[Turning milestones into measurement](https://d-score.org/dbook1) |
II.	  | [Tuning instruments to unity](https://stefvanbuuren.name/dbook2) |
III.	| [Tailoring tests to fit the occasion](https://stefvanbuuren.name/dbook3) |
IV.	  | [Taking off the hood](https://stefvanbuuren.name/dbook4) |

Editors: [Stef van Buuren](https://stefvanbuuren.name), [Iris Eekhout](https://www.iriseekhout.com)

This repository holds the sources for booklet I.
",2022-08-12
https://github.com/D-score/dbook2,"# D-score booklet II: Tuning instruments to unity

Children learn to walk, speak, and think at an astonishing pace. The D-score captures this process as a one-number summary. The D-score booklets explain why we need the D-score, how we construct it, and how we calculate it. Application of the D-score enables comparisons in child development across populations, groups and individuals.

We are preparing four D-score booklets under the following titles:

|     |                                      |
| ---:|:------------------------------------ |
I.    |	[Turning milestones into measurement](https://d-score.org/dbook1) |
II.	  | [Tuning instruments to unity](https://d-score.org/dbook2) |
III.	| [Tailoring tests to fit the occasion](https://stefvanbuuren.name/dbook3) |
IV.	  | [Taking off the hood](https://stefvanbuuren.name/dbook4) |

Editors: [Stef van Buuren](https://stefvanbuuren.name), [Iris Eekhout](https://www.iriseekhout.com)

This repository holds the sources for booklet II.
",2022-08-12
https://github.com/D-score/dscore,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# dscore

<!-- badges: start -->

[![Lifecycle:
maturing](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://www.tidyverse.org/lifecycle/#maturing)
[![CRAN
status](https://www.r-pkg.org/badges/version/dscore)](https://CRAN.R-project.org/package=dscore)
[![](http://cranlogs.r-pkg.org/badges/dscore)](https://cran.r-project.org/package=dscore)
[![](https://img.shields.io/badge/github%20version-1.6.0-orange.svg)](https://github.com/d-score/dscore)
<!-- badges: end -->

The *D*-score is a numerical score that measures generic development in
children. You may use the *D*-score to analyze and predict development
of children similar to measures like height and weight.

The `dscore` package contains tools to

-   Map your item names to the GSED convention
-   Calculate *D*-score from item level responses
-   Transform the *D*-scores into DAZ, age-standardised Z-scores

The required input consists of *item level* responses on milestones from
widely used instruments for measuring child development.

## Installation

You can install the development version from
[GitHub](https://github.com/) with:

``` r
install.packages(""remotes"")
remotes::install_github(""d-score/dscore"")
```

## Overview

You may estimate the *D*-score and the *D*-score age-adjusted Z-score
(DAZ) from child data on developmental milestones. Four steps are
needed:

1.  Identify whether the `dscore` package covers your measurement
    instrument;
2.  Map your variable names to the GSED 9-position schema;
3.  Calculate *D*-score and DAZ;
4.  Summarise your results.

The `dscore` package provides various function that support these steps.
See [Getting started](https://d-score.org/dscore/articles/start.html)
for more details.

## Resources

### Books and reports

1.  [*D*-score: Turning milestones into
    measurement](https://d-score.org/dbook1/)
2.  [Inventory of 147 instruments for measuring early child
    development](https://documents.worldbank.org/en/publication/documents-reports/documentdetail/384681513101293811/a-toolkit-for-measuring-early-childhood-development-in-low-and-middle-income-countries):
    Fernald et al. ([2017](#ref-fernald2017))

### Keys

1.  Project with `dutch` key, 0-2 years: van Buuren
    ([2014](#ref-vanbuuren2014))
2.  Project with `gcdg` key: Weber et al. ([2019](#ref-weber2019))
3.  Project with `gsed` key: GSED team (Maureen Black, Kieran Bromley,
    Vanessa Cavallera (lead author), Jorge Cuartas, Tarun Dua
    (corresponding author), Iris Eekhout, Günther Fink, Melissa
    Gladstone, Katelyn Hepworth, Magdalena Janus, Patricia Kariger,
    Gillian Lancaster, Dana McCoy, Gareth McCray, Abbie Raikes, Marta
    Rubio-Codina, Stef van Buuren, Marcus Waldman, Susan Walker and Ann
    Weber) ([2019](#ref-gsedteam2019))

### Methodology

1.  Interval scale: Jacobusse, van Buuren, and Verkerk
    ([2006](#ref-jacobusse2006))
2.  Adaptive testing: Jacobusse and van Buuren
    ([2007](#ref-jacobusse2007))

### Shiny app

If you want to calculate the D-score on your own data, and you’re not an
`R` user, you might wish to check out our interactive Shiny
[dcalculator](https://tnochildhealthstatistics.shinyapps.io/dcalculator/)
app.

## Acknowledgement

This study was supported by the Bill & Melinda Gates Foundation. The
contents are the sole responsibility of the authors and may not
necessarily represent the official views of the Bill & Melinda Gates
Foundation or other agencies that may have supported the primary data
studies used in the present study. The authors wish to recognize the
principal investigators and their study team members for their generous
contribution of the data that made this tool possible and the members of
the Ki team who directly or indirectly contributed to the study: Amina
Abubakar, Claudia R. Lindgren Alves, Orazio Attanasio, Maureen M. Black,
Maria Caridad Araujo, Susan M. Chang-Lopez, Gary L. Darmstadt, Bernice
M. Doove, Wafaie Fawzi, Lia C.H. Fernald, Günther Fink, Emanuela
Galasso, Melissa Gladstone, Sally M. Grantham-McGregor, Cristina
Gutierrez de Pineres, Pamela Jervis, Jena Derakhshani Hamadani,
Charlotte Hanlon, Simone M. Karam, Gillian Lancaster, Betzy Lozoff,
Gareth McCray, Jeffrey R Measelle, Girmay Medhin, Ana M. B. Menezes,
Lauren Pisani, Helen Pitchik, Muneera Rasheed, Lisy Ratsifandrihamanana,
Sarah Reynolds, Linda Richter, Marta Rubio-Codina, Norbert Schady,
Limbika Sengani, Chris Sudfeld, Marcus Waldman, Susan P. Walker, Ann M.
Weber and Aisha K. Yousafzai.

### Literature

<div id=""refs"" class=""references csl-bib-body hanging-indent"">

<div id=""ref-fernald2017"" class=""csl-entry"">

Fernald, L. C. H., E. Prado, P. Kariger, and A. Raikes. 2017. “A Toolkit
for Measuring Early Childhood Development in Low and Middle-Income
Countries.”
<https://documents.worldbank.org/en/publication/documents-reports/documentdetail/384681513101293811/a-toolkit-for-measuring-early-childhood-development-in-low-and-middle-income-countries>.

</div>

<div id=""ref-gsedteam2019"" class=""csl-entry"">

GSED team (Maureen Black, Kieran Bromley, Vanessa Cavallera (lead
author), Jorge Cuartas, Tarun Dua (corresponding author), Iris Eekhout,
Günther Fink, Melissa Gladstone, Katelyn Hepworth, Magdalena Janus,
Patricia Kariger, Gillian Lancaster, Dana McCoy, Gareth McCray, Abbie
Raikes, Marta Rubio-Codina, Stef van Buuren, Marcus Waldman, Susan
Walker and Ann Weber). 2019. “The Global Scale for Early Development
(GSED).” *Early Childhood Matters*.
<https://earlychildhoodmatters.online/2019/the-global-scale-for-early-development-gsed/>.

</div>

<div id=""ref-jacobusse2007"" class=""csl-entry"">

Jacobusse, G., and S. van Buuren. 2007. “Computerized Adaptive Testing
for Measuring Development of Young Children.” *Statistics in Medicine*
26 (13): 2629–38.
<https://stefvanbuuren.name/publication/jacobusse-2007/>.

</div>

<div id=""ref-jacobusse2006"" class=""csl-entry"">

Jacobusse, G., S. van Buuren, and P. H. Verkerk. 2006. “An Interval
Scale for Development of Children Aged 0-2 Years.” *Statistics in
Medicine* 25 (13): 2272–83.
<https://stefvanbuuren.name/publication/jacobusse-2006/>.

</div>

<div id=""ref-vanbuuren2014"" class=""csl-entry"">

van Buuren, S. 2014. “Growth Charts of Human Development.” *Statistical
Methods in Medical Research* 23 (4): 346–68.
<https://stefvanbuuren.name/publication/van-buuren-2014-gc/>.

</div>

<div id=""ref-weber2019"" class=""csl-entry"">

Weber, A. M., M. Rubio-Codina, S. P. Walker, S. van Buuren, I. Eekhout,
S. Grantham-McGregor, M. C. Araujo, et al. 2019. “The D-Score: A Metric
for Interpreting the Early Development of Infants and Toddlers Across
Global Settings.” *BMJ Global Health* 4: e001724.
<https://gh.bmj.com/content/bmjgh/4/6/e001724.full.pdf>.

</div>

</div>
",2022-08-12
https://github.com/D-score/gsedread,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# gsedread

<!-- badges: start -->

[![Lifecycle:
experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
<!-- badges: end -->

The goal of gsedread is to read validation data of the project Global
Scales for Early Development (GSED).

## Installation

<!-- If you have been marked as a collaborator on GitHub for this repository, generate a personal access token as in <https://github.com/settings/tokens>. Add a line  -->
<!-- ```{r eval=FALSE} -->
<!-- GITHUB_PAT=ghp_vC82..................... -->
<!-- ``` -->
<!-- with your token in the file `.Renviron` in your home directory. Restarting R adds the environmental variable GITHUB_PAT to your session. Then install the `gsedread` package from GitHub as follows:  -->

Install the `gsedread` package from GitHub as follows:

``` r
install.packages(""remotes"")
remotes::install_github(""d-score/gsedread"")
```

There is no CRAN version.

## Example

You need access to the WHO SharePoint site and sync the data to a local
OneDrive. In the file `.Renviron` in your home directory add a line
specifying the location of your synced OneDrive, e.g.,

    ONEDRIVE_GSED='/Users/username/Library/CloudStorage/OneDrive-Sharedlibraries-WorldHealthOrganization/CAVALLERA, Vanessa - GSED Validation 2021_phase I'

After setting the environmental variable `ONEDRIVE_GSED`, restart R, and
manually check whether you are able to read the OneDrive directory.

``` r
dir(Sys.getenv(""ONEDRIVE_GSED""))
#>  [1] ""Bangladesh Validation""                             
#>  [2] ""Baseline Analysis - OLD - NOV 2021""                
#>  [3] ""Final Phase 1 Data - May 10th 2022""                
#>  [4] ""GSED Final Collated Phase 1 Data Files 18_05_22""   
#>  [5] ""GSED PHASE 1 DATA COLLECTED LOG""                   
#>  [6] ""GSED_data_quality_1_output_LF_TEST.csv""            
#>  [7] ""GSED_data_quality_1_output.csv""                    
#>  [8] ""GSED_phase1_merged_11_11_21.csv""                   
#>  [9] ""interim DAZ values combined.csv""                   
#> [10] ""Interim validation data_phase I_May2021""           
#> [11] ""Master_data_dictionary_MAIN_v0.9.1_2021.04.22.xlsx""
#> [12] ""Pakistan Validation""                               
#> [13] ""Pemba Validation""                                  
#> [14] ""QUALITATIVE DATA PHASE 1 MAY 2022""                 
#> [15] ""Stop rule change exploration""
```

The following commands reads all SF data from
`GSED Final Collated Phase 1 Data Files 18_05_22` directory and returns
a tibble with one record per administration.

``` r
library(gsedread)
data <- read_sf()
dim(data)
#> [1] 6228  160
```

Count the number of records per file:

``` r
table(data$file)
#> 
#>                ban_sf_2021_11_03 ban_sf_new_enrollment_17_05_2022 
#>                             1421                               72 
#>     ban_sf_predictive_17_05_2022                pak_sf_2022_05_17 
#>                              473                             1761 
#> pak_sf_new_enrollment_2022_05_17     pak_sf_predictive_2022_05_17 
#>                               72                              459 
#>                tza_sf_2021_11_01 tza_sf_new_enrollment_10_05_2022 
#>                             1427                               74 
#>     tza_sf_predictive_10_05_2022 
#>                              469
```

Process variable names user-friendly alternative:

``` r
rename_vector(colnames(data)[c(1:3, 19, 21:25)], lexout = ""gsed2"", trim = ""Ma_SF_"")
#> [1] ""file""      ""gsed_id""   ""parent_id"" ""date""      ""gpalac001"" ""gpacgc002""
#> [7] ""gpafmc003"" ""gpasec004"" ""gpamoc005""
```

## Operations

The package reads and processes GSED data. It does not store data. The
`read_sf()` and `read_lf()` functions takes the following actions:

1.  Constructs the paths to the files OneDrive sync file;
2.  Reads all specified datasets in a list;
3.  Internally specifies the desired format for each column;
4.  Specifies the available date and data-time formats per file;
5.  Recodes empty, `NA`, `-8888`, `-8,888.00` and `-9999` values as
    `NA`;
6.  Repairs problems with mixed data-time formats in the adaptive
    Pakistan data;
7.  Stacks the datasets to one tibble and adds columns `file` and `adm`;
8.  Removes records without a `GSED_ID`.

Item renaming with `rename_variables()` relies on the item translation
table at
<https://github.com/D-score/gsedread/blob/main/inst/extdata/itemnames_translate.tsv>.
",2022-08-12
https://github.com/daanreijnders/arctic-connectivity,"# Ocean Surface Connectivity in the Arctic: Capabilities and Caveats of Community Detection in Lagrangian Flow Networks

This repository contains Python scripts and notebooks used in my MSc research project on investigating connectivity in the surface of the Arctic Ocean. Author is Daan Reijnders unless specified otherwise.

Publication (open access):

Reijnders, D., van Leeuwen, E. J., & van Sebille, E. (2021). Ocean surface connectivity in the Arctic: Capabilities and caveats of community detection in Lagrangian flow networks. *Journal of Geophysical Research: Oceans*, 126, e2020JC016416. https://doi.org/10.1029/2020JC016416

## Data used
Hydrodynamical data from the `GLOBAL_REANALYSIS_PHY_001_030` dataset from the Copernicus Marine Environment Monitoring Service (CMEMS) is used. Data is loaded above 60N. Data is used between 1993 and 2018.

## Environment
Packages used can be found in the import list of each file. The Conda environment used in this thesis can be found in the `meta` directory. 

## Main pipeline
1. Particles are initialized with the `community.particles` class.
2. A `parcels.fieldset` is initialized with `fieldsetter_cmems` script.
3. Particles are advected using [Parcels](https://github.com/OceanParcels/parcels) through the `advectParticles` script.
4. A Lagrangian flow network is constructed using the `community.countBins` and `community.transMat` classes.
5. The community detection algorithm *Infomap* (version 1.0.0-beta.51) is applied on the Lagrangian flow network (`.net` file). Options used:
    * `-d` specifies that the network is directed
    * `-k` include self-edges
    * `--clu` print a .clu file with the top cluster ids for each node
    * `--markov-time` to specify the markov-time (almost always 2)
    * `-N 20` always run Infomap 20 times and choose the best solution
    * `-s` random seed. Almost always `314159`, unless seeking degenerate solutions. Then the range of seeds is 1..100.
6. The resulting community description (`.clu` file) is loaded using `community.countBins`.
7. Results are plotted and further analyzed in notebooks in `community_detection` directory.

Transition matrices (`npz`), corresponding network descriptions (`.net`) and community divisions (`.clu`) are persistenly stored at https://public.yoda.uu.nl/science/UU01/IN0OU9.html.

## Research Abstract
To identify barriers to transport in a fluid domain, community detection algorithms from network science have been used to divide the domain into clusters that are sparsely connected with each other. In a previous application to the closed domain of the Mediterranean Sea, communities detected by the _Infomap_ algorithm have barriers that often coincide with well-known oceanographic features. We apply this clustering method to the surface of the Arctic and subarctic oceans and thereby show that it can also be applied to open domains. First, we construct a Lagrangian flow network by simulating the exchange of Lagrangian particles between different bins in an icosahedral-hexagonal grid. Then, _Infomap_ is applied to identify groups of well-connected bins. The resolved transport barriers include naturally occurring structures, such as the major currents. As expected, clusters in the Arctic are affected by seasonal and annual variations in sea-ice concentration. An important caveat of community detection algorithms is that many different divisions into clusters may qualify as good solutions. Moreover, while certain cluster boundaries lie consistently at the same location between different good solutions, other boundary locations vary significantly, making it difficult to assess the physical meaning of a single solution. We therefore consider an ensemble of solutions to find persistent boundaries, trends and correlations with surface velocities and sea-ice cover.
",2022-08-12
https://github.com/daanreijnders/bound-to-the-data,"# Bound to the Data

This is Python module with set of functions as classes to download oceanographic environemental forecast data (atmospheric, waves, currents, biogeochemistry). It is written by Daan Reijnders for Edward Clydesdale Thomson's project ”Bound to the Miraculous“, and includes routines to access atmospheric and oceanographic environmental forecast data at a particular (lon, lat) coordinate pair and export this to a JSON file.


## Structure
The `python` folder contains the code. Almost every function is described with a docstring.
 - `bound_to_the_data.py` contains the main code. Uses downloaded data to interpolate environmental conditions to a specific point.
    
    It takes positional arguments 
    - `longitude` (between -180 and 180)
    - `latitude` (between from -90 to 90). 
    
    Additionally, the following options can be given:
    - `--timestamp`: date as string ('YYYY-MM-DD-HH-MM') for which the data should be valid. If not specified, data is downloaded for the current time. Note that times should be specified close to the current date, as the analysis and forecasts have a limited lead time, while old data may be discarded from servers.
    - `--cred`: path to [Copernicus Marine Environment Monitoring Service](https://marine.copernicus.eu) credential file (a JSON file with `username` and `password` keys.)
    - `--download_atlantic`: bool that specifies whether data should be downloaded over the whole North Atlantic (lon = [-90, 23]
        lat = [0, 80]), which may be useful for caching. The total cache will then be about 300 mb. 
    - `--filename`: specifies the filename for the JSON output (overrides default).
    - `--force_download`: forces a fresh data download, ignoring cache.
    - `--ignore_cache_validity`: do not check whether the cache has valid timesteps for the requested data. This can be useful to load from cache in case of a data outage.
- `downloader.py`: main downloader code. Downloads data the data sources listed below.
- `physics.py`: some physics functions. It contains a parameterization for fog risk, and the dispersion relation for cappilary-gravity waves (relating wave frequency (from the data source) to wavenumber and, in turn, wavelength).
- `tools.py`: miscellaneous helper tools.

A `cache` folder will be created automatically. Bathymetry data and the most recent model data (atmospheric, wave, physics, and biogeochemistry) are stored here.
An `output` folder will be created automatically.

## CMEMS Credentials
Data from the [Copernicus Marine Environment Monitoring Service (CMEMS)](https://marine.copernicus.eu) can only be accessed using a registered CMEMS account. You can obtain an account [here](https://resources.marine.copernicus.eu/registration-form). The login credentials (username and password) should be stored in a JSON file, which has `username` and `password` keys. An example is given in `example-credentials.json`. The script looks for a `credentials.json` by default, but a path to an alternative file can be passed using the `--cred` flag (see the previous section).

## Usage
Make sure that [Conda](https://docs.conda.io/en/latest/) is installed. Then create an environment using the `environment.yml` template:
```bash
conda env create -f environment.yml
```
This creates a conda environment, by default named `bound`. It can be activated using `conda activate bound`.

Then you can use the data downloader as follows:
```bash
python3 python/bound_to_the_data.py -30 25
```
This will download and export a JSON file with data at 30°E, 25°N, approximated at the current time.

Useful flags:
 - `--filename` specifies the filename for the JSON output
 - `cred` specifies the filename of the `credentials.json`
parser.add_argument('--filename', metavar='Filename', help=""filename for JSON output"", type=str, default=None)
    parser.add_argument('--timestamp', metavar='timestamp string YYYY-MM-DD-HH-MM', type=str, default=None, 
                        help=""Timestamp for which to load the data. Format: YYYY-MM-DD-HH-MM"")
    parser.add_argument('--cred', metavar='CMEMS_credentials file', type=str, default=None,
                        help='Copernicus Marine Environment Monitoring Services credential file. This should be a JSON file with `username` and `password`.')
    parser.add_argument('--download_atlantic', action='store_true', help='Download the whole of the North Atlantic domain.')
    parser.add_argument('--force_download', action='store_true', help=""Force a fresh download from server, ignoring cached data"")
    parser.add_argument('--ignore_cache_validity', action='store_true', help=""Use cached data, no matter if the requested time is not available in it."")


Please use the root of this repository as your working directory. Filepaths are relative to this.


## Output
Output is saved to a JSON file in the `output` folder and contains longitude, latitude and a timestamp in its filename. Each variable is saved with `data`, `description` and `unit` keys, so that the data is self-explaining.


## Data Sources
This software assumes the availability of forecast data from the following sources:
 - NOAA National Weather Service, National Centers for Environmental Prediction, Global Forecast System (GFS) - https://www.nco.ncep.noaa.gov/pmb/products/gfs/
    - From https://thredds.ucar.edu
 - Copernicus Marine Environment Monitoring Service (CMEMS) - https://marine.copernicus.eu
    - [Global Ocean 1/12 degree Physics Analysis and Forecast](https://resources.marine.copernicus.eu/product-detail/GLOBAL_ANALYSIS_FORECAST_PHY_001_024)
    - [Global Ocean Waves Analysis and Forecast](https://resources.marine.copernicus.eu/product-detail/GLOBAL_ANALYSIS_FORECAST_WAV_001_027/INFORMATION)
    - [Global Ocean Biogeochemistry Analysis and Forecast](https://resources.marine.copernicus.eu/product-detail/GLOBAL_ANALYSIS_FORECAST_BIO_001_028/INFORMATION)


## Bugs and future functionality
Any bugs and future functionality plans should be described as issues.

",2022-08-12
https://github.com/daanreijnders/isoneutral-dispersion,"<a href=https://doi.org/10.24416/UU01-RXA2PB><img src=""https://img.shields.io/badge/doi-10.24416%2FUU01--RXA2PB-blue""></a>

# Simulating Lagrangian Subgrid-Scale Dispersion on Neutral Surfaces in the Ocean
This repository holds the data generation and analysis scripts for the manuscript 'Simulating Lagrangian Subgrid-Scale Dispersion on Neutral Surfaces in the Ocean'. 
https://github.com/daanreijnders/isoneutral-dispersion


A snapshot of this repository is stored at https://doi.org/10.24416/UU01-RXA2PB

Corresponding publication:

Reijnders, D., Deleersnijder, E., & Sebille, E. (2022). Simulating Lagrangian Subgrid‐Scale Dispersion on Neutral Surfaces in the Ocean. Journal of Advances in Modeling Earth Systems, 14(2). https://doi.org/10.1029/2021MS002850


## `ACC_mitgcm_config`
Contains MITgcm configuration files for the coarse and fine model, including their spinups. This includes the compilation code and namelists.

## `ACC_particle_experiments`
Contains Python scripts for the Lagrangian simulations carried out with [Parcels](https://github.com/OceanParcels/parcels) (experiments with a tracer patch, and with particles initialized on a lattice), and analysis scripts pertaining to these simulations and their output.

## `data_processing`
Contains scripts and notebooks for processing the data. This includes creating coarsened fields, computing discrete derivatives, and computing the quantities necessary for the LS parameterization.

## `idealized_experiments`
Experiments using the idealized set-up. 

## `kernels`
Functions used by [Parcels](https://github.com/OceanParcels/parcels) to drive the Markov models.

## `misc`
Contains a PDF copy of the documentation for MITgcm's ACC channel simulation. At the date of writing, this documentation is also available online at https://mitgcm.readthedocs.io/en/latest/examples/reentrant_channel/reentrant_channel.html.

## `tools`
Miscellaneous helper functions.

## `verification`
Notebooks that inspect the MITgcm model output.

## `visualization`
For some of the figures in the manuscript. Others are located under the `ACC_particle_experiments` > `analysis` notebooks, or under `idealized_experiments/3D_3D_idealized_isopycnal_Markov1.ipynb`


",2022-08-12
https://github.com/daanreijnders/snippets,"# snippets
",2022-08-12
https://github.com/daanreijnders/trapping_internal_tides,"# Trapping of Internal Tides in a Channel Model

Two tutorials of the Utrecht University course NS-MO447M Waves in Geophysical Fluids will be devoted to recovering and extending the result published in the paper

> Drijfhout, S., & Maas, L. R. M. (2007). Impact of Channel Geometry and Rotation on the Trapping of Internal Tides. *Journal of Physical Oceanography*, *37*(11), 2740–2763. https://doi.org/10.1175/2007JPO3586.1

[which you can download here](https://webspace.science.uu.nl/~maas0131/files/drijfhoutmaas07jpo%5bsmallpdf.com%5d.pdf).

You will do so using the simple ocean circulation model named “MICOM” (Miami Isopycnic Coordinate Ocean Model Output), written in Fortran, which is the main computer language for nearly all ocean, atmosphere, and climate models.

This repository holds the model code and files that you can use for analysis. This README also contains the instructions for a small report that forms the fifth assignment of the course.



## Model description

The model is configured as a channel with an open boundary at one, oceanward side, and a continental slope at the opposite side. The length of the channel is 1200 km, the width 191.25 km. There are 43 layers of 100-m depth, so that total depth in the middle of the channel is 4300 m. 

A barotropic tidal wave, having a typical period of 12 h, enters the channel from the open boundary. Over the continental slope its mainly horizontally-moving barotropic tidal current aquires a vertical component, as the flow is forced to follow the bottom. This vertical velocity displaces isopycnals up and downward out-of-equilibrium. The gravitational restoring force subsequently generates outward-propagating internal tides. 

The paper discusses 4 experiments. In two experiments the earth’s rotation is neglected and the Coriolis frequency is set to 0. In the other two experiments we use an <img src=""https://render.githubusercontent.com/render/math?math={f}""> plane with <img src=""https://render.githubusercontent.com/render/math?math={f = 10^{-4}\ \rm{s}^{-1}}"">. Both cases are run with two channel configurations: in one case the channel has a flat bottom and vertical walls in the cross-channel direction; in the other case the channel has a parabolic cross-channel bottom profile with maximum depth of 4300 m and sloping sidewalls. When <img src=""https://render.githubusercontent.com/render/math?math={f=0}""> the buoyancy frequency is chosen as <img src=""https://render.githubusercontent.com/render/math?math={N = 3.05 \times 10^{-3}\ \rm{s}^{-1}}"">. When <img src=""https://render.githubusercontent.com/render/math?math={f = 10^{-4}\ \rm{s}^{-1}}"">,  <img src=""https://render.githubusercontent.com/render/math?math={N= 2.2 \times 10^{-3}\ \rm{s}^{-1}}"">. 

The model files in this repository are configured for the case <img src=""https://render.githubusercontent.com/render/math?math={f = 10^{-4}\ \rm{s}^{-1}}"">, <img src=""https://render.githubusercontent.com/render/math?math={N = 2.2 \times 10^{-3}\ \rm{s}^{-1}}"">. 



##  Downloading and building the model

You are used to working with Python, which is an interpreted programming language. This means that your instructions are not immediately understood by your computer, but are interpreted and translated to machine code on-the-fly as you execute a script. In contrast, Fortran is a compiled programming language. Before you run a program, your entire code needs to be translated to machine code first. This extra step allows your code to run incredibly efficiently whenever it is executed.

To run the model on your machine, you will first need to compile it. This is also called *building* the model. The model should be able to build on **MacOS** and **Linux** with the Fortran compiler *gfortran* (part of the *GCC* compiler collection) installed. We have also tested the model on Gemini, the Science department’s computer cluster. If you are using MacOS, you may first need to install the Command Line Tools using `xcode-select --install`. Then, you can install _gfortran_ as part of _GCC_ using the package manager _[Homebrew](https://brew.sh)_ (after installing Homebrew, use `brew install gcc`). If you use **Windows**, you may install **Linux within Windows** (see [these instructions](https://docs.microsoft.com/en-us/windows/wsl/about)). If you are unable to build the model on your own computer, feel free to use the Gemini cluster. In that case, make sure to read the specific instructions below.

<details>
  <summary>⚠️ **Instructions for using the Gemini cluster [Click me]**</summary>

  ### Logging in
  1. Open a Terminal.
  2. Connect to the Gemini cluster by typing `ssh 1234567@gemini.science.uu.nl` using your Solis-ID in place of 1234567.
  3. Type your Solis-ID password.
  4. You're in! Your home directory is `/nethome/1234567`. It has a quotum of 2GB. For temporarily storing large amounts of data, create a scratch folder on the scratch disk: `mkdir /scratch/1234567`. Gemini consists of several computing nodes. Each node has its own `scratch` disk.  This means you may have to copy files between nodes. The default (login) node is `science-bs35`. The other node for regular, scheduled jobs is `science-bs37`. You can switch to this node by typing `ssh science-bs37`, and exit it again by typing `exit`. 

  ### Executing commands
Now you can execute shell commands as usual. Note that Gemini is a cluster, which means that you are sharing resources with other users. When executing small jobs (e.g. copying files, running small scripts, building the model), you can do so as usual. **However, when running larger jubs, such as running the model, you should make use of the queueing system.** This way, you're getting adequate computational resources for your _job_, and by using a queue, you won't be hogging resources from other users. 

  ### Submitting a job to the queue
  1. To tell the queueing system what your _job_ is comprised of, you should first create a *job script*, e.g. `my_job.sh`. The `sh` extension indicates that this is a shell script. An example job script looks as follows
  ```bash
  #/bin/bash

  # SGE: Set the job name
  #$ -N wgf_model
  # SGE: this flag exports all active environment variables to the job
  #$ -V
  # SGE: time limit and queue. You probably don't need to change this
  #$ -l h_rt=2:00:00 
  #$ -q all.q
  # SGE: your Email here, for job notification
  #$ -M my.student.email@uu.nl
  # SGE: when do you want to be notified (b : begin, e : end, s : error)?
  #$ -m e 
  #$ -m s
  # SGE: ouput in the current working dir
  #$ -wd /scratch/1234567/wgf_model/

  # Navigate to the right directory and run the model
  cd /scratch/1234567/wgf_model/ # Navigate to where the model is stored
  ./micom1.x # This line starts the model execution
  
  ```
  2. Submit the job using `qsub /path/to/my_job.sh`.
  3. You can inspect the job status using `qstat.`
  4. If you need to delete the job, check the id using `qstat` and use `qdel 123` with 123 being the job id.

  ### Running Jupyter Lab on the cluster
  You can use Jupyter Lab on the cluster. This allows you to easily analyze the model output. 
  1. To do so, you must first load _Conda_: `module load miniconda/3`. Initialize Conda by typing `conda init bash`. You may need to open another bash-shell: type `bash`. You can tell that Conda is loaded when `(base)` is being shown in front of the interpreter.
  2. Start Jupyter: `jupyter lab --no-browser.`
  3. Take note of the Jupyter port number that has been assigned (the four digits in the X's in http://127.0.0.1:XXXX) and the token (the long string after `token=`).
  4. Open a new terminal window or tab on your local computer. In this terminal we set up an SSH tunnel.
  5. Pick a random number YYYY between 8000 and 9000. This will be our SSH port number for the tunnel. Try another number if something fails.
  6. On your local machine, type `ssh -A -L YYYY:localhost:XXXX 1234567@gemini.science.uu.nl`
  7. Open a browser on your local computer and go to `localhost:YYYY`, where `YYYY` is your chosen portnumber. When asked for a password/token, use the one that you noted in step 2.

More info can be found here: https://github.com/OceanParcels/UtrechtTeam/wiki/How-to-run-parcels-on-lorenz,-gemini-and-cartesius#gemini

</details>




1. You can download the files in this repository by running `git clone https://github.com/daanreijnders/trapping_internal_tides.git`
2. Navigate to the `model` directory: `cd trapping_internal_tides/model/`
3. Build the model with the `make` command



## Model components

You will find the following files in the model directory:

- `*.f` files. These are the source codes for the model. The main code is `micom-ssd.f`, and the other files serve as subroutines.
- `makefile` that contains instructions for the compiler
- `*.o` files that are created after compiling the code with the makefile
- `micom1.x` file created after compiling the code with the makefile. This is the executable for running the model
- `*.h` (3x) files where some variables are defined common to various subroutines and the main part
- `micom.in` file setting a few free parameters.
- `thetas` file describing the densities of the 43 layers.



## Running the model

With the command `./micom1.x`, the model starts running. **If you are working on the cluster, do not execute this command directly from the shell. Instead, use the queue system** See the information at *⚠️ Instructions for using the Gemini cluster* above.

Three types of output files will be created;

- 16 `analyse_****` files
- 2 `average_****` files
- 1 `restart_****` file

 The MICOM model is a so-called isopycnic model. It consists of layers with constant density but the depth of the interface between the layers is variable (in contrast to so-called constant z-models, where the density of layers is variable, but their interfaces are fixed in time).

- The `analyse`-files output the time varying depth of the 43 interfaces at 16 moments in time during the last day (1.5 hour output). This is enough to resolve the dominant frequency of internal waves forced by an external wave with fixed frequency of 12 hrs.
- The `average`-files describe the 1-day average of the model-state. You only need the last day.
- The `restart`-file is needed when you want to prolong your run



## Reading and analyzing the model output

In the analysis folder, you will find a `micom_tools.py` file, that contains a simple Python class with a couple of methods to read in the data and to compute the amplitude and phase of the dominant internal wave. Amplitudes are already divided by 980.6 to convert them to centimeters (<img src=""https://render.githubusercontent.com/render/math?math={g = 9.806 \ \rm{m/s^2}}""> is the gravitational acceleration); the phase is defined between <img src=""https://render.githubusercontent.com/render/math?math={-\pi}""> and <img src=""https://render.githubusercontent.com/render/math?math={\pi}"">. The `tutorial_micom_tools.ipynb` notebook will guide you through the functionality of the tool. Make sure to have Numpy, Matplotlib and Xarray installed.



## Assignment

In the first tutorial you will try to recover some of the figures in Drijfhout and Maas (2007) and possibly other figures from the same 4 runs. We ask you to

- Motivate your choice of figures (choose 6-8).
- Explain why you must adapt <img src=""https://render.githubusercontent.com/render/math?math={N}""> and total runtime when <img src=""https://render.githubusercontent.com/render/math?math={f}""> changes
- Why do you see trapping of internal waves in the present set-up and not in other set-ups (different <img src=""https://render.githubusercontent.com/render/math?math={f}""> values and bottom profiles)?
- What is the story you want to tell (see the first bullet) and what are your main conclusions?

 In tutorial 1 you do not need to change the set-up (but you may and can win extra brownie points if you do). In tutorial 2 you will have to change the set-up. Here are the places you will have to make changes in the code (although the code is in Fortran, the required changes are so simple that you can make them without detailed understanding of the code).

- To change the bottom profile, in `cyclo.f` you must edit `pmer` (line 57) and `poos` (line 49) to define the bottom profile in respectively length and width.
- To change <img src=""https://render.githubusercontent.com/render/math?math={f}""> you must edit `geopar.f` (line 22-23)
- To change <img src=""https://render.githubusercontent.com/render/math?math={N}""> you must edit `thetas`. The values in thetas represent the potential densities (assuming a linear equation of state, where density solely depends on potential temperature), and are determined as (<img src=""https://render.githubusercontent.com/render/math?math={\sigma_0-1000)/1000}"">. That means that <img src=""https://render.githubusercontent.com/render/math?math={1000*\sigma_0}""> runs from 26.0 till 28.071 <img src=""https://render.githubusercontent.com/render/math?math={\rm{kg}/{m}^3}"">. 
- To change the length of the run you must edit `micom.in`. The 5 values are explained in `micom_ssd.f` where `micom.in` is read on line 27. You can search for their names to understand what they steer. The first 2 values refer to `day1` and `day2` and the model runs from `day1` to `day2`.

Note that you may need to recompile the program using `make`.

In tutorial 2 we ask you to configure at least 1 of the 3 other set-ups discussed in Drijfhout and Maas (2007) (if you have already done so in practical 1 we ask you to choose a second one), and at least 1 configuration that was **NOT** discussed in Drijfhout and Maas (2007). You may think of the following options: 

- What happens when you change the forcing frequency? Are the results invariant for this parameter? And what happens if you add the new and old frequency in the forcing? How linear is the response?
  You can change the forcing frequency in `boundpb.f` and `boundvb.f` on line 30 (`sin(tsec*2.*pi/(12.*3600.))`).
- What happens if you choose steeper or less steep bottom profiles in the cross-channel direction? What happens if you add random small-scale perturbations to the bottom?
- What happens if you turn the channel, or to make it easy, make <img src=""https://render.githubusercontent.com/render/math?math={f}""> a variable function of *x*?
- What happens if you make <img src=""https://render.githubusercontent.com/render/math?math={N}""> stronger or weaker or no longer a constant but a function of *z*?
- Maybe you want to change another parameter. Which one would you choose (try to argue, even if you have no time to do this)?

Again we ask you:

- To motivate your choice of figures (choose 2-4) for each of the 2 runs with 1 parameter changed.
- What is the story you want to tell (see bullet 1) and what are your main conclusions?

The deadline for this assignment is **May 31st, 2022**.
",2022-08-12
https://github.com/daob/automtmm,"The plan:
----------
1. ESS SPSS file is read and preprocessed; transformed into cov matrices
2. User specifies input templates and program (LISREL, Mplus, ...
3. Input files generated from templates
4. Input files run
5. Parameters estimates and their cov matrices extracted from outputs, 
	summary report made (can be qualities, even meta-analysis of qualities)
6. Git commit is done
7. User can go back to 2. or even 1.,
8. History is visible via qgit, can even branch and merge, 
 	keeping track of reports.
",2022-08-12
https://github.com/daob/EPC-interest-ranking-paper,"Repository for the paper ""Evaluating measurement invariance in categorical data latent variable models with the EPC-interest""
==========================


This repository contains latex, R code, Latent GOLD 5 code, and raw data corresponding to the paper ""Evaluating measurement invariance in categorical data latent variable models with the EPC-interest"", DOI: [10.1093/pan/mpv020](http://dx.doi.org/10.1093/pan/mpv020/"").

These materials have also been deposited in the Dataverse archive corresponding to the published paper. 

Copyright to the published version (not hosted here) is held by Oxford University Press. All rights to the current files are reserved by the authors (Oberski, Vermunt & Moors).",2022-08-12
https://github.com/daob/JruleMplus,,2022-08-12
https://github.com/daob/lavaan.survey.fiml,"# lavaan.survey.fiml
## Complex survey standard errors for Structural Equation Models in R

This is an experimental package, whose functionality will eventually be merged into lavaan.survey. 
Currently it estimates standard errors for Structural Equation Models (SEM) estimated using lavaan in R, 
accounting for clustering and stratification. It also allows for models with missing data estimated using FIML, 
where lavaan.survey requires multiple imputation to deal with missing data.

It does not (yet) account for weights, so there is no point estimation in this package (see lavaan.survey for that). 
",2022-08-12
https://github.com/daob/Measurement-error-models,"
# [SURV730] Measurement error models
<a href=https://www.datacamp.com/teach/repositories/52621483/go target=""_blank""><img src=""https://s3.amazonaws.com/assets.datacamp.com/img/github/content-engineering-repos/course_button.png"" width=""150""></a>
<a href=http://www.datacamp.com/teach/repositories target=""_blank""><img src=""https://s3.amazonaws.com/assets.datacamp.com/img/github/content-engineering-repos/dashboard_button.png"" width=""150""></a>

These are the three <a href=https://www.datacamp.com target=""_blank"">DataCamp</a> assignments to go with SURV730 in the JPSM online/IPSDS program. Please see <a href=""http://jpsmonline.umd.edu/course/view.php?id=57"">JPSM online</a> for more information, slides, videos, and the syllabus.

",2022-08-12
https://github.com/daob/vaccine-misclassification,"# Misclassification models

### How to use

1. Clone the reposity
2. Open the Rproj file in Rstudio (for example by double-clicking it)
3. Open the `R/simulation.R` file
4. Run it

### Description of setup

* _Simulates_ data with the following setup: 
    - Outcome is ""event"" (0/1)
    - Outcome is misclassified. So there is an unobserved true outcome, `y`, and an observed, potentially error-prone, outcome `ystar`
    - Predictor `x` is ""vaccine"" (0/1)
    - Everything, including misclassfication table, is parameterized as a logistic regression. So parameters in these tables are log-linear.
* Defines _estimators_ you might apply to such data. Currently the following are implemented:
    - ""Naive"", pretending there is no misclassfication: a logistic regression of oobserved `ystar` on `x`. And the relative risk calculated on the observed `ystar`
    - ""MLE"": this formulates the (true) model, including misclassification. This model is estimated by maximizing the marginal log-likelihood directly. 
    - ""EM"": this formulates the same (true) model as MLE, but uses a different estimation method (EM), which is usually more stable.  
* Defines and runs a _simulation study_ (experiment) in which the following factors are varieed:
    - Sample size `n`
    - The sensitivity and specificity of the event registration (reworked into loglinear paramters `tau` and `lambda`)
    - The overall base event rate, parameterized with logit parameter `alpha`
    - The true effect size, with logistic regression coefficient `beta` (fixed at 0.2 for the moment)
    - The overall vaccination rate (fixed at 0.7 for the moment)

At the moment, only nondifferential error is examined.

### Results so far

""Obviously"", the MLE is unbiased, especially as `n` increases and/or `alpha` gets closer to zero (more events observed). However, in terms of mean-squared-error, it is almost never worthwhile to use the MLE. This is because, in this simple nondifferential setup, only the specificity, which we can assume to be excellent, is relevant to bias in `beta`. So there is little to no payoff for trading unbiasedness for the considerably larger variance in the MSE relative to the naive estimator. 

![Mean absolute error illustration](https://github.com/daob/vaccine-misclassification/blob/dcbe1cff67edcddcac11420ae1a4f42bb9b2ae23/images/mean_absolute_error_big.pdf)

The same results may not hold for relative risk (not examined yet).

The same results will not hold for differential error.",2022-08-12
https://github.com/dasscheman/bar,"Yii 2 gebaseerd barkassa systeem.
============================

Deze aplicatie is gemaakt voor het beheren van de bar voor kleine verenigingen.

Installatie:

Onder buntu kun je alle benodigde pakketten met apt installeren:
`sudo apt install composer php-cli php-gd php-intl php-mysql php-xml mariadb-server`

Database aanmaken:
`mysql -u root -p -e 'CREATE DATABASE barkassa'`

clone met git en maak de config/db.php:
```
<?php

return [
    'class' => 'yii\db\Connection',
    'dsn' => 'mysql:host=localhost;dbname=barkassa',
    'username' => 'root',
    'password' => '',
    'charset' => 'utf8',
];
```

 En de config/email.php:

```
<?php

return [
    'class' => 'Swift_SmtpTransport',
    'host' => 'localhost', // e.g. smtp.mandrillapp.com or smtp.gmail.com
    'username' => 'username',
    'password' => 'password',
    'port' => '587', // Port 25 is a very common port too
    'encryption' => 'tls', // It is often used, check your provider or mail
];
```

Vanaf de root
- `composer global require ""fxp/composer-asset-plugin""`
- `composer install`
- `php yii migrate/up --migrationPath=@yii/rbac/migrations`
- `php yii migrate/up --migrationPath=@vendor/dektrium/yii2-user/migrations`
- `php yii migrate/up`
- `php yii serve`

Ga met je browser naar http://localhost:8080

Log in met de username `beheerder` en wachtwoord `beheerder` en wijzig het wachtwoord.
Deze gebruiker kan gebruikers toevoegen en rechten zetten op andere gebruikers.",2022-08-12
https://github.com/dasscheman/BierBlog,"## Bierblog
Site voor brouwerijzeist.nl


""type"": ""path"",
""url"": ""/home/daan/DevelopLaravel/laravel-blog"",
""options"": {
    ""symlink"": true
}



        {
            ""type"": ""vcs"",
            ""url"": ""git@github.com:dasscheman/laravel-blog.git""
        },
",2022-08-12
https://github.com/dasscheman/glazenwassers,"# glazenwassers
Glazenwasser is een spel dat gebruik maakt van het Yii2 framework. Hieronder staat hoe je dat insatlleerd. 

Vragen kun je sturen naar glazenwasser@biologenkantoor.nl

Yii 2 Basic Application Template
================================

Yii 2 Basic Application Template is a skeleton Yii 2 application best for
rapidly creating small projects.

The template contains the basic features including user login/logout and a contact page.
It includes all commonly used configurations that would allow you to focus on adding new
features to your application.


DIRECTORY STRUCTURE
-------------------

      assets/             contains assets definition
      commands/           contains console commands (controllers)
      config/             contains application configurations
      controllers/        contains Web controller classes
      mail/               contains view files for e-mails
      models/             contains model classes
      runtime/            contains files generated during runtime
      tests/              contains various tests for the basic application
      vendor/             contains dependent 3rd-party packages
      views/              contains view files for the Web application
      web/                contains the entry script and Web resources



REQUIREMENTS
------------

The minimum requirement by this application template that your Web server supports PHP 5.4.0.


INSTALLATION
------------

### Install from an Archive File

Extract the archive file downloaded from [yiiframework.com](http://www.yiiframework.com/download/) to
a directory named `basic` that is directly under the Web root.

You can then access the application through the following URL:

~~~
http://localhost/basic/web/
~~~


### Install via Composer

If you do not have [Composer](http://getcomposer.org/), you may install it by following the instructions
at [getcomposer.org](http://getcomposer.org/doc/00-intro.md#installation-nix).

You can then install this application template using the following command:

~~~
php composer.phar global require ""fxp/composer-asset-plugin:1.0.0-beta4""
php composer.phar create-project --prefer-dist --stability=dev yiisoft/yii2-app-basic basic
~~~

Now you should be able to access the application through the following URL, assuming `basic` is the directory
directly under the Web root.

~~~
http://localhost/basic/web/
~~~


CONFIGURATION
-------------

### Database

Edit the file `config/db.php` with real data, for example:

```php
return [
    'class' => 'yii\db\Connection',
    'dsn' => 'mysql:host=localhost;dbname=yii2basic',
    'username' => 'root',
    'password' => '1234',
    'charset' => 'utf8',
];
```

**NOTE:** Yii won't create the database for you, this has to be done manually before you can access it.

Also check and edit the other files in the `config/` directory to customize your application.
",2022-08-12
https://github.com/dasscheman/hike-app,"<p align=""center"">
    <a href=""https://github.com/yiisoft"" target=""_blank"">
        <img src=""https://avatars0.githubusercontent.com/u/993323"" height=""100px"">
    </a>
    <h1 align=""center"">Yii 2 Basic Project Template</h1>
    <br>
</p>

Yii 2 Basic Project Template is a skeleton [Yii 2](http://www.yiiframework.com/) application best for
rapidly creating small projects.

The template contains the basic features including user login/logout and a contact page.
It includes all commonly used configurations that would allow you to focus on adding new
features to your application.

[![Latest Stable Version](https://img.shields.io/packagist/v/yiisoft/yii2-app-basic.svg)](https://packagist.org/packages/yiisoft/yii2-app-basic)
[![Total Downloads](https://img.shields.io/packagist/dt/yiisoft/yii2-app-basic.svg)](https://packagist.org/packages/yiisoft/yii2-app-basic)
[![Build Status](https://travis-ci.org/yiisoft/yii2-app-basic.svg?branch=master)](https://travis-ci.org/yiisoft/yii2-app-basic)

DIRECTORY STRUCTURE
-------------------

      assets/             contains assets definition
      commands/           contains console commands (controllers)
      config/             contains application configurations
      controllers/        contains Web controller classes
      mail/               contains view files for e-mails
      models/             contains model classes
      runtime/            contains files generated during runtime
      tests/              contains various tests for the basic application
      vendor/             contains dependent 3rd-party packages
      views/              contains view files for the Web application
      web/                contains the entry script and Web resources



REQUIREMENTS
------------

The minimum requirement by this project template that your Web server supports PHP 5.4.0.


INSTALLATION
------------

### Install via Composer

If you do not have [Composer](http://getcomposer.org/), you may install it by following the instructions
at [getcomposer.org](http://getcomposer.org/doc/00-intro.md#installation-nix).

You can then install this project template using the following command:

~~~
php composer.phar create-project --prefer-dist --stability=dev yiisoft/yii2-app-basic basic
~~~

Now you should be able to access the application through the following URL, assuming `basic` is the directory
directly under the Web root.

~~~
http://localhost/basic/web/
~~~

### Install from an Archive File

Extract the archive file downloaded from [yiiframework.com](http://www.yiiframework.com/download/) to
a directory named `basic` that is directly under the Web root.

Set cookie validation key in `config/web.php` file to some random secret string:

```php
'request' => [
    // !!! insert a secret key in the following (if it is empty) - this is required by cookie validation
    'cookieValidationKey' => '<secret random string goes here>',
],
```

You can then access the application through the following URL:

~~~
http://localhost/basic/web/
~~~


### Install with Docker

Update your vendor packages

    docker-compose run --rm php composer update --prefer-dist
    
Run the installation triggers (creating cookie validation code)

    docker-compose run --rm php composer install    
    
Start the container

    docker-compose up -d
    
You can then access the application through the following URL:

    http://127.0.0.1:8000

**NOTES:** 
- Minimum required Docker engine version `17.04` for development (see [Performance tuning for volume mounts](https://docs.docker.com/docker-for-mac/osxfs-caching/))
- The default configuration uses a host-volume in your home directory `.docker-composer` for composer caches


CONFIGURATION
-------------

### Database

Edit the file `config/db.php` with real data, for example:

```php
return [
    'class' => 'yii\db\Connection',
    'dsn' => 'mysql:host=localhost;dbname=yii2basic',
    'username' => 'root',
    'password' => '1234',
    'charset' => 'utf8',
];
```

**NOTES:**
- Yii won't create the database for you, this has to be done manually before you can access it.
- Check and edit the other files in the `config/` directory to customize your application as required.
- Refer to the README in the `tests` directory for information specific to basic application tests.


TESTING
-------

Tests are located in `tests` directory. They are developed with [Codeception PHP Testing Framework](http://codeception.com/).
By default there are 3 test suites:

- `unit`
- `functional`
- `acceptance`

Tests can be executed by running

```
vendor/bin/codecept run
```

The command above will execute unit and functional tests. Unit tests are testing the system components, while functional
tests are for testing user interaction. Acceptance tests are disabled by default as they require additional setup since
they perform testing in real browser. 


### Running  acceptance tests

To execute acceptance tests do the following:  

1. Rename `tests/acceptance.suite.yml.example` to `tests/acceptance.suite.yml` to enable suite configuration

2. Replace `codeception/base` package in `composer.json` with `codeception/codeception` to install full featured
   version of Codeception

3. Update dependencies with Composer 

    ```
    composer update  
    ```

4. Download [Selenium Server](http://www.seleniumhq.org/download/) and launch it:

    ```
    java -jar ~/selenium-server-standalone-x.xx.x.jar
    ```

    In case of using Selenium Server 3.0 with Firefox browser since v48 or Google Chrome since v53 you must download [GeckoDriver](https://github.com/mozilla/geckodriver/releases) or [ChromeDriver](https://sites.google.com/a/chromium.org/chromedriver/downloads) and launch Selenium with it:

    ```
    # for Firefox
    java -jar -Dwebdriver.gecko.driver=~/geckodriver ~/selenium-server-standalone-3.xx.x.jar
    
    # for Google Chrome
    java -jar -Dwebdriver.chrome.driver=~/chromedriver ~/selenium-server-standalone-3.xx.x.jar
    ``` 
    
    As an alternative way you can use already configured Docker container with older versions of Selenium and Firefox:
    
    ```
    docker run --net=host selenium/standalone-firefox:2.53.0
    ```

5. (Optional) Create `yii2_basic_tests` database and update it by applying migrations if you have them.

   ```
   tests/bin/yii migrate
   ```

   The database configuration can be found at `config/test_db.php`.


6. Start web server:

    ```
    tests/bin/yii serve
    ```

7. Now you can run all available tests

   ```
   # run all available tests
   vendor/bin/codecept run

   # run acceptance tests
   vendor/bin/codecept run acceptance

   # run only unit and functional tests
   vendor/bin/codecept run unit,functional
   ```

### Code coverage support

By default, code coverage is disabled in `codeception.yml` configuration file, you should uncomment needed rows to be able
to collect code coverage. You can run your tests and collect coverage with the following command:

```
#collect coverage for all tests
vendor/bin/codecept run -- --coverage-html --coverage-xml

#collect coverage only for unit tests
vendor/bin/codecept run unit -- --coverage-html --coverage-xml

#collect coverage for unit and functional tests
vendor/bin/codecept run functional,unit -- --coverage-html --coverage-xml
```

You can see code coverage output under the `tests/_output` directory.
",2022-08-12
https://github.com/dasscheman/hike_development,"# hike_development
",2022-08-12
https://github.com/dasscheman/photobooth,"photobooth
=======================

A DIY photo booth using a Raspberry Pi that automatically sends animated gifs local site: https://github.com/dasscheman/pibooth
And photo's are printed. 

This is based on: 
http://www.drumminhands.com/2014/06/15/raspberry-pi-photo-booth/
https://github.com/safay/RPi_photobooth/


",2022-08-12
https://github.com/dasscheman/pibooth,"Yii 2 Basic Project Template
============================

Yii 2 Basic Project Template is a skeleton [Yii 2](http://www.yiiframework.com/) application best for
rapidly creating small projects.

The template contains the basic features including user login/logout and a contact page.
It includes all commonly used configurations that would allow you to focus on adding new
features to your application.

[![Latest Stable Version](https://poser.pugx.org/yiisoft/yii2-app-basic/v/stable.png)](https://packagist.org/packages/yiisoft/yii2-app-basic)
[![Total Downloads](https://poser.pugx.org/yiisoft/yii2-app-basic/downloads.png)](https://packagist.org/packages/yiisoft/yii2-app-basic)
[![Build Status](https://travis-ci.org/yiisoft/yii2-app-basic.svg?branch=master)](https://travis-ci.org/yiisoft/yii2-app-basic)

DIRECTORY STRUCTURE
-------------------

      assets/             contains assets definition
      commands/           contains console commands (controllers)
      config/             contains application configurations
      controllers/        contains Web controller classes
      mail/               contains view files for e-mails
      models/             contains model classes
      runtime/            contains files generated during runtime
      tests/              contains various tests for the basic application
      vendor/             contains dependent 3rd-party packages
      views/              contains view files for the Web application
      web/                contains the entry script and Web resources



REQUIREMENTS
------------

The minimum requirement by this project template that your Web server supports PHP 5.4.0.


INSTALLATION
------------

### Install from an Archive File

Extract the archive file downloaded from [yiiframework.com](http://www.yiiframework.com/download/) to
a directory named `basic` that is directly under the Web root.

Set cookie validation key in `config/web.php` file to some random secret string:

```php
'request' => [
    // !!! insert a secret key in the following (if it is empty) - this is required by cookie validation
    'cookieValidationKey' => '<secret random string goes here>',
],
```

You can then access the application through the following URL:

~~~
http://localhost/basic/web/
~~~


### Install via Composer

If you do not have [Composer](http://getcomposer.org/), you may install it by following the instructions
at [getcomposer.org](http://getcomposer.org/doc/00-intro.md#installation-nix).

You can then install this project template using the following command:

~~~
php composer.phar global require ""fxp/composer-asset-plugin:~1.1.1""
php composer.phar create-project --prefer-dist --stability=dev yiisoft/yii2-app-basic basic
~~~

Now you should be able to access the application through the following URL, assuming `basic` is the directory
directly under the Web root.

~~~
http://localhost/basic/web/
~~~


CONFIGURATION
-------------

### Database

Edit the file `config/db.php` with real data, for example:

```php
return [
    'class' => 'yii\db\Connection',
    'dsn' => 'mysql:host=localhost;dbname=yii2basic',
    'username' => 'root',
    'password' => '1234',
    'charset' => 'utf8',
];
```

**NOTES:**
- Yii won't create the database for you, this has to be done manually before you can access it.
- Check and edit the other files in the `config/` directory to customize your application as required.
- Refer to the README in the `tests` directory for information specific to basic application tests.
",2022-08-12
https://github.com/dasscheman/yii2-app-basic,"Yii 2 Basic Project Template
============================

Yii 2 Basic Project Template is a skeleton [Yii 2](http://www.yiiframework.com/) application best for
rapidly creating small projects.

The template contains the basic features including user login/logout and a contact page.
It includes all commonly used configurations that would allow you to focus on adding new
features to your application.

[![Latest Stable Version](https://poser.pugx.org/yiisoft/yii2-app-basic/v/stable.png)](https://packagist.org/packages/yiisoft/yii2-app-basic)
[![Total Downloads](https://poser.pugx.org/yiisoft/yii2-app-basic/downloads.png)](https://packagist.org/packages/yiisoft/yii2-app-basic)
[![Build Status](https://travis-ci.org/yiisoft/yii2-app-basic.svg?branch=master)](https://travis-ci.org/yiisoft/yii2-app-basic)

DIRECTORY STRUCTURE
-------------------

      assets/             contains assets definition
      commands/           contains console commands (controllers)
      config/             contains application configurations
      controllers/        contains Web controller classes
      mail/               contains view files for e-mails
      models/             contains model classes
      runtime/            contains files generated during runtime
      tests/              contains various tests for the basic application
      vendor/             contains dependent 3rd-party packages
      views/              contains view files for the Web application
      web/                contains the entry script and Web resources



REQUIREMENTS
------------

The minimum requirement by this project template that your Web server supports PHP 5.4.0.


INSTALLATION
------------

### Install via Composer

If you do not have [Composer](http://getcomposer.org/), you may install it by following the instructions
at [getcomposer.org](http://getcomposer.org/doc/00-intro.md#installation-nix).

You can then install this project template using the following command:

~~~
php composer.phar global require ""fxp/composer-asset-plugin:^1.2.0""
php composer.phar create-project --prefer-dist --stability=dev yiisoft/yii2-app-basic basic
~~~

Now you should be able to access the application through the following URL, assuming `basic` is the directory
directly under the Web root.

~~~
http://localhost/basic/web/
~~~


### Install from an Archive File

Extract the archive file downloaded from [yiiframework.com](http://www.yiiframework.com/download/) to
a directory named `basic` that is directly under the Web root.

Set cookie validation key in `config/web.php` file to some random secret string:

```php
'request' => [
    // !!! insert a secret key in the following (if it is empty) - this is required by cookie validation
    'cookieValidationKey' => '<secret random string goes here>',
],
```

You can then access the application through the following URL:

~~~
http://localhost/basic/web/
~~~


CONFIGURATION
-------------

### Database

Edit the file `config/db.php` with real data, for example:

```php
return [
    'class' => 'yii\db\Connection',
    'dsn' => 'mysql:host=localhost;dbname=yii2basic',
    'username' => 'root',
    'password' => '1234',
    'charset' => 'utf8',
];
```

**NOTES:**
- Yii won't create the database for you, this has to be done manually before you can access it.
- Check and edit the other files in the `config/` directory to customize your application as required.
- Refer to the README in the `tests` directory for information specific to basic application tests.



TESTING
-------

Tests are located in `tests` directory. They are developed with [Codeception PHP Testing Framework](http://codeception.com/).
By default there are 3 test suites:

- `unit`
- `functional`
- `acceptance`

Tests can be executed by running

```
vendor/bin/codecept run
``` 

The command above will execute unit and functional tests. Unit tests are testing the system components, while functional
tests are for testing user interaction. Acceptance tests are disabled by default as they require additional setup since
they perform testing in real browser. 


### Running  acceptance tests

To execute acceptance tests do the following:  

1. Rename `tests/acceptance.suite.yml.example` to `tests/acceptance.suite.yml` to enable suite configuration

2. Replace `codeception/base` package in `composer.json` with `codeception/codeception` to install full featured
   version of Codeception

3. Update dependencies with Composer 

    ```
    composer update  
    ```

4. Download [Selenium Server](http://www.seleniumhq.org/download/) and launch it:

    ```
    java -jar ~/selenium-server-standalone-x.xx.x.jar
    ```

    In case of using Selenium Server 3.0 with Firefox browser since v48 or Google Chrome since v53 you must download [GeckoDriver](https://github.com/mozilla/geckodriver/releases) or [ChromeDriver](https://sites.google.com/a/chromium.org/chromedriver/downloads) and launch Selenium with it:

    ```
    # for Firefox
    java -jar -Dwebdriver.gecko.driver=~/geckodriver ~/selenium-server-standalone-3.xx.x.jar
    
    # for Google Chrome
    java -jar -Dwebdriver.chrome.driver=~/chromedriver ~/selenium-server-standalone-3.xx.x.jar
    ``` 
    
    As an alternative way you can use already configured Docker container with older versions of Selenium and Firefox:
    
    ```
    docker run --net=host selenium/standalone-firefox:2.53.0
    ```

5. (Optional) Create `yii2_basic_tests` database and update it by applying migrations if you have them.

   ```
   tests/bin/yii migrate
   ```

   The database configuration can be found at `config/test_db.php`.


6. Start web server:

    ```
    tests/bin/yii serve
    ```

7. Now you can run all available tests

   ```
   # run all available tests
   vendor/bin/codecept run

   # run acceptance tests
   vendor/bin/codecept run acceptance

   # run only unit and functional tests
   vendor/bin/codecept run unit,functional
   ```

### Code coverage support

By default, code coverage is disabled in `codeception.yml` configuration file, you should uncomment needed rows to be able
to collect code coverage. You can run your tests and collect coverage with the following command:

```
#collect coverage for all tests
vendor/bin/codecept run -- --coverage-html --coverage-xml

#collect coverage only for unit tests
vendor/bin/codecept run unit -- --coverage-html --coverage-xml

#collect coverage for unit and functional tests
vendor/bin/codecept run functional,unit -- --coverage-html --coverage-xml
```

You can see code coverage output under the `tests/_output` directory.
",2022-08-12
https://github.com/dasscheman/yii2-leaflet-character-icon,"Adding a character to an icon
",2022-08-12
https://github.com/davidcortesortuno/alma_tutorials_mumax3_workshop,"# ALMA Tutorials: MuMax3 Workshop

Author: David Cortés-Ortuño

Utrecht University, Netherlands <img src=""https://upload.wikimedia.org/wikipedia/en/thumb/2/26/Utrecht_University_logo.svg/1280px-Utrecht_University_logo.svg.png"" height=30 width=30 style=""display: inline-block;"">

## Repository

This repository contains the basic material for the MuMax3 workshop for the
ALMA Tutorials web series. 

In the `notebook` folder you can find a notebook that can be run in the Google
Colab machines. In addition, you can find a notebook with the data analysis
shown durin the workshop. 

## Extras

To analyse `ovf` files and MuMax3 data I recommend you take a look at the
[OOMMFPy](https://github.com/davidcortesortuno/oommfpy) library :smile:

You can also convert your data files to Python using `mumax3-convert -numpy`.

## Credits to: 

- [MuMax3](https://mumax.github.io/) 
- The Dynamat Team from Ghent University 
- The original [MuMax3 Workshop](https://mumax.ugent.be/mumax3-workshop/)
- Google 
",2022-08-12
https://github.com/davidcortesortuno/aps_search,"# aps_search

A script to search through APS journals. For now, we only print the latest twenty papers published with the relevant keyword.

Example:

```
david@tarro:: python prb_search.py --keyword skyrmion
2016-10-13  Quantum skyrmions in two-dimensional chiral magnets
            Rina Takashima, Hiroaki Ishizuka, and Leon Balents
            PRB
            http://journals.aps.org//prb/pdf/10.1103/PhysRevB.94.134415
--------------------------------------------------------------------------------
2016-10-07  Skyrmions with Attractive Interactions in an Ultrathin Magnetic Film
            Levente Rózsa, András Deák, Eszter Simon, Rocio Yanes, László Udvardi, László Szunyogh, and Ulrich Nowak
            PRL
            http://journals.aps.org//prl/pdf/10.1103/PhysRevLett.117.157205
--------------------------------------------------------------------------------
2016-10-05  Emergent geometric frustration of artificial magnetic skyrmion crystals
            Fusheng Ma, C. Reichhardt, Weiliang Gan, C. J. Olson Reichhardt, and Wen Siang Lew
            PRB
            http://journals.aps.org//prb/pdf/10.1103/PhysRevB.94.144405
            
...

```
",2022-08-12
https://github.com/davidcortesortuno/conference_style,"# conference_style
LaTeX style for an abstract book

This style generates and abstract book. The idea is to script (e.g. using Python) the creation of the sections, taking the information from a database (CSV file for example). The book has the structure:
  
  - Front Page
  - License
  - Table of Contents
  - Talks Schedule
  - Poster Schedule
  - Sections with the conference subjects
  - Abstracts

In the `minimal_example.tex` file is explained roughly how the sections are made. A PDF is provided with the final result.
To compile the `tex` file, we need `lualatex` and the `Lato` fonts installed in the system.

The `cls` file is not very tidy, because it is based in a previous work, but it has comments to where change every section.




The files are licensed under the Creative Commons Attribution-NonCommercial 4.0 International License.
",2022-08-12
https://github.com/davidcortesortuno/latex_template_naranja,"# Naranja LaTeX template

![](jpg/preview.jpg)

Naranja is a LaTeX class for reports, theses, etc.  This repository includes an
`example_report.tex` file as a template to start using the class. 
A [PDF file](pdf/report_example.pdf) with the compiled `tex` file is in the
`pdf` folder.

To compile the example just run `make`.

## Fonts

This class uses `URW garamond` for the main text and maths. This font can be
installed from a `lua` script in
[https://www.tug.org/fonts/getnonfreefonts/](https://www.tug.org/fonts/getnonfreefonts/).
Download it and run it with: 

    texlua install-getnonfreefonts

assuming you have TeXLive installed.

If you want to use the normal Modern LaTeX fonts, just comment the 


    % \renewcommand{\rmdefault}{ugm}
    % \renewcommand{\sfdefault}{ugm}
    % \usepackage[urw-garamond]{mathdesign}

lines in `naranja.cls`.

In addition, `lato` fonts are used for the headings. Future versions of
`naranja.cls` might use this font for the main text.

## Requirements

`naranja` is based on `komascript` with `tocstyle`. It seems `tocstyle` can
change in the future so this dependency might be updated in future versions of
the template. 

I suggest to compile a report that uses this class, using a recent version of
TeXLive, since packages as `lato` fonts are not available in old TeXLive
versions. The following is a list of packages used in `naranja.cls`:

    komascript      % base class 
    scrlayer        % chapter styling
    aurical         % font for chapter numbers
    ugm             % urw gramond font for main text
    textcomp        % text symbols
    lato            % font for headings
    tocstyle        % for table of contents. It comes with komascript
    tocbibind       % bibliography at the TOC
    minitoc         % mini table of contents in chapter heading
    amsmath
    colortbl        % colours for tables
    makecell        % tables
    hyperref        % links
    cite
    caption
    array
    geometry
    tikz
    tikzpagenodes
    pdfpages
    etoolbox
    
In Linux, different distributions have `extra` TeXLive packages to easily get
most of the dependencies for this class.

## TODO

* Figure and table labels should be at the right or left side according
  if the page is odd or even
* Update table of contents or `tocstyle`
* Update to a sans serif font (?)

---

This template is licensed under the [MIT License](LICENSE)
",2022-08-12
https://github.com/davidcortesortuno/master_thesis_physics_utfsm_2013,"# Influencia de las interacciones de Dzyaloshinskii-Moriya en el espectro de ondas de espín de láminas ferromagnéticas delgadas

English: *Influence of Dzyaloshinskii-Moriya interactions on the spin wave
spectrum of thin ferromagnetic films* 

My Master thesis was written in 2013 at the Universidad Técnica Federico Santa
María in Chile, under the supervision of Prof. Pedro Landeros. At that time I
used LyX to typeset the thesis which made the process much easier. The thesis
is written in spanish and contains original ideas about spin waves in thin
ferromagnetic films with the Dzyaloshinskii-Moriya interaction (DMI). The
approach is to assume a continuous material thus micromagnetics is the main
theoretical framework for the derivations. Units in this work are cgs. This
work was published as a paper in

https://doi.org/10.1088/0953-8984/25/15/156001

At the time the thesis was written there was a growing interest in materials
with DMI, in particular because of the recent experimental verification of
skyrmions in bulk ferromagnetic systems with a broken crystal symmetry. In my
thesis we focused on thin films and there is an attempt to obtain the
continuous DMI expression of 2D films (few monolayers). Nowadays (2021) these
expressions are well known and there has been a substantial number of studies
that have analysed and measured the magnitude of the DMI. Notably, our
theoretical derivations predicted the asymmetry on the spin wave spectrum that
could be verified using Brillouin Light Scattering. This has been proved in
multiple studies now.

The original thesis can be found in the `pdf` folder. The source file requires
updating of the references.

# Copyright

The thesis is copyrighted by the UTFSM in Chile. If you want to cite any of the
results, you can cite the aforementioned paper or using the following
bibtex entry:

    
    @mastersthesis{Cortes2013,
      author       = {Cort{\'e}s-Ortu{\~n}o, David}, 
      title        = {Influence of Dzyaloshinskii-Moriya interactions on the spin wave spectrum of thin ferromagnetic films},
      school       = {Departamento de F{\'i}sica, Universidad T{\'e}cnica Federico Santa Mar{\'i}a},
      year         = 2013,
      address      = {Avenida Espa{\~n}a 1680, Valpara{\'i}so, Chile},
      month        = 4,
    }
",2022-08-12
https://github.com/davidcortesortuno/mayavi_surface_povray,"![](surf_test.png)
",2022-08-12
https://github.com/davidcortesortuno/nb_cat,"# nb_cat

Print Jupyter notebooks on a terminal. We provide a `setup.py` file to
install the package using `pip`, which adds an executable to your
`../bin/` folder and call this script using `nb_cat FILE`.

If we execute the test notebook:

```
@: nb_cat test_nb.ipynb

--------------------------------------------------------------------------------

[M]:   # Test notebook


--------------------------------------------------------------------------------

[C]:   # Import some libraries
       import numpy as np


--------------------------------------------------------------------------------

[C]:   np.arange(7)

[Out]: array([0, 1, 2, 3, 4, 5, 6])

--------------------------------------------------------------------------------

...

```

## TODO:

* Convert LaTeX strings to unicode using:
  https://github.com/phfaist/pylatexenc/blob/master/pylatexenc/latex2text.py
  That repo needs to be updated to be compatible with Python 3

* Check for Matplotlib Figures or Images
",2022-08-12
https://github.com/davidcortesortuno/nebm_plot_tools,"# NEBM Plot Tools

Plot library for the NEBM data produced by Fidimag (and Finmag)

The functionality is demonstrated in the [Tutorial](tutorial/tutorial_fidimag.ipynb).

 <br />
---

This library is licensed under the MIT License (see ``LICENSE`` file) <br />
David Cortés-Ortuño, Hans Fangohr <br />
University of Southampton <br />
",2022-08-12
https://github.com/davidcortesortuno/oommfpy,"[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2611194.svg)](https://doi.org/10.5281/zenodo.2611194)
![GH Actions Test](https://github.com/davidcortesortuno/oommfpy/actions/workflows/manual-test.yml/badge.svg?)

```
   .+------+-------+-------+-------+-------+-------+-------+
 .'      .'      .'      .'      .'      .'      .'      .'|
+---+--+'------+'------+'------+'------+'------+'------+'  -
|      |       |       |       |       |       |       |   |
|   O  +   O   +   M   +   M   +   F   +   P   +   Y   +   +
|      |       |       |       |       |       |       | .'
+------+'------+'------+'------+-------+-------+-------+'
```

# OOMMFPy

A very minimal and simple Python library to read and extract data from OOMMF
magnetisation files `omf`, which are also used in MuMax3. In addition to this
library we provide tools to plot `omf` files and convert them to `vtk` files.

Highlights:

- Read `omf` files in any format
- Can also read `ovf` files and MuMax3 files
- Painless conversion of the data in an `omf` file into Numpy arrays for data
  analysis
- Fast calculation (using Numpy) of the skyrmion number in a slice of the
  system in any plane orientation (`xy`, `xz`, `yz`)
- Fast reading of `omf` files in binary format (using Numpy's `fromfile`)
- Minimal and super fast tool to convert `omf` files to VTK format
- Plot functions
- Early support for Paraview plugin: read `omf` files directly!

## Install

The easiest is to use `pip` or `poetry` to install the package from
[PyPI](https://pypi.org/project/oommfpy)

    pip install oommfpy

The Github address can also be directly used to install the package via `pip`

    pip install git+https://github.com/davidcortesortuno/oommfpy

Alternatively, a `setup.py` file is provided to install this library

    git clone https://github.com/davidcortesortuno/oommfpy
    cd oommfpy
    pip install ./

If successful, the `plot_omf` and `omf2vtk` tools are installed in the
corresponding `bin` directory and can be called from the command line.

A C library is built with the installation process, thus the setup file tries
to install Cython if is not present in the system.

### Paraview plugin

A first version of a reader for Paraview is added in this last version. For now
the installation is a bit of a hack:

- After installing the `oommfpy` library, locate the `oommfpy` folder from
  the`site-packages` directory

- Download the latest version of Paraview with Python > 3.8 support

- Copy the `oommfpy` directory into the Paraview Python `site-packages` folder.
  For example, for Paraview 5.9.0 installed in the `home` folder:

  ```
  cp -r oommfpy $HOME/ParaView-5.9.0-MPI-Linux-Python3.8-64bit/lib/python3.8/site-packages/
  ```

- Open Paraview and go to `Tools -> Manage Plugins -> Load New` and select the
  Python file in the `tools/` folder of `oommfpy` (you can clone the
  repository)

- Now you can open any `omf` file without converting to VTK!

## Documentation

For now check the `doc/ipynb` folder which contains a tutorial with basic
functionality. To load a file with a magnetisation field, which is found more
commonly in simulations, use the `MagnetisationData` class. To load any field,
such as the dipolar field, use the `FieldData` class.

Scripts to convert `omf` to VTK can be called directly as, for example,

```
omf2vtk -i omfs/my_oommf_output.omf -o test.vtk
```

The input path can also be a directory or a path with a wildcard, *e.g.*
`omfs/*.omf`. This method assumes the files in the path come from the same
simulation as the tool loads the mesh from the first file in the path and then
only updates the magnetisation fields.

Similar options are provided for the `plot_omf` function. Use the `--help` for
details.

## TODO

- [ ] More tests
- [ ] Add pyproject.toml file to avoid manual installation of Cython in setup.py
- [ ] More options to plotting library
- [ ] Print `z` coordinate when computing sk number
- [ ] Allow Periodic boundaries for the skyrmion number calculation
- [ ] Add typing check
- [ ] Support for multiple OS

# Citation

If you find this library useful, please cite this repository as:

```
@Misc{Cortes2019,
  author       = {David Cort{\'e}s-Ortu{\~n}o},
  title        = {OOMMFPy},
  howpublished = {Zenodo doi:10.5281/zenodo.2611194. Github: https://github.com/davidcortesortuno/oommfpy},
  year         = {2019},
  doi          = {10.5281/zenodo.2611194},
  url          = {https://doi.org/10.5281/zenodo.2611194},
}
```
",2022-08-12
https://github.com/davidcortesortuno/oommf_sk_number,"[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1296536.svg)](https://doi.org/10.5281/zenodo.1296536)

# WARNING:

This tool is not developed anymore and the project has been migrated to OOMMFPy:

https://github.com/davidcortesortuno/oommfpy



# OOMMF Skyrmion Number

Python library for the calculation of the skyrmion number
or topological charge from OMF files produced by OOMMF.

## Instructions 

The OMF files have to be in text format and regular grids. You can obtain
this format directly from OOMMF using the OOMMF tools:

    tclsh oommf.tcl avf2ovf -grid reg my_file.omf my_file_in_text.omf

The format is not important, can be `ovf`, `omf`, etc.

The Python library was designed to be used in external scripts that
can process multiple files or folders. Examples are in the `script` folder.
The main Class for loading and processing the files is called
`SkNumberOOMMF`.

All the calculations are performed using vectorised operations with the Numpy
library.

For now, the Q number can only be computed in a slice of the system in
the XY plane. Works well for 2D systems.

## Basic usage

To test the library, we can use an interactive console as IPython to
analyse a particular `omf` file, say, `my_file.omf`:

```python

import oommf_sk_number as oskn

# Load the class. z_index indicate the layer number
oommf_data = oskn.SkNumberOOMMF('my_file.omf', z_index=0)
print('Sk number =', oommf_data.compute_sk_number())

# Plot the charge density
oommf_data.plot_charge_density('muy_file_charge.png')
```

Options can be passed to the plot methods.

## Version

This is a very first version of the library. There are many options that still
need to be implemented/polished.

## TODO


- [ ] Multiple tests
- [ ] More documentation
- [ ] Create proper system wide installation (setup.py)
- [ ] Check labels on plots
- [ ] Allow slices of the system in different directions
- [ ] Calculate coordinates
- [ ] Allow Periodic boundaries
- [ ] Print z coordinate specified by the z-index

# Citation

Cite this repository as:

```
@Misc{Cortes2018,
  author       = {David Cort{\'e}s-Ortu{\~n}o},
  title        = {OOMMF Skyrmion Number},
  howpublished = {Zenodo doi:10.5281/zenodo.1296536. Github: https://github.com/davidcortesortuno/oommf_sk_number},
  year         = {2018},
  doi          = {10.5281/zenodo.1296536},
  url          = {https://doi.org/10.5281/zenodo.1296536},
}

```
",2022-08-12
https://github.com/davidcortesortuno/paper-2016-Cortes-etal-Thermal-stability-and-topological-protection-of-skyrmions,"[![DOI](https://www.zenodo.org/badge/73332983.svg)](https://www.zenodo.org/badge/latestdoi/73332983)

# NEBM Simulations

This repository contains all the necessary scripts to run the Nudged Elastic
Band method (NEBM) atomistic simulations, using
[Fidimag](http://computationalmodelling.github.io/fidimag/).

## System

![](figs/skyrmion.jpg)

The system under study is an ``80 nm X 40 nm X 0.25 nm`` cobalt nanotrack which
is translated into a lattice of ``320 X 185`` spins which are hexagonally
arranged. 

Magnetic parameters are:

    D     :: VARIABLE meV   DMI constant
    J     :: 27.026   meV   Exchange constant
    mu_s  :: 0.846    mu_B  Magnetic moment (in Bohr magneton units)
    k_u   :: 0.0676   meV   Anisotropy

The simulations are specified for different DMI constants. Folders and scripts
have the ``D`` parameter in ``J m**-2`` which, in the atomistic simulations,
have the following equivalences:

    Micromagnetic           Atomistic

    D = 26e-4 J m**-2  -->  D = 0.586 meV
    D = 28e-4 J m**-2  -->  D = 0.631 meV
    D = 30e-4 J m**-2  -->  D = 0.676 meV
    D = 32e-4 J m**-2  -->  D = 0.721 meV
    D = 34e-4 J m**-2  -->  D = 0.766 meV
    D = 36e-4 J m**-2  -->  D = 0.811 meV
    D = 38e-4 J m**-2  -->  D = 0.856 meV

## Simulations

There are three different NEBM simulations:

1. Skyrmion annihilation at the boundary

2. Skyrmion collapse or skyrmion destruction by a singularity, from linear
interpolations as initial state

3. Climbing image NEBM for the NEBM simulations of 2., which results in the
skyrmion collapse 

For simulation 2., we can obtain either a skyrmion collapse transition or a
skyrmion destruction by a singularity. The latest is observed for DMI
magnitudes equal or larger than ``0.721 meV``.

# Instructions

To run the simulations, we need:

1. Run the relaxation scripts in ``sims/relaxation/`` to obtain both, the
skyrmion and ferromagnetic states as ``npy`` files. There is a script for every
magnetic configuration and DMI constant. Hence, if we want the simulations for
``D=0.721 meV``, for example, we need to run

    bash stripe_320x185_Co_fm-up_D32e-4_h25e-2nm.sh
    bash stripe_320x185_Co_sk-down_D32e-4_h25e-2nm.sh

2. With the relaxed states we can now run the NEBM simulations in
``sims/nebm``.  Since we have 3 different simulations we have three options:

## 1. Boundary Annihilation

![](figs/boundary_annihilation.jpg)

For this we need to manually generate the initial state for the NEBM. Thus,
following the previous example, if we want to run the simulation for 
``D=0.721 meV``, we need to 
        
i. Move to ``sims/nebm/neb_stripe_320x185_Co_sk-down_fm-up_D32e-4_h25e-2nm_GEODESIC``

ii. Run the ``generate_sk-disp_initial_state.sh`` script. This will create
an ``sk_disp_npys`` folder with the images for the NEBM initial state

iii. Run the ``initiate_simulation_skdisp_k1e4.sh`` script. This will generate
the ``neb_stripe_320x185_Co_sk-down_fm-up_D32e-4_h25e-2nm_GEODESIC_energy.ndt`` and
``neb_stripe_320x185_Co_sk-down_fm-up_D32e-4_h25e-2nm_GEODESIC_dYs.ndt`` files with
the data of the energy bands for every iteration of the NEBM.

## 2. Linear interpolations

![](figs/singularity.jpg)

For this case, the initial state for the NEBM, which are linear interpolations
on the spherical angles that define the spins directions, is automatically
generated.  Thus it is only necessary to move to the simulation folder, for
example,
``sims/nebm/neb_stripe_320x185_Co_sk-down_fm-up_D32e-4_h25e-2nm_GEODESIC``, and
run the ``initiate_simulation_k1e4.sh`` script. As we mentioned before, some
simulations will relax towards a skyrmion collapse and others to the
destruction mediated by a singularity.

## 3. Climbing image NEBM

![](figs/collapse.jpg)

These simulations are the continuation of the **Linear interpolations**
simulations.  Since the climbing image is based on taking the largest energy
image in the band and redefining the forces on it, we need to identify this
point.

According to our results, we have already identified the climbing images and
they are specified on every simulation script (we expect that anyone who runs
the simulations will also obtain the same largest energy points). Thus,
following the previous examples, if we want to run the simulation for 
``D=0.721 meV``, we need to first run the corresponding **Skyrmion collapse**
simulation and then the ``initiate_simulation_climbing-15_k1e4.sh`` script,
which uses the 15th image of the band as climbing image.

# Docker

This repository also contains a Docker file to run a simple example or all the
simulations to completely reproduce the data from the paper. We provide in the
main folder a `Makefile` to automatically-easily run them.

When trying to run any of the simulations, the first step of the script is to
build a Docker container with Fidimag installed on it, together with all the
necessary tools to produce the plots. You can take a look at the ``Makefile``
in the ``docker/`` folder for details.

By default, the simulations run with OpenMP using 2 threads.

## Example

A simple example produces the data for the case of a DMI constant of ``D = 0.721 meV`` 
(equivalent to ``D = 3.2 mJ m**-2``). This can be called as simply as

    make example

These simulations reproduce the three afore mentioned transitions and two kind
of plots for every case. One of the plots shows the energy band for the last
step of the NEBM and the other is a sequence of snapshots of the NEBM band
images, which shows the skyrmion destruction process. Specifically, the
snapshots are the magnetisation field coloured according to the out of plane
component (``z``) of the spins. The figures are saved in the ``nebm_figures``
folder.

The plot for the energy band when using linear interpolations as initial state
is:

![](figs/example_energy_band.png)

And the snapshots look like

![](figs/nebm_example_snapshots.jpg)

Be aware that the system has a significant number of spins, thus the
simulations will take a while to finish.

## Paper data

To obtain the paper data, running the three different simulations for every DMI value
mentioned in the publication, the instruction is just to run

    make run_all

This process will probably take a couple of weeks to finish. If you want to use
more than 2 threads for OpenMP, just edit the line in ``docker/Dockerfile``
which says `ENV OMP_NUM_THREADS=2``.

Every energy band will be plotted in separate files, but if you want to plot
all the DMI cases (for a single transition) in a single file, you can call the
plot library with `python plot/plot_energy_bands.py --D_list 26 28 32 ...`` for
example. See the plot library for more details.
",2022-08-12
https://github.com/davidcortesortuno/paper-2018-chiral_magnonic_crystals,"[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1193279.svg)](https://doi.org/10.5281/zenodo.1193279)

# Chiral Magnonic Crystals: Micromagnetic Simulations

This repository has all the necessary tools to reproduce the micromagnetic
simulations from the publication: **Chiral Magnonic Crystals: Unconventional Spin-Wave
Phenomena Induced by a Periodic Dzyaloshinskii-Moriya Interaction** from *R. A.
Gallardo, D. Cortés-Ortuño, T. Schneider, A. Roldán-Molina, F. Ma, R. E. Troncoso,
K. Lenz, H. Fangohr, J. Lindner and P. Landeros*.

## System

The system under study is a 3000 nm long and 200 nm wide Permalloy stripe with
a periodic DMI, which can be obtained by patterning periodic arrays of heavy
metal wires on top of the sample.

The periodicity in the sample depends on three factors: the periodicity
parameter `a`, the width of the ferromagnetic material `w` and the width `w_D`
of the heavy metal wires, which induce a periodic interfacial DMI.

For example, for a system with `a=100 nm`, if we specify the heavy metal widths
as `w_D=a/2`, we obtain the pattern:

![](images/simulation_system_a100nm_w50nm.png)

Magnetic parameters of permalloy are:

```
    Ms = 0.658e6    MA/m
    A  = 11.1e-12   J/m
    D  = 3e-3       J/m^2  (this is variable and depends on the
                            induced DMI from the heavy metal wire)
```

## Simulations

We use OOMMF to simulate the magnonic waveguides and follow the methods from
[1] to compute the spin wave dispersion relation.

In our study, we fix the periodicity to a value of `a=100 nm` and we set
different heavy metal widths `w_D` of `a`, `a/2`, `a/4` and `0` (these
parameters can be changed from the OOMMF scripts in `sim` through the
`a` and `w` parameters). The width of the regions in the ferromagnet
without DMI is `w=a - w_D`.

The first step in the simulations is to relax the sample with a strong magnetic
field of `B=0.25 T` to saturate the magnetisation along the `y` direction,
which is along the sample width.

After relaxation, we excite the system using a `sinc` pulse of `0.025 T` of
magnitude with a cutoff frequency of `60 GHz` and centered at `50 ps`. We apply
this excitation in a `4 nm` wide region at the middle of the stripe, as shown
in the first figure. The system dynamics during the excitation process
is saved every `0.5 ps` during `8000` steps, i.e. for `4 ns`.


## Damping

The theory is based on infinitely long and wide stripes. In the simulation we
use a finite width of 200 nm. We set a damping of `0.01` magnitude that
exponentially grows damping towards the width edges as shown in:

![](images/exponential_damping_along_width.png)

By setting the damping in this way, we can obtain clearer images of the
spin wave spectrum of the system by avoiding reflection of the spin
waves at the boundary [2]. In our system, a significant noise comes from the
width edges rather from the edges at the extremes of the long side of
the stripe.

## Spectrum

Using the scripts in the `data_plot` folder we extract the spin components
along the sample length at the middle of the stripe for every time step.
Consequently, we apply a 2D Fourier transform with a `hanning` window function.
To plot the spectrum we use either a `squared` or `log` scale for the power
intensity.

A `log` scaled plot for the spectrum of the system with `w_d=50 nm` is shown
below:

![](images/spectra_w50_log10.png)


# Docker

If Docker is installed in your system, all the simulations and data processing 
scripts can be run automatically run using

```bash
    make run_all
```

This command builds a Docker container using the `joommf/oommf` image,
runs the simulations in the `sim` folder and generates the data using
the scripts in the `data_plot` folder.

# Extensions

Simulation scripts from this repository are specified with the original OOMMF extension for interfacial DMI. This module works for confined geometries. These simulations can be modified for the analysis of infinite films by using periodic boundaries with the `Oxs_PeriodicRectangularMesh` mesh class and the module provided in this [repository](https://github.com/joommf/oommf-extension-dmi-cnv), wich accepts periodic meshes. Results of systems with an *infinite* width should be equivalent to the figures shown in the paper associated to this Supplementary Material, which were simulated using the MuMax3 software with periodic boundaries along the y-direction of the waveguide.

# Cite

If you want to cite this repository you can refer to this bibtex entry:

```
@Misc{Cortes2018,
  author       = {David Cort{\'e}s-Ortu{\~n}o and Hans Fangohr},
  title        = {Data set for ``Chiral Magnonic Crystals: Unconventional Spin-Wave Phenomena Induced by a Periodic Dzyaloshinskii-Moriya Interaction''},
  howpublished = {Zenodo doi:10.5281/zenodo.1193279. Github: https://github.com/davidcortesortuno/paper-2018-chiral\_magnonics},
  year         = {2018},
  doi          = {10.5281/zenodo.1193279},
  url          = {https://doi.org/10.5281/zenodo.1193279},
}

```

# References

[1] Venkat et al. *Proposal for a Standard Micromagnetic Problem: Spin Wave
Dispersion in a Magnonic Waveguide*. IEEE Trans. Magn., 46, 1 (524-529). 2013.

[2] Venkat et al. *Mesh Size and Damped Edge Effects in Micromagnetic Spin Wave
Simulation*. Preprint at arXiv:1405.4615. 2014.
",2022-08-12
https://github.com/davidcortesortuno/paper-2019_nanoscale_skyrmions_target_states_confined_geometries,"[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1438396.svg)](https://doi.org/10.5281/zenodo.1438396)
[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/davidcortesortuno/paper-2019_nanoscale_skyrmions_target_states_confined_geometries/master)

# Data set for: Nano-scale magnetic skyrmions and target states in confined geometries 

![](images/hex_island_sk_helix.png)

This repository contains the libraries and scripts to completely reproduce the
simulations of the publication *Nano-scale magnetic skyrmions and target states in confined geometries* by D. Cortés-Ortuño, N. Romming, M. Beg, K. von Bergmann, A. Kubetzka, O. Hovorka, H. Fangohr and R. Wiesendanger. 

These simulations are based on the
[Fidimag](http://doi.org/10.5334/jors.223) code [1] which can perform discrete spin
simulations.

Images of the experimental data used in the simulations are located in the
`Romming_data` folder. These images are copyrighted and were obtained by the
group of [R.  Wiesendanger](http://www.nanoscience.de/HTML/) from the
University of Hamburg.

## Scripts

Simulation scripts are located in the `simulation` folder, where they are
separated in three categories:

- **relaxation**: scripts that directly relax a given initial state for a
  particular geometry (the experimental island, hexagons or truncated
  triangles). Magnetic parameters can also be modified, see the `bash` scripts
  for details. The library containing all the available options and initial
  states is `hexagonal_fidimag.py`, which uses `argparse`.  The *relaxation*
  directory also contains the scripts to fully reproduce the phase diagrams
  shown in [1]. These simulations require substantial simulation time and disk
  space, thus it is recommended to start them with precaution.

- **hysteresis**: these scripts take the initial state from the *relaxation*
  simulations (specific paths to `npy` files might require updating the final
  step number) and perform a sequential field sweep of the system, according to
  the specified options. Scripts are written as `cfg` Python config files and
  the main library with the options is `hexagonal_hysteresis.py`.

- **NEBM**: these scripts also rely on the simulations from the *relaxation*
  folder to specify the initial state. The main library in this case is
  `hexagonal_neb_fidimag.py`, which uses the GNEBM implementation from Fidimag.

A library to create a mesh/simulation from an image is given in
`sim_from_image.py`. In our case we use the islands from experiments in the
`Romming_data` folder. A copy of the numpy array containing the spin data,
which was btained with this library for the island system, is stored in the
`mu_s` directory.

A library to create meshes with different geometries is given in the
`mesh_geometries` folder.


# Widget

An interactive widget is provided in the `simulation_widget_nb.ipynb` notebook
which runs the `simulation_widget.py` library. This widget is a proof of
concept for the extensibility of Fidimag, in this case with the IPython widgets
library. The notebook shows a simulation for the island system reproduced from
experiments but further options are available in `simulation_widget.py`

# Notebooks

Jupyter notebooks with the analysis of the data from the simulations are
provided in the `notebooks` directory. Notebooks can be viewed directly from
Github. In case you run the notebooks the paths to simulation files need to be
updated according to the simulation files produced in your machine (final time
steps might change) and the folders in your system (some notebooks might be
outdated). Notebooks with GNEBM results require the `nebm_plot_tools` library.

In this repository we provide 10 notebooks:

- **Hexagons NEBM skyrmion collapse.ipynb**: GNEBM results for the skyrmion
  collapse transition in hexagonal islands of varying size, applied field and
  boundary condition. Results are based on the simulations located in
  `simulations/NEBM/nebm_2Dhex_hexagons_PdFe-Ir_sk-down-collapse_B-sweep_DT/`
  and
  `simulations/NEBM/nebm_2Dhex_hexagons_PdFe-Ir_sk-down-collapse_B-sweep_pinned_boundary_DT/`

- **Hexagons NEBM skyrmion escape.ipynb**: GNEBM results for the skyrmion escape
  transition in hexagonal islands with free boundaries and varying size and
  applied field. Results are based on the simulations located in
  `simulations/NEBM/nebm_2Dhex_hexagons_PdFe-Ir_sk-down-escape_B-sweep_DT/`

- **Hexagons NEBM tgt-state-up skyrmion-down collapse.ipynb**: GNEBM results for
  the decay of a target state into a skyrmion in hexagonal islands of varying
  size, applied field and boundary condition. Results are based on the
  simulations located in
  `simulations/NEBM/nebm_2Dhex_hexagons_PdFe-Ir_tgt-st-up_sk-down_collapse_B-sweep_DT/`
  and
  `simulations/NEBM/nebm_2Dhex_hexagons_PdFe-Ir_tgt-st-up_sk-down_B-sweep_pinned_boundary_DT/`

- **Hexagons PdPdFe energies_positive_fields.ipynb**: Notebook analysing the
  energy of ferromagnetic orderings, skyrmions and target states in hexagonal
  islands, as a function of island size, applied field and boundary condition.
  The notebook also shows snapshots of the relaxed configurations. Results are
  based on the simulations located in
  `simulations/relaxation/hexagons_size_variation_DT/` and
  `simulations/relaxation/hexagons_size_variation_pinned_boundary_DT/`

- **Hexagons_phase_diagram.ipynb**: Phase diagram with the lowest energy
  configurations in hexagonal islands with free boundaries, as a function of
  island size and applied field. The phase diagram is shown both using the
  topological charge and the snapshots of the ground states. Results are based
  on the simulations located in
  `simulations/relaxation/hexagons_phase_diagram_B_L/`

- **Hexagons_phase_diagram_pinned_bs.ipynb**: Same than the previous phase
  diagram notebook but in islands with a ferromagnetic rim. Results are based
  on the simulations located in
  `simulations/relaxation/hexagons_phase_diagram_B_L_pinned_bs/`

- **image_rotated NEBM target state-skyrmion.ipynb**: GENBM results for the decay
  of target states into skyrmions in quasi-hexagonal islands fabricated
  experimentally. Results are shown as a function of the applied field.
  Simulation files are located in
  `simulations/NEBM/nebm_2Dhex_image-rotated_PdFe-Ir_tgt-st-up_sk-down_B-sweep_DT/`
  and
  `simulations/NEBM/nebm_2Dhex_image-rotated_PdFe-Ir_tgt-st-3PI_tgt-st-2PI_B-sweep_DT/`

- **image_rotated snapshots-positive_fields.ipynb**: Results for the simulation
  of hysteresis-like field sweep processes in quasi-hexagonal islands
  fabricated experimentally. Results are shown for different initial states and
  the energy of the configurations found during the simulated sweep process are
  compared with the energy of magnetic configurations found during a field
  sweep experiment. Results are based on the files located in
  `simulations/hysteresis/image_rotated_DT/`

- **image_rotated_energies_positive_fields.ipynb**: Notebook analysing the
  energies and topological charge of different magnetic configurations in
  quasi-hexagonal islands fabricated experimentally. Results are compared with
  the energies of configurations observed during a field sweep experiment.  In
  addition, the notebook provides details about the translation of the mesh
  from experiment into the simulations. Snapshots of different magnetic
  orderings are also shown along the notebook. Simulation files are
  located in `simulations/relaxation/image_rotated_B-sweep_DT/`

- **image_rotated_relaxed_from_image.ipynb**: Notebook showing the simulation of
  quasi-hexagonal islands fabricated experimentally. Simulations are directly
  compared with the experimental observations, which are based on SP-STM
  measurements. Simulation files are
  located in `simulations/relaxation/image_rotated_relax-from-image/`. Experimental
  images are provided in the `Romming_data/` folder

# Cite

If you find this material useful please cite us (you might need the LaTeX's
`url` package)

    @Misc{Cortes2019,
      author       = {David Cort{\'e}s-Ortu{\~n}o and Niklas Romming and Marijan Beg and Kirsten von Bergmann and Andr{\'e} Kubetzka and Ondrej Hovorka and Hans Fangohr and Roland Wiesendanger},
      title        = {{Data set for: Nano-scale magnetic skyrmions and target states in confined geometries}},
      howpublished = {Zenodo \url{doi:10.5281/zenodo.1438396}. Github: \url{https://github.com/davidcortesortuno/paper-2019_nanoscale_skyrmions_target_states_confined_geometries}},
      year         = {2019},
      doi          = {10.5281/zenodo.1438396},
      url          = {https://doi.org/10.5281/zenodo.1438396},
    }

# References

[1] Bisotti, M.-A., Cortés-Ortuño, D., Pepper, R., Wang, W., Beg, M., Kluyver,
T., & Fangohr, H. (2018). Fidimag – A Finite Difference Atomistic and
Micromagnetic Simulation Package. Journal of Open Research Software, 6(1), 22.
DOI: http://doi.org/10.5334/jors.223
",2022-08-12
https://github.com/davidcortesortuno/paper-2020_real-space_imaging_of_confined_magnetic_skyrmion_tubes,"# Data set for: Real-space Imaging of Confined Magnetic Skyrmion Tubes

Run the notebooks online: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/davidcortesortuno/paper-2019_resonant_xray_imaging_confined_magnetic_skyrmion_tubes/master)

Zenodo:                   [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3618835.svg)](https://doi.org/10.5281/zenodo.3618835)


![](figures/sk_tubes_vorticity_im.png)

This repository contains the scripts, notebooks and experimental data to
reproduce the figures, simulations and numerical data shown in **Real-space
Imaging of Confined Magnetic Skyrmion Tubes** by *M. T. Birch, D.
Cortés-Ortuño,  L. A. Turnbull, M. N.  Wilson, F. Groß, N.  Träger, A.
Laurenson, N. Bukin, S. H. Moody, M.  Weigand, G. Schütz, H. Popescu, R. Fan,
P. Steadman, J. A. T. Verezhak, G.  Balakrishnan, J. C. Loudon, A. C.
Twitchett-Harrison, O. Hovorka, H. Fangohr, F. Ogrin, J.  Gräfe and P. D.
Hatton*.

Both simulation and experimental data analysis are performed using Python with
the Matplotlib, Jupyter, Scipy, Numpy and h5py libraries [1,2,3,4,5].

Jupyter notebooks are provided to process the experimental data and reproduce
the STXM, X-Ray Holography and LTEM images, which are shown as Figures 2, 3, 4
and 5 in the paper.

Simulation scripts are based on the finite difference micromagnetic code OOMMF [6]
with the extension to simulate DMI for materials with symmetry class *T*:
[oommf-extension-dmi-t](https://github.com/joommf/oommf-extension-dmi-t)

The analysis of OOMMF's output files, which are in the `OMF` format, are
processed using the [OOMMFPy](https://github.com/davidcortesortuno/oommfpy)
library [7], which can calculate the topological charge in a 2D slice. 

Three-dimensional visualisations of the magnetic states are performed using
Paraview. In order to get VTK files for visualisation, convert the `OMF` files
into `.vtk` using the `OOMMFPy` library.


## Simulations

Simulation scripts are located in [sims/oommf/](sims/oommf). Each of the
subfolders in this directory contains an OOMMF `mif` simulation script, and a
bash script to run the simulations used in the paper. These simulation scripts
are parametrised so they can be customised by tuning the magnetic parameters
and the protocols (such as the range of fields in the field sweep simulations).

### Simulation details

- `sk_tubes_helices_FeGe_Lx1000nm_Ly100nm_Lz1000nm`: simulation of three skyrmion tubes at different magnetic fields. The initial state is specified using three paraboloid solutions. The background can be either a conical solution or uniform field along the tubes. These simulations have a different coordinate system than the other simulations. Here, the `z` direction is along the tubes and the sample thickness is along the `y` direction, so in the data notebooks, the coordinate system is rotated to make `z` along the sample thickness and `y` along the skyrmion tubes. Simulations used in the paper start from the solution with a uniform backgound (files are named `no-helix`).
- `conical_FeGe_Lx1000nm_Ly100nm_Lz1000nm`: simulation of conical phase using a spin spiral initial state
- `mixed_phases_FeGe_Lx1000nm_Ly100nm_Lz1000nm_ROT`: simulation of spirals in 45 degrees, a skyrmion lattice and a field polarised region in the same system
- `m_random_FeGe_Lx1000nm_Ly100nm_Lz1000nm`: simulation of random states at zero field. In addition, scripts to run field sweep simulations in the `Bz` and `By` directions are included. Field sweep simulations start from the random equilibrium states and finish at a strong magnetic field around 300 or 400 mT.
- `sk_lattice_helices_FeGe_Lx1000nm_Ly100nm_Lz1000nm`: simulation of a skyrmion lattice at different fields. The initial state is specified using a triple-Q spiral, which is a well known analytical model for a skyrmion lattice.
- `spirals_helices_FeGe_Lx1000nm_Ly100nm_Lz1000nm`: simulation of a spiral/helical solution at zero (or any) field.

## Notebooks

Notebooks with the analysis of the data are in the [notebooks](notebooks)
folder. These notebooks rely heavily on the
[OOMMFPy](https://github.com/davidcortesortuno/oommfpy), Jupyter, Matplotlib,
Scipy and Numpy libraries. 

The best documented and self-explained notebook is the
[sk_tubes_field_sweep.ipynb](notebooks/sk_tubes_field_sweep.ipynb) Jupyter file
where simulations from three tubes and a field sweep process are analysed. The
notebooks show the calculation of averaging the magnetisation through the
thickness, isocontours of the skyrmion tubes, distance between skyrmions and
the sample edges, distance between skyrmion-skyrmion, topological charge
density and profiles of slices of the skyrmion tube. Notice that the coordinate
system from the data used in this notebook is rotated, so `y->z`, `z->y` and
`x->-x`.

The rest of the notebooks follow this procedure: load the `omf` files from
OOMMF using OOMMFPy, plot slices of the sample, compute the average of the out
of plane magnetisation component across the thickness of the sample, save this
data, plot this data.

## Experimental data

To reproduce the Figures obtained from the experimental data in the paper,
notebooks are provided in corresponding subfolders in the
[experimental](experimental) directory. These subfolders contain data from both
Holography and STXM images. Specifically, the notebooks in the `Figure_x`
directories show how the STXM data is processed and compared with the results
of the simulations and LTEM and X-Ray Holography images. Data files obtained
from simulations are provided as text files and can be reproduced from the
OOMMF simulations (see above). Additionally, the notebooks in the
[experimental/holography_reconstruction](experimental/holography_reconstruction)
contain the methods to process and generate the X-Ray Holography images.

## Reproducible

You can run the Jupyter notebooks online by pressing the Binder badge at the
top of this README file. Launching the notebooks in this way creates a
temporary copy of this repository, allowing it to be run and altered in real
time in your web browser. Notebooks in the [experimental](experimental) folder
can reproduce most of the Figures shown in the publication.

# Cite

If you find this material useful please cite us (you might need the LaTeX's
`url` package)

    @Misc{Birch2020,
      author       = {M. T. Birch and D. Cort\'es-Ortu\~no and  L. A. Turnbull and M. N. Wilson and F. Gro\ss and N. Tr\""ager and A.~Laurenson and N. Bukin and S. H. Moody and M. Weigand and G. Sch\""utz and  H. Popescu and R. Fan and P.~Steadman and J.~A.~T.~Verezhak and G. Balakrishnan and J. C. Loudon and A.~C.~Twitchett-Harrison and O. Hovorka and H. Fangohr and F. Y. Ogrin  and J. Gr\""afe and P. D. Hatton},
      title        = {{Data set for: Real-space Imaging of Confined Magnetic Skyrmion Tubes}},
      howpublished = {Zenodo \url{doi:10.5281/zenodo.3618835}. Github: \url{https://github.com/davidcortesortuno/paper-2020_real-space_imaging_of_confined_magnetic_skyrmion_tubes}},
      year         = {2020},
      doi          = {10.5281/zenodo.3618835},
      url          = {https://doi.org/10.5281/zenodo.3618835},
    }

# References

[1] *Matplotlib: A 2D Graphics Environment*. J. D. Hunter. Computing in Science
& Engineering, vol. 9, no. 3, pp. 90-95, 2007.

[2] *IPython: A System for Interactive Scientific Computing*. Fernando Pérez
and Brian E. Granger. Computing in Science & Engineering, 9, 21-29 (2007),
DOI:10.1109/MCSE.2007.53 (publisher link)

[3] *SciPy: Open Source Scientific Tools for Python, 2001-*. Jones E, Oliphant
E, Peterson P, et al., http://www.scipy.org/ [Online; accessed 2019-08-09].

[4] *A guide to NumPy*. Travis E, Oliphant. USA: Trelgol Publishing, (2006).

[5] *Python and HDF5*. Collette, A. O’Reilly Media (2013).

[6] *OOMMF User's Guide, Version 1.0*. M.J. Donahue and D.G. Porter.
Interagency Report NISTIR 6376, National Institute of Standards and Technology,
Gaithersburg, MD (Sept 1999) 

[7] *OOMMFPy*. D. Cortés-Ortuño. Zenodo doi:10.5281/zenodo.2611194. Github:
https://github.com/davidcortesortuno/oommfpy (2019)

[8] *Binder 2.0 - Reproducible, Interactive, Sharable Environments for Science
at Scale*. Jupyter et al. Proceedings of the 17th Python in Science Conference.
doi://10.25080/Majora-4af1f417-011 (2018)
",2022-08-12
https://github.com/davidcortesortuno/paper-2021_bloch_point_mediated_skyrmion_annihilation_in_three_dimensions,"Zenodo:                   [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4384569.svg)](https://doi.org/10.5281/zenodo.4384569)

# Data Set

This repository contains both the experimental data and the simulation scripts
to reproduce the results of [1]: *Topological defect-mediated skyrmion
annihilation in three dimensions* by M. T. Birch, D. Cortés-Ortuño, N. D. Khanh,
S.  Seki, A. Štefančič, G. Balakrishnan, Y. Tokura and P. D. Hatton.

![](images/sk_transition.jpg)

# Experimental data

Raw data from the figures of the paper, in the form of MPMS3 Data Files (`.dat`
files), are supplied in the `Experimental_Data` directory.

# Simulations

Simulations are based on the Fidimag [2] code and consist in two steps:

##  1. Find equilibrium states

These are found in `sims/equilibrium_states/` and use the main simulation
script `eq_state_relaxation.py`. Parameters are specfied in Bash files which
contain the range of field used to obtain the equilibrium states. Initial
profiles of the states are also found in the main Python script.

In the case of the skyrmion embedded in the helical phase, scripts to create
the initial configuration are found in
`sims/equilibrium_states/cubic_anisotropy/helix_sk_tube/sim.py` and in
`sim_hyst.py`. These scripts take the helical states (from a `npy` file) after
relaxation and create a skyrmion tube at the centre of the sample.

##  2. GNEBM simulations

After finding the equilibrium states, the magnetic configurations (stored in
`npy` files) are used to run the GNEBM to find the Minimum Energy Paths between
them. Simulations are based on the main Python script `sims/gnebm/gnebm.py`.

# Jupyter Notebooks

Notebooks where the energy of the configurations is analysed and where
visualisations of the configurations are shown, can be found in the `notebooks`
directory. Brief descriptions of the methods used in the Jupyter notebooks are
provided. The main methodology is to create a simulation object from Fidimag,
load `npy` files with the magnetic configurations and then analyze the data.
Visualisation of isosurfaces are performed using SciKit image [3] and plots of
the states are performed using Matplotlib [4]. Data analysis is also done via
Numpy arrays [5].


# Cite

If you find this material useful please cite us (you might need the LaTeX's
`url` package)

    @Misc{Birch2021,
      author       = {M. T. Birch and D. Cort\'es-Ortu\~no},
      title        = {{Data set for: Bloch point-mediated skyrmion annihilation in three dimensions}},
      howpublished = {Zenodo \url{doi:10.5281/zenodo.4384569}. Github: \url{https://github.com/davidcortesortuno/https://github.com/davidcortesortuno/paper-2021_bloch_point_mediated_skyrmion_annihilation_in_three_dimensions}},
      year         = {2021},
      doi          = {10.5281/zenodo.4384569},
      url          = {https://doi.org/10.5281/zenodo.4384569},
    }

# References

[1]  Birch, M. T., Cortés-Ortuño, D., Khanh, N. D., Seki, S., Štefančič, A.,
Balakrishnan, G., Tokura, Y. and Hatton, P. D. *eprint* **arXiv:2012.14813**
[https://arxiv.org/abs/2012.14813]

[2] Bisotti, M.-A., Cortés-Ortuño, D., Pepper, R., Wang, W., Beg, M., Kluyver,
T. and Fangohr, H., 2018. *Fidimag – A Finite Difference Atomistic and
Micromagnetic Simulation Package.* Journal of Open Research Software, 6(1),
p.22. DOI: http://doi.org/10.5334/jors.223

[3] Stéfan van der Walt, Johannes L. Schönberger, Juan Nunez-Iglesias, François
Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu and the
scikit-image contributors. scikit-image: Image processing in Python. PeerJ
2:e453 (2014) https://doi.org/10.7717/peerj.453 

[4] J. D. Hunter, ""Matplotlib: A 2D Graphics Environment"", Computing in Science
& Engineering, vol. 9, no. 3, pp. 90-95, 2007.

[5] Harris, C.R., Millman, K.J., van der Walt, S.J. et al. Array programming
with NumPy. Nature 585, 357–362 (2020). DOI: 0.1038/s41586-020-2649-2.
",2022-08-12
https://github.com/davidcortesortuno/paper-2022_toggle-like_current_induced_bp_dynamics_3d_skyrmion_strings,"[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6393340.svg)](https://doi.org/10.5281/zenodo.6393340)
![CC BY-NC-SA 4.0][cc-by-nc-sa-shield]

[cc-by-nc-sa-shield]: https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg

# Data Set

This repository contains both the experimental data and the simulation scripts
to reproduce the results of [1]: *Toggle-like current-induced Bloch point
dynamics of 3D skyrmion strings in a room-temperature nanowire* by M. T. Birch,
D. Cortés-Ortuño, K. Litzius, S. Wintz, F. Schulz, M. Weigand, A. Štefančič, D.
Mayoh, G. Balakrishnan, P.D. Hatton, G. Schütz. A preprint version of this
publication can be found at https://www.researchsquare.com/article/rs-1235546/v1

![](data_analysis/twotubes.png)

---

# Experimental data

Please refer to the README in the [Experimental Data](Experimental_Data)
directory.

Cloning this repository will not load the experimental data files
automatically. You need to install `git lfs` in the cloned repository and then
`git pull`.

# Simulations

Simulations are based on the MuMax3 [2] code and can be found in the
`sims/mumax3` directory. Simulations are written directly in Go (`go` files) or
in MuMax's scripting language (`mx3` files). Simulations written in Go require
to be compiled with a working installation of MuMax3 by running, for example,

```shell
go build skyrmion_lattice_Co8Zn9Mn3/sk_lattice_D_0d6e-3_A_5d7295e-12_LD_120nm_ROT.go
```

There are different simulations in the [sims/mumax3/](sims/mumax3) directory

- **skyrmion_lattice_Co8Zn9Mn3** : Simulation of a skyrmion lattice. In the
  manuscript it is used the rotated lattice given by the script ending in `ROT`

- **skyrmion_memory_device** : Sequence of skyrmion strings of different length

- **two_tubes_center_Co8Zn9Mn3** : 2 skyrmion strings at the centre of the
  sample using different magnitudes of the exchange and DMI constant such that
  the helical length of the system is kept at 120 nm. This is to find the
  optimal values for the parameters to make the skyrmion string size comparable
  to experimental data.

- **two_tubes_field-sweep_Co8Zn9Mn3_anisotropy** : Starting from two tubes at
  the centre of the sample, separated by a small distance of 5% of the length
  of the film, at a field of By=-100 mT, these simulations do a field sweep
  decreasing the field magnitude to 0 mT, for different values of a uniaxial
  anisotropy with hard axis in the x-direction. This is done in order to check
  a threshold value where helices propagating in the x-direction are formed.

- **two_tubes_separated_field-sweep_Co8Zn9Mn3_anisotropy** : Similarly than
  before but using a specific magnitude of anisotropy and using different
  separation of the initial two skyrmion strings. The anisotropy value was
  chosen from the threshold value of the previous simulations.

- **two_tubes_field-sweep_Co8Zn9Mn3_NEW** : Field sweep starting from two
  skyrmion strings at the centre of the sample, at a field of By=-100mT, for
  two different initial separation of the skyrmions. The field sweeps are
  performed both increasing and decreasing the field magnitude.

- **two_tubes_separation_Co8Zn9Mn3** : Energy minimisation of two skyrmion
  strings solutions initially separated by different distances, at different
  applied field strengths. Field is applied along the film width, in the
  y-direction. This folder includes simulations of the conical state.


# Jupyter Notebooks

Notebooks where the energy of the configurations is analysed and where
visualisations of the configurations are shown, can be found in the
[data_analysis](data_analysis) directory. Data analysis of the MuMax3 [2]
simulation outputs is performed using the OOMMFPy library [5]. Plots of the
simulations are performed using Matplotlib [3]. Data analysis is done via Numpy
arrays [4]. Visualizations are done using Paraview [6] and PyVista [7].

In the notebooks the main method is to compute the mean value of the out of
plane component of the magnetization across the film thickness, which is in the
z-direction. This is done to compare the results with the experimental data.
The average is computed by loading the magnetization data from `ovf` files
using OOMMFPy and then using Numpy array operations.

- **two_tubes_DMI_Exchange_vis.ipynb** : Data analysis of the simulation of two
  skyrmion strings using different values of exchange and DMI. Additionally, it
  is included visualisation of the skyrmion strings using PyVista. This
  notebook is well documented and can be used as a reference for the methods
  used in the other notebooks.

- **conical_state.ipynb** : Analysis of the conical state for a field applied
  in the direction of the film width. Results with and without anisotropy.

- **sk_lattice_vs_field.ipynb** : Simulation of skyrmion lattice in the
  xy-plane.

- **skyrmion_device.ipynb** : Visualisation of skyrmion strings of different
  length.

- **two_tubes_separation_vs_field.ipynb** : Compute images of the field sweep
  applied to the film with two skyrmion strings, starting from skyrmions
  separated by increasing distances.

- **two_tubes_separation_vs_field-CONICAL.ipynb** : Like before but looking
  at simulations where a conical background was specified in the initial state.

- **two_tubes_vs_anisotropies.ipynb** : Field sweep on a film with two skyrmion
  strings starting at a field of By=-100 mT, using different values of uniaxial
  anisotropy with hard axis in the x-direction.


# Cite

If you find this material useful please cite us (you might need the LaTeX's
`url` package)

    @Misc{Birch2021,
      author       = {M. T. Birch and D. Cort\'es-Ortu\~no},
      title        = {{Data set for: Toggle-like current-induced Bloch point dynamics of 3D skyrmion strings in a room-temperature nanowire}},
      howpublished = {Zenodo \url{doi:10.5281/zenodo.6393340}. Github: \url{https://github.com/davidcortesortuno/paper-2022_toggle-like_current_induced_bp_dynamics_3d_skyrmion_strings}},
      year         = {2022},
      doi          = {10.5281/zenodo.6393340},
      url          = {https://doi.org/10.5281/zenodo.6393340},
    }

# References

[1]  Birch M, Cortés-Ortuño D, Litzius K, et al. Toggle-like current-induced
Bloch point dynamics of 3D skyrmion strings in a room-temperature nanowire.
Research Square; 2022. PREPRINT (Version 1). DOI: 10.21203/rs.3.rs-1235546/v1.
https://www.researchsquare.com/article/rs-1235546/v1

[2] Vansteenkiste, A. et al. The design and verification of MuMax3. AIP
Advances 4, 107133 (2014).

[3] J. D. Hunter, ""Matplotlib: A 2D Graphics Environment"", Computing in Science
& Engineering, vol. 9, no. 3, pp. 90-95, 2007.

[4] Harris, C.R., Millman, K.J., van der Walt, S.J. et al. Array programming
with NumPy. Nature 585, 357–362 (2020). DOI: 0.1038/s41586-020-2649-2.

[5] Cortés-Ortuño, D. OOMMFPy Python module. doi: 10.5281/zenodo.2611194 (2019).
[https://github.com/davidcortesortuno/oommfpy]

[6] Ayachit, U. The ParaView Guide: A Parallel Visualization Application,
Kitware, 2015, ISBN 978-1930934306

[7] Sullivan et al. PyVista: 3D plotting and mesh analysis through a
streamlined interface for the Visualization Toolkit (VTK). Journal of Open
Source Software, 4(37), 1450 (2019).
",2022-08-12
https://github.com/davidcortesortuno/paper_biskyrmions_bubbles,"[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1420447.svg)](https://doi.org/10.5281/zenodo.1420447)


# Supplementary data for: Do Images of Biskyrmions Show Type-II Bubbles?

![](images/bubble_lattice.png)

This repository contains the micromagnetic simulation scripts for the publication **Do Images of Biskyrmions Show Type-II Bubbles?** by *J. C. Loudon, A. C. Harrison, D. Cortes-Ortuno, M. T. Birch, L. A. Turnbull, A. Stefancic, F. Ogrin, E. Burgos-Parra, A. Campbell, H. Popescu, M. Beg, O. Hovorka, H. Fangohr, P. A. Midgley, G. Balakrishnan and P. D. Hatton*, [Advanced Materials *31* 1806598 (2019), https://doi.org/10.1002/adma.201806598](https://doi.org/10.1002/adma.201806598)


Simulations are based on the OOMMF code [1] and its Jupyter interface JOOMMF [2]

# Simulations

Simulation scripts are located in the `sims/oommf` folder, where there are different directories:

- `bubble_lattice_A20pJm-1_mu0Ms648e-3_field-sweep`: simulation of lattice of bubbles in a cuboid sample, starting from an array of hexagonally arranged dots with the out of plane magnetisation direction in the `-z` direction on a background with `m_z=1`

- `bubble_lattice_A-sweep_mu0Ms-sweep_field-sweep`: simulation of a lattice of bubbles by varying the exchange, saturation magnetisation and applied field. These simulations help to find a suitable range of magnetic parameters to stabilise bubbles and compare them to the experimental data 

- `film_random_A20pJm-1_mu0Ms648e-3`: multiple simulations starting from different random initial configurations (random;y oriented spins)

- `film_random_A20pJm-1_mu0Ms648e-3_field-sweep`: (hysteresis-like) field sweep process starting from the random configurations obtained from `film_random_A20pJm-1_mu0Ms648e-3`. These simulations show the formation of spirals and different types of bubbles as the field is increased

- `isolated_typeII_bubble_A20pJm-1_mu0Ms648e-3_field-sweep`: simulation of isolated type-II bubbles in a cuboid sample to analyse their size and applied field range of stability. Simulations are performed for different cuboid sizes. See the `images` folder for different type-II bubble visualisations

![](images/bubble.png)
![](images/TYPEII_BUBBLE_OVERVIEW.png)

Since simulations are based in OOMMF [2], the scripts are given as `.mif` text files. Makefiles are also provided, but tsome paths might require updating. Scripts assume you can run OOMMF using the `oommf` system variable/command.

# Notebooks

Notebooks analysing/processing the data from the simulations are provided in the `notebooks` directory

# Example

An interactive example is given in the main folder to simulate a type-II bubble

# Cite

If you find this material useful please cite us:

```
@Misc{Cortes2019,
  author       = {David Cort{\'e}s-Ortu{\~n}o and James C. Loudon and Marijan Beg and Ondrej Hovorka and Hans Fangohr},
  title        = {{Supplementary data for: Do Images of Biskyrmions Show Type-II Bubbles?}},
  howpublished = {Zenodo doi:10.5281/zenodo.1420447. Github: https://github.com/davidcortesortuno/paper_biskyrmions_bubbles},
  year         = {2019},
  doi          = {10.5281/zenodo.1420447},
  url          = {https://doi.org/10.5281/zenodo.1420447},
}
```

# References

[1] M. J. Donahue and D. G. Porter, *OOMMF Users Guide, Version 1.0*, Tech. Rep. (National Institute of Standards and Technology, Gaithersburg, MD, 1999).

[2] M. Beg, R. A. Pepper, and H. Fangohr, *User interfaces for computational science: A domain specific language for OOMMF embedded in python*. AIP Advances 7, 056025 (2017), https://doi.org/10.1063/1.4977225.
",2022-08-12
https://github.com/davidcortesortuno/plomo_template,"# Plomo

A modern University of Southampton PhD thesis template

## Description

This collection of LaTeX files is a modern template for a PhD thesis. The
format of this template follows the rules of the University of Southampton for
the publication of a thesis (margins, front pages, etc.).  Still, I recommend
to check the rules in the official University of Southampton library online
documents.

The template is based on `Komascript` with the `Lato` font, which is in the
official CTAN LaTeX packages repository. Math fonts are customised to use
`Lato` font numbers and `cmbright` math symbols.

A `Makefile` is provided so you can compile the files using `make` or `make
nomen` in case of specifying a nomenclature.

All the necessary chapters for a thesis are in the `chapters` directory.

An example PDF of how the thesis would look like is in the `example` directory

I recommend using a recent TeXLive (or another TeX distribution) version to
avoid problems of compilation. I tested this template in Arch Linux with
TeXLive 2017.  Most of the package are in the official repositories of this
Linux distribution. Otherwise, you have to manually install packages from CTAN.

This template is licensed under a BSD 2-Clause License:

Copyright (c) 2018, David Ignacio Cortés Ortuño

I'd appreciate acknowledgment where possible ;)
",2022-08-12
https://github.com/davidcortesortuno/povray_vectorfield,"# Povray Vector Field

These scripts generate a vector field using Povray and Python.  The spins
directions are colourised according to their out of plane direction, using
Matplotlib.

The files are:

1. **generate_povray_inc.py**: This will generate a skyrmion (the vector field)
   using Fidimag and save the result to the **skyrmion.inc** file. This file
contains macros for each spin, called `spins`, which has 6 components: the 3
coordinates in space, the spins directions and the colours in rgb format.  We
provide the `.inc` file so we don't have to install Fidimag.

2. **generate_spins.pov**: This is the Povray file with the scene settings

3. **generate.ini**: Configurations to render the POV file

4. **Makefile**: Easily render the POV file with the corresponding settings and
   crop the final image file.

![](figs/skyrmion.png)


",2022-08-12
https://github.com/davidcortesortuno/skyrmion_model_widget,"# skyrmion_model_widget

The interactive widget illustrates a mathematical description of a structure
known as skyrmion. Skyrmions are magnetic configurations resembling vortices
that can be found in a specific range of magnetic materials where an inversion
symmetry is absent.  These configurations can be as small as a few nanometres
and are stable against perturbations, for example from temperature, because of
their peculiar topological properties. This also means that they can be
manipulated. For instance, they can be moved with electric currents. A skyrmion
can be described using a classical model, where the angular momentum of every
atom in a magnetic system, which is known as a spin, is represented by a three
dimensional vector.  The spin is a physical property that specifies the
magnetic properties of a magnetic material. Every spin can be understood as a
tiny magnet whose orientation is given by the vector orientation. From the
interaction of neighbouring spins, which is determined by the properties of the
magnetic material under study, different magnetic configurations can arise,
such as the vortex-like pattern of a skyrmion. 

The interactive plot shows a top view of a skyrmion in a triangular arrangement
of atoms. The plot shows the in-plane orientation of the spin vector of every
atom. The spins are coloured according to the out of plane vector component,
which is labelled as the z-direction. Because of the triangular arrangement of
atoms, their positions can be modelled as hexagons. The skyrmion state can have
different configurations depending on the interactions present on the system.
These configurations can be described by three parameters known as chirality,
vorticity and helicity, which determine the orientation of the spin vectors.
The chirality modifies the sense of rotation of the spins. The helicity can
modify the structure from a hedgehog-like pattern, when it is 0, to a
whirl-like pattern when it is equal to pi. And, finally, the vorticity changes
the pattern between a vortex or anti-vortex pattern. 

This linear model of a skyrmion has been useful to understand the properties of
skyrmions in magnetic disks made of FeGe
[https://www.nature.com/articles/srep17137]. Magnetic skyrmions have recently
been an important topic of study because of their potential application to
novel technologies, such as spin based magnetic recording devices.
",2022-08-12
https://github.com/davidcortesortuno/sundials_examples,"# Sundials examples

This is an attempt to undertsand how to use CVODE from Sundials

* The `src` and `include` files are from Sundials 2.6.2
  ( http://computation.llnl.gov/projects/sundials/sundials-software )

* The pendulum example has been taken from:
  http://www.cs.nyu.edu/courses/spring09/G22.2112-001/hw/hw10ex/hw10ex.pdf

  This files will be modified and has been updated to work with the latest
  Sundials version
",2022-08-12
https://github.com/DerekKarssenberg/test,"# test
test repository
test changes
",2022-08-12
https://github.com/DH-IT-Portal-Development/django-shared-core,"# Django DH-IT Core library

These Django apps implement shared code for DH-IT Django projects. Developed in collaboration with the [UiL OTS Labs](https://github.com/UiL-OTS-labs)

## Currently targeting:
- Python 3.9
- Python 3.10 (experimental)
- Django 3.x
- Django 4.0 (experimental)

Older versions are not supported, please use a older release if needed

# Included libraries
These libraries have been completely integrated into this codebase

## django-encrypted-model-fields 
Modified for better Django 2.2 support and some additional tweaks.

Source: https://gitlab.com/lansharkconsulting/django/django-encrypted-model-fields/

Licenced under the MIT licence, see `cdh/core/fields/LICENSE`.

## django-js-urls
Modified to better suit our usage

Source: https://github.com/impak-finance/django-js-urls

Licenced under the MIT license, see `cdh/core/js_urls/LICENSE`

# Partly included libraries
These libraries are partly integrated into this codebase. 
This means that we still use the original package as a dependency, but parts of it have been copied and 
modified into this codebase as overrides. 

## vbuild
Partly overriden for integration into a larger Django infrastructure (the ``cdh.vue`` app)

Source: https://github.com/manatlan/vbuild

Licensed under the MIT license, see https://github.com/manatlan/vbuild/blob/master/LICENSE
",2022-08-12
https://github.com/DH-IT-Portal-Development/ethics,"======
FEtC-H
======

Ethical Committee web application in Django

Introduction
------------

This Django_ project allows a user to apply a research project for ethical review.
It was custom-tailored for the `Faculty Ethics Committee - Humanities`_ (FEtC-H) of `Utrecht University`_.

Documentation
-------------

(Somewhat) detailed HTML documentation can be found in the ``docs`` folder.
A HTML version of the docs can be generated by sphinx, and a prebuild version is located in the ``docs/_build/html``
folder.

As a hasty programmer sometimes forgets things, it's recommended you rebuild the documentation yourself once in a while.
You can do this by exectuting the following commands in the ``docs`` folder::

    make clean # autodoc gets confused if you don't clean the existing folder
    make html

You can make any Sphinx supported format you want of course. Just replace the ``html`` with the desired format.

Language
--------

The main language of this web application is Dutch, as it's aimed towards the mostly avid Dutch-speaking researchers of Utrecht University.
However, since October 2016, there is a full English translation available, compiled by `Anna Asbury`_.
Translations in other languages are welcome, of course.

.. _Django: https://www.djangoproject.com/
.. _Faculty Ethics Committee - Humanities: https://fetc-gw.wp.hum.uu.nl
.. _Utrecht University: https://www.uu.nl
.. _Anna Asbury: http://www.annaasbury.com/",2022-08-12
https://github.com/DH-IT-Portal-Development/procreg,"# ProcReg: A processing registry for research

This repository houses the processing registry in development at the Utrecht University Faculty of Humanities. It is a Django project making use of the DH-IT [Django Shared Core](https://github.com/DH-IT-Portal-Development/django-shared-core).
",2022-08-12
https://github.com/DieStok/Basic-Machine-Learning-for-Bioinformatics,"# Basic-Machine-Learning-for-Bioinformatics
ML course materials for bioinformatics students following the course Basic Machine Learning for Bioinformatics at UU. <br>

# Topics covered per day (lectures and practicals)
* Day 1: Linear regression, gradient descent, introduction to linear algebra
* Day 2: Logistic regression, regularisation, ROC curve, introduction to neural networks (NNs)
* Day 3: NN Backpropagation algorithm, convolutional neural networks explained, guest speaker on deep learning in Oxford Nanopore sequencing (13:15-14:05)
* Day 4: K-means clustering, hierarchical clustering, deep dive into phylogenetics
* Day 5: Problems with high-dimensional data, Principal Component Analysis (PCA)
* Day 6: Working with scikit-learn, introduction to Keras and TensorFlow, project introduction and start

# Dependencies and running the practicals.
The material assumes a local installation of Anaconda, including the packages `numpy`, `scipy`, `pandas`, `sklearn`, `biopython`, `pandas-plink`, `tensorflow`, `notebook`, `matplotlib`, and `seaborn`.

# More information
For more information and resources, read the [course reader](CourseReaderMLBasics2021_Final.pdf).

# Words of thanks
Greatly inspired by/based on [Andrew Ng's course on Coursera](https://www.coursera.org/learn/machine-learning/home/welcome). The PCA part is based on [Prof. Victor Lavrenko's excellent lecture series](https://www.youtube.com/watch?v=IbE0tbjy6JQ&list=PLBv09BD7ez_5_yapAg86Od6JeeypkS4YM). Many thanks are owed to [Dr. Jeroen de Ridder](https://www.umcutrecht.nl/en/research/researchers/de-ridder-jeroen-j) for expert assistance. I thank [Dr. ir. Bas van Breukelen](https://www.uu.nl/staff/BvanBreukelen) for long-term assistance and [Prof. Dr. Berend Snel](https://tbb.bio.uu.nl/snel/group.html) for comments on the phylogenetics part. Any errors remain my own (and, with your help, will hopefully be noticed and rectified soon).
",2022-08-12
https://github.com/DieStok/MHCPathwayNaiveBayesianClassifier,"# MHCPathwayNaiveBayesianClassifier
Major research project intended to infer candidate genes involved in the MHC pathway from publically available data and a positive set of known MHC pathway genes.
Read the [report](ImportantDocumentation/Writing/FinalReport_DieterStoker4159853_DiscoveringnovelMHCpathwaycandidatesusinganaïveBayesianclassifier.pdf) for details on the work and methods. Programming contains dataset and scripts used to process the data. Note that the code was not made to be publication-ready and represents my first foray into completing a large-scale bioinformatic research project. Hence, the code will not work out-of-the-box. Conducted under the auspices of Dr. Can Kesmir and Dr. T.J.P. van Dam. 

Note: part of the data was too large to be uploaded, specifically the (split) database of human TFBS motifs by Kheradpour and Kellis, and part of the unprocessed immunohistochemistry data. This data is available upon request.
",2022-08-12
https://github.com/DieStok/PracticalEssentialsCourseUUSeptember212021,"# PracticalEssentialsCourseUUSeptember212021
Contains the files you need for the practical of the Essentials Course for the Bioinformatics and Biocomplexity Master on the 21st of September 2021.

Subject matter covered: k-means clustering, hierarchical clustering, PCA, tSNE and UMAP. 
Author: Dieter Stoker
",2022-08-12
https://github.com/DieStok/PredictingDNALoopsWithChromatinMarksRandomForests,"# PredictingDNALoopsWithChromatinMarksRandomForests
Contains the [report](ImportantDocumentation/writing/DieterFinalReportChromatinLoopPredictionWithRFClassifiers_ShortInternshipMCLS_2019.pdf) and code for my minor internship on predicting DNA loop formation using chromatin marks at and between loop anchors using Random Forest classifiers. Conducted under the auspices of Dr. Jeroen de Ridder and Dr. Amin Allahyar. Thanks to Geert Geeven, Valerio Bianchi and Wouter de Laat for data and/or fruitful discussion. Due to being unpublished, the data for the DNA loops in heart tissue is not included in this repository. Please refer to this BioRxiv post for details on this data: https://www.biorxiv.org/content/10.1101/705715v1. Since the classifier works on GM12878 and Heart loops together and does not work with only half the data, I have opted to keep all data under wraps for now. It is available upon request. The code for the Drosophila deep learning replication experiment mentioned in Supplementary data 1 is not in this repository as it was a side-project, but is available upon request as well. run_script.sh was made by Dr. Amin Allahyar.
",2022-08-12
https://github.com/DorienHuijser/cd2-survey,"# Vocabularies Survey Connecting Data in Child Development (CD2)

Hello! This repository contains the code to process raw data from the CD2 vocabularies survey for each participating cohort study into a much more readable and useful format. It also contains these results, but for privacy reasons (email addresses, IP addresses and demographic information), the raw survey data is not published here. 

Below you will find what this survey is about and how the data are processed.

You can read the script in html-form [here](docs/CD2-vocabularies-survey-v1.2.html). The script itself is written in Rmd and can be found [here](CD2-vocabularies-survey-v1.2.Rmd)

**Table of contents**
  * [About the CD2 project](#about-the-cd2-project)
  * [About the CD2 vocabularies survey](#about-the-cd2-vocabularies-survey)
  * [Structure of the survey](#structure-of-the-survey)
  * [Structure of the datafile](#structure-of-the-datafile)
  * [Functionality of this code](#functionality-of-this-code)
  * [Dependencies](#dependencies)
  * [Installation](#installation)
  * [Usage](#usage)
  * [License](#license)
  * [Contributing and contact](#contributing-and-contact)

## About the CD2 project
Connecting Data in Child Development (CD2) is an infrastructure project funded by the Platform Digitale Infrastructuur - Social Sciences and Humanities. The project aims to harmonize the metadata from 6 Dutch developmental child cohort studies within the <a href=""https://individualdevelopment.nl/"" target=""_blank"">Consortium on Individual Development (CID)</a>. The end result of this harmonization will consist of an online portal where one can find, among others, what data was collected in these cohort studies and how to get access to them (of note, the portal will not contain actual data). Additionally, the metadata underlying this web portal will be findable not only through the web portal itself, but also through existing infrastructures such as ODISSEI (for the social scientific metadata) and HEALTH-RI (for the biomedical metadata). You can read the <a href=""https://pdi-ssh.nl/nl/funded-projects-2/gehonoreerde-projecten/connecting-data-in-child-development-cd2/"" target=""_blank"">project description here</a>.

## About the CD2 vocabularies survey
As part of making the CID metadata findable, data will be labeled with **keywords** and **categories**. Because there is no fully suitable controlled vocabulary available that fits the wealth of data in CID, we plan to complement existing vocabularies with our own. To create such a vocabulary, input from the entire CID community is needed to help us determine the relevant keywords and categories that researchers use to search for all the different types of data within CID. The CD2 vocabularies survey therefore asks the respondent (CID researcher) to 1) provide keywords and 2) choose relevant categories for a subset of experiments within their own cohort.

## Structure of the survey
The full survey can be found in `CD2_vocabularies_survey_qusetions.pdf` in the `docs` folder. Importantly, depending on their cohort, respondents answer 2 questions which are **repeated for 25 measures/instruments of their cohort** (e.g., experiments, questionnaires, etc.) using Qualtrics's Loop and Merge functionality:
- `Which keywords would you assign to this measure? Please separate your keywords with a comma` (open text question)
- `Choose one or multiple categories that you think fit best and rank them according to their relevance using numbers (1 = most relevant, 2 = second most relevant, etc.).` (ranking question with 3 additional custom fields)

## Structure of the datafile
The data file is an extremly wide datafile, because for each of the 6 cohorts, there is a total of around 80-200 measures/instruments (differs per cohort) that could be shown. Although the survey for an individual respondent will only show a random selection of 25 of these, the resulting datafile contains them all and is > 13000 columns wide. Not exactly readable!

The datafile contains 3 main types of variables:
1. Keywords question: `[instrument_number]_[cohort]_Keywords`: Keywords string response separated by commas.
2. Category rankings: `[instrument_number]_[cohort]_Cat_[category-number]`: The response is a number delineanating the priority given to the category.
3. Category custom categories: `[instrument_number]_[cohort]_Cat_1[2/3/4]_TEXT`: A string indicating the custom category that was provided.

## Functionality of this code
The code can be found in `docs` and does the following:
1. Set the parameters (e.g., filename, additional intrument numbers files, cohort names, etc.).
2. Read in the data.
3. Create mappings: lists of numbers and instrument names.
4. For each cohort, put the data from the Keywords question in a flat, usable format.
5. For each cohort, put the data from the Category rankinkg question in a flat, usable format.
6. For each cohort, combine the procesed data from the Keywords and Category ranking questions into one processed datafile. These can be found in `data/processed`

## Dependencies
Dependencies used can be found in the `renv.lock` file.

## Installation
Feel free to reuse this code by <a href=""https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository"" target=""_blank"">cloning the repository</a>. Warning: you will most likely have to adapt the code tremendously, since the code is currently tailored towards this specific use case. 

## Usage
The code is located in the `CD2-vocabularies-survey-v1.1.Rmd` file located in the `docs` folder. You need the following to run the code:
- R or R Studio
- The raw datafile (not included in this public repository)
- The instrument number files as used during the survey (located in `docs/instrumentnrs`)

All relevant files are read in by the code. Because the code is R Markdown, you can find a lot of explanation about how the code works in there as well.

## License
This project is licensed under the terms of the [MIT License](/LICENSE.md)

## Contributing and contact
To contribute, feel free to open an issue or a pull request in this repository. Alternatively, you can email <a href=""https://www.uu.nl/staff/DCHuijser"" target=""_blank"">Dorien Huijser</a> for comments or questions.
",2022-08-12
https://github.com/DorienHuijser/dorienhuijser,"## 👋 Hi there! 
[![Website](https://img.shields.io/badge/Website-399180?style=flat-square&logo=icloud&logoColor=white)](https://www.dorienhuijser.com/)
[![Twitter](https://img.shields.io/badge/Twitter-1DA1F2?style=flat-square&logo=twitter&logoColor=white)](https://twitter.com/DorienHuijser) 
[![LinkedIn](https://img.shields.io/badge/Linkedin-informational?style=flat-square&logo=linkedin&logoColor=white)](https://linkedin.com/in/DorienHuijser) 
[![ORCID](https://img.shields.io/badge/ORCID-darkgreen?style=flat-square&logo=orcid)](https://orcid.org/0000-0003-3282-8083)
[![OSF](https://img.shields.io/badge/Open_Science_Framework-29bcf3?style=flat-square&logo=open-access)](https://osf.io/n6ba2/)
[![University profile](https://img.shields.io/badge/University_profile-FFCD00?style=flat-square&logo=gmail&logoColor=white)](https://www.uu.nl/staff/DCHuijser)

Some facts about me:
- 💼 I am  a research data manager (she/her) at Utrecht University.  
- ✍️ I use Git(hub) mostly for documentation, not so much for hardcore coding!
- 🧠 I was schooled to be a Neuroscientist, but switched to being data support staff.
<br>

| 🔭 Keywords |  💻 Techy stuff | 🌱 Looking to learn |
| --- | ---| --- |
| Research Data Management | Git(hub) | R |
| Data Privacy | Markdown | Python |
| Metadata | HTML | **... and more** |
| Project Management | R | |
| Open Science | | |

### ⭐ Stats  
![Stats](https://github-readme-stats.vercel.app/api?username=DorienHuijser&show_icons=true)
![Your Repository's Stats](https://github-readme-stats.vercel.app/api/top-langs/?username=DorienHuijser&theme=white)
",2022-08-12
https://github.com/DorienHuijser/ExplDataAn_CourseProjectWeek4,"# Course project week 4 - Exploratory Data Analysis course
This repository contains files for the Course Project (week 4) from Exploratory Data Analysis on Coursera.

The files are:

1. **plot1.R and plot1.png**: Have total PM2.5 emissions from PM2.5 decreased in the United States from 1999 to 2008 (1999, 2002, 2005, 2008)? 
2. **plot2.R and plot2.png**: Have total emissions from PM2.5 decreased in the Baltimore City, Maryland (""fips==""24510"") from 1999 to 2008? 
3. **plot3.R and plot3.png**: Of the four types of sources indicated by the type (point, nonpoint, onroad, nonroad) variable, which of these four sources have seen decreases in emissions from 1999–2008 for Baltimore City? Which have seen increases in emissions from 1999–2008?
4. **plot4.R and plot4.png**: Across the US, how have emissions from coal combustion-related sources changed from 1999–2008?
5. **plot5.R and plot5.png**: How have emissions from motor vehicle sources changed from 1999–2008 in Baltimore City?
6. **plot6.R and plot6.png**: Compare emissions from motor vehicle sources in Baltimore City with emissions from motor vehicle sources in Los Angeles County, California (fips == ""06037""). Which city has seen greater changes over time in motor vehicle emissions?",2022-08-12
https://github.com/DorienHuijser/GettingCleaningDataCourseProject,"# GettingCleaningDataCourseProject
This repository contains the data, code and information necessary to reproduce the tidy dataset for the Getting and Cleaning Data Course on Coursera.
The instruction text for this project can be found [here](https://www.coursera.org/learn/data-cleaning/peer/FIZtT/getting-and-cleaning-data-course-project)

Files in this repository:

- Readme.md: the current readme file
- Codebook.md: codebook for the tidy datasets that are created with the run_analysis.R script: which variables represent what?
- tidy_dataset_1.txt: the first dataset that is created with the run_analysis.R script
- tidy_dataset_summary.txt: the second dataset that is created with the run_analysis.R script containing summary measures from the first dataset
- run_analysis.R: an R script that gets and cleans the original data and produces the two tidy datasets described above

## The run_analysis.R script
This script performs the following actions:

1. **Prepare the data**: set the working directory (change to your own working directory!) and  [download](https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip) and unzip the original data to be transformed
2. **Merge the training and test datasets** to create one dataset: load the necessary original data, name the variables and combine the test and training datasets into one large dataset containing 561 variables and 10299 observations
3. **Extract the measurements on the mean and standard deviations** for each measurement
4. **Label** the dataset with descriptive variable names
5. **Use descriptive activity names** to name the acitivities in the dataset: instead of numbers indicating the activities (walking, laying, sitting, etc.), replace them for actual words. In this step, the first dataset ""tidy_dataset_1.txt"" is created
6. **Create a second tidy dataset** with the average of each variable for each activity and each subject. In this step, the second dataset is created: ""tidy_dataset_summary.txt""",2022-08-12
https://github.com/DorienHuijser/MRIsharingguide,"# Brain MRI data sharing guide for Dutch researchers

With the increasing amount of data sharing initiatives, researchers collecting brain MRI data cannot and do not want to stay behind. However, many researchers do not know where to start, what they can share and where, where they can find information or support, etc. Therefore, we developed a brain MRI data sharing guide for researchers in the Netherlands who are new to open science and data sharing.

- **You can find the static version of the MRI data sharing guide [here](https://doi.org/10.5281/zenodo.3822289)**
- **You can find an interactive version of the Brain MRI data sharing guide [here](http://www.dorienhuijser.com/MRIsharingguide/)**

## Project development

- In October 2019, we had a [hackathon](https://www.universiteitleiden.nl/open-science-community-leiden/news/oscl) at Leiden University about sharing MRI data via the [Open Science Community Leiden](https://www.universiteitleiden.nl/open-science-community-leiden). Here, we formed a group of research supporters and researchers. In the next months, we developed a [first version](https://github.com/DorienHuijser/DecisionTreeMRIData/blob/master/old/20200125_DecisionTree_FlowChart_v1.4.bmp) of the guide.
- During an [OpenMR Benelux 2020](https://openmrbenelux.github.io/) [hackathon](https://github.com/OpenMRBenelux/openmrb2020-hackathon/issues/4), we worked further on the guide.
- The first version of the guide has now been published on Zenodo: https://doi.org/10.5281/zenodo.3822289 (June 19, 2020)
- After the first version was published, we started collaborating with colleagues from VU Amsterdam, in order to make the guide clearer and provide supporting information. We've also created an [interactive version](http://www.dorienhuijser.com/MRIsharingguide/) that does not require downloading.

## How to contribute

If you have feedback about this guide, feel free to contact me (Dorien Huijser) via [Twitter](https://twitter.com/DorienHuijser) or [work email](mailto:huijser@essb.eur.nl)

### :tada: Thanks to all [contributors](https://github.com/DorienHuijser/DecisionTreeMRIData/blob/master/contributors.md)! :tada:
",2022-08-12
https://github.com/duyguislakoglu/ai-ml-dl-resources,"# Artificial Intelligence/Machine Learning/Deep Learning Resources for Different Levels 

## General

1- Find your new project from <a href=""https://paperswithcode.com"">paperswithcode</a> 

2- Read <a href=""https://ec.europa.eu/newsroom/dae/document.cfm?doc_id=60419"">Ethics Guidelines for Trustworthy AI</a> 

3- Join communities: <a href=""https://huggingface.co""> HuggingFace</a>, <a href=""https://mltokyo.ai""> ML Tokio</a> and <a href=""https://www.fast.ai""> FastAI</a>

## Courses
1- Stanford CS231n (https://cs231n.github.io)

2- https://www.fast.ai

3- https://cds.nyu.edu/deep-learning/ 

4- https://web.stanford.edu/~jurafsky/slp3/

5- https://cs224w.stanford.edu  

6- Tübingen Machine Learning https://www.youtube.com/channel/UCupmCsCA5CFXmm31PkUhEbA/videos

7- TUM Computer Vision https://www.youtube.com/playlist?list=PLog3nOPCjKBneGyffEktlXXMfv1OtKmCs

8- https://huggingface.co/course/chapter1 

9- Essence of linear algebra https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab

10- http://mitliagkas.github.io/ift6085-dl-theory-class-2019/ 

11- AI&ML Oxford https://twitter.com/wooldridgemike/status/1406691675498553354

12- https://missing.csail.mit.edu 

## Books 
1- https://www.deeplearningbook.org 

2- https://mml-book.github.io

3- https://huyenchip.com/ml-interviews-book/ 

## Meetings
1- https://iml.web.cern.ch/meetings 

2- Mila Tea Talks https://mila.quebec/en/cours/rdv/

3- Koç University AI Meetings https://ai.ku.edu.tr/ai-meetings/

## Blogs/Channels
1- http://colah.github.io/posts/2015-08-Understanding-LSTMs

2- The Illustrated Transformer – Jay Alammar https://jalammar.github.io/illustrated-transformer/

3- Yannic Kilcher YouTube Channel https://www.youtube.com/c/YannicKilcher

4- Henry AI Labs	https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw?view_as=subscriber

5- Jeremy Jordan https://www.jeremyjordan.me

6- Machine Learning Mastery https://machinelearningmastery.com

7- Analytics Community https://www.analyticsvidhya.com/blog/

8- Blog - neptune.ai https://neptune.ai/blog

9- Chai Time Data Science https://www.youtube.com/c/chaitimedatascience

## Blog posts
1- https://machinelearningmastery.com/practical-guide-to-gan-failure-modes/ 

2- https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/

## Collections
1- https://deepmind.com/learning-resources

2- https://madewithml.com 

3- https://twitter.com/omarsar0 

4- https://twitter.com/arunprakashml/status/1404759268717334529 

5- https://github.com/duyguislakoglu/awesome-graph-ml

## Videos
1- https://slideslive.com/38955135/geometric-deep-learning-the-erlangen-programme-of-ml 

## Slides
1- https://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-40-introduction-to-neural-computation-spring-2018/lecture-notes/MIT9_40S18_Lec20.pdf 

## PhD
1- European Lab for Learning & Intelligent Systems https://ellis.eu

2- Computer Science Research at Max Planck Institutes | Computer and Information Science Research Germany Europe https://www.cis.mpg.de

3- Max-Planck-Institut für Informatik: CS@SAAR https://www.mpi-inf.mpg.de/cssaar

4- IMPRS-IS International Max Planck Research School for Intelligent Systems https://imprs.is.mpg.de

5- The Hybrid Intelligence Centre https://www.hybrid-intelligence-centre.nl

## Internships
1- https://www.zurich.ibm.com/greatminds/ 

## Podcast
1- https://twimlai.com 

## Programs
1- <a href=""https://github.com/dangkhoasdc/awesome-ai-residency""> AI Residency programs</a>
",2022-08-12
https://github.com/duyguislakoglu/awesome-graph-ml,"# awesome-graph-ml

## Courses

*1-* <a href=""http://web.stanford.edu/class/cs224w/"">Stanford CS224W</a> / <a href=""http://snap.stanford.edu/class/cs224w-videos-2019/"">2019 Winter Lecture Videos</a>
     
*2-* <a href=""https://cs.mcgill.ca/~wlh/comp766/"">Mcgill Comp766</a>

*3-* <a href=""https://jian-tang.com/teaching/graph2019"">Jian Tang</a>

## Books

*1-* <a href=""https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf"">William L. Hamilton. (2020). Graph Representation Learning. Morgan & Claypool, forthcoming</a>

*2-* <a href=""https://cse.msu.edu/~mayao4/dlg_book/""> DLG </a>

## Collections

*1-*  <a href=""https://github.com/naganandy/graph-based-deep-learning-literature"">graph-based-deep-learning-literature</a>

*2-*  <a href=""https://deep-learning-drizzle.github.io/#graphnn"">deep-learning-drizzle</a>

*3-*  <a href=""https://qdata.github.io/deep2Read//aReadingsIndexByCategory/#2Graphs"">qdata Deep Learning Readings</a>

*4-*  <a href=""https://t.me/graphML"">Graph Machine Learning Telegram Group</a>

## Workshops

*1-* <a href=""https://grlplus.github.io"">GRL+</a> <a href=""https://slideslive.com/icml-2020/graph-representation-learning-and-beyond-grl"">Videos</a>

*2-* <a href=""https://grlearning.github.io"">GRL</a>

*3-* <a href=""http://gdl-israel.github.io/"">GDL Israel</a>

*4-* <a href=""http://www.mlgworkshop.org/2020/""> MLG </a>

## Slides

*1-* <a href=""https://www.dropbox.com/s/appafg1fumb6u7f/tutorial_CVPR17_DL_graphs.pdf?dl=0"">Xavier Bresson tutorial_CVPR17_DL_graphs</a>

## Blog Posts

*1-* <a href=""https://towardsdatascience.com/@michael.bronstein"">Michael Bronstein</a>

*2-* <a href=""https://medium.com/@BorisAKnyazev"">Boris Kynazev</a>

*3-* <a href=""https://app.wandb.ai/yashkotadia/gatedgcn-pattern/reports/Intro-to-Graph-Neural-Networks-with-GatedGCN--VmlldzoyMDg4MjA"">Introduction to GNNs with GatedGCN</a>

## Tutorials

*1-*  <a href=""https://github.com/sw-gong/GNN-Tutorial"">Shunwang Gong GNN Tutorial</a>

*2-*  <a href=""https://github.com/mims-harvard/graphml-tutorials"">mims-harvard graphml-tutorials</a>

## Videos

*1-* <a href=""https://www.youtube.com/watch?v=MIAbDNAxChI"">Jure Leskovec ""Deep Learning on Graphs""</a>

*2-* <a href=""https://www.youtube.com/watch?v=8kTxTX0eBRA"">Geometric Deep Learning - Michael Bronstein - MLSS 2020, Tübingen</a>

*3-* <a href=""https://www.youtube.com/watch?v=v3jZRkvIOIM"">Xavier Bresson: ""Convolutional Neural Networks on Graphs""</a>

*4-* <a href=""https://www.youtube.com/watch?v=Rr0pBFGcnjw&list=PL05umP7R6ij1qBaWovWYINzgFZJrBey4L&index=14&t=0s"">Stefanie Jegelka: Representation and Learning in Graph Neural Networks</a> 

*5-* <a href=""https://www.youtube.com/watch?v=fpb3j33RfTc&feature=youtu.be"">Focused Lecture - Graph Neural Networks with Petar Velickovic</a>

*6-* <a href=""https://www.youtube.com/watch?v=9XoCQn34tXo&feature=youtu.be"">Learning the Structure of Graph Neural Networks | Mathias Niepert | heidelberg.ai</a>

## Podcasts

*1-* <a href=""https://www.youtube.com/watch?v=Qtgep2CEExY"">TWiML Joan Bruna & Michael Bronstein Interview - Geometric Deep Learning</a>

*2-* <a href=""https://buff.ly/39nVvIY"">TWiML Graph ML Research at Twitter with Michael Bronstein</a>

## Tools

*1-* <a href=""https://github.com/rusty1s/pytorch_geometric"">Pytorch Geometric</a>

*2-* <a href=https://github.com/dmlc/dgl>DGL</a>

*3-* <a href=""https://github.com/alibaba/graph-learn"">graph-learn</a>

",2022-08-12
https://github.com/duyguislakoglu/eulers_monopoly,"![Ekran Resmi 2019-12-12 10 42 47](https://user-images.githubusercontent.com/37613999/70694090-d8d70680-1ccf-11ea-9cac-0982afe29f63.png)
",2022-08-12
https://github.com/duyguislakoglu/julia_graph_convolutional_networks,"
The Julia/Knet implementation of [<span class=""underline"">Semi-supervised Classification With Graph Convolutional Networks</span>](https://arxiv.org/pdf/1609.02907.pdf) [1].

## Usage

```julia train.jl```

## Parameters

      --dataset: The name of the dataset
      --model: The name of the model: gcn, gcn_cheby or dense
      --epochs: Number of epochs to train
      --lr: Initial learning rate
      --weight_decay: Weight for L2 loss on embedding matrix
      --hidden: Number of units in hidden layer
      --pdrop: Dropout rate (1 - keep probability) 
      --window_size: Tolerance for early stopping (# of epochs)
      --load_file: The path to load a saved model
      --num_of_runs: The number of randomly initialized runs 
      --save_epoch_num: The number of epochs to save the model 
      --chebyshev_max_degree: Maximum Chebyshev polynomial degree 

## Colab 
[<span class=""underline"">Colab link</span>](https://colab.research.google.com/drive/1yoe5yyJg-7gJ70Zp2AcIG0X1ey_XOoK3?authuser=1#scrollTo=Xd2rEUHRuFkn&uniqifier=2)

## Original Code

[<span class=""underline"">https://github.com/tkipf/gcn</span>](https://github.com/tkipf/gcn)

[<span class=""underline"">https://github.com/tkipf/pygcn</span>](https://github.com/tkipf/pygcn)


## Datasets

**1-** [<span class=""underline"">Citeseer</span>](https://github.com/kimiyoung/planetoid/tree/master/data)

**2-** [<span class=""underline"">Cora</span>](https://github.com/kimiyoung/planetoid/tree/master/data)

**3-** [<span class=""underline"">Pubmed</span>](https://github.com/kimiyoung/planetoid/tree/master/data)

**4-** [<span class=""underline"">NELL</span>](http://www.cs.cmu.edu/~zhiliny/data/nell_data.tar.gz)

https://github.com/kimiyoung/planetoid

## References

[1] Thomas N. Kipf, Max Welling. 2017, [<span class=""underline"">Semi-supervised Classification With Graph Convolutional Networks</span>](https://arxiv.org/pdf/1609.02907.pdf), In *International Conference on Learning Representations (ICLR)*
",2022-08-12
https://github.com/dzamrsky/SWAMPy-tools,"# SWAMPy tools

Welcome to the SWAMPy tools QGIS plugin repository! Swampy helps users to build coastal groundwater models (2D only so far) using the Flopy library to implement SEAWAT models. The user can  quickly set up a SEAWAT model at a desired location along the global coastline via an interface that splits the setup stages into multiple tabs. Starting at the geographical location and grid size defintion, followed by hydrogeological settings, specifying initial conditions, stress periods and boundary conditions and finally defining SEAWAT parameters before running the model itself. Once the model is succesfully terminated the user can plot the groundwater salinity distribution at each model time step.

Swampy development was funded via the Future Deltas programme of Utrecht University and via Deltares research institute and is still under further development which will bring new features in the future. If you have any issues with installing/using swampy or have suggestions for features you would like to have included in future versions please contact me at daniel.zamrsky@gmail.com! 

<b>Please beware that you need to have a QGIS version 3.16 or higher to use this plugin! (No guarantees for lower versions of QGIS)<b>

Hope you will enjoy the SWAMPy tools to the fullest! 
<br>
<br>
<br>
<p align=""center""><img src=""https://github.com/danzamrsky/SWAMPy-tools/blob/main/swampy_icon.png"" height=""600"" width=""600"" ></p>
",2022-08-12
https://github.com/EgilFischer/Bluetongue,"# Bluetongue
Agent-based model simulating spatial bluetongue transmission between farms and social interactions for decision making to vaccinate
",2022-08-12
https://github.com/EgilFischer/GenericInfectiousDiseaseModels,"# Generic infectious disease models
This project contains R-scripts for SIR type models and extensions. 

## File structure
```
.
└──GenericInfectiousDiseaseModels
    ├── src
    ├── output
    ├── data
    ├── README.md
    ├── LICENSE.md
    ├── CITATION.md
    └── .gitignore
```
    
## License

This project is licensed under the terms of the [MIT License](/LICENSE.md)

## Citation

Please as [Citation](/CITATION.MD)

## Contact
e.a.j.fischer@uu.nl
",2022-08-12
https://github.com/EgilFischer/mink-cat,"# SARS-CoV-2 infection in cats and dogs in infected mink farms
Anna E. van Aart, Francisca C. Velkers, Egil A.J. Fischer, et al 2021

Risk factor analyses and transmission estimation for SARS-CoV2 on cats and dogs living on mink farms in the Netherlands

[Analyses code for R](ParameterEstimation.R) 

[Final size calculation](FinalSizeFastImplementation.R) this file is not used in the paper, but can be used to estimate cat to cat transmission

[Data file](/Dataset_hond-katFV20210128EF20210202.xlsx) 


## License

This project is licensed under the terms of the [GNU General Public License v3.0](/LICENSE.md)

## Citation

Please [cite this project as described here](/CITATION)
",2022-08-12
https://github.com/EgilFischer/MITAR,"# MITAR
Repository containing dynamical models for the MITAR project

R-scripts to compare pair-formation models of conjugation with bulk-conjugation models,
used in Alderliesten JB, Zwart MP, de Visser JAGM, Stegeman A, Fischer EAJ. 2022.
Second compartment widens plasmid invasion conditions: two-compartment pair-formation
model of conjugation in the gut. Journal of Theoretical Biology 533:110937.
* pairformation.R 
* pairformation2comp.R

The first file is used to model conjugation in a single compartment, the second file
to model conjugation in two compartments, with migration between them.
Select one parameter set from the section 'Parameter values' at a time,
run the section 'Run simulations', and run the appropriate output section.
Results of the simulations are saved as Comma Separated Values (.csv) files.
Graphs are created and, if the variabe 'saveplots' in the section 'Plotting and
simulation options' is set to TRUE, saved as .png-files.
If the variable 'plotdataapproxbulk' is set to TRUE, the data used to
approximate the bulk-conjugation rates is plotted.

R-scripts for multispecies models of conjugation:
* multispecies.R
* multispeciespinnewspecies.R

These scripts consider three different scenarios regarding how the plasmid-bearing
population is introduced: in the most-abundant species, in the least-abundant species,
or in a species that was not yet present. These models are still under development.
",2022-08-12
https://github.com/EgilFischer/TransmissionEstimation,"# Estimation of NDV transmission in vaccinated and unvaccinated animals
R-Scripts used for ""A vector vaccine reduces transmission of Newcastle disease virus in commercial broiler chickens with maternally derived antibodies"".
These scripts- contain rmarkdown for the following analyses:

- parametric survival analyses for the infectious period
- non-parametric tests for the total excretion of virus (AUC)
- glm for estimation of transmission coefficient
- calculation of R0
",2022-08-12
https://github.com/ekatrukha/AIS,"AIS
===
Axon Initial Segment characterization/alignment tools

Matlab routines for analyzing fluorescence intensity profile along axon's initial segment (in 1D).<br />
Written similar to <a href=""http://www.ncbi.nlm.nih.gov/pubmed/20543823"">Grubb MS, Burrone J. Nature 2010 paper</a>, check out authors <a href=""http://www.grubblab.org/Matlab-scripts.php"">own version</a>.<br />
For description and manual refer to <a href=""https://github.com/ekatrukha/AIS/wiki""><strong>Wiki page</strong></a>.<br />
<img src=""http://katpyxa.info/software/AIS/emblem.png""><br />
Includes three modules:
<ul>
<li><strong>AISquantPlot</strong> for characterizing and inspecting individual profiles</li>
<li><strong>AISquantBatch</strong> for analyzing many profiles in batch</li>
<li><strong>AISalign</strong> for averaging and aligning multiple staining of two different proteins with respect to each other</li>
</ul>
<br />
Also includes <a href=""http://www.mathworks.nl/matlabcentral/fileexchange/209-padadd/content/padadd.m"">padadd</a> routine by Dave Johnson.

<hr />
Developed in <a href=""http://cellbiology.science.uu.nl/""> Cell Biology group</a> of Utrecht University.    
<a href=""mailto:katpyxa@gmail.com"">E-mail</a> for any questions. 
",2022-08-12
https://github.com/ekatrukha/Axonal_transport_model,"# Axonal_transport_model

Simple mathematical model of bidirectional active transport in axons (written in Mathematica).

General assuptions are:  
1) axonal microtubules are unidirectional (plus ends to periphery/growth cone)  
2) there are different ratios of kinesins/dyneins per single cargo  
   
It corresponds to equations described in the publication:  
LF Gumy, EA Katrukha, LC Kapitein, CC Hoogenraad <a href=""http://onlinelibrary.wiley.com/doi/10.1002/dneu.22121/full"">New insights into mRNA trafficking in axons</a> Developmental neurobiology, 2014   

It consists of two files, first is to find how average probability density function (PDF) changes over time:  
<b>part1_average_PDF.nb</b>   
<img src=""http://katpyxa.info/software/Axonal_transport_model/part1_ill.gif""> <img src=""http://katpyxa.info/software/Axon_transport_logo.png"">  

The second part simulates stochastic individual cargo movements:   
<b>part2_stochastic_MonteCarlo.nb</b>  
<img src=""http://katpyxa.info/software/Axonal_transport_model/part2_stochastic_MonteCarlo.png""> <img src=""http://katpyxa.info/software/Axonal_transport_model/multiple_partice_diffusion.gif"">


Developed in <a href='http://cellbiology.science.uu.nl/'>Cell Biology group</a> of Utrecht University.  
E-mail <a href=""mailto:katpyxa@gmail.com"">for any questions</a>.
",2022-08-12
https://github.com/ekatrukha/BigTrace,"[![](https://github.com/ekatrukha/bigtrace/actions/workflows/build-main.yml/badge.svg)](https://github.com/ekatrukha/bigtrace/actions/workflows/build-main.yml)

BigTrace
===

<br />
<img src=""https://katpyxa.info/software/BigTrace.png"" align=""right"" style=""padding:100px""/> This is <a href=""http://www.gnu.org/licenses/gpl.html"">open-source</a> <a href=""https://fiji.sc/"">FIJI</a> plugin for tracing of curvilinear structures in volumetric microscopy datasets using <a href=""https://forum.image.sc/t/bigvolumeviewer-tech-demo/12104"">BigVolumeViewer</a> for visualization). 
<br />
<br />
For description and manual refer to <a href=""https://github.com/ekatrukha/BigTrace/wiki""><strong>Wiki page</strong></a>.  <br />
Main topics:
<ul>
<li> <a href=""https://github.com/ekatrukha/BigTrace/wiki/How-to-install-plugin"">How to install plugin</a></li>
<li> <a href=""https://github.com/ekatrukha/BigTrace/wiki/How-to-use-plugin"">How to use plugin</a></li>
<li> <a href=""https://github.com/ekatrukha/BigTrace/wiki/How-to-cite-plugin%3F"">How to cite plugin?</a></li>
</ul>
<br />
The plugin is usable, but under ""alpha-stage"" development, so there are frequent updates and changes.
<br />
<br />

<img src=""https://katpyxa.info/software/BigTrace/bigtrace_example.gif"" />

Developed in <a href='http://cellbiology.science.uu.nl/'>Cell Biology group</a> of Utrecht University.  
<a href=""mailto:katpyxa@gmail.com"">E-mail</a> for any questions.
",2022-08-12
https://github.com/ekatrukha/ComDet,"ComDet
===

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4281064.svg)](https://doi.org/10.5281/zenodo.4281064)
<br />
<img src=""http://katpyxa.info/software/ComDet/comdet_emblem.png"" align=""right"" style=""padding:100px""/> This is <a href=""http://www.gnu.org/licenses/gpl.html"">open-source</a> <a href='http://rsbweb.nih.gov/ij/'>ImageJ</a> plugin for finding and/or analyzing colocalization of bright intensity spots (particles, vesicles, comets, dots, etc) in images with heterogeneous background (microscopy, astronomy, engineering, etc).
<br />
<br />
For description and manual refer to <a href=""https://github.com/ekatrukha/ComDet/wiki""><strong>Wiki page</strong></a>.  
Main topics:
* [How to install plugin](https://github.com/ekatrukha/ComDet/wiki/How-to-install-plugin)
* [How to use plugin (Text Tutorial)](https://github.com/ekatrukha/ComDet/wiki/How-to-use-plugin-%28Text-Tutorial%29)
* [How does detection work?](https://github.com/ekatrukha/ComDet/wiki/How-does-detection-work%3F)
* [How to cite plugin?](https://github.com/ekatrukha/ComDet/wiki/How-to-cite-plugin%3F)
<br />
<br />
Developed in <a href='http://cellbiology.science.uu.nl/'>Cell Biology group</a> of Utrecht University.  
<a href=""mailto:katpyxa@gmail.com"">E-mail</a> for any questions.
",2022-08-12
https://github.com/ekatrukha/ContourLines,"# ContourLines
[ImageJ](https://imagej.nih.gov/ij/)/[FIJI](http://fiji.sc/) plugin generating contour lines with equal spacing on top of an image (using overlay).  
Based/inspired by [streamlines](https://github.com/anvaka/streamlines) project by [anvaka](https://github.com/anvaka) and [this publication](http://web.cs.ucdavis.edu/~ma/SIGGRAPH02/course23/notes/papers/Jobard.pdf) (use it for citation).
   
![contourexample](http://katpyxa.info/software/ContourLines/CL2.gif ""logo"")

## How to install plugin

You need to download and install [ImageJ](https://imagej.nih.gov/ij/download.html) or [FIJI](http://fiji.sc/#download) on your computer first.

For FIJI
* add https://sites.imagej.net/Ekatrukha/ to the list of update sites, as follows
* go to Help -> Update and press ""Manage update sites"" button
* press ""Add update site"" button and put the following link there https://sites.imagej.net/Ekatrukha/

For ImageJ
* Download and copy [the latest version of plugin](https://github.com/ekatrukha/ContourLines/blob/master/target/ContourLines_-0.0.4.jar?raw=true) into the *plugins* folder of your ImageJ. (for example, in Windows look for *C:\Program Files\ImageJ\plugins*)

Restart ImageJ/Fiji and plugin will appear in *Plugins* menu

## Parameters

After plugin launch, rendering parameters window will appear  

![paramwindows](http://katpyxa.info/software/ContourLines/CL_parameters_dialog_v.0.0.4.png ""parameters window"")

### Smoothing radius
To find the ""vector field of an image"", plugin calculates convolution of image with [derivative of Gaussian](http://campar.in.tum.de/Chair/HaukeHeibelGaussianDerivatives) in *x* and *y* to estimate intensity gradient at each point. You can specify smoothing radius (SD of Gaussian). The larger the value, less small intensity details are available. For example, here are two images, the left one is build with smoothing radius of 2.0, while the right one with smoothing radius of 10.0 pixels.

![Rad2](http://katpyxa.info/software/ContourLines/smoothing_2_line_0.05_distance_3_x.png ""Rad2"")  ![Rad10](http://katpyxa.info/software/ContourLines/smoothing_10_line_0.05_distance_3_x.png ""Rad10""). 

Notice that the small spot in the right top corner ""disappeared"".

### Line integration step

This parameter defines, how precisely contour generation routine follows gradient vector field of the image. Smaller value result in more precise contour lines, but also take longer time to generate them. Example below illustrates integration step of 0.05 (left) versus 0.5 pixels (right).

![Step005](http://katpyxa.info/software/ContourLines/smoothing_2_line_0.05_distance_3_x.png ""Step005"")  ![Step05](http://katpyxa.info/software/ContourLines/smoothing_2_line_0.5_distance_3_x.png ""Step05""). 

### Distance between lines

Well, how densely you want contour lines to be plotted. Left image is 3 px, right image is 10 px.

![dist3](http://katpyxa.info/software/ContourLines/smoothing_2_line_0.05_distance_3_x.png ""Step005"")  ![dist10](http://katpyxa.info/software/ContourLines/smoothing_2_line_0.05_distance_10_x.png ""dist10""). 

### Stop line at fraction of distance

This parameter is a fraction of previous and it defines when the line integration will stop. I.e. end of each line cannot come closer than this distance to already existing lines. For example, on the left image parameter's value is 0.5, while on the right it is 0.1.  

![stop05](http://katpyxa.info/software/ContourLines/smoothing_2_line_0.05_distance_3_x.png ""Stop05"")  ![stop01](http://katpyxa.info/software/ContourLines/smoothing_2_line_0.05_distance_3_single_color_end_0.1.png ""stop01""). 

### Use single color

With this option checked, plugin will use current selected color (in toolbar or Color Picker) to build lines. Example:

![singlecolor](http://katpyxa.info/software/ContourLines/smoothing_2_line_0.05_distance_3_single_color_x.png ""single color"")

### Use color lookup tables (LUT)

Plugin will use one of the installed ImageJ/Fiji lookup tables (LUTs) to color code each contour with its average intensity. Pay attention, that current minimum and maximum of intensity would be taken from ""Brightness/Contrast"" settings of the image. In works better if your image uses ""dark-to-bright"" LUT and you check ""Invert LUT"" option for contours. In this case brighter lines would be on top of dark image areas and vice versa. In the example below the left image uses inverted ""Thermal"" LUT for contours, while contour LUT of the image on the right is not inverted.

![invertlut](http://katpyxa.info/software/ContourLines/smoothing_2_line_0.05_distance_3_x.png ""invertlut"")  ![notinvertlut](http://katpyxa.info/software/ContourLines/smoothing_2_line_0.05_distance_3_not_inverted_thermal_x.png ""notinvertlut""). 

### Remove open contours

Plugin will remove open contours (contours with free ends). Examples are below, left image includes open contours, while the right one not.

![withopen](http://katpyxa.info/software/ContourLines/smoothing_2_line_0.05_distance_3_x.png ""withopen"")  ![noopen](http://katpyxa.info/software/ContourLines/smoothing_2_line_0.05_distance_3_single_color_closed_only.png ""noopen""). 

## Updates history

* v.0.0.4 added criteria for line integration end. Added a possibility to remove open contours.

---
Developed in [Cell Biology group](http://cellbiology.science.uu.nl/) of Utrecht University.  
Email katpyxa @ gmail.com for any questions/comments/suggestions.
",2022-08-12
https://github.com/ekatrukha/Correlescence,"# Correlescence
[![DOI](https://zenodo.org/badge/78357244.svg)](https://zenodo.org/badge/latestdoi/78357244)

<img src=""http://katpyxa.info/software/Correlescence_logo.png"" align=""right"" style=""padding:100px""/> This is open-source [ImageJ](https://imagej.nih.gov/ij/index.html) plugin for different spatial/temporal correlation analysis of images (stacks and hyperstacks).
It includes 2D cross-correlation for stacks and 1D/2D STICS analysis. 
The plugin is in active development, some parts will appear and disappear.
<br />
<br />
For description and manual refer to <a href=""https://github.com/ekatrukha/Correlescence/wiki""><strong>Wiki page</strong></a>.  
Main topics:
* [How to install plugin](https://github.com/ekatrukha/Correlescence/wiki/How-to-install-plugin)
* [Main features/functions](https://github.com/ekatrukha/Correlescence/wiki)
* [How to cite plugin?](https://github.com/ekatrukha/Correlescence/wiki/How-to-cite-plugin%3F)
<br />
<br />
Developed in <a href='http://cellbiology.science.uu.nl/'>Cell Biology group</a> of Utrecht University.  
<a href=""mailto:katpyxa@gmail.com"">E-mail</a> for any questions.
",2022-08-12
https://github.com/ekatrukha/CurveTrace,"# CurveTrace
Curve tracing ImageJ plugin


[![DOI](https://zenodo.org/badge/165858856.svg)](https://zenodo.org/badge/latestdoi/165858856)

<img src=""http://katpyxa.info/software/CurveTrace/CurveTrace_logo.png"" align=""right"" style=""padding:100px""/> This is <a href=""http://www.gnu.org/licenses/gpl.html"">open-source</a> <a href='http://rsbweb.nih.gov/ij/'>ImageJ</a> plugin for extracting tracing curves on images. It includes Carsten Steger's <a href=""http://www.sciencedirect.com/science/article/pii/S107731421200118X"">algorithm</a> plus additional modules currently under development.
<br />
<br />
For description and manual refer to <a href=""https://github.com/ekatrukha/CurveTrace/wiki""><strong>Wiki page</strong></a>.
<br />
<br />
Main topics:
* [How to install plugin](https://github.com/ekatrukha/CurveTrace/wiki/How-to-install-plugin)
* [Main features/functions](https://github.com/ekatrukha/CurveTrace/wiki)
* [How to cite plugin?](https://github.com/ekatrukha/CurveTrace/wiki/How-to-cite-plugin%3F)
<br />
<br />
Developed in <a href='http://cellbiology.science.uu.nl/'>Cell Biology group</a> of Utrecht University.  
<a href=""mailto:katpyxa@gmail.com"">E-mail</a> for any questions.
",2022-08-12
https://github.com/ekatrukha/DoM_Utrecht,"DoM_Utrecht
===============

<strong>Detection of Molecules</strong> (DoM) plugin for ImageJ, Utrecht University

[![DOI](https://zenodo.org/badge/6157101.svg)](https://zenodo.org/badge/latestdoi/6157101)

<img src=""http://katpyxa.info/software/DoM_logox.png"" align=""right"" style=""padding:100px""/>This is <a href=""http://www.gnu.org/licenses/gpl.html"">open-source</a> <a href='http://rsbweb.nih.gov/ij/'>ImageJ</a> plugin for analysis of single molecule microscopy images (superrsolution PALM/STORM, etc).
<br />
<br />
For description and manual refer to <a href=""https://github.com/ekatrukha/DoM_Utrecht/wiki""><strong>Wiki page</strong></a>. 
Main topics:
* [How to install plugin](https://github.com/ekatrukha/DoM_Utrecht/wiki/How-to-install-plugin)
* [How to use plugin](https://github.com/ekatrukha/DoM_Utrecht/wiki/How-to-use-plugin-%28Main-menu%29)
* [How to cite plugin](https://github.com/ekatrukha/DoM_Utrecht/wiki/How-to-cite-plugin%3F)
<br />
<br />
<br />
We use free license of Java Profiler <a href=""http://www.ej-technologies.com/products/jprofiler/overview.html"">JProfiler</a> to estimate perfomance of plugin. 
<br />
<br />

Developed in <a href='http://cellbiology.science.uu.nl/'>Cell Biology group</a> of Utrecht University.  
<a href=""mailto:katpyxa@gmail.com"">E-mail</a> for any questions.
",2022-08-12
https://github.com/ekatrukha/KymoResliceWide,"KymoResliceWide
===============
[![DOI](https://zenodo.org/badge/19349865.svg)](https://zenodo.org/badge/latestdoi/19349865)
<br />
<img src=""http://katpyxa.info/software/KymoResliceWide/KymoResliceWide_logo.png"" align=""right"" style=""padding:100px""/> This is <a href=""http://www.gnu.org/licenses/gpl.html"">open-source</a> <a href='http://rsbweb.nih.gov/ij/'>ImageJ</a> plugin making kymographs of maximum or average intensity with wide lines, polylines or curves (freehand selection).
<br />
<br />
For description and manual refer to <a href=""https://github.com/ekatrukha/KymoResliceWide/wiki""><strong>Wiki page</strong></a>.
Main topics:
* [How to install plugin](https://github.com/ekatrukha/KymoResliceWide/wiki/How-to-install-plugin)
* [How to use plugin (Text Tutorial)](https://github.com/ekatrukha/KymoResliceWide/wiki/How-to-use-plugin-%28Text-Tutorial%29)
* [How to cite plugin?](https://github.com/ekatrukha/KymoResliceWide/wiki/How-to-cite-plugin%3F)
* [Why another kymograph plugin?](http://katpyxa.info/feedbacks/?p=26)
<br />
<br />
Developed in <a href='http://cellbiology.science.uu.nl/'>Cell Biology group</a> of Utrecht University.  
<a href=""mailto:katpyxa@gmail.com"">E-mail</a> for any questions.
",2022-08-12
https://github.com/ekatrukha/MTA,"Multiscale Trend Analysis
===============

<img src=""http://katpyxa.info/software/MTA_logo.png"" align=""right"" style=""padding:100px""/> 

Matlab and python code performing multiscale trend analysis based on piecewise linear approximations of time series according to the paper ""<a href=""http://arxiv.org/pdf/physics/0305013.pdf"">Multiscale Trend Analysis</a>"" by I.Zaliapin, A.Gabrielov, V.Keilis-Borok. 
<br />
<br />
Here is <a href=""http://katpyxa.info/feedbacks/?p=73"">my blog</a> post with some details.
<br />
<br />
To start, run and explore 'mta_example.m' (Matlab) or 'MTA-analysis.py'.
<br />
<br />
There are some distinction in this realization comparing to the paper, check the code! 
<br />
<br />
<hr />
Written in <a href='http://cellbiology.science.uu.nl/'>Cell Biology group</a> of Utrecht University.  
<a href=""mailto:katpyxa@gmail.com"">E-mail</a> for any questions.
",2022-08-12
https://github.com/ekatrukha/PTU_Reader,"# PTU_Reader
[ImageJ](https://imagej.nih.gov/ij/)/[FIJI](http://fiji.sc/) plugin reading PicoQuant ptu/pt3 FLIM TTTR image files.

It is based/upgraded from [Pt3Reader](http://imagejdocu.tudor.lu/doku.php?id=plugin:inputoutput:picoquant_.pt3_image_reader:start) plugin developed by François Waharte and [Matlab code](https://github.com/PicoQuant/PicoQuant-Time-Tagged-File-Format-Demos/blob/master/PTU/Matlab/Read_PTU.m) from PicoQuant.  
![PTU_Reader logo](http://katpyxa.info/software/PTU_Reader_logo.png ""logo"")

## How to install plugin

1. You need to download and install [ImageJ](https://imagej.nih.gov/ij/download.html) or [FIJI](http://fiji.sc/#download) on your computer first.
2. [Download *PTU_Reader_...jar*](https://github.com/ekatrukha/PTU_Reader/blob/master/PTU_Reader_0.0.9_.jar?raw=true) and place it in the ""plugins"" folder of ImageJ/FIJI.
3. Plugin will appear as *PTU_Reader* in ImageJ's *Plugins* menu.

## How to run plugin

1. Click *PTU_Reader* line in ImageJ's *Plugins* menu.
2. In ""Open File"" dialog choose file with (ptu/pt3) extension to load.
3. Plugin will read file's header and provide you with following options:  
![Menu](http://katpyxa.info/software/PTU_Reader/Menu2.png ""Menu"")
4. Choose what images/stacks you want to load (see detailed description below) and click *OK*.

## Output #1: lifetime ordered stack

This is 8-bit stack containing 4096 frames *(since v.0.0.7 it will be trimmed to max observed value)*. Each frame corresponds to a lifetime value. Why 4096? It is number of time registers in PicoQuant module. To get real time value in nanoseconds (ns), you need to know frequency of laser pulses during acquisition. Suppose laser frequency is 40MHz. That means distance between pulses is 1/40x10^6 = 25 ns. That means this period is splitted by 4096 intervals, i.e. time difference between frames = 25/4096 ~ 6.1 picoseconds (ps, 10^-12).

**NB:** Since v.0.0.7 stack will be trimmed to max observed value, i.e. 3210 or something else, to save a bit of memory.

The intensity of pixel at *x*,*y* position corresponds to the number of photons with this lifetime during the *whole acquisition*.  
Don't forget to do *Image->Adjust->Brightness/Contrast* to see the signal.

For example, to get a FLIM decay curve of some image area that would look like this:  
![Curve example](http://katpyxa.info/software/PTU_Reader/Curve_example.png ""curve"")  

select rectangular ROI and go to *Image->Stacks->Plot Z-axis profile*. You can make plot *y*-axis logarithmic by clicking *More>>Set Range..*

In addition, there are two loading options: ""*Load whole stack*"" and ""*Load binned*"". First option assembles all photons in one stack with 4096 frames (lifetimes). ""Load binned"" creates Hyperstack where z coordinate corresponds to 4096-format lifetime, while ""*time*"" corresponds to binned frame intervals specified by ""*Bin size in frames*"" parameter below. Last option can generate HUGE files (since imagewidth x imageheight x 4096 x binned frames), so be aware about it.

If you choose ""*Load only frame range*"" option at the bottom of the dialog, it will also restrict detected photons to the specified range.

## Output #2: Intensity and Lifetime Average stacks
Lifetime acquisition often happens over multiple frames. If this option is checked in the menu, plugin will provide intensity stack for each frame (if binning is 1) or summarized intensity of multiple frames for the bin size larger than 1.  

In addition, for the same number of binned frames, it will generate average lifetime value map.  
**Important**: the lifetime value is in the same 4096 register format! Use the same math/laser frequency recalculation to get value in nanoseconds. Whole image/stack math can be done using ImageJ *Process->Math* options.

So you can observe average lifetime change during acquisition. It is recommended to you different LUT to highlight its changes (*Image->Lookup Tables->Spectrum* or *Rainbow RGB*)

These two stacks are in 32-bit format.

You can restrict the interval of loaded data by selecting ""*Load only frame range*"" checkbox and providing the range of frames to load.

## It can not read my files! What about pt2? There is error!
Send me example of your file by email, describe the problem and I'll try to incorporate it to the plugin.

## Updates history
v.0.0.9 (2020.11) Thanks to Robert Hauschild feedback, corrected PT3 file format reading issues (actually it was diabled before). Added version number to the plugin menu, next to its name.

v.0.0.8 (2020.09) Thanks to Emma Wilson feedback, corrected some HydraHarp/TimeHarp260 T3 file format issues. Corrected channel names in the exported stack.

v.0.0.7 (2019.02) Thanks to Marco Dalla Vecchia feedback, now HydraHarp/TimeHarp260 T3 file format is supported. Plus plugin works correctly with multi-channel FLIM data. The error of dtime=0 is fixed. Added progress bar for initial data assessment.

v.0.0.6 (2018.03) Thanks to Tanja Kaufmann feedback, data reading is updated. Now there are two modes of reading, depending if the Frame marker is present. Plus LineStart and LineStop marker values are read from the header. + WRAPAROUND value is changed to 65536.

v.0.0.5 (2017.05) Thanks to Shunsuke Takeda feedback, the error of ""missing first frame"" is eliminated.  

v.0.0.4 (2017.04) Thanks to Bruno Scocozza feedback, frame marker bug during loading is fixed now. Plus, ""frame range"" and lifetime binning options are added.  

v.0.0.2 (2017.03) file dialog changed to system default (now works on Mac, no need in java library).

---
Developed in [Cell Biology group](http://cellbiology.science.uu.nl/) of Utrecht University.  
Email katpyxa @ gmail.com for any questions/comments/suggestions.
",2022-08-12
https://github.com/ekatrukha/radialitymap,"# Radiality map 


[![DOI](https://zenodo.org/badge/82709541.svg)](https://zenodo.org/badge/latestdoi/82709541) <br/>
[ImageJ](https://imagej.nih.gov/ij/)/[FIJI](http://fiji.sc/) macro that splits image to radial and non-radial components using output of [OrientationJ](http://bigwww.epfl.ch/demo/orientation/) plugin.
![radialitymap logo](http://katpyxa.info/software/radialitymap_logo.png ""logo"")

## How to install and run macro

1. You need to download and install [ImageJ](https://imagej.nih.gov/ij/download.html) or [FIJI](http://fiji.sc/#download) on your computer first.
  * you will need ImageJ version 1.48h or higher for macro to work.
2. [Download](http://bigwww.epfl.ch/demo/orientation/OrientationJ_.jar) OrientationJ_.jar and place it in the ""plugins"" folder of ImageJ/FIJI. 
3. [Download](https://raw.githubusercontent.com/ekatrukha/radialitymap/master/radiality_script_v2_0_20191017.ijm) macro file ""*radiality_script...ijm*"" and store it somewhere on your hard disk.
4. Load your image to ImageJ/FIJI. Currently macro works only with 8-/16-/32-bit images.
  * regular RGB images can be converted in ImageJ using *Image->Type->...bit* command  
![example image](http://katpyxa.info/software/radialitymap/image_example.png ""example image"")  
5. Choose the origin of coordinates (center with respect to what image considered radial) using ""Point"" selection tool:
![point tool](http://katpyxa.info/software/radialitymap/fiji_point_tool.png ""point tool"")
![example image with dot](http://katpyxa.info/software/radialitymap/image_with_point.png ""example image with dot"")
6. Go to *Plugins->Macros->Runs..* menu and choose previously downloaded macro file (""*radiality_script...ijm*"" from step 3).
  * If you try *Plugins->Macros->Install..* and choose macro file, it will appear as a separate row in *Plugins->Macros* menu  
![parameters window](http://katpyxa.info/software/radialitymap/parameters_window.png ""parameters window"") 
7. Choose values of parameters. For ""gaussian window"" a good initial guess is an approximate thickness of lines on your image. 
Try values 2 times higher or less and see result. 
The difference in ""Map calculation method"" is not so critical, pick the best one judging from final result.
8. Done!

## Output images

Macro/script generates five extra images. Here is description:  

1. Local orientation map (32-bit) generated by OrientationJ with local directionality angle in each pixel in radians:  
![orientation map](http://katpyxa.info/software/radialitymap/orientation_map.png ""orientation map"")  
2. Local radiality map (32-bit) ranging from 0 to 1. It is absolute value of cosine of difference between the local orientation angle and the angle of vector drawn from the new origin of coordinates (marked by Point tool) to the current pixel position  
![radiality map](http://katpyxa.info/software/radialitymap/radialitymap.png ""radiality map"")
3. Multiplication of original image and radiality map (32-bit), i.e. ""radial"" intensity component. By default it uses ""Fire"" color lookup table, but can be anything else.  
![radiality mult map](http://katpyxa.info/software/radialitymap/radialitymap_mult.png ""radiality mult map"")
4. Local non-radiality map (32-bit) ranging from 0 to 1. It is just one minus radiality map  
![nonradiality map](http://katpyxa.info/software/radialitymap/nonradialitymap.png ""nonradiality map"")
5. Multiplication of original image and non-radiality map (32-bit), i.e. ""non-radial"" intensity component. By default it uses ""Fire"" color lookup table, but can be anything else.  
![nonradiality mult map](http://katpyxa.info/software/radialitymap/nonradialitymap_mult.png ""nonradiality mult map"")

In general, you can combine radial/non-radial images (3 and 5) together with different LUTs using *Image->Color->Merge Channels..* command of ImageJ (cyan=radial, magenta=non-radial):  
![composite map](http://katpyxa.info/software/radialitymap/composite_maps.png ""composite map"")


## Example application to microtubules shapes

Here is image of a cytoplast at round micropattern kindly provided by [Manuel Thery](http://www.cytomorpholab.com/index.php?page=lab-members) (gray) and separation result (cyan=radial, magenta=non-radial):  
![cytoplast](http://katpyxa.info/software/radialitymap/cytoplasts_decomposed.png ""cytoplast"")  

It is easy to see some artefact at the center. The density of microtubules is high there, they overlap over each other and so it is impossible to distinguish individual filaments shapes. But line features can be enchanced using [Hessian FeatureJ](https://imagescience.org/meijering/software/featurej/hessian/) function (smallest eighen value, no absolute comparison), so shapes can be distinguished a bit better:  
![cytoplast hessian](http://katpyxa.info/software/radialitymap/cytoplasts_decomposed_hessian.png ""cytoplast after hessian"")

## How to cite this macro?

You can use Zenodo DOI for the link to the specific release (see DOI badge on the top of the page):

> _Katrukha E. 2019, RadialityMap macro for ImageJ, v0.2, Zenodo, doi:10.5281/zenodo.6817259_

## Updates history

2019.10.17 Thanks to Junnan Fang feedback macro is updated to the current version of OrientationJ

---
Developed in [Cell Biology group](http://cellbiology.science.uu.nl/) of Utrecht University.  
Email katpyxa @ gmail.com for any questions/comments/suggestions.

",2022-08-12
https://github.com/ekatrukha/RegisterNDFFT,"# RegisterNDFFT

Registration of ND images using FFT. 
Work in progress.
",2022-08-12
https://github.com/ekatrukha/rods-detection-in-noisy-images,"Rods detection in noisy images
===

<img src=""http://katpyxa.info/software/rods_detection_logo.png"" align=""right"" style=""padding:100px""/>This is a collection of [ImageJ](https://imagej.nih.gov/ij/)/[FIJI](http://fiji.sc/) macros for detection of rods (straight line segments of approximately same length) in noisy images and movies. Developed mostly for <i>in vitro</i> microtubule [gliding assays](https://www.youtube.com/watch?v=yRjU-bgfL0I), but can be applied to anything else (rod-shaped bacteria, etc). Uses [Template matching plugin]( https://sites.google.com/site/qingzongtseng/template-matching-ij-plugin) by Qingzong Tseng. 

In addition, contains Matlab scripts to link detections to tracks based on modified [SimpleTracker](https://nl.mathworks.com/matlabcentral/fileexchange/34040-simple-tracker) code by Jean-Yves Tinevez and tools to find tracks with directional runs (based on [this paper](https://www.nature.com/articles/ncomms14772)).  

For description and manual refer to <a href=""https://github.com/ekatrukha/rods-detection-in-noisy-images/wiki""><strong>Wiki page</strong></a>.   
Main topics:   
* **[How to install and run rods detection (ImageJ macro)](https://github.com/ekatrukha/rods-detection-in-noisy-images/wiki/How-to-install-and-run-rods-detection-%28ImageJ-macro%29)** 
* **[How to compose detection to tracks (Matlab)](https://github.com/ekatrukha/rods-detection-in-noisy-images/wiki/How-to-compose-detection-to-tracks-%28Matlab%29)**
* **[How does detection work?](https://github.com/ekatrukha/rods-detection-in-noisy-images/wiki/How-does-detection-work%3F)**
* **[Rods enchancement filter](https://github.com/ekatrukha/rods-detection-in-noisy-images/wiki/Rods-enhancement-filter-%28ImageJ-macro%29)**

Alternatively, to detect and track rods in movies with high signal-to-noise ratio, you can use [FIESTA](https://www.bcube-dresden.de/fiesta/wiki/Configuration) package from Stephan Diez lab, based on [this publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3117161/).


<br />
<br />
Developed in <a href='http://cellbiology.science.uu.nl/'>Cell Biology group</a> of Utrecht University.   
 
<a href=""mailto:katpyxa@gmail.com"">E-mail</a> for any questions.
",2022-08-12
https://github.com/ekatrukha/rods_bending,"rods bending
============

This package is able to calculate the shape of thin flexible elastic rod under load.

It can handle large loads and nonlinear deformations of rod.

It was developed to describe the shape of microtubules bending
under the force of kinesins in this paper:   
Doodhi H, Katrukha EA, Kapitein LC, Akhmanova A. [Mechanical and geometrical constraints control kinesin-based microtubule guidance](http://www.ncbi.nlm.nih.gov/pubmed/24462000) Curr Biol. 2014 Feb 3;24(3):322-8.  


<img src=""http://katpyxa.info/software/rods_bending/rods_bending.gif""/> 

In supplemental material you can find quite detailed description of theory and numerical realization behind it.  
But actually it is pretty general and can be applied to any other thin rod (macroscopic too).

Main parameters that you need to know are:   
- length of the rod  
- its flexural rigidity  
- magnitude of force applied to one end (the other one is clamped)  
- direction of the force  

Theory behind it is described in ""[Theory of Elasticity](https://archive.org/details/TheoryOfElasticity)"" by L.D. Landau & E.M. Lifshitz (see Chapter 2), but it is very compressed and hard to read explanation.

If you want to know more about banding rods (different clamping, following force, etc), I would recommend to read the book of E.V. Popov ""Theory and calculation of flexible elastic rods"", but it is available only [in Russian](http://katpyxa.info/software/rods_bending/Popov%20E.V.%20Teoriya%20i%20raschet%20gibkih%20uprugih%20sterzhnej%20(Nauka,%201986)(ru)(K)(600dpi)(T)(296s)_PCem_.djvu).

<img src=""http://katpyxa.info/software/rods_bending/popov.png"" align=""right"" />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
Developed in <a href='http://cellbiology.science.uu.nl/'>Cell Biology group</a> of Utrecht University.  
E-mail <a href=""mailto:katpyxa@gmail.com"">for any questions</a>.
",2022-08-12
https://github.com/ekatrukha/TrackMate2MTrackJ,"TrackMate2MTrackJ
===============

<img src=""http://katpyxa.info/software/TrackMate2MTrackJ_logo.png"" align=""right"" style=""padding:100px""/> 

This is <a href=""http://www.gnu.org/licenses/gpl.html"">open-source</a> Java software to convert <a href=""http://fiji.sc/TrackMate"">TrackMate</a> XML Export trajectories to <a href=""http://fiji.sc/MTrackJ"">MTrackJ</a> format.   
Also contains Matlab function for the <a href=""https://github.com/ekatrukha/TrackMate2MTrackJ/raw/master/importMTrackJTracks.m"">import of MTrackJ tracks</a>.
<br />
<br />
<a href=""https://github.com/ekatrukha/TrackMate2MTrackJ/raw/master/bin/TrackMate2MTrackJ_0.0.4.jar""><strong>Download jar file</strong></a> (please, note that you need <a href=""http://www.oracle.com/technetwork/java/javase/downloads/index.html"">Java</a> installed to run it).  
For Windows systems you need to additionally download <a href=""https://raw.githubusercontent.com/ekatrukha/TrackMate2MTrackJ/master/TrackMate2MTrackJ_start.bat"">this bat file</a> to the same folder and use it to run the converter.

For the description refer to the <a href=""https://github.com/ekatrukha/TrackMate2MTrackJ/wiki""><strong>Wiki page</strong></a>.
<br />
<hr />
Developed in <a href='http://cellbiology.science.uu.nl/'>Cell Biology group</a> of Utrecht University.  
<a href=""mailto:katpyxa@gmail.com"">E-mail</a> for any questions.
",2022-08-12
https://github.com/ekatrukha/ZstackDepthColorCode,"# Z-stack Depth Color Code

[ImageJ](https://imagej.nih.gov/ij/)/[FIJI](http://fiji.sc/) plugin to colorcode Z-stacks/hyperstacks (8-,16-,32 bit). Allows to uses available LUTs + invert them. 

The plugin creates an output as Composite or RGB stack. So there is conversion to 8-bit, depending on the current Brightness/Contrast settings.

**NB:** After installation plugin appears in *Plugins->Stacks->Z-stack Depth Colorcode* menu.

It is loosely based on [Temporal-Color Code](https://imagej.net/Temporal-Color_Code) macro (but does not create Z-projection) and it is rewritten version of Z_Code_Stack function from [FIJI Cookbook](https://github.com/fiji/cookbook).

Generated colorcoded stacks can be visualized by different 3D renders ([3D Viewer](https://imagej.nih.gov/ij/plugins/3d-viewer/) or [3Dscript](https://bene51.github.io/3Dscript/) plugin).

Example: EB comets (by Boris Shneyer), grayscale before: 


![EB BW](http://katpyxa.info/software/ZstackDepthColorCode/EB_colored_BW.gif ""EB stack BW"")


and coded with Thermal LUT, after:


![EB color](http://katpyxa.info/software/ZstackDepthColorCode/EB_colored_thermal.gif ""EB stack color"")



## How to install plugin

To install plugin in FIJI:

* add https://sites.imagej.net/Ekatrukha/ to the list of update sites, as follows
* go to Help -> Update and press ""Manage update sites"" button
* press ""Add update site"" button and put the following link there https://sites.imagej.net/Ekatrukha/

To install plugin manually in ImageJ:

* download and copy the [latest version of plugin](https://github.com/ekatrukha/ZstackDepthColorCode/raw/main/target/ZstackDepthColorCode_-0.0.2.jar) 
* plugin will appear in _Plugins->Stacks->Z-stack Depth Colorcode_ menu

## Updates history
2021.02.17 (v.0.0.2) Added LUT image generation option. Fixed colors for inverted LUT and some window appearance. 
 
2021.01.11 (v.0.0.1) First version. 

---
Developed in [Cell Biology group](http://cellbiology.science.uu.nl/) of Utrecht University.  
Email katpyxa @ gmail.com for any questions/comments/suggestions.
",2022-08-12
https://github.com/ellenhamaker/DSEM-book-chapter,# DSEM-book-chapter,2022-08-12
https://github.com/ellenhamaker/RI-CLPM,"# RI-CLPM & Extensions

This repository and its Github Pages have been moved to [jeroendmulder.github.io/RI-CLPM](jeroendmulder.github.io/RI-CLPM). This repository's Github Page now serves as an automatic redirect. 


",2022-08-12
https://github.com/emmekeaarts/mHMMbayes,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# mHMMbayes

With the  package mHMMbayes you can fit multilevel hidden Markov models.
The multilevel hidden Markov model (HMM) is a generalization of the
well-known hidden Markov model, tailored to accommodate (intense)
longitudinal data of multiple individuals simultaneously. Using a
multilevel framework, we allow for heterogeneity in the model parameters
(transition probability matrix and conditional distribution), while
estimating one overall HMM. The model has a great potential of
application in many fields, such as the social sciences and medicine.
The model can be fitted on multivariate data with a categorical
distribution, and include individual level covariates (allowing for
e.g., group comparisons on model parameters). Parameters are estimated
using Bayesian estimation utilizing the forward-backward recursion
within a hybrid Metropolis within Gibbs sampler. The package also
includes various options for model visualization, a function to simulate
data and a function to obtain the most likely hidden state sequence for
each individual using the Viterbi algorithm.

Please do not hesitate to contact me if you have any questions regarding
the package.

## Installation

You can install mHMMbayes from github with:

``` r
# install.packages(""devtools"")
devtools::install_github(""emmekeaarts/mHMMbayes"")
```

## Usage

This is a basic example which shows you how to run the model using
example data included with the package, and how to simulate data. For a
more elaborate introduction, see the vignette “tutorial-mhmm”
accompanying the package.

``` r
library(mHMMbayes)

##### Simple 2 state model
# specifying general model properties
m <- 2
n_dep <- 4
q_emiss <- c(3, 2, 3, 2)

# specifying starting values
start_TM <- diag(.8, m)
start_TM[lower.tri(start_TM) | upper.tri(start_TM)] <- .2
start_EM <- list(matrix(c(0.05, 0.90, 0.05,
                          0.90, 0.05, 0.05), byrow = TRUE,
                         nrow = m, ncol = q_emiss[1]), # vocalizing patient
                  matrix(c(0.1, 0.9,
                           0.1, 0.9), byrow = TRUE, nrow = m,
                         ncol = q_emiss[2]), # looking patient
                  matrix(c(0.90, 0.05, 0.05,
                           0.05, 0.90, 0.05), byrow = TRUE,
                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist
                  matrix(c(0.1, 0.9,
                           0.1, 0.9), byrow = TRUE, nrow = m,
                         ncol = q_emiss[4])) # looking therapist

 # Run a model without covariates. 
 # Note that normally, a much higher number of iterations J would be used
 set.seed(23245)
 out_2st <- mHMM(s_data = nonverbal, 
              gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss), 
              start_val = c(list(start_TM), start_EM),
              mcmc = list(J = 11, burn_in = 5))
#> [1] 10
#> [1] ""total time elapsed in minutes 0.42""
 
out_2st
#> Number of subjects: 10 
#> 
#> 11 iterations used in the MCMC algorithm with a burn in of 5 
#> Average Log likelihood over all subjects: -1639.443 
#> Average AIC over all subjects: 3306.885 
#> 
#> Number of states used: 2 
#> 
#> Number of dependent variables used: 4
summary(out_2st)
#> State transition probability matrix 
#>  (at the group level): 
#>  
#>              To state 1 To state 2
#> From state 1      0.934      0.066
#> From state 2      0.058      0.942
#> 
#>  
#> Emission distribution for each of the dependent variables 
#>  (at the group level): 
#>  
#> $p_vocalizing
#>         Category 1 Category 2 Category 3
#> State 1      0.031      0.943      0.025
#> State 2      0.766      0.110      0.134
#> 
#> $p_looking
#>         Category 1 Category 2
#> State 1      0.221      0.779
#> State 2      0.100      0.900
#> 
#> $t_vocalizing
#>         Category 1 Category 2 Category 3
#> State 1      0.802      0.084      0.106
#> State 2      0.049      0.924      0.031
#> 
#> $t_looking
#>         Category 1 Category 2
#> State 1      0.041      0.959
#> State 2      0.292      0.708

# Run a model including a covariate 
# Here, the covariate (standardized CDI change) predicts the emission 
# distribution for each of the 4 dependent variables:
n_subj <- 10
xx <- rep(list(matrix(1, ncol = 1, nrow = n_subj)), (n_dep + 1))
for(i in 2:(n_dep + 1)){
 xx[[i]] <- cbind(xx[[i]], nonverbal_cov$std_CDI_change)
}
out_2st_c <- mHMM(s_data = nonverbal, xx = xx, 
                 gen = list(m = m, n_dep = n_dep, q_emiss = q_emiss), 
                 start_val = c(list(start_TM), start_EM),
                 mcmc = list(J = 11, burn_in = 5))
#> [1] 10
#> [1] ""total time elapsed in minutes 0.42""

 
 ### Simulating data
 # simulating data for 10 subjects with each 100 observations
 n_t <- 100
 n <- 10
 m <- 3
 q_emiss <- 4
 gamma <- matrix(c(0.8, 0.1, 0.1,
                   0.2, 0.7, 0.1,
                   0.2, 0.2, 0.6), ncol = m, byrow = TRUE)
 emiss_distr <- matrix(c(0.5, 0.5, 0.0, 0.0,
                         0.1, 0.1, 0.8, 0.0,
                         0.0, 0.0, 0.1, 0.9), nrow = m, ncol = q_emiss, byrow = TRUE)
 set.seed(1253)
 data1 <- sim_mHMM(n_t = n_t, n = n, m = m, q_emiss = q_emiss, gamma = gamma, 
                   emiss_distr = emiss_distr, var_gamma = 1, var_emiss = 1)
 head(data1$states)
#>      subj state
#> [1,]    1     2
#> [2,]    1     2
#> [3,]    1     2
#> [4,]    1     2
#> [5,]    1     2
#> [6,]    1     2
 head(data1$obs)
#>      subj observation
#> [1,]    1           2
#> [2,]    1           3
#> [3,]    1           1
#> [4,]    1           2
#> [5,]    1           2
#> [6,]    1           2


 # simulating subject specific transition probability matrices and emission distributions only
 n_t <- 0
 n <- 5
 m <- 3
 q_emiss <- 4
 gamma <- matrix(c(0.8, 0.1, 0.1,
                   0.2, 0.7, 0.1,
                   0.2, 0.2, 0.6), ncol = m, byrow = TRUE)
 emiss_distr <- matrix(c(0.5, 0.5, 0.0, 0.0,
                         0.1, 0.1, 0.8, 0.0,
                         0.0, 0.0, 0.1, 0.9), nrow = m, ncol = q_emiss, byrow = TRUE)
 set.seed(549801)
 data2 <- sim_mHMM(n_t = n_t, n = n, m = m, q_emiss = q_emiss, gamma = gamma, 
                   emiss_distr = emiss_distr, var_gamma = 1, var_emiss = 1)
 data2
#> $subject_gamma
#> $subject_gamma[[1]]
#>        [,1]   [,2]   [,3]
#> [1,] 0.6302 0.2849 0.0849
#> [2,] 0.1817 0.7714 0.0469
#> [3,] 0.2164 0.1738 0.6098
#> 
#> $subject_gamma[[2]]
#>        [,1]   [,2]   [,3]
#> [1,] 0.7819 0.1235 0.0945
#> [2,] 0.0747 0.9015 0.0238
#> [3,] 0.3285 0.4705 0.2011
#> 
#> $subject_gamma[[3]]
#>        [,1]   [,2]   [,3]
#> [1,] 0.6228 0.1443 0.2329
#> [2,] 0.5242 0.3106 0.1652
#> [3,] 0.5215 0.1167 0.3618
#> 
#> $subject_gamma[[4]]
#>        [,1]   [,2]   [,3]
#> [1,] 0.5726 0.1054 0.3220
#> [2,] 0.1751 0.5438 0.2811
#> [3,] 0.2109 0.1686 0.6204
#> 
#> $subject_gamma[[5]]
#>        [,1]   [,2]   [,3]
#> [1,] 0.8227 0.1212 0.0561
#> [2,] 0.2029 0.5990 0.1982
#> [3,] 0.0902 0.3200 0.5898
#> 
#> 
#> $subject_emmis
#> $subject_emmis[[1]]
#>        [,1]   [,2]   [,3]   [,4]
#> [1,] 0.4916 0.5083 0.0001 0.0000
#> [2,] 0.0572 0.1810 0.7618 0.0000
#> [3,] 0.0000 0.0000 0.0629 0.9371
#> 
#> $subject_emmis[[2]]
#>        [,1]   [,2]   [,3]   [,4]
#> [1,] 0.1451 0.8549 0.0000 0.0000
#> [2,] 0.0510 0.1518 0.7972 0.0000
#> [3,] 0.0000 0.0000 0.0514 0.9486
#> 
#> $subject_emmis[[3]]
#>        [,1]   [,2]   [,3]   [,4]
#> [1,] 0.2158 0.7842 0.0000 0.0000
#> [2,] 0.1865 0.1831 0.6304 0.0000
#> [3,] 0.0001 0.0002 0.5793 0.4204
#> 
#> $subject_emmis[[4]]
#>        [,1]   [,2]   [,3]   [,4]
#> [1,] 0.3268 0.6732 0.0000 0.0000
#> [2,] 0.0501 0.0867 0.8631 0.0000
#> [3,] 0.0000 0.0000 0.1194 0.8806
#> 
#> $subject_emmis[[5]]
#>        [,1]   [,2]   [,3]   [,4]
#> [1,] 0.7268 0.2731 0.0000 0.0001
#> [2,] 0.1303 0.2549 0.6148 0.0000
#> [3,] 0.0000 0.0000 0.1085 0.8915

 set.seed(10893)
 data3 <- sim_mHMM(n_t = n_t, n = n, m = m, q_emiss = q_emiss, gamma = gamma, 
                   emiss_distr = emiss_distr, var_gamma = .5, var_emiss = .5)
 data3
#> $subject_gamma
#> $subject_gamma[[1]]
#>        [,1]   [,2]   [,3]
#> [1,] 0.7958 0.0461 0.1581
#> [2,] 0.3042 0.4663 0.2295
#> [3,] 0.1396 0.6520 0.2084
#> 
#> $subject_gamma[[2]]
#>        [,1]   [,2]   [,3]
#> [1,] 0.6221 0.0834 0.2946
#> [2,] 0.1143 0.8430 0.0427
#> [3,] 0.1414 0.3805 0.4780
#> 
#> $subject_gamma[[3]]
#>        [,1]   [,2]  [,3]
#> [1,] 0.7416 0.0554 0.203
#> [2,] 0.1403 0.7937 0.066
#> [3,] 0.1915 0.0975 0.711
#> 
#> $subject_gamma[[4]]
#>        [,1]   [,2]   [,3]
#> [1,] 0.6333 0.0932 0.2736
#> [2,] 0.1127 0.6909 0.1964
#> [3,] 0.1058 0.3872 0.5070
#> 
#> $subject_gamma[[5]]
#>        [,1]   [,2]   [,3]
#> [1,] 0.7610 0.1833 0.0557
#> [2,] 0.0781 0.8920 0.0300
#> [3,] 0.2269 0.1116 0.6615
#> 
#> 
#> $subject_emmis
#> $subject_emmis[[1]]
#>        [,1]   [,2]   [,3]   [,4]
#> [1,] 0.6359 0.3641 0.0000 0.0000
#> [2,] 0.2302 0.2625 0.5073 0.0000
#> [3,] 0.0000 0.0000 0.0326 0.9674
#> 
#> $subject_emmis[[2]]
#>        [,1]   [,2]   [,3]   [,4]
#> [1,] 0.6471 0.3528 0.0000 0.0000
#> [2,] 0.1977 0.2782 0.5240 0.0001
#> [3,] 0.0000 0.0000 0.2446 0.7554
#> 
#> $subject_emmis[[3]]
#>        [,1]   [,2]   [,3]   [,4]
#> [1,] 0.7053 0.2946 0.0000 0.0000
#> [2,] 0.1433 0.0626 0.7940 0.0001
#> [3,] 0.0000 0.0000 0.0274 0.9726
#> 
#> $subject_emmis[[4]]
#>        [,1]   [,2]   [,3]   [,4]
#> [1,] 0.8119 0.1880 0.0000 0.0000
#> [2,] 0.0704 0.0669 0.8627 0.0000
#> [3,] 0.0000 0.0000 0.1557 0.8443
#> 
#> $subject_emmis[[5]]
#>        [,1]   [,2]   [,3]   [,4]
#> [1,] 0.5968 0.4032 0.0000 0.0000
#> [2,] 0.1023 0.1158 0.7819 0.0000
#> [3,] 0.0000 0.0000 0.1358 0.8642
```
",2022-08-12
https://github.com/franciscapessanha/Asteroids-clone,"
# Asteroids clone

[![Watch the video](https://img.youtube.com/vi/JiAo3qK28fA/maxresdefault.jpg)](https://youtu.be/JiAo3qK28fA)

## How to play
The objective of the game is to destroy the asteroids and enemy spaceships. 
The player can perform propulsion and rotation movements as well as firing. 
### Commands
* Movement of the ship: arrow keys and `WASD`
* Shoot: `SPACE` key
* Pause: `ALT` key
* Return to the home menu: `ESC` key

## Scoring system
* Large asteroids: 20 points
* Medium asteroids: 50 points
* Small asteroids: 100 points
* Enemy spaceships: 200 points

Note: Points will only be attributed when damage is caused by the player, i.e. any asteroid destroyed by the enemy ships will not 
be taken into account for calculating the final score.

*Assets designed by freepik - www.freepik.com*
",2022-08-12
https://github.com/franciscapessanha/Hand-pose-recognition,"
# Hand Pose Recognition

In the present work, we applied a computer vision approach,  based on the hand position and outline, to recognition of simple hand gestures: digits from zero to five and four gestures, the OK, not OK, pointer and all right. 

## Demo
![demo image](https://github.com/franciscapessanha/HandPoseRecognition/blob/master/hand.gif)


## Installation
```bash
pip install -r requirements.txt
```
## Running

#### Video Capture Device (WebCam):

```bash
python3 main.py <video-capture-device-id>
```
if run without parameters it will default to video capture device with id 0

#### Specific File

```bash
python3 main.py <file-path>
```
only `.mp4`, `.jpg` and `.png` files are supported

## Usage
When first opened press `ENTER` to sample hand skin color using the mouse, confirm the region by pressing `ENTER` again.

The threshold values calculated from this sample are saved locally on a 'thresholds' file, and are used by default the next time the program is opened. To change this values, the user can calibrate or re-sample the skin color.

To calibrate the values, press `C` to open the calibration window, confirm calibration by pressing `ENTER`.

To re-sample skin color and calculate new values, press `S` to open the sampling window, confirm the region by pressing `ENTER`.

Press `Space` to pause/resume if it's a video input.

Press `Esc` to quit the program.

## Team
 - [Margarida Abranches](https://github.com/margaridaabranches)
- [Maria Francisca Pessanha](https://github.com/franciscapessanha)
- [Paulo Correia](https://github.com/pipas)
",2022-08-12
https://github.com/franciscapessanha/Lusitannia,"# In search of Lusitannia

Joining the history of Portugal to legends and myths of medieval fantasy, ""In search of
Lusitannia ""proposes a fictional universe of adventure and magic, emphasizing the cultural importance
of music.

[![Watch the video](https://img.youtube.com/vi/qfQAher7kiI/maxresdefault.jpg)](https://youtu.be/qfQAher7kiI)

## Team
- **Implementation**: [Francisca Pessanha](https://github.com/franciscapessanha)
- **Design and Story writing**: [Dan Martini](https://github.com/martinidan)
- **Sound Design and Soundtrack**: [Paulo Teixeira](https://github.com/PauloTeixeira94) and [Marcelo Sousa](https://github.com/marcelodesousa)
",2022-08-12
https://github.com/franciscapessanha/Porto-landmarks-recognition,"
# Porto's Landmarks Recognition
The present work proposes an approach for classification and location detection of five Porto landmarks: *Torre dos Clérigos*,
*Casa de Serralves*, *Casa da Música*, *Câmara do Porto*, *Ponte da Arrábida* based on Deep Convolutional Neural Networks Algorithms. 

Additionally, *Casa da Música* images were classified according to the viewpoint from each the photography was taken. 

## Team
- [Francisca Pessanha](https://github.com/franciscapessanha)
- [Margarida Abranches](https://github.com/margaridaabranches)
- [Paulo Correia](https://github.com/pipas)
",2022-08-12
https://github.com/franciscapessanha/Pulmonary-nodules-analysis,"# Segmentation and Texture Analysis of Pulmonary Nodules
Development of an algorithm that allows the segmentation of nodules and the characterization of their texture in non-solid, sub-solid and solid. 

A pipeline for two-dimensional and three-dimensional computed tomography images was design and both tradicional approachs (feature extraction and SVM/kNN training) and Convolutional Neural Networks were implemented.

## Team
- [Francisca Pessanha](https://github.com/franciscapessanha)
- [Hugo Barros](https://github.com/hugobarros96)
- [Margarida Borges](https://github.com/margaridaborgespereira)
- [Rita Moura](https://github.com/ritamoura96)

",2022-08-12
https://github.com/franciscapessanha/Sheep-game,"# Animals Affect: Can you read sheep facial expressions? We build algorithms that can!

""Animals in pain do show reliable, if at times, very subtle signs of distress. A recently developed Sheep Pain Facial Expression Scale (SPFES) can help handlers accurately and objectively determine the degree of pain an animal is experiencing."" Can you identify the signs?

## How to play
The objective of the game is to classify sheep facial expressions as happy (""no pain"") or sad (""pain"").

## Data
The data will be saved in the Data folder in the format `<day>_<month>_<year>_<hour>_<minutes>_<seconds>.csv`


*Assets designed by freepik - www.freepik.com*
",2022-08-12
https://github.com/FridoF/PyTom,"# PytomGUI

PyTom is a toolbox developed for interpreting cryo electron tomography data. All steps from reconstruction, localization, alignment and classification are covered with standard and improved methods.

## Getting Started



### Prerequisites

PyTomGUI is designed to run on linux systems, but can also be installed on MacOSX.It requires the following software package to be installed:

```
- python (>= 3.7 )
- openmpi 
- fftw3
- gcc (version 5-7) 
- numpy
- boost
- lxml, libxml2, libxstl. 
- swig (>= 3.0.12)
- PyQt5
- pyqtgraph
- motioncor2 ( >=1.2.1)
- imod (>=4.10.25)
- mrcfile
```

### Installing

To install PyTomGUI please clone the most recent version by executing the following command 

```
git clone --recursive https://github.com/FridoF/PyTomPrivate.git pytom
```

After a succesful clone enter the new directory and go to pytomc
```
cd PyTomPrivate/pytomc
```

Here you will find an installation script name compile.py. Please run this script with python3. 
```
python3.7 compile.py --pythonVersion 3.7 --target all 
```

If certain dependencies are not found, chances are there you either have not installed them, or that the installation paths to the dependencies are not found automatically. 

It is possible to add installation paths manually by adding them after a respective flag (include dirs after --includeDir, lib dirs after --libDir and exe dirs after --exeDir). More than one path can be added per flag, where each path should be separated by a space. Note that the order of the paths can make a difference.

```
python3.7 compile.py --pythonVersion 3.7 --target all --exeDir [path_to_your_exe_dir1] [path_to_your_exe_dir2] --libDir [path_to_your_lib_dir1] [path_to_your_lib_dir2] --includeDir [path_to_your_exclude_dir1] [path_to_your_exclude_dir2] 
```

To locate missing dependencies try:

```
locate Python.h
```

This results in something similar to:
```
/usr/local/Cellar/python/3.7.2_1/Frameworks/Python.framework/Versions/3.7/include/python3.7m/Python.h
/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/include/python2.7/Python.h
```

Now update the includeDir flag to:

```
python3.7 compile.py --pythonVersion 3.7 --target all --includeDir /usr/local/Cellar/python/3.7.2_1/Frameworks/Python.framework/Versions/3.7/include/python3.7m/
```

## Versioning

For the versions available, see the [tags on this repository](https://gschot@bitbucket.org/gschot/pytom/tags). 

## Authors

* **Gijs van der Schot** - *PyTomGUI* 
* **Thomas Hrabe**       - *PyTom* 
* **Yuxiang Chen**       - *PyTom*
* **Friedrich Forster**  - *PyTom* 

See also the list of [contributors](https://gschot@bitbucket.org/gschot/pytom/contributors) who participated in this project.

## License

Copyright (c) 2021

Utrecht University

http://www.pytom.org

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

The complete license can be obtained from 
http://www.gnu.org/licenses/gpl-2.0.html.
",2022-08-12
https://github.com/ganropa/drought,"# [Droughts CAMELS]

## Project organization
- PG = project-generated
- HW = human-writable
- RO = read only
```
.
├── .gitignore
├── CITATION.md
├── LICENSE.md
├── README.md
├── requirements.txt
├── bin                <- Compiled and external code, ignored by git (PG)
│   └── external       <- Any external source code, ignored by git (RO)
├── config             <- Configuration files (HW)
├── data               <- All project data, ignored by git
│   ├── processed      <- The final, canonical data sets for modeling. (PG)
│   ├── raw            <- The original, immutable data dump. (RO)
│   └── temp           <- Intermediate data that has been transformed. (PG)
├── docs               <- Documentation notebook for users (HW)
│   ├── manuscript     <- Manuscript source, e.g., LaTeX, Markdown, etc. (HW)
│   └── reports        <- Other project reports and notebooks (e.g. Jupyter, .Rmd) (HW)
├── results
│   ├── figures        <- Figures for the manuscript or reports (PG)
│   └── output         <- Other output for the manuscript or reports (PG)
└── src                <- Source code for this project (HW)

```


## License

This project is licensed under the terms of the [MIT License](/LICENSE.md)
",2022-08-12
https://github.com/ganropa/Examples,"# Examples
My First codes
",2022-08-12
https://github.com/gbeckers/agldata,"=======
agldata
=======

|Docs Status|

.. contents::

What is agldata?
----------------
A python library for published data from artificial grammar learning or
statistical learning studies. Data can be efficiently accessed in a
structured way, for example to use them for (meta-) analyses.

You can easily add data sets by entering them in a file in a user-friendly
format (`YAML <https://yaml.org/>`__). See `example here
<https://github.com/gbeckers/agldata/tree/master/agldata/datafiles
/wilsonetal_2013_jneurosci.yaml>`__.

Current version is: 0.1.0

**This library is in aplha stage, is not ready and not fully tested. It should
not be used for research yet.** If you are nevertheless interested in using if
for your research, consider collaborating with the main author of the library
(see contact).

Documentation
-------------
A first attempt to documentation can be found here:
https://agldata.readthedocs.io/

Install
-------
agldata depends on Python 3.6 or higher, and one external library (pyaml).

To install the latest release::

    $ pip install agldata

To use the latest version from GitHub, I recommend working in
Anaconda, in which case you could first create a separate
conda environment, called, e.g. ""agl""::

    $ conda create -n agl python=3.8 jupyterlab git pyyaml=5.1

Then install agldata from GitHub::

    $ pip install git+https://github.com/gbeckers//agldata@master


To switch to this new environment in a terminal:

Linux and MacOS::

    $ source activate agltest

Windows::

    $ conda activate agltest


If you want to remove the conda environment later::

    $ conda env remove -n agl


Contributing
------------
To add data from studies, see see https://agldata.readthedocs.io/en/latest/contributing.html
for instructions on how to prepare data files.
To get them merged into the library or to fix bugs/add functionality, it is best to use a the
Github pull request workflow. It is described here:
https://github.com/processing/processing/wiki/Contributing-to-Processing-with-Pull-Requests

Included studies
----------------

See: https://agldata.readthedocs.io/en/latest/studies.html

Copyright and License
---------------------
:copyright: Copyright 2019-2021, Gabriel Beckers, Utrecht University.
:license: 3-Clause Revised BSD License, see LICENSE.txt for details.

Data from original studies has been entered by: Gabriel Beckers, Simon Buil.

Contact
-------
Gabriel Beckers, Utrecht University, https://www.gbeckers.nl

.. |Docs Status| image:: https://readthedocs.org/projects/agldata/badge/?version=latest
   :target: https://agldata.readthedocs.io/en/latest/?badge=latest
   :alt: Documentation Status",2022-08-12
https://github.com/gbeckers/Birdwatcher,"Birdwatcher
===========

|Github CI Status| |Appveyor Status| |PyPi version| |Docs Status| |Repo Status|
|Codecov status|

.. image:: docs/images/banner.gif
  :align: center
  :width: 720

Birdwatcher is a Python computer vision library for analyzing animal behavior
in a Python scientific computing environment.

Birdwatcher should help you getting up and running quickly when building
analysis code or tools for specific measurements. It provides high-level
functionality that is common in video analysis, such as reading and writing
videos into and from numpy arrays, applying processing algorithms such as
background subtraction, morphological transformation, resizing, drawing on
frames etc. Much of the underlying video and image processing is based on
`FFmpeg <https://www.ffmpeg.org/>`__ and `OpenCV <https://opencv.org/>`__,
but Birdwatcher is a lot easier to use for many tasks because its
higher-level implementation of functionality as compared to these tools.

Despite its name, Birdwatcher is not only for birds. We also successfully
analyzed dog behavior, and it could be used on anything that moves. It is
being used in our lab but still under heavy development, and should be
considered alpha software.

Code can be found on GitHub: https://github.com/gbeckers/Birdwatcher .

Documentation can be found at https://birdwatcher.readthedocs.io .

It is developed by Gabriel Beckers and Carien Mol, at Experimental Psychology,
Utrecht University. It is open source, freely available under the `New BSD License
<https://opensource.org/licenses/BSD-3-Clause>`__ terms.


Installation Birdwatcher package
--------------------------------

Birdwatcher depends on Python 3.6 or higher, and a number of libraries. As
long as there is no official release. It is best to use the github master
branch. The older (alpha) versions on PyPi are outdated.

**User installation**

1) We recommend using Anaconda for installation. Install Anaconda from https://www.anaconda.com/ .

2) Open Anaconda prompt in terminal.

3) Create new environment for Birdwatcher (name is up to you, in example
   here 'mybirdwatcher'). We install Jupter lab and ffmpeg at the same time::

    $ conda create -n mybirdwatcher python=3.8 jupyterlab ffmpeg

4) Switch to this new environment:

   Linux and MacOS::

    $ source activate mybirdwatcher

   Windows::

    $ conda activate mybirdwatcher

5) Install Birdwatcher master branch from git repo::

    $ pip install git+https://github.com/gbeckers/birdwatcher@master


**Dependencies**

The following dependencies are automatically taken care of when you
install Birdwatcher from GitHub using the pip method above:

- numpy
- matplotlib
- darr
- opencv-python
- opencv-contrib-python

It further depends on:

- ffmpeg (including ffprobe)

If you do not use the conda way above to install ffmpeg, you need to
install it yourself (https://www.ffmpeg.org/).


Test
----

To run the test suite:

.. code:: python

    >>>import birdwatcher as bw
    >>>bw.test()
    ..................................
    ----------------------------------------------------------------------
    Ran 33 tests in 12.788s

    OK

    <unittest.runner.TextTestResult run=33 errors=0 failures=0>


Documentation
-------------

https://birdwatcher.readthedocs.io

Examples
--------

See `jupyter notebook directory
<https://github .com/gbeckers/Birdwatcher/tree/master/notebooks>`__.

Contributions
-------------
Sita ter Haar and Dylan Minekus helped exploring the application of movement
detection algorithms.

.. |Repo Status| image:: https://www.repostatus.org/badges/latest/active.svg
   :alt: Project Status: Active – The project has reached a stable, usable state and is being actively developed.
   :target: https://www.repostatus.org/#active
.. |Github CI Status| image:: https://github.com/gbeckers/Birdwatcher/actions/workflows/python_package.yml/badge.svg
   :target: https://github.com/gbeckers/Birdwatcher/actions/workflows/python_package.yml
.. |Appveyor Status| image:: https://ci.appveyor.com/api/projects/status/github/gbeckers/darr?svg=true
   :target: https://ci.appveyor.com/project/gbeckers/birdwatcher
.. |PyPi version| image:: https://img.shields.io/badge/pypi-0.2.0-orange.svg
   :target: https://pypi.org/project/birdwatcher/
.. |Docs Status| image:: https://readthedocs.org/projects/birdwatcher/badge/?version=latest
   :target: https://birdwatcher.readthedocs.io/en/latest/
.. |Codecov status| image:: https://codecov.io/gh/gbeckers/Birdwatcher/branch/master/graph/badge.svg?token=829BH0NSVM
   :target: https://codecov.io/gh/gbeckers/Birdwatcher


",2022-08-12
https://github.com/gbeckers/Darr,"Darr
====

|Github CI Status| |Appveyor Status| |PyPi version| |Conda Forge|
|Codecov Badge| |Docs Status| |Zenodo Badge|

Darr is a Python library that stores NumPy arrays on disk in a way that is
simple and self-documented, which makes them easily accessible from a wide
range of computing environments. Arrays are automatically kept up-to-date
with a full explanation of how data is stored, including code to read
itself in languages such as R, Julia, IDL, Matlab, Maple, and Mathematica,
or in Python/Numpy without Darr (see `example
<https://github.com/gbeckers/Darr/tree/master/examplearrays/arrays
/array_int32_2D.darr>`__). Keeping data universally readable and documented is
a pillar of good scientific practice, and a good idea in general. More
rationale for a tool-independent approach to numeric array storage is provided
`here <https://darr.readthedocs.io/en/latest/rationale.html>`__.

Under the hood, Darr uses NumPy memory-mapped arrays, which is a widely
established and trusted way of working with disk-based numerical arrays, and
which makes Darr fully NumPy compatible. This enables efficient out-of-core
read/write access to potentially very large arrays. What Darr adds is that it
automatically keeps your arrays fully documented, open, and thus widely
readable. Further, Darr adds functionality to make your life easier in other
ways, such as the support for ragged arrays, the ability to create arrays from
iterators, append and truncate functionality, and the easy use of metadata.

Flat binary files and (JSON) text files are accompanied by a README text file
that explains how the array and metadata are stored (`see example arrays
<https://github.com/gbeckers/Darr/tree/master/examplearrays/>`__).
It is trivially easy to share your arrays with others or with yourself when
working in different computing environments because they always contains clear
documentation of the specific data at hand, including code to read it.
Does your colleague want to try out an interesting algorithm in R or Matlab
on your arrays?  No need to export anything or to provide elaborate
explanation. No dependence on complicated formats or specialized libraries.
No looking up things. A copy-paste of a few lines of code from the
documentation stored with the data is sufficient. Self-documentation and code
examples are automatically updated as you change your arrays when working
with them.

See this `tutorial <https://darr.readthedocs.io/en/latest/tutorialarray.html>`__
for a brief introduction, or the
`documentation <http://darr.readthedocs.io/>`__ for more info.

Darr is currently pre-1.0, still undergoing development. It is open source and
freely available under the `New BSD License
<https://opensource.org/licenses/BSD-3-Clause>`__ terms.

Features
--------
-  Data is stored purely based on flat binary and text files, maximizing
   universal readability.
-  Automatic self-documention, including copy-paste ready code snippets for
   reading the array in a number of popular data analysis environments, such as
   Python (without Darr), R, Julia, Octave/Matlab, GDL/IDL, and Mathematica
   (see `example array
   <https://github.com/gbeckers/Darr/tree/master/examplearrays/arrays/array_int32_2D.darr>`__).
-  Disk-persistent array data is directly accessible through `NumPy
   indexing <https://numpy.org/doc/stable/reference/arrays.indexing.html>`__
   and may be larger than RAM and that is easily appendable.
-  Supports ragged arrays.
-  Easy use of metadata, stored in a widely readable separate
   `JSON <https://en.wikipedia.org/wiki/JSON>`__ text file.
-  Many numeric types are supported: (u)int8-(u)int64, float16-float64,
   complex64, complex128.
-  Integrates easily with the `Dask <https://dask.pydata.org/en/latest/>`__
   library for out-of-core computation on very large arrays.
-  Minimal dependencies, only `NumPy <http://www.numpy.org/>`__.

Con's:

-  No compression, although compression for archiving purposes is supported.

Installation
------------

Darr depends on Python 3.6 or higher and NumPy 1.12 or higher.

Install Darr from PyPI::

    $ pip install darr

Or, install Darr via conda::

    $ conda install -c conda-forge darr

To install the latest development version, use pip with the latest GitHub
master::

    $ pip install git+https://github.com/gbeckers/darr@master


Documentation
-------------
See the `documentation <http://darr.readthedocs.io/>`_ for more information.

Contributing
------------
Any help / suggestions / ideas / contributions are welcome and very much
appreciated. For any comment, question, or error, please open an issue or
propose a pull request.


Other interesting projects
--------------------------
If Darr is not exactly what you are looking for, have a look at these projects:

-  `asdf <https://github.com/asdf-format/asdf>`__
-  `exdir <https://github.com/CINPLA/exdir/>`__
-  `h5py <https://github.com/h5py/h5py>`__
-  `pyfbf <https://github.com/davidh-ssec/pyfbf>`__
-  `pytables <https://github.com/PyTables/PyTables>`__
-  `zarr <https://github.com/zarr-developers/zarr>`__



Darr is BSD licensed (BSD 3-Clause License). (c) 2017-2022, Gabriël
Beckers

.. |Github CI Status| image:: https://github.com/gbeckers/Darr/actions/workflows/python_package.yml/badge.svg
   :target: https://github.com/gbeckers/Darr/actions/workflows/python_package.yml
.. |Appveyor Status| image:: https://ci.appveyor.com/api/projects/status/github/gbeckers/darr?svg=true
   :target: https://ci.appveyor.com/project/gbeckers/darr
.. |PyPi version| image:: https://img.shields.io/badge/pypi-0.5.4-orange.svg
   :target: https://pypi.org/project/darr/
.. |Conda Forge| image:: https://anaconda.org/conda-forge/darr/badges/version.svg
   :target: https://anaconda.org/conda-forge/darr
.. |Docs Status| image:: https://readthedocs.org/projects/darr/badge/?version=stable
   :target: https://darr.readthedocs.io/en/latest/
.. |Repo Status| image:: https://www.repostatus.org/badges/latest/active.svg
   :alt: Project Status: Active – The project has reached a stable, usable state and is being actively developed.
   :target: https://www.repostatus.org/#active
.. |Codacy Badge| image:: https://api.codacy.com/project/badge/Grade/c0157592ce7a4ecca5f7d8527874ce54
   :alt: Codacy Badge
   :target: https://app.codacy.com/app/gbeckers/Darr?utm_source=github.com&utm_medium=referral&utm_content=gbeckers/Darr&utm_campaign=Badge_Grade_Dashboard
.. |Zenodo Badge| image:: https://zenodo.org/badge/151593293.svg
   :target: https://zenodo.org/badge/latestdoi/151593293
.. |Codecov Badge| image:: https://codecov.io/gh/gbeckers/Darr/branch/master/graph/badge.svg?token=BBV0WDIUSJ
   :target: https://codecov.io/gh/gbeckers/Darr
",2022-08-12
https://github.com/gbeckers/jadeR,"# jadeR for Python
## Blind source separation of real signals

jadeR implements JADE, an Independent Component Analysis (ICA) algorithm
developed by Jean-Francois Cardoso. More information about JADE can be
found among others in: Cardoso, J. (1999) High-order contrasts for
independent component analysis. Neural Computation, 11(1): 157-192.

More information about ICA can be found among others in Hyvarinen A.,
Karhunen J., Oja E. (2001). Independent Component Analysis, Wiley. Or at the
website http://www.cis.hut.fi/aapo/papers/IJCNN99_tutorialweb/

The Python code here was translated into NumPy from the original Matlab Version
1.8 of May 2005 by Gabriel Beckers, http://gbeckers.nl . After that, two 
corrections were made by David Rivest-Hénault to make the code become 
equivalent at machine precision to that of jadeR.m, after which I put the code 
on Github.",2022-08-12
https://github.com/gbeckers/ndarrayarray,"Ndarrayarray
============

|Repo Status|

**WARNING: this package is still experimental**

This package enables you to work with a disk-based, memory mapped array of NumPy numeric
ndarrays, each of which may have an arbitrary number of dimensions and shape.

It extends the concept of a ragged (or 'jagged') array, which is an array of arrays
with different lengths, to an array of arrays with any dimensionality and shape.

Since ndarrayarrays are memory-mapped from disk they can be (much) larger than RAM.

The Ndarrayarray package depends on the `Darr <https://github.com/gbeckers/Darr/>`__ package.

Ndarrayarray is very early in development. It is open source and freely available under
the `New BSD License <https://opensource.org/licenses/BSD-3-Clause>`__ terms.

Ndarrayarray is BSD licensed (BSD 3-Clause License). (c) 2022, Gabriël
Beckers

Example
-------

.. code:: python

    >>> import numpy as np
    >>> ndarrayarray as ndaa
    >>> a = ndaa.create_ndarrayarray('testndaa.darr', dtype='float64', overwrite=True)
    >>> a.append(np.ones((3,1,3)))
    >>> a.append(2 * np.ones((2,4)))
    >>> a.append(3 * np.ones(5))
    >>> a[0]
    array([[[1., 1., 1.]],
           [[1., 1., 1.]],
           [[1., 1., 1.]]])
    >>> a[2]
    array([3., 3., 3., 3., 3.])

.. |Repo Status| image:: https://www.repostatus.org/badges/latest/wip.svg
   :alt: Project Status: WIP – Initial development is in progress, but there has not yet been a stable, usable release suitable for the public.
   :target: https://www.repostatus.org/#wip

",2022-08-12
https://github.com/gbeckers/Praat_scripts,"Praat scripts
=============

A collection of Praat scripts that I wrote a long time ago. I haven't updated 
them in a long time but perhaps they are still useful to some.


Usage
-----

In Praat ( download at https://http://www.fon.hum.uva.nl/praat/ ), open script 
and run. Many have some form of documentation included.

",2022-08-12
https://github.com/gbeckers/pyagl,"pyagl
=====

A python library for statistical and artificial grammar learning (AGL)
analyses.

This software is used for our own research, but is freely available to
others for use or contributions.

We are at an incipient stage where just one model for word segmentation
(PARSER) is implemented, as well as more general string analysis functions.

Pyagl has its roots in, and will supersede, two related earlier projects:
`aglcheck <https://github.com/gjlbeckers-uu/aglcheck>`__ by Gabriël Beckers.
The PARSER module was initiated by Bror-E, in `PARSER-for-Python
<https://github.com/Bror-E/PARSER-for-Python>`__, but has since then evolved
to a large extent.

Pyagl is currently pre-1.0, still undergoing significant development. It is
open source and freely available under the
`New BSD License <https://opensource.org/licenses/BSD-3-Clause>`__ terms.


Installation
------------

As long as there is no official release I recommend working in Anaconda.

Create an environment::

    $ conda create -n agltest pip python=3.6 jupyterlab git pyaml pandas

Switch to this new environment:

Linux and MacOS::

    $ source activate agltest

Windows::

    $ conda activate agltest

Install the pyagl master repo::

    $ pip install  git+https://github.com/gbeckers//pyagl@master


If you want to remove the conda environment later::

    $ conda env remove -n agltest


Documentation
-------------

Not there yet.


pyagl is BSD licensed (BSD 3-Clause License). (c) 2019-2020, Gabriël Beckers.
",2022-08-12
https://github.com/gbeckers/sound,"Sound
=====

**NOTE: DO NOT USE THIS VERSION YET, in middle of refactoring.**

*Sound* is a package for working with acoustic data in Python. It is designed
for scientific use cases, but it may also be of interest to soundscape
recordists, or anyone who wants to work efficiently with very long sounds,
with very many sounds, or with metadata.

There already good tools for working with audio files in Python. However,
audio files can be cumbersome to work with in applications they were not
designed for. Some examples include the ability to work efficiently with very
long recordings (think many hours, days, weeks), with zillions of short sound
events, with non-integer sampling rates (needed to precisely synchronize
acoustic data with other data), with absolute sound levels or absolute time, or
various types of metadata. These things do not matter when working with your
average music song, but they do matter in science and other applications.
*Sound* is intended to solve this problem.

In its simplest form, *Sound* work with (collections of) normal audio files.
However it improves their usefulness by organizing important metadata in
separate text-based files. For more heavy-duty work, *Sound* works with
`Darr <https://darr.readthedocs.io/en/latest>`__-based data, which is a format
designed for scientific use and supports very efficient random access
reading/writing of numeric data. This way, you can efficiently work very
large recordings (that won't fit in RAM memory), or zillions of recorded
sound episodes which would otherwise be inefficiently stored in zillions of
separate files.

*Sound* is in its early stages of development (alpha) stage. It forms the basis
of, and is complemented, by the *SoundLab* library for sound analysis and
visualization, and *SoundStimBuilder* library for those who construct auditory
stimuli, e.g. for scientific experiments.

Sound is BSD licensed (BSD 3-Clause License). (c) 2020, Gabriël Beckers


",2022-08-12
https://github.com/gbeckers/soundlab,"Soundlab
========

Soundlab is a python package for the analysis and visualization of sound data.
Created with scientific use in mind. It is in its early stages of
development (alpha) stage.

Do not use this yet. I am working on a first version...

",2022-08-12
https://github.com/gbeckers/soundstimbuilder,"Soundstimbuilder
================

Soundstimbuilder is a python package for creating experimental sound stimuli.

This package is in alpha stage. Do not use yet outside our lab.


Installation
------------
It is best to install in a separate conda environment (see below):

To install the latest development version, use pip with the latest GitHub
master: ::

    $ pip install git+https://github.com/gbeckers/soundstimbuilder@master

If you already have an older version installed, you may have to uninstall
 first (not sure, but can't hurt):
 
    $ pip uninstall soundstimbuilder
 
Conda environment
-----------------
It is best to first create a separate Anaconda environment for installation, for now with a number of packages
with specific versions.

Most packages are in the default channel but you will also need one package from conda-forge. This channel may
need to be appended to conda's channel list (if it is already appended, then the next is still safe): ::

    $ conda config --append channels conda-forge

In a terminal, use the following to create an environment called ""sndbld"": ::

    $ conda create -n sndbld python=3.8 jupyter=1.0 scipy=1.4 darr=0.3 pandas=1.0 matplotlib=3.1

You can also create this environment from Anaconda Navigator, without using a terminal.

Testing
-------

To run the test suite: ::

    >>> import soundstimbuilder as sb
    >>> sb.test()
    .
    ----------------------------------------------------------------------
    Ran 1 test in 0.000s
    
    OK
    <unittest.runner.TextTestResult run=1 errors=0 failures=0>
    
    >>>
    
Note that tests still have to be written, but the testing frame work is in place.",2022-08-12
https://github.com/georkap/ego_md_mtl,"# Multi-dataset Multitask Egocentric Action Recognition

Code for paper Multi-dataset Multitask Egocentric Action recognition (https://ieeexplore.ieee.org/document/9361177)

## Abstract
For egocentric vision tasks such as action recognition, there is a relative scarcity of labeled data. This increases the risk of overfitting during training. In this paper, we address this issue by introducing a multitask learning scheme that employs related tasks as well as related datasets in the training process. Related tasks are indicative of the performed action, such as the presence of objects and the position of the hands. By including related tasks as additional outputs to be optimized, action recognition performance typically increases because the network focuses on relevant aspects in the video. Still, the training data is limited to a single dataset because the set of action labels usually differs across datasets. To mitigate this issue, we extend the multitask paradigm to include datasets with different label sets. During training, we effectively mix batches with samples from multiple datasets. Our experiments on egocentric action recognition in the EPIC-Kitchens, EGTEA Gaze+, ADL and Charades-EGO datasets demonstrate the improvements of our approach over single-dataset baselines. On EGTEA we surpass the current state-of-the-art by 2.47%. We further illustrate the cross-dataset task correlations that emerge automatically with our novel training scheme.

## Requirements
torch==1.6.0  
dsntnn==0.5.3  
opencv_python==4.4.0.44  
matplotlib==3.3.1  
scipy==1.5.0  
numpy==1.19.1  
pandas==0.24.2  
torchvision==0.7.0  
scikit_learn==0.24.1  

## Citation
If you do, please cite our paper. 

G. Kapidis, R. Poppe and R. C. Veltkamp, ""Multi-Dataset, Multitask Learning of Egocentric Vision Tasks,"" in IEEE Transactions on Pattern Analysis and Machine Intelligence, doi: 10.1109/TPAMI.2021.3061479.

@ARTICLE{9361177,  
  author={G. {Kapidis} and R. {Poppe} and R. C. {Veltkamp}},  
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},  
  title={Multi-Dataset, Multitask Learning of Egocentric Vision Tasks},  
  year={2021},  
  volume={},  
  number={},  
  pages={1-1},  
  doi={10.1109/TPAMI.2021.3061479}}  
a
",2022-08-12
https://github.com/georkap/faster-rcnnwv.pytorch,"# A *Faster* Pytorch Implementation of Faster R-CNN

## Introduction

This project is a *faster* pytorch implementation of faster R-CNN, aimed to accelerating the training of faster R-CNN object detection models. Recently, there are a number of good implementations:

* [rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn), developed based on Pycaffe + Numpy

* [longcw/faster_rcnn_pytorch](https://github.com/longcw/faster_rcnn_pytorch), developed based on Pytorch + Numpy

* [endernewton/tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn), developed based on TensorFlow + Numpy

* [ruotianluo/pytorch-faster-rcnn](https://github.com/ruotianluo/pytorch-faster-rcnn), developed based on Pytorch + TensorFlow + Numpy

During our implementing, we referred the above implementations, especailly [longcw/faster_rcnn_pytorch](https://github.com/longcw/faster_rcnn_pytorch). However, our implementation has several unique and new features compared with the above implementations:

* **It is pure Pytorch code**. We convert all the numpy implementations to pytorch!

* **It supports multi-image batch training**. We revise all the layers, including dataloader, rpn, roi-pooling, etc., to support multiple images in each minibatch.

* **It supports multiple GPUs training**. We use a multiple GPU wrapper (nn.DataParallel here) to make it flexible to use one or more GPUs, as a merit of the above two features.

* **It supports three pooling methods**. We integrate three pooling methods: roi pooing, roi align and roi crop. More importantly, we modify all of them to support multi-image batch training.

* **It is memory efficient**. We limit the image aspect ratio, and group images with similar aspect ratios into a minibatch. As such, we can train resnet101 and VGG16 with batchsize = 4 (4 images) on a sigle Titan X (12 GB). When training with 8 GPU, the maximum batchsize for each GPU is 3 (Res101), totally 24.

* **It is faster**. Based on the above modifications, the training is much faster. We report the training speed on NVIDIA TITAN Xp in the tables below.

### What we are doing and going to do

- [x] Support both python2 and python3 (great thanks to [cclauss](https://github.com/cclauss)).
- [x] Add deformable pooling layer (mainly supported by [Xander](https://github.com/xanderchf)).
- [x] Support pytorch-0.4.0.
- [x] Support tensorboardX.
- [ ] Support pytorch-0.4.1 or higher.

## Other Implementations

* [Feature Pyramid Network (FPN)](https://github.com/jwyang/fpn.pytorch)

* [Mask R-CNN](https://github.com/roytseng-tw/mask-rcnn.pytorch) (~~ongoing~~ already implemented by [roytseng-tw](https://github.com/roytseng-tw))

* [Graph R-CNN](https://github.com/jwyang/graph-rcnn.pytorch) (extension to scene graph generation)

## Tutorial

* [Blog](http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/) by [ankur6ue](https://github.com/ankur6ue)

## Benchmarking

We benchmark our code thoroughly on three datasets: pascal voc, coco and imagenet-200, using two different network architecture: vgg16 and resnet101. Below are the results:

1). PASCAL VOC 2007 (Train/Test: 07trainval/07test, scale=600, ROI Align)

model    | #GPUs | batch size | lr        | lr_decay | max_epoch     |  time/epoch | mem/GPU | mAP
---------|--------|-----|--------|-----|-----|-------|--------|-----
[VGG-16](https://www.dropbox.com/s/6ief4w7qzka6083/faster_rcnn_1_6_10021.pth?dl=0)     | 1 | 1 | 1e-3 | 5   | 6   |  0.76 hr | 3265MB   | 70.1
[VGG-16](https://www.dropbox.com/s/cpj2nu35am0f9hp/faster_rcnn_1_9_2504.pth?dl=0)     | 1 | 4 | 4e-3 | 8   | 9  |  0.50 hr | 9083MB   | 69.6
[VGG-16](https://www.dropbox.com/s/1a31y7vicby0kvy/faster_rcnn_1_10_625.pth?dl=0)     | 8 | 16| 1e-2 | 8   | 10  |  0.19 hr | 5291MB   | 69.4
[VGG-16](https://www.dropbox.com/s/hkj7i6mbhw9tq4k/faster_rcnn_1_11_416.pth?dl=0)     | 8 | 24| 1e-2 | 10  | 11  |  0.16 hr | 11303MB  | 69.2
[Res-101](https://www.dropbox.com/s/4v3or0054kzl19q/faster_rcnn_1_7_10021.pth?dl=0)   | 1 | 1 | 1e-3 | 5   | 7   |  0.88 hr | 3200 MB  | 75.2
[Res-101](https://www.dropbox.com/s/8bhldrds3mf0yuj/faster_rcnn_1_10_2504.pth?dl=0)    | 1 | 4 | 4e-3 | 8   | 10  |  0.60 hr | 9700 MB  | 74.9
[Res-101](https://www.dropbox.com/s/5is50y01m1l9hbu/faster_rcnn_1_10_625.pth?dl=0)    | 8 | 16| 1e-2 | 8   | 10  |  0.23 hr | 8400 MB  | 75.2 
[Res-101](https://www.dropbox.com/s/cn8gneumg4gjo9i/faster_rcnn_1_12_416.pth?dl=0)    | 8 | 24| 1e-2 | 10  | 12  |  0.17 hr | 10327MB  | 75.1  


2). COCO (Train/Test: coco_train+coco_val-minival/minival, scale=800, max_size=1200, ROI Align)

model     | #GPUs | batch size |lr        | lr_decay | max_epoch     |  time/epoch | mem/GPU | mAP
---------|--------|-----|--------|-----|-----|-------|--------|-----
VGG-16     | 8 | 16    |1e-2| 4   | 6  |  4.9 hr | 7192 MB  | 29.2
[Res-101](https://www.dropbox.com/s/5if6l7mqsi4rfk9/faster_rcnn_1_6_14657.pth?dl=0)    | 8 | 16    |1e-2| 4   | 6  |  6.0 hr    |10956 MB  | 36.2
[Res-101](https://www.dropbox.com/s/be0isevd22eikqb/faster_rcnn_1_10_14657.pth?dl=0)    | 8 | 16    |1e-2| 4   | 10  |  6.0 hr    |10956 MB  | 37.0

**NOTE**. Since the above models use scale=800, you need add ""--ls"" at the end of test command.

3). COCO (Train/Test: coco_train+coco_val-minival/minival, scale=600, max_size=1000, ROI Align)

model     | #GPUs | batch size |lr        | lr_decay | max_epoch     |  time/epoch | mem/GPU | mAP
---------|--------|-----|--------|-----|-----|-------|--------|-----
[Res-101](https://www.dropbox.com/s/y171ze1sdw1o2ph/faster_rcnn_1_6_9771.pth?dl=0)    | 8 | 24    |1e-2| 4   | 6  |  5.4 hr    |10659 MB  | 33.9
[Res-101](https://www.dropbox.com/s/dpq6qv0efspelr3/faster_rcnn_1_10_9771.pth?dl=0)    | 8 | 24    |1e-2| 4   | 10  |  5.4 hr    |10659 MB  | 34.5

4). Visual Genome (Train/Test: vg_train/vg_test, scale=600, max_size=1000, ROI Align, category=2500)

model     | #GPUs | batch size |lr        | lr_decay | max_epoch     |  time/epoch | mem/GPU | mAP
---------|--------|-----|--------|-----|-----|-------|--------|-----
[VGG-16](http://data.lip6.fr/cadene/faster-rcnn.pytorch/faster_rcnn_1_19_48611.pth)    | 1 P100 | 4    |1e-3| 5   | 20  |  3.7 hr    |12707 MB  | 4.4

Thanks to [Remi](https://github.com/Cadene) for providing the pretrained detection model on visual genome!

* Click the links in the above tables to download our pre-trained faster r-cnn models.
* If not mentioned, the GPU we used is NVIDIA Titan X Pascal (12GB).

## Preparation


First of all, clone the code
```
git clone https://github.com/jwyang/faster-rcnn.pytorch.git
```

Then, create a folder:
```
cd faster-rcnn.pytorch && mkdir data
```

### prerequisites

* Python 2.7 or 3.6
* Pytorch 0.4.0 (**now it does not support 0.4.1 or higher**)
* CUDA 8.0 or higher

### Data Preparation

* **PASCAL_VOC 07+12**: Please follow the instructions in [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to prepare VOC datasets. Actually, you can refer to any others. After downloading the data, creat softlinks in the folder data/.

* **COCO**: Please also follow the instructions in [py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to prepare the data.

* **Visual Genome**: Please follow the instructions in [bottom-up-attention](https://github.com/peteanderson80/bottom-up-attention) to prepare Visual Genome dataset. You need to download the images and object annotation files first, and then perform proprecessing to obtain the vocabulary and cleansed annotations based on the scripts provided in this repository.

### Pretrained Model

We used two pretrained models in our experiments, VGG and ResNet101. You can download these two models from:

* VGG16: [Dropbox](https://www.dropbox.com/s/s3brpk0bdq60nyb/vgg16_caffe.pth?dl=0), [VT Server](https://filebox.ece.vt.edu/~jw2yang/faster-rcnn/pretrained-base-models/vgg16_caffe.pth)

* ResNet101: [Dropbox](https://www.dropbox.com/s/iev3tkbz5wyyuz9/resnet101_caffe.pth?dl=0), [VT Server](https://filebox.ece.vt.edu/~jw2yang/faster-rcnn/pretrained-base-models/resnet101_caffe.pth)

Download them and put them into the data/pretrained_model/.

**NOTE**. We compare the pretrained models from Pytorch and Caffe, and surprisingly find Caffe pretrained models have slightly better performance than Pytorch pretrained. We would suggest to use Caffe pretrained models from the above link to reproduce our results.

**If you want to use pytorch pre-trained models, please remember to transpose images from BGR to RGB, and also use the same data transformer (minus mean and normalize) as used in pretrained model.**

### Compilation

As pointed out by [ruotianluo/pytorch-faster-rcnn](https://github.com/ruotianluo/pytorch-faster-rcnn), choose the right `-arch` in `make.sh` file, to compile the cuda code:

  | GPU model  | Architecture |
  | ------------- | ------------- |
  | TitanX (Maxwell/Pascal) | sm_52 |
  | GTX 960M | sm_50 |
  | GTX 1080 (Ti) | sm_61 |
  | Grid K520 (AWS g2.2xlarge) | sm_30 |
  | Tesla K80 (AWS p2.xlarge) | sm_37 |

More details about setting the architecture can be found [here](https://developer.nvidia.com/cuda-gpus) or [here](http://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/)

Install all the python dependencies using pip:
```
pip install -r requirements.txt
```

Compile the cuda dependencies using following simple commands:

```
cd lib
sh make.sh
```

It will compile all the modules you need, including NMS, ROI_Pooing, ROI_Align and ROI_Crop. The default version is compiled with Python 2.7, please compile by yourself if you are using a different python version.

**As pointed out in this [issue](https://github.com/jwyang/faster-rcnn.pytorch/issues/16), if you encounter some error during the compilation, you might miss to export the CUDA paths to your environment.**

## Train

Before training, set the right directory to save and load the trained models. Change the arguments ""save_dir"" and ""load_dir"" in trainval_net.py and test_net.py to adapt to your environment.

To train a faster R-CNN model with vgg16 on pascal_voc, simply run:
```
CUDA_VISIBLE_DEVICES=$GPU_ID python trainval_net.py \
                   --dataset pascal_voc --net vgg16 \
                   --bs $BATCH_SIZE --nw $WORKER_NUMBER \
                   --lr $LEARNING_RATE --lr_decay_step $DECAY_STEP \
                   --cuda
```
where 'bs' is the batch size with default 1. Alternatively, to train with resnet101 on pascal_voc, simple run:
```
 CUDA_VISIBLE_DEVICES=$GPU_ID python trainval_net.py \
                    --dataset pascal_voc --net res101 \
                    --bs $BATCH_SIZE --nw $WORKER_NUMBER \
                    --lr $LEARNING_RATE --lr_decay_step $DECAY_STEP \
                    --cuda
```
Above, BATCH_SIZE and WORKER_NUMBER can be set adaptively according to your GPU memory size. **On Titan Xp with 12G memory, it can be up to 4**.

If you have multiple (say 8) Titan Xp GPUs, then just use them all! Try:
```
python trainval_net.py --dataset pascal_voc --net vgg16 \
                       --bs 24 --nw 8 \
                       --lr $LEARNING_RATE --lr_decay_step $DECAY_STEP \
                       --cuda --mGPUs

```

Change dataset to ""coco"" or 'vg' if you want to train on COCO or Visual Genome.

## Test

If you want to evlauate the detection performance of a pre-trained vgg16 model on pascal_voc test set, simply run
```
python test_net.py --dataset pascal_voc --net vgg16 \
                   --checksession $SESSION --checkepoch $EPOCH --checkpoint $CHECKPOINT \
                   --cuda
```
Specify the specific model session, chechepoch and checkpoint, e.g., SESSION=1, EPOCH=6, CHECKPOINT=416.

## Demo

If you want to run detection on your own images with a pre-trained model, download the pretrained model listed in above tables or train your own models at first, then add images to folder $ROOT/images, and then run
```
python demo.py --net vgg16 \
               --checksession $SESSION --checkepoch $EPOCH --checkpoint $CHECKPOINT \
               --cuda --load_dir path/to/model/directoy
```

Then you will find the detection results in folder $ROOT/images.

**Note the default demo.py merely support pascal_voc categories. You need to change the [line](https://github.com/jwyang/faster-rcnn.pytorch/blob/530f3fdccaa60d05fa068bc2148695211586bd88/demo.py#L156) to adapt your own model.**

Below are some detection results:

<div style=""color:#0000FF"" align=""center"">
<img src=""images/img3_det_res101.jpg"" width=""430""/> <img src=""images/img4_det_res101.jpg"" width=""430""/>
</div>

## Webcam Demo

You can use a webcam in a real-time demo by running
```
python demo.py --net vgg16 \
               --checksession $SESSION --checkepoch $EPOCH --checkpoint $CHECKPOINT \
               --cuda --load_dir path/to/model/directoy \
               --webcam $WEBCAM_ID
```
The demo is stopped by clicking the image window and then pressing the 'q' key.

## Authorship

This project is equally contributed by [Jianwei Yang](https://github.com/jwyang) and [Jiasen Lu](https://github.com/jiasenlu), and many others (thanks to them!).

## Citation

    @article{jjfaster2rcnn,
        Author = {Jianwei Yang and Jiasen Lu and Dhruv Batra and Devi Parikh},
        Title = {A Faster Pytorch Implementation of Faster R-CNN},
        Journal = {https://github.com/jwyang/faster-rcnn.pytorch},
        Year = {2017}
    }

    @inproceedings{renNIPS15fasterrcnn,
        Author = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
        Title = {Faster {R-CNN}: Towards Real-Time Object Detection
                 with Region Proposal Networks},
        Booktitle = {Advances in Neural Information Processing Systems ({NIPS})},
        Year = {2015}
    }
",2022-08-12
https://github.com/georkap/hand_track_classification,"# hand_track_classification

This repository is used for classification of hand tracking images from egocentric videos of the Epic Kitchens [PDF](https://arxiv.org/pdf/1804.02748.pdf) [Project page](https://epic-kitchens.github.io/2018) dataset. 
Using pytorch 0.4 currently

## Steps taken
We used tracking of hands by detection with a Yolov3 hand detector [Git](https://github.com/AlexeyAB/darknet) and the Sort tracker [PDF](https://arxiv.org/pdf/1602.00763.pdf) to fill in the gaps in the detections, 
in order to produce continuous hand tracks.
Then we turn the hand tracks into images which we classify using a resnet (18, 50, 101) backbone into the 120 verb classes of Epic Kitchens.
These pipelines are covered by ```main_cnn``` and its variations

The points per frame of each track can also be used as input to an lstm and be classified into the verb classes of Epic Kitchens
These pipelines are covered by ```main_lstm``` and its variations 

For practical reasons for my project I added support for the MFNET network [Git](https://github.com/cypw/PyTorch-MFNet) [PDF](https://arxiv.org/abs/1807.11195) to train directly on the RGB images of Epic Kitchens
without direct hand support yet. Pipelines are covered by ```main_mfnet``` and its variations

## Usage

### Data Loading
It is practical to create symlinks ""mklink /D target origin"" into the base folder and have the split\\*.txt files point to the relative path to the base folder. 

### Create split files
.txt files that register the locations of the actual files that are used for training along with other pieces of information e.g. class, number of frames etc.

### Arguments
For a comprehensive list of arguments see ```utils\argparse_utils.py ```

Examples:
CNN training
```usage: main_cnn.py [-h] [--base_output_dir BASE_OUTPUT_DIR]
                   [--model_name MODEL_NAME] [--verb_classes VERB_CLASSES]
                   [--noun_classes NOUN_CLASSES] [--batch_size BATCH_SIZE]
                   [--clip_gradient] [--no_resize]
                   [--inter {linear,cubic,nn,area,lanc,linext}] [--bin_img]
                   [--pad] [--pretrained]
                   [--resnet_version {18,34,50,101,152}] [--channels {RGB,G}]
                   [--feature_extraction] [--double_output]
                   [--dropout DROPOUT] [--mixup_a MIXUP_A] [--lr LR]
                   [--lr_type {step,multistep,clr,groupmultistep}]
                   [--lr_steps LR_STEPS [LR_STEPS ...]] [--momentum MOMENTUM]
                   [--decay DECAY] [--max_epochs MAX_EPOCHS]
                   [--gpus GPUS [GPUS ...]] [--num_workers NUM_WORKERS]
                   [--eval_freq EVAL_FREQ] [--mfnet_eval MFNET_EVAL]
                   [--eval_on_train] [--save_all_weights] [--save_attentions]
                   [--logging] [--resume]
                   train_list test_list
```
LSTM training
```
usage: main_lstm.py [-h] [--base_output_dir BASE_OUTPUT_DIR]
                    [--model_name MODEL_NAME] [--verb_classes VERB_CLASSES]
                    [--noun_classes NOUN_CLASSES] [--batch_size BATCH_SIZE]
                    [--lstm_feature {coords,coords_dual,coords_polar,coords_diffs,vec_sum,vec_sum_dual}]
                    [--no_norm_input] [--lstm_clamped]
                    [--lstm_input LSTM_INPUT] [--lstm_hidden LSTM_HIDDEN]
                    [--lstm_layers LSTM_LAYERS]
                    [--lstm_seq_size LSTM_SEQ_SIZE] [--lstm_dual]
                    [--lstm_attn] [--only_left] [--only_right]
                    [--double_output] [--dropout DROPOUT] [--mixup_a MIXUP_A]
                    [--lr LR] [--lr_type {step,multistep,clr,groupmultistep}]
                    [--lr_steps LR_STEPS [LR_STEPS ...]] [--momentum MOMENTUM]
                    [--decay DECAY] [--max_epochs MAX_EPOCHS]
                    [--gpus GPUS [GPUS ...]] [--num_workers NUM_WORKERS]
                    [--eval_freq EVAL_FREQ] [--mfnet_eval MFNET_EVAL]
                    [--eval_on_train] [--save_all_weights] [--save_attentions]
                    [--logging] [--resume]
                    train_list test_list
```
MFNet training
```
usage: main_mfnet.py [-h] [--base_output_dir BASE_OUTPUT_DIR]
                     [--model_name MODEL_NAME] [--verb_classes VERB_CLASSES]
                     [--noun_classes NOUN_CLASSES] [--batch_size BATCH_SIZE]
                     [--clip_gradient] [--clip_length CLIP_LENGTH]
                     [--frame_interval FRAME_INTERVAL] [--pretrained]
                     [--pretrained_model_path PRETRAINED_MODEL_PATH]
                     [--double_output] [--dropout DROPOUT] [--mixup_a MIXUP_A]
                     [--lr LR] [--lr_type {step,multistep,clr,groupmultistep}]
                     [--lr_steps LR_STEPS [LR_STEPS ...]]
                     [--momentum MOMENTUM] [--decay DECAY]
                     [--max_epochs MAX_EPOCHS] [--gpus GPUS [GPUS ...]]
                     [--num_workers NUM_WORKERS] [--eval_freq EVAL_FREQ]
                     [--mfnet_eval MFNET_EVAL] [--eval_on_train]
                     [--save_all_weights] [--save_attentions] [--logging]
                     [--resume]
                     train_list test_list
```

## Download hand tracks dataset
[Google Drive link](https://drive.google.com/file/d/1TSo2H6_bGuLdh6WgKYU9OJLoPAETcbPi/view?usp=sharing)

## Contact
Georgios Kapidis
georgios{dot}kapidis{at}noldus{dot}nl
g{dot}kapidis{at}uu{dot}nl
georgios{dot}kapidis{at}gmail{dot}com
",2022-08-12
https://github.com/georkap/object-based-location-classification,"# object-based-location-classification

## Introduction

This repository contains training and evaluation code to train ANN and LSTM networks for location classification based on detected objects on a video frame.
We use Yolo for object detections but anything can be used to produce the classification input as long as the format in the train files is not modified.

The code is tested and working on PyTorch 0.4.0 on Windows 10.

## Installation

Clone the repository to your computer
``` 
git clone https://github.com/georkap/object-based-location-classification 
```
We include the object detections for the 20 object classes of the adl dataset for detection threshold 0.3 (case d2d 0.3) for all the videos of the ADL dataset.

## Dependencies
* pytorch 0.4
* numpy
* sklearn

## Licence
Our code and data are available under the MIT licence. If you use them for scientific research please consider citing our paper:
```
@inproceedings{kapidis_where_2018,
  title = {Where {{Am I}}? {{Comparing CNN}} and {{LSTM}} for {{Location Classification}} in {{Egocentric Videos}}},
  doi = {10.1109/PERCOMW.2018.8480258},
  booktitle = {2018 {{IEEE International Conference}} on {{Pervasive Computing}} and {{Communications Workshops}} ({{PerCom Workshops}})},
  author = {Kapidis, G. and Poppe, R. W. and van Dam, E. A. and Veltkamp, R. C. and Noldus, L. P. J. J.},
  month = mar,
  year = {2018},
  pages = {878-883}
}
```

## Contact
g.kapidis{at}uu.nl
",2022-08-12
https://github.com/gerb-ster/1x7-2x3-Buffer,"# 2×3 – 1×7 Buffer

A simple but effective little module. An active buffer build around the TL074 & TL072 IC’s. The output of the last output of the first block is linked to the input of the second block. So when there is nothing connected to the second input you have a 1×7 buffer.

Ps. On the silkscreen is a small typo; it should note “2×3 / 1×7” instead of “2×3 / 1×6”, this is fixed in the schematics & PCB design.

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/5-Channel-Mixer,"# 5 Channel Mixer

This one is not that special, it’s just a simple 5 channel mixer build around a TL074 and a TL072, very useful though.

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/CoronDS8-Clone,"# CoronDS8 Clone

This is a clone of the famous Coron DS8 DrumSytnh unit from the seventies. I've adapted it 
to a Eurorack module and included a small mod, a on-off-on 'weird' toggle switch, which 
generates even spacier drum sounds.

## Status: (Almost) Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype
- done: fully test the prototype
- todo: buid a proper faceplate",2022-08-12
https://github.com/gerb-ster/Curstis-VCO,"# Curtis-VCO

This module is based on the classic CEM3340 VCO chip, although I’m using a AS3340 clone. I’ve took some inspiration from the Maximus VCO design from Thomas Henry. The CEM3340 outputs 3 waveforms with different output levels. To achieve a uniform 10 vpp output around 0 volt, I used a series of op-amps. In the first step the 3 levels is attenuated to 10 vpp, in the next step it’s being offset with 5v to make it swing around 0 volt. I’ve used a trimpot to fine-tune the offset.

The triangle output is then used to ‘wave shape’ a sine output. For this, I also used a classic design from Thomas.

I’ve already made some changes to the PCB design, there was an issue with the power rails on the IO board (hence the bodge wire), some transistor values where wrong, and I made some more room for a good timing cap.

### Update 18-05-2022

Small changes have been made to the io-board to fix an issue with PWM range knob. If you have an older board, you can remove R51 from the ioboard and replace it with a wire and replace R49 with a 91k resistor.

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/Curtis-Dual-VCA,"# Curtis-Dual-VCA

A dual VCA module based on the CEM 3360 IC

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/Curtis-VC-ADSR,"# Curtis VC ADSR

A voltage controlled ADSR enveloppe generator based on the CEM 3310

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/CV-Utils,"# CV-Utils

A simple CV utilities module

## Status: Work in Progress

- done: first schematic & board design
- todo: breadboard and test
- todo: design frontpanel
- todo: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/Echo-Reverb,"# Echo/Reverb

Next, another effect module. I bought a bunch of PT2399 IC’s from China so I made his Echo/Reverb unit. It’s basically a clone of the Echo unit from Synthrotek.

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/Elka-Synthex-Curtis-VCF,"# Elka Synthex 'Curtis' VCF

A clone of the VCF found in the Elka Synthex around the CEM 3320 IC. This project is still 'work in progress', a
prototype has been build, but it's not quite bug free. 

_the current design needs to be debugged_

## Status: Work in Progress

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB 
- todo: build (working) prototype

## Links

https://bitbucket.org/XNOTOX/synthex-vcf/src/master/ 
https://electricdruid.net/cem3320-filter-designs/ 
https://www.instructables.com/How-to-Program-an-Attiny85-From-an-Arduino-Uno/ 
",2022-08-12
https://github.com/gerb-ster/EurorackBreakOutBoard,"# Eurorack BreakOut Board

A simple breakout board to help development of Eurorack modules. Clips on your breadboard and
provides a power connector, power indicator lights and some I/O terminals.

## Features

- 4 IO ports
- Power indicator led for both +12v and -12v rails
- Selectable ground rails


## Sponsoring

This project was kindly sponsored by [PCBWay](https://www.pcbway.com/?from=gerbster). They offer a fast and very affordable PCB manufacturing service. I find that their ordering process is very intuitive and the boards are of exceptional quality, so I highly recommend them.",2022-08-12
https://github.com/gerb-ster/MegaPercussionSynth,"# Mega Percussion Synth

A eurorack adaptation of Thomas Henry's MegaPercussionSynth

## Status: Work in Progress

- done: first schematic & board design
- todo: breadboard and test
- todo: design frontpanel
- todo: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/MiniMoog-VCF,"# YUSynth MiniMoog VCF

No system is complete without some good VCF’s. And one excellent source for some good VCF is designs is, offcourse, the website of Yven Usson, also known as YuSynth. And this VCF is no exception. It’s a clone of the famous VCF found in the MiniMoog. I’ve made an eurorack version.

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/ModularSequencer,"# Modular Sequencer

A modular sequencer consisting of a control module and 1 or more linkable io-modules. Al daisychained together.

## Notes

- All connected by a parallel SPI bus.
- contains 1 serial connected line for discovery
- a termination jumper for the last io-board
- all io-boards start up in a discovery mode
- if io board has termination jumper it sends signal
- control boards send an ID which is registered by io board
- io board goes out of discovery mode and sets discovery line high
- this triggers the same process in the ajasoned io-board module
- this goes on until control board senses a discovery line high
- play mode is then activated
- control board sends a gate active command; board_id + port + on / off
- io board scans inputs, gate on/off, cv value & gate length for all of its 4 ports and sends it to the control board.

## features

### Main Control Board

- tempo pot
- tempo external input (bpm)
- tempo CV input (?)
- start / stop button
- start / stop trigger inputs (?)
- CV output
- CV Portamento
- Gate Output

### IO Board

- 4x CV level pot
- 4x Gate length pot
- 4x Gate on/off toggle switch
- 4x Step indicator LED


## Links

https://www.instructables.com/ATTiny84-I2C-Slave-Arduino-UNO/
https://thecustomizewindows.com/2019/03/how-to-invert-signal-for-arduino-high-to-low-or-the-reverse/
https://docs.arduino.cc/learn/communication/wire",2022-08-12
https://github.com/gerb-ster/MS20-VCF,"# Korg MS20 VCF

A(nother) eurorack adaptation of this classic Korg VCF from the MS20. 

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/Noise-Clock-Sample-and-Hold,"# Noise, Clock and S&H

This module is a sort of 3-in-1 module, containing 4 types of noise, a clock generator based on a 555 IC and a sample-and-hold section. It’s based on / inspired by the Noise & S&H module by Niklas Ronnberg. It has a slew function, which is really nice, use it quit a lot.

The noise colors are White, Blue, Pink & Red. They are created by filtering the white noise. There are 2 trimmer pots on the back, one to set the overal noise volume and one to set the pulse length of the clock output.

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype

## Pot Values

Since the earlier schematic didn't include the pot values, here they are (schematics have also been updated)

Slew & Attn: 100k lin
Rate: 1M log",2022-08-12
https://github.com/gerb-ster/RayWilson-Dual-VCA,"# Ray Wilson's Dual VCA

Somewhere I read that you can never have enough VCA’s, and I’ve build a number of them over the years. This one is a straight up clone of a design by the late Ray Wilson from Music From Outer Space. Really nice and versatile module.

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/RingMod,"# YUSynth Ring Modulator

This module is based on the design from YuSynth, found at http://yusynth.net/Modular/EN/RINGMOD/index.html. Nothing really special, just a cool handy ring modulator. The sound sample is created using the sine outputs of two VCO’s.

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/Ronnberg-LFO,"# Ronnberg LFO

This is my take on the LFO module designed by Niklas Ronnberg. I’ve changed the range switch with an on-off-on type switch. This way, in the off position, the timing cap consist of a single .001 uF cap, the other 2 positions either add an extra .068uF or 1uF cap. This increases the selectable range quit a bit. I’ve also added a triangle-to-sine converter based on a design by Thomas Henry.

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/SCM-140-ADSR,"# SCM 140 ADSR

This module is a straight up clone of the ADSR module found it the famous Roland System-100m modular synthesizer from the ’70’s. It consists of 2 identical ADSR generators with a positive and a negative output. I’ve build this some years ago, there are other simpler designs out there, however, it was a really good learning experience.

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/Steiner-VCF,"# YUSynth Steiner VCF

Another one of the wonderful VCF designs from YuSynth. The VCF ‘core’ is the same as the design from YuSynth, I just replaced the rotary switch with a ‘digital’ version controlled by a Atmel ATTINY22 and a ADG412 digital SPST switch. This allows you to select 2 more modes, namely HP+BP and LP+BP, next to HP, BP, LP and AP (LP+BP+HP).

The original design:

http://yusynth.net/Modular/EN/STEINERVCF/index-v2.html

## Status: Work in Progress

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype
- todo: final debugging
- todo: rewrite the firmware in pure C",2022-08-12
https://github.com/gerb-ster/TH-555-VCO,"# Thomas Henry 555 VCO

The famous VCO module by Thomas Henry based on the 555 timer IC. This was the first module I ever designed a PCB for, which, in hindsight, was not the smartest move. It took me quit a while to get it bug-free, but now that it works, I love it! Over the years I read a lot of articles and booklets by Thomas Henry and if you’re into this synth diy stuff, I highly recommend checking out him and his work. If you want to build/buy one yourself, there is also an excellent version out there by a guy named Fonitronik, there is a link below.

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/TH-566-VCO,"# Thomas Henry 566 VCO

Some years ago I bought a booklet called ‘making music with the 566’ by Thomas Henry. The first time I read it, I understood maybe 10 percent. Since then, every year or so, I’d read it again, and it started making more and more sense. And last year I decided it was time to try and make a PCB for it. So here it is. I’ve added a wave shaper to turn the triangle wave into a saw wave. I made some minor mistakes in the first PCB design, but these are fixed in the schematics below.

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/TR82-mk2-Kicks,"# TR82 mk2 'Kicks'

Second version of the TR82 project; proper PCB's this time! I've combined the BassDrums and the SnareDrum to 
a single board. I've added a OpAmp buffer to the output and changed the output impedance to a more standard
1k. Next to that I've also added a Gate-to-Trigger circuit based on a design by Ken Stone. It converts the 
rising edge of a Gate signal into a short trigger pulse. By changin the level of the gate/trigger input you
can control how 'hard' you hit the drums. Higher levels means more 'accent'.

The Designs and Mods of the drum voices are the same as in the 'original' TR82.

There is a 'internal trigger' connection added to the mainboard of the PCB. This can be used to hook it up to
a seperate controller board. Maybe :). 

### Links

https://www.gerbster.nl/tr82/  
https://www.elby-designs.com/webtek/cgs/cgs24/cgs24_gatetotrigger.html

## Status: Work in Progress

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- todo: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/TR82-mk2-Sequencer,"# TR82 mk2 'sequencer'

A 16-step sequencer for the TR82 mk2 drum computer.

## Status: Work in Progress

- done: first schematic & board design
- todo: breadboard and test
- todo: design frontpanel
- todo: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/Vulcan-DCO,"# Vulcan DCO

## Status: (Almost) finished*

- done: draw first schematic
- done: breadboard and test
- done: board design
- done: design frontpanel
- done: order PCB and build prototype

*) I only need to finish building a proper faceplate; otherwise this project is done.

## Theory

This DCO module is based on an article written by Alethea Flowers about the Roland Juno series. 
The idea behind this specific DCO is that a (analog) ramp generator is being controlled by 
a (digital) microcontroller. A clock signal comming from the micro controller will control the 
frequency of the ramp generator. The nature of this perticular circuit is such, that an increase of 
the frequency means an decrease of the output amplitude of the generated waveform. To counter 
this effect, a DAC is used to compensate the input voltage. If you want more in-depth information
about this, I would highly recommend you read the above mentioned article. A link is provided below.


## Requirements

For this module I aimed at the following specs

- 1v/oct control voltage (-3v to +5v)
- 8 octave range (A0 to A8)
- Triangle, Saw, Sine, Pulse, Square and Sub-Square outputs
- 1k output impedance
- 10 Vpp around 0 volts output
- PWM control for the Pulse output


## Internals

Since this systems requires a micro controller to control it, I've choosen the ATTiny84 running at 20Mhz
from Microchip for this purpose. It has all peripherals (ADC, timers, SPI) on board and it has a relative
small footprint. For the DAC I've choosen a MCP4921 which is connected to the ATTiny through SPI. To generate
a stabel reference voltage, a TL431 shunt voltage regulator is used. For all other tasks, scaling and shifting
the CV and wave generators, TL082/TL084 op-amps are used.


## General Workings

The control voltage comming in is scaled down by 2 and shifted down to 0v to translate the -3v/+5v into 0v/+4v.
Diode clamps are used to make sure the signal doesn't go below 0v or +5v. This signal is then fed to the 10bit 
ADC of the microchip, which has a AREF of 4v, so 0v has a value of 0, while 4v will have a value of 1024. This 
value is then used to determine the input voltage of the ramp genator using the DAC and the clock signal which 
controls it's frequency. The generated ramp wave is then used to generate the sine, triangle, and pulse waves
using several wave shapers. The square and sub square waves are taken directly from the clock signal comming 
from the microchip.

A simple block diagram outlining the inner workings

![alt text](./images/BlockDiagram-VulcanDCO.png ""Block Diagram"")

## The Microchip

The most interresting part of this system is the microchip controlling the ramp generator. All other parts are
more or less standard building blocks which you will find in various other VCO schematics. As mentioned above,
the microchip uses it's ADC to convert the incoming control voltage to a 10bit value which is used to control
the ramp generator. To do this, every one of these 1024 values needs to be translated to a certain clock 
frequency and an input voltage. Since this is not a linear scale, some math has to be done. I've tried to break
it down in the following sections:

### CV to Frequencies

I've used this table to translate the control voltages to frequencies. I've also added the CV value after shifting
and scaling and the correspondending ADC values. 

| CV | Norm. CV | Note | Hz | ADC Value  |
|---|---|---|---|---|
| -3v | 0.0v | A0 | 27.5 Hz | 0
| -2v | 0.5v | A1 |   55 Hz | 127
| -1v | 1.0v | A2 |  110 Hz | 255
| 0v | 1.5v | A3 |  220 Hz | 383
| 1v | 2.0v | A4 |  440 Hz | 511
| 2v | 2.5v | A5 |  880 Hz | 639
| 3v | 3.0v | A6 | 1760 Hz | 767
| 4v | 3.5v | A7 | 3520 Hz | 895
| 5v | 4.0v | A8 | 7040 Hz | 1023

The formula used here is:

```
frequency = (2 ^ (2 * normalized_control_voltage)) * 27.5
```

### Setting the Clock Frequency

For the clock signal I've used one of the Timers (timer1) of the ATTiny84. It is set up in such a
way that it generates a square wave at pin 7 with a frequency set by manipulating a compare mode
value. If you want more specifics and want to know what registers are set, please check out the 
source code, comments are provided. For every duty cycle of the squeare wave we need 2 pulses, one
to turn the output high, one to turn it low. This formula is used ('F_CPU' is the clock speed of the
CPU (20Mhz) and 'current_prescaler' is the prescaler used in this timer(8)):

```
timer_value = (F_CPU / current_prescaler) / (frequency * 2)
```

For more information about using the timer of an ATTiny84, links are provided below.


### Input Voltage Compensation

As mentioned above, we need to compensate the input voltage of the ramp generator to make sure the
amplitude of the output remains the same. To do this we use a DAC. The DAC generates a voltage between
0v and 4v, using a 12bit value (0-4096). For this we also use the 4.00v shunt voltage regulator as a
voltage reference. This voltage is then inverted and multiplied with a factor 3 to cover a range of
0v to -12v. This way the ramp generator will output a 12vpp ramp between 0v and 12v.

The formula to transform a frequency to the appropriate DAC voltage can be found in the article by
Alethea Flowers, she explains it in much better details then I can do, so if you're interested in that,
the link can be found below.

```
C = 1nF
R = 200k Ohm
Vout = +12v

time = 1 / frequency
Vin = -(C * R * Vout) / time
```

#### However... 

After a lot of testing and doing measurements I found out that this formula works, but it kind of 
starts failing for the lower frequencies. Below 300Hz you will need a higher input voltage to get a consistent
output voltage. To make it easier for myself I've picked an 10Vpp instead of 12Vpp for my measurements. I've
plotted these numbers on a graph and it turned out pretty linear so when i used the formula which came out
this graph, the low end output voltage are much more consistent.

I'm not a mathematician so why or how this formula works, i don't know :-)

```
Vin = (0.697151065 * frequency) + 40.82
```

![alt text](./images/HztoMv-Graph.png ""Hz to mV Graph"")


### Lookup Tables

If you look through the source code you won't find any of the above decribed formulas. This is because 
I've used 2 lookup tables to speed things up. Although the ATTiny84 could do the Math involved, this is 
not really necessary. Every one of the 1024 values coming from ADC will translate into a 'fixed' timer and 
DAC value. So there is no real need to calculate these values on the fly. These lookup tables are basically 
2 giant arrays, 1024 elements in size, which contain all pre-calculated values. These arrays are stored in
the Flash memory of the ATTiny since the normal RAM is to small. 

In the 'utils' folder you can find the ```generator.php``` file which is used to generate those tables and 
code. I've choosen to use PHP for this, because this is my 'native' language :-).


## Waveshapers

The Square and Sub-Square outputs are derived directly from the Clock signal using various op-amps and a 
CD4013 Dual Flip-Flop IC. This schematic is taken from a design for the 'making music with a 566' by
Thomas Henry. Next, the Saw Wave coming out of the Ramp generator is used to shape it into a Triangle wave.
This Triangle wave is then used to generate a Sine wave. The Saw wave is also used to create a Pulse wave.
There is a CV input and pot to control the Pulse Width Modulation. For these wave shapers I've used 
designs from Thomas Henry, Ray Wilson and Yves Usson (YUSynth). Links are found below.


## Calibration

There are 2 trimmerpots that need to be set, in order to calibrate the Ramp Generator Core. 

CV_OFFSET => -1.5v  
VREF_SET => 4.00v  


## Links

<https://blog.thea.codes/the-design-of-the-juno-dco/>  
<https://www.geogebra.org/graphing/mfdyqrfj>  
<https://www.bristolwatch.com/ccs/TL431A.htm>  
<https://github.com/JChristensen/tinySPI>  
<https://github.com/MetreIdeas/ATtinyx4A_ADC_Example>  
<https://eleccelerator.com/avr-timer-calculator/>  
<https://maxembedded.wordpress.com/2011/06/28/avr-timers-timer1/>  
<https://www.nongnu.org/avr-libc/user-manual/pgmspace.html>  
<https://www.arduino.cc/reference/tr/language/variables/utilities/progmem/>  
<https://www.birthofasynth.com/Thomas_Henry/Pages/VCO-1.html>  
<https://yusynth.net/Modular/EN/VCO/index.html>  ",2022-08-12
https://github.com/gerb-ster/WaveFolder,"# WaveFolder

This effect module is based on a design/schematic I found on YouTube, by a guy named Adamski. It’s a nice module to generate some interesting sound effects.

## Status: Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/YUSynth-LFO,"# YUSynth LFO

I'm working on some new modules, one of them is this VC-LFO adaptation from YUSynth. As far as I can determine this is a VCO that generates a saw waveform which is then run thru some wave shapers to create a saw, tri, pulse and sine wave.

I've made some modifications:

- I've powered it with +/- 12v instead of the +/- 15v.
- I've ommited the range switch section, after breadboarding it I found the range with the 1 uF cap quite sufficient.
- I also changed some resistor values to make sure the output is 10vpp around 0v for all waveforms.
- I rearranged the wave shapers a bit; the tri/sine section comes after the saw buffer, this way the tri/sine section outputs a 10vpp around 0v without the need for extra components.

In the YUSynth design there was 1 unused op-amp section; I've used that one to invert the saw into an extra ramp output.

The original design:
http://yusynth.net/Modular/EN/LFO/VC-LFO2.html


## Status: Work in Progress

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- todo: order PCB and build prototype",2022-08-12
https://github.com/gerb-ster/YUSynth-VC-Stereo-Mixer,"# YUSynth-VC-Stereo-Mixer

This is my eurorack adaptation of the Mixout-VC Panner from YUSynth. I've ommited
the headphone out the VU meters. This is done because to compact the boards a bit
and to save space on the panels.

I've broken it up into 3 boards, one VoiceBoard for every channel, a mainboard 
containing the mixer and the Bass & Trebble and finally small daughter board for the
left/right output jacks.

The breaks up of the boards is done in such a way that you can decide for yourself
how many channels/voice you want to add.

The original design:
http://yusynth.net/Modular/EN/MIXOUT/index.html

## Status: (Almost) Finished

- done: first schematic & board design
- done: breadboard and test
- done: design frontpanel
- done: order PCB and build prototype
- todo: build frontpanel",2022-08-12
https://github.com/gerkovink/evaluation,"# Towards a standardized evaluation of imputation routines

This repository is dedicated to the manuscript 'Towards a standardized evaluation of imputation routines', written by Hanne Oberman and Gerko Vink.

The manuscript can be found in the `Manuscript` folder. The reporting guidelines formulated in the manuscript can be found in the `Checklist` folder. Both are also available from [www.gerkovink.com/evaluation](https://www.gerkovink.com/evaluation/).

- Hanne (31-03-2022)",2022-08-12
https://github.com/gerkovink/FUSO2022,"# Kenniscentrum Open Educatie (KOE)

Repository behorende bij de ontwikkeling van een facultaire USO aanvraag in de ronde 2022. 

De aanvraag kan [hier worden gedownload](https://github.com/gerkovink/FUSO2022/blob/main/Formulier%20voor%20aanvragen%20FSO%20projecten%2022-23.doc?raw=true) en [hier worden bekeken](https://github.com/gerkovink/FUSO2022/blob/main/Formulier%20voor%20aanvragen%20FSO%20projecten%2022-23.pdf)


## Links
De volgende onderwijsvoorbeelden worden doorgelinkt in het voorstel:

- [Youtube colleges onder open licentie](https://www.youtube.com/watch?v=1x3HW1RczAo)
- Materialen voor [Supervised Learning and Visualization](https://github.com/gerkovink/slv) en de bijbehorende [course page](https://www.gerkovink.com/slv)
- Materialen voor [Markup Languages and Reproducible Programming in Statistics](https://github.com/gerkovink/markup) en de bijbehorende [course page](https://www.gerkovink.com/markup) en [studentenpagina](https://github.com/gerkovink/markup2021)
- [Kennisclips M&S](https://www.youtube.com/c/MethodologyStatisticsFSWUU)
- [Theory Construction and Statistical Modeling](https://cjvanlissa.github.io/TCSM/)
- Course page voor [Battling the curse of dimensionality](https://uudav.nl)
- [Applied Data Analysis and Visualization](https://adav-course-2021.netlify.app/)
- [Practicum Data Analyse](https://www.youtube.com/watch?v=4jJSm4m9UVc&list=PLOQIOFZl7uYaFtuWgEVIeN97zei5ok5G5)
",2022-08-12
https://github.com/gerkovink/INFOMDA1-2021,"# INFOMDA1 2021 course collaboration

Please submit your work as a pull request to this repo. Your PR should be a `RStudio` project with at least the following **folder** and file (-->) structure for assignments in the corresponding assignments folder

---

**YOURNAMES_Assignment_#**

  --> `Assignment_#.Rmd` <br>
  --> `Assignment_#.html` <br>
  --> `data.*` <br>
  --> `Assignment_#.Rproj`<br>

---

and the following structure for practicals in the corresponding practical folder

---

**YOURNAME_P#**

  --> `P#_yourname.Rmd` <br>
  --> `P#_yourname.html` <br>
  --> `data_if_needed.*` <br>
  --> `P#.Rproj` <br>

---
",2022-08-12
https://github.com/gerkovink/JOS_Unit-nonresponse_and_Inference,"# JOS_Unit-nonresponse_and_Inference
Simulation archive for the manuscript `How to obtain valid inference under unit non-response?'


- The four folders represent the four methods investigated. 
- In each of the folders you can find:

  - The script: '1.Execute.R'. This file starts with loading the required packages

  - The script: '1.Execute.R' also loads the relevant simulation functions, which can all be found in the 
  directory: `\functions`. 

  - Next, it sets the global simulation parameters, and then it loads a seperate
  `.R` script for every simulation condition.  These scripts can all be found in the 
  directory `\Simulation conditions`.

  - Every condition script starts with setting local parameters. The population data is generated by the `createdata.R` script.

- In the simulation function:
For weighting and sample imputation, a sample is drawn from the population dataset.
Next, a part of the data is made missing. For masss imputation and CCA, the
sampling step is skipped and missingness is directly induced. In the next step,
the relevant method is applied, and an outfile is created saving the relevant
output. This process is replicated 1000 times. The outfile containing output of 
the 1000 replications is saved under ""Workspaces/Simulation"".

- With the ""Evaluate.R"" script, we obtain the relevant output from each .Rdata file
from each simulation condition and save it under ""Workspaces/Evaluation"".


All the best, 

Laura, Gerko and Joop
",2022-08-12
https://github.com/gerkovink/markup2020,"<!-- README.md is generated from README.Rmd. Please edit that file -->

# markup 2020 <a href='https://github.com/gerkovink/markup2020'><img src='markup_sticker_SMALL.png' align=""right"" height=""139"" /></a>

Course collaboration for 202000010.

You need to solve this conflict in a generic manner such that your
classmates can also merge their forked branch into this master branch.
So think about a solution on your end.

Gerko

-----
",2022-08-12
https://github.com/gerkovink/markup2021,"<!-- README.md is generated from README.Rmd. Please edit that file -->

# markup 2021 <a href='https://github.com/gerkovink/markup2021'><img src='sticker.png' align=""right"" height=""139"" /></a>

Course collaboration for 202000010.

------------------------------------------------------------------------
",2022-08-12
https://github.com/gerkovink/MarkupLanguages2019,"# MarkupLanguages2019

This is the file to append. Add your name to this list:

- gerko
- Vera
- anne
- pascal
- Naomi
- Gerbrich
- Sanne
- Hanne",2022-08-12
https://github.com/gerkovink/mice-inator,"# mice-inator
What the future would send to protect today's `mice`
",2022-08-12
https://github.com/gerkovink/MICEmeeting,"# MICEmeeting
The mice meeting repo

## How to use this repo
Under projects (one for every member) you can add issues to discuss. Then everything is nicely logged and visible. 
",2022-08-12
https://github.com/gerkovink/miceVignettes,"# miceVignettes

A detailed series of vignettes that walk you through solving realistic inference problems with [`mice`](https://cran.r-project.org/web/packages/mice/index.html).

We suggest going through these vignettes in the following order

- [1. Ad Hoc methods and the mice algorithm](https://www.gerkovink.com/miceVignettes/Ad_hoc_and_mice/Ad_hoc_methods.html)
- [2. Convergence and pooling](https://www.gerkovink.com/miceVignettes/Convergence_pooling/Convergence_and_pooling.html)
- [3. Inspecting how the observed data and missingness are related](https://www.gerkovink.com/miceVignettes/Missingness_inspection/Missingness_inspection.html)
- [4. Passive imputation and post-processing](https://www.gerkovink.com/miceVignettes/Passive_Post_processing/Passive_imputation_post_processing.html)
- [5. Combining inferences](https://www.gerkovink.com/miceVignettes/Combining_inferences/Combining_inferences.html)
- [6. Imputing multi-level data](https://www.gerkovink.com/miceVignettes/Multi_level/Multi_level_data.html)
- [7. Sensitivity analysis with mice](https://www.gerkovink.com/miceVignettes/Sensitivity_analysis/Sensitivity_analysis.html)
- [8. `futuremice`: Wrapper for parallel MICE imputation through futures](https://www.gerkovink.com/miceVignettes/futuremice/Vignette_futuremice.html)
- [9. Multivariate PMM for imputing multiple columns simultaneously](https://www.gerkovink.com/miceVignettes/mpmm/mpmm-vignette.html)

---

All the best,

[Gerko](https://www.gerkovink.com) and [Stef](http://www.stefvanbuuren.nl)

---

Useful links:

- [`mice` on CRAN](https://cran.r-project.org/web/packages/mice/index.html)
- [`mice` on GitHub](https://github.com/stefvanbuuren/mice)
- [`mice` in JStatSoft](https://www.jstatsoft.org/article/view/v045i03)
- [`parlMICE` on GitHub](https://github.com/gerkovink/parlMICE) - [vignette](https://www.gerkovink.com/parlMICE/Vignette_parlMICE.html)
",2022-08-12
https://github.com/gerkovink/OpenScienceFund2022,"# OpenScienceFund2022
See https://www.uu.nl/onderzoek/open-science/open-science-fund

Working title: automatic report generation for incomplete data analysis. 

Deliberately open. 

![](https://www.gerkovink.com/images/pubdom.png)
",2022-08-12
https://github.com/gerkovink/parlMICE,"# parlMICE is now part of `mice` in `R`. 

**There is an improved version of `parlMICE`: `mice::parlmice`. Development of `parlmice` will not continue on `gerkovink/parlMICE`, but rather move to `mice` development. No need to keep them seperated.**

**All the best,**

**[Gerko](https://www.gerkovink.com) and [Rianne](https://github.com/RianneSchouten)**

The latest (development) version of `mice` can be installed by running

```
install.packages(""devtools"")
devtools::install_github(repo = ""stefvanbuuren/mice"")
```

---

A wrapper function that can tremendously speed up the `mice` algorithm in `R`. 

Please refer to [this vignette](https://gerkovink.github.io/parlMICE/Vignette_parlMICE.html) for detailed instructions about the use of `parlMICE`.

---


---

Useful links:

- [`miceVignettes` on GitHub](https://gerkovink.github.io/miceVignettes/)
- [`mice` on CRAN](https://cran.r-project.org/web/packages/mice/index.html)
- [`mice` on GitHub](https://github.com/stefvanbuuren/mice)
- [`mice` in JStatSoft](https://www.jstatsoft.org/article/view/v045i03)
",2022-08-12
https://github.com/gerkovink/Pooling_MI,"DATA ARCHIVE
===

Pooling multiple imputations when the sample happens to be the population

---
This repository contains all necessary files to replicate the simulation study for the manuscript ""Pooling multiple imputations when the sample happens to be the population"" by Gerko Vink and Stef van Buuren. This manuscript can be found at http://arxiv.org/pdf/1409.8542v1.pdf. Software requirements are R (http://www.r-project.org}) and R-package mice (version 2.21, http://cran.r-project.org/web/packages/mice/index.html). 

---

| Files/Folders          | Description   |
| -----------------      | ------------- |
|Execute.R              |R-script to run the simulation study|
|2. Make.table.R        |R-script to extract the tabulated data as presented in the manuscript|
|3. Make.figure.R       |R-script to make the figure as presented in the manuscript|
|Reference PDF.pdf      |The reference pdf titled ""Pooling multiple imputations when the sample happens to be the population"" by Gerko Vink and Stef van Buuren|
|/Functions             |Folder containing the functions used by '1. Execute.R'|
|/Simulation conditions |Folder that contains the simulation conditions executed by '1. Execute.R'|
|/Workspaces            |'Folder containing the final simulated data R-workspace as obtained by running '1. Execute.R' |

For any help with the files in this archive, please contact Gerko Vink (g.vink@uu.nl). 
",2022-08-12
https://github.com/gerkovink/ResearchSeminar_1,"MSBBSS11: Research Seminar
===

Exercise 5

---
This repository might contain useful information for anyone who’d like to pursue a career where reproducibility of code and results is desired. It is designed as a tutorial for the graduate students of the selective master program [**Methodology and Statistics for the Behavioural, Biomedical and Social Sciences**](http://www.uu.nl/masters/en/methodology-and-statistics-behavioural-biomedical-and-social-sciences) in the `MSBBSS11: Research Seminar` course. 

This is by no means a full overview of markdown, GitHub, reproducibility, or anything else, for that matter: the aim is to get you started and allow you to develop a workflow that rocks your boat while simultaneously allowing others to follow your train of thought by simply looking at your project’s documentation. ‘

I’d like to give you some pointers in the form of tips:

---

#### Tip 1 - Work in projects and project folders. 

Most things we (we as in statisticians) do have a representation in digital space. For example, if a client comes to me with a missing data problem I have a very distinct workflow of a) checking and editing the input data, b) designing the missing data models, c) joining these models by means of algorithms to impute the missing data and d) evaluating the joint data. I use scripting languages (such as R) and programming languages (such as C(++) or Python). The code I produce is updated and executed (run) frequently, results are produced and stored on the fly, and the state of the project at timepoint B may be very different from the state at timepoint A. Using a dedicated folder for every project makes tracking the code, the changes to the code and the states of the project over time much more convenient. 

---

#### Tip 2 - Don't make your projects too comprehensive. 
Simple. It's easier to manage a (giant) project if you break it up into smaller parts

---

#### Tip 3 - Document your code
A year from now, when looking at your code, syntax or project, you probably won't be able to remember what you did, what your code does, or why you coded something like you did. Unless you document your code and make it insightful. When coding your project, try to adhere to some coding convention: the [Google style guides](https://github.com/google/styleguide) are very useful resources and provide you with well-designed style conventions on all the major programming and scripting languages. I suggest you study at least the [R Style Guide](https://google.github.io/styleguide/Rguide.xml) as this might be the language you will use most during your thesis.

---

#### Tip 4 - Open a GitHub account
Its simple, free and even the paid version is free when you are a student (i.e. the fee is waived). You can apply for a student discount [here](https://education.github.com/discount_requests/new). There is also a [Student Developer Pack](https://education.github.com/pack) that helps you learn new coding languages and provides you with a comprehensive software toolkit to do so - also free. And you can host your own professional site on [GitHub Pages](https://pages.github.com). Also for free!

Why GitHub? It is a great platform to share your work/projects as open source repositories for others to use or work with. You can work with multiple collaborators on the same project repository and all version history is automatically controlled. So when sh&t hits the fan: you can simply go back in time. If you would like to keep things closed source and work alone or together in a private repository: the paid version (yes - the one that is free as a student) allows for that, too. In short; many developers work together on GitHub and there is a reason why they choose this platform. Also, there are apps for Mac and Windows and its use is extensively documented by GitHub and by the community itself. 

---

#### Tip 5 - Document changes to your code
With GitHub this is really easy - the framework does this for you. After all, it is a repository service build on the version control software [`Git`](https://git-scm.com), created by Linus Torvalds. See for yourself: click this `Readme.MD` file and have a look at the history of the file. If you click on a commit (a change in the document) - you can even compare the committed source to the previous source!

---

#### Tip 6 - Use version control software
If you use GitHub; you're done! congrats. If not, please integrate `Git` into `R-Studio`, or place your `R-Studio` projects into something that does version control, such as [Google Drive](https://www.google.com/drive/), [DropBox](https://www.dropbox.com/), [OneDrive](https://onedrive.live.com), [SurfDrive](https://www.surfdrive.nl/en) or any of the other major cloud-based file hosting platforms around.

---

#### Tip 7 - Learn Markdown
Use [markdown](http://daringfireball.net/projects/markdown/) or [rmarkdown](http://rmarkdown.rstudio.com) to communicate your project to the masses (or clients). It is easy, it allows for weaving your text and code into a single document and it makes the things you do much more reproducible. It exports as anything you'd like - although I would suggest HTML files as they are cross-device readable. Oh, and this `Readme.MD` file is also written in markdown - as well as directly online editable in GitHub! As are most major code files. 

---

#### Tip 8 - Go with the flow
Your own workflow, that is! If you have decided on a foundation for your project workflow, all that rest is a workflow itself. I like to develop all my projects according to [this nested workflow](https://github.com/gerkovink/Pooling_MI), because it results in an effortlessly archivable project folder. But choose whatever floats your boat. 

---

All the best, 

- [Gerko](https://www.gerkovink.com)



",2022-08-12
https://github.com/gerkovink/Xplain,"Welcome! 

This is a minimal example of a book based on R Markdown and **bookdown** (https://github.com/rstudio/bookdown). 

This template provides a skeleton file structure that you can edit to create your book. 

The contents inside the .Rmd files provide some pointers to help you get started, but feel free to also delete the content in each file and start fresh.

Additional resources:

The **bookdown** book: https://bookdown.org/yihui/bookdown/

The **bookdown** package reference site: https://pkgs.rstudio.com/bookdown
",2022-08-12
https://github.com/Gionimo/InteractiveNarrator,"# Interactive Narrator
**Developed @ Utrecht University**

Visualize your user stories to improve understanding of the project, inspect system functionality 
and improve communication among stakeholders.

Please visit [this page](http://interactivenarrator.science.uu.nl) to start using the application.

This application uses the Visual Narrator (https://github.com/marcelrobeer/visualnarrator) to generate a conceptual model which is then
visualized using Vis.js and is presented to the user in the browser from where the visualization can be adapted to cater to the 
user's needs.

![alt tag](https://github.com/Gionimo/InteractiveNarrator/blob/master/Screenshot%20Interactive%20Narrator2.png)


**License**

This product is free to use for commercial and non-commercial purposes.

**How To Start Developing**
1. create a ubuntu virtual machine
open a terminal in /Documents and run:
2. apt install python-pip
3. apt-get install python3-dev
4. apt install virtualenv
5. virtualenv -p python3 inarrator (this will create a separate dev. environment with the name inarrator)
6. in the newly created directory open a terminal and activate 
the virtual enironment with `source bin/activate` (do this everytime you need to install/update something)
7. pip install git
8. pip install -r requirements.txt (this will install all the required packages)
9. python -m spacy download en_core_web_md (download the spacy NLP language model)
 

**Telling Python interpreter where to look for packages**
Open a Linux terminal and navigate to your root directory with `~` and then type `sudo nano .profile`
which should open a file with some lines after which you add to the bottom of this file:

`export PYTHONPATH=$PYTHONPATH:/home/path/on/your/computer/yourvirtualenvironment/:/home/path/on/your/computer/yourvirtualenvironment/VisualNarrator`

If after this Python throws import errors it might not be able to find the packages.
A workaround is to add this line after line 22 in app.py and after line 6 in post.py:
   
   `sys.path.append('/path/on/your/computer/yourvirtualenvironment/VisualNarrator')`
   
   **and**
   
Add after line 6 in app.py:
 
   `# sys.path.append('/home/path/on/your/computer/inarrator')`
    
To tell Python where to look for de VisualNarrator package on your computer.

Notes:
- the project has been prepared for using Flask-Security but isn doing so yet. This is why
some commented code is present.
- message flashing is not working, although some code to start working with it is present
- deleting per sprint is ready to be implemented (see method in app.py) but needs looking into as currently 
deleting entities from one sprint also deletes them while they are actually in other sprints too.
solution: only delete entity (concept) when it is in a use story that is in only one sprints
- downloading the PNG of the network now gives a transparent image. This should be reworked to contain a white 
background. Some experimental code exists in visualization.js
- various print statements exist to help logging during development

TO DO LIST:
- improve rendering performance on mobile devices (change settings for algorithm)
- add clustering (method already in place)
- increase security for users (flask_security module already in place but not in use)
- enable delete per sprint (code already present, just needs tweaking. see app.py delete_sprint())
- add progress bar and hide rendering of the visualization until it's done
- add ceiling AND bottom to the weight filter
- add support for themes and epics
- improve support for touch events
- add more succes confirmations
- add a tutorial
- enable drawing on the visualization on touch devices
- enable downloading of the reports VN generates.
- add detection mechanism for redundencies/inconsistencies/dependencies such as color alerts
",2022-08-12
https://github.com/giovannivarr/Complexity-of-Locally-Fair-Allocations-on-Graphs,"# Complexity of Locally Fair Allocations on Graphs

This repository contains the source code of the experiments we have discussed in the MoL thesis ""Complexity of Locally Fair Allocations on Graphs"".

The file `requirements.txt` contains all the necessary libraries and tools (with the relative version) to run the code. Directory `images` contains all images that were generated from the experiments.

Inside the `code` directory you can find all the code written to perform the experiments. In it is also included the IPython notebook `experiments.ipynb`, which can be used to run experiments similar to those presented in the thesis.


## Installation

1. Clone the repository

```
git clone https://github.com/giovannivarr/Complexity-of-Fair-Allocations-on-Graphs
```

2. Install all dependencies listed in `requirements.txt`

```
pip install -r requirements.txt
```


## Usage

As already mentioned the `experiments.ipynb` notebook contains ready-to-run examples of the experiments we have performed.

In the following sections we will briefly show what can be done with the implemented functions, presenting some use-cases for each file in the `code` directory.


### `graph_generator.py`

In this file we have implemented some functions to generate graphs which were not included in the `NetworkX` library (at least up to the version we have used, i.e. version `2.5`). For example, to generate a line of 5 vertices, it suffices to run the following code:

```
G = generate_line(5)
```


### `utility_generator.py`

This file contains the two functions we have implemented to generate the utility of an agent for each item. Utilities can be generated by drawing samples either from a uniform or a normal distribution.

For the sake of our example, assume that we want to generate the utility function of an agent for `5` items, drawing the values from a uniform distribution which minimum value is `-2` and maximum value `1`. Observe that by how the library `scipy` defines the uniform distribution, we have to provide in this case as inputs `-2` for the `loc` parameter and `3` for the `scale` one (since the maximum value is given by the sum `loc + scale`):

```
u = generate_uniform_utilities_agent_based(-2, 3, 5)
```

Observe that the utility functions' type is `numpy.ndarray`.


### `item_allocation_generator.py`

This file contains the functions which can be used to generate an item allocation. There are four different criteria to define the item allocation (as presented in the thesis), though all are called by a fifth function, `generate_allocation`. In order to compute an allocation we just need a utility profile and a criterion. The possible options for the criterion are the following:

- `max_utilitarian`
- `min_enviness`
- `min_unproportionality`
- `random`

Hence, given a utility profile `u`, to compute an item allocation that minimizes enviness it suffices to run the following code:

```
allocation = generate_allocation(u, 'min_enviness')
```


### `position_assignment_generator.py`

The last file contains various ways to compute position assignments. At this point, we will assume that we have the following variables are set:
- `agents`, i.e. the number of agents;
- `utilities`, i.e. the utility profile (note that these are the utilities for the single items);
- `item_allocation`, i.e. the item allocation;
- `G`, i.e. the graph (which type is `networkx.Graph` and which nodes' type is `int`).

First of all, it is possible to generate all possible position assignments of the agents on the given graph. To do so, we just do the following:

```
assignments = generate_all_assignments(agents, G.nodes())
```

We can also compute the utilities of each assigned bundle for each agent:

```
allocated_utils = compute_utilities(agents, utilities, item_allocation)
```

Now, notice that for each criterion we have defined two functions, one which simply answers whether there is a position assignment that satisfies the criterion (with the fixed item allocation) and one which also finds all of them (if there are any). For the sake of our example, let us consider local envy-freeness. Thanks to what we have done so far, we are now ready to know whether there is a position assignment which satisfies LEF or find it:

```
exists_lef = exists_envy_free_assignment(agents, assignments, allocated_utils, G)
lef_assignments = find_envy_free_assignment(agents, assignments, allocated_utils, G)
```

Observe that for local envy-freeness it is also possible to check whether there is an LEF position assignments using a function which defines an ILP. In this case, as one can imagine, the list of all possible position assignments `assignments` is not needed.

As a final example, we will also show how to use the proofs of Theorems 2 and 8 to find whether there is a position assignment that either satisfies LEF or LEF1 in case the social graph is a tree. Notice that the following three steps, i.e. those before we will effectively show how to call the functions which follow the algorithms of the two proofs, are all performed in these two final functions, thus they can be omitted.

First, we can compute the agent-types. These can either be the EF ones or EF1 ones (see Definitions 21 and 22 respectively) based on the criterion we are considering. For our example, we will again consider local envy-freeness, thus the EF agent-types. To compute the agent-types, it suffices to do the following:

```
ef_agent_types = find_envy_agent_types(agents, allocated_utils)
```

Also, we can compute the vertex-types of the tree (see Definition 20). Notice that the tree must be a directed one (i.e. it must be a `networkx.DiGraph`), as otherwise the function will not work properly as explained in the comment below the definition of such function. In our case, the tree must be directed starting from the root and going down towards the leaves. Given a tree `T`, to find the list it suffices to call the following function:

```
vertex_types = find_vertex_types(T)
```

Given the vertex-types we can compute, for each pair of vertex-types *t*, *t'*, how many vertices of type *t'* are child vertices of a vertex of type *t*. From now on, we denote with `r` the root of the tree. Note that the following function computes this value for any possible pair, and not a single one.

```
vertex_types_edges = find_vertex_types_edges(T, r, vertex_types)
```

As anticipated before, it is possible to check whether there is a position assignment that respects LEF or LEF1 using the two FPT algorithms we provided in the thesis.

```
model_exists_lef = parameterized_tree_lef_position_assignment(T, r, agents,  allocated_utils)
model_exists_lef1 = parameterized_tree_lef1_position_assignment(T, r, agents, allocated_utils, utilities, item_allocation)
```

Notice that the two functions return a MIP model, meaning that they are not booleans telling whether there is or not an assignment (unlike the previous functions we have shown). To know whether the model `m` has a solution (and thus whether there is a position assignment), it suffices to do the following:

```
from mip import OptimizationStatus
m.optimize() == OptimizationStatus.OPTIMAL
```
",2022-08-12
https://github.com/giovannivarr/RetiElaboratori16-17,"# RetiElaboratori16-17

[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-sa/4.0/)

Appunti del corso di Reti degli Elaboratori (AA 2016-2017)

Dentro la cartella src/ trovate il file sorgente .tex con cui è stato generato
il pdf. Correzioni ed aggiunte sono ben gradite!
",2022-08-12
https://github.com/giovannivarr/SynthesisingRMsMARL,"# SynthesisingRMsMARL
Repository containing code of the experiments appearing in the paper ""Synthesising Reward Machines for Cooperative Multi-Agent Reinforcement Learning"".

# How to run the code
There are two possible ways of running our code:
1. By running the file `run.py`, where the variable `experiment` specifies which kind of experiment to perform;
2. By running the file `run_compare.py`, specifying the experiment using always the variable `experiment`. Experiments from these files compare reward machines obtained generated using MCMAS against those crafted by hand from [[1]](#1). 


# Acknowledgements 
Many of the files are originally or have been adapted from other files in the repository at [github.com/cyrusneary/rm-cooperative-marl](https://github.com/cyrusneary/rm-cooperative-marl). We thank the authors for their availability in sharing the code and for their original work in RM-based MARL. 

## References
<a id=""1"">[1]</a> 
Neary, C., Xu, Z., Wu, B., & Topcu, U. (2021, May). 
Reward Machines for Cooperative Multi-Agent Reinforcement Learning. 
In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems (pp. 934-942).
",2022-08-12
https://github.com/gjlbeckers-uu/aglcheck,"========
aglcheck
========

.. contents::

What is aglcheck?
-----------------
*aglcheck* is a python library for analyzing fragment-based, non-grammatical 
similaritites between strings used in artificial grammar learning (AGL) 
experiments. It has lower-level functions to analyze and compare individual
strings, but probably even more useful is its functionality to easily compare
sets of strings and visualize the results in image plots, or in tables that
highlight matching string fragments in color. Further, similarity values
between sets of strings sets are available as data frames for statistical
analyses, for example to test for association with behavioral responses, in
the Python library Pandas_.

.. _Pandas: http://pandas.pydata.org

The goal of *aglcheck* is to enable the fast identification of potential
problems in experimental designs. It can be useful for those who design an
AGL experiment, but also for those who review an AGL manuscript for a
scientific journal or want to evaluate the likelihood of alternative
explanations in published work in which the possibility of fragment-based
similarity may not have been (sufficiently) considered by authors and
reviewers.

*aglcheck* is already useful from this point of view, but it is still being
developed. A future goal is to make it trivially easy to use, perhaps even
without installing a Python environment (i.e. through a Jupyter Notebook
webserver). For now you need a Python environment, though (see below).

Another explicit goal is to make the software very reliable. Users should have
high confidence that the results are correct, given that they are designing
scientific experiments or checking the experimental design or results of
others. A testing framework is in place, and test cases for the most important
algorithms are included. In addition, everything is programmed in Python, and
the code is open source, so it is easy to inspect what is going on under the
hood.

Lastly, *aglcheck*, is easy to extend. With a few lines of Python you can
contribute your own algorithm, which can then be used within a framework that
does string reading and visualization for you.


Example
-------
An example of how visualization can be useful, consider the following strings,
used in Wilson et al. (2015) Nat. Commun. [http://dx.doi.org/10.1038/ncomms9901]:

+-----------+-------+---------+
| type      | label | string  |
+===========+=======+=========+
|           | E1    |  acf    |
|           +-------+---------+
|           | E2    |  acfc   |
|           +-------+---------+
|           | E3    |  acgf   |
|           +-------+---------+
|           | E4    |  acgfc  |
|           +-------+---------+
| Exposure  | E5    |  adcf   |
| strings   +-------+---------+
|           | E6    |  adcfc  |
|           +-------+---------+
|           | E7    |  adcfcg |
|           +-------+---------+
|           | E8    |  adcgf  |
|           +-------+---------+
|           | E9    |  acgfcg |
+-----------+-------+---------+
|           | CT1   |  acgfc  |
| Grammar   +-------+---------+
| Cosistent | CT2   |  adcfcg |
| Test      +-------+---------+
| strings   | CT3   |  acfcg  |
|           +-------+---------+
|           | CT4   |  adcgfc |
+-----------+-------+---------+
|           | VT1   |  afgcd  |
| Grammar   +-------+---------+
| Violating | VT2   |  afcdgc |
| Test      +-------+---------+
| strings   | VT3   |  fadgc  |
|           +-------+---------+
|           | VT4   |  dcafgc |
+-----------+-------+---------+

In this string design, there is a bias in shared maximum fragment between
grammar consistent and gramar violating test string length and corresponding
duration, which is much more easily seen in the image plots below than in the
table above:

.. image:: example_figures/example_fig_sharedchunklength_1.png
    :width: 100%

Quantifications and visualizations can be produced based on a very simple,
human readable and writable text file (yaml format) that lists the strings of
interest, and, optionally, defines categories to be compared and other
information (see example_)

.. _example: https://github.com/gjlbeckers-uu/aglcheck/blob/master/aglcheck/datafiles/wilsonetal_natcomm_2015.yaml

aglcheck can produce HTML tables that highlight in color the specific
similarities between individual strings (e.g., see table_)

.. _table: https://rawgit.com/gjlbeckers-uu/aglcheck/master/example_figures/example_table.html

*aglcheck* was initially written to analyze string sets for potential confounds
based on acoustic similarity in a sample of 9 AGL studies in nonhuman animals
for the scientific paper:

Beckers, G.J.L., Berwick B.C., Okanoya, K. and Bolhuis, J.J. (2016) What do
animals learn in artificial grammar studies? *Neuroscience & Biobehavioral
Reviews* [http://dx.doi.org/10.1016/j.neubiorev.2016.12.021]

See the supplementary information of this paper to see the results of such
analyses: here_.

.. _here: https://rawgit.com/gjlbeckers-uu/aglcheck/master/stimulussets_analyzed/suppl_info_beckers_etal_2016_jneurobiorev_revision2.html

These were produced with version 0.1.0, which is saved as a separate branch on
github. However, the current wider objective is to provide visualization
software that can be used to analyze AGL string set design more generally.


Development status
------------------
This is beta software. It does what it was initially was designed for, and
should also be usable for other applications. The lack of formal documentation
is the biggest hurdle, but there is a tutorial jupyter notebook that should be
sufficient as an example of how to use aglcheck.

A testing framework is in place and test cases for the most important
algorithms are included. To run the test, use 'aglcheck.test()'.

The 0.1.x series is intended to remain compatible with the the jupyter
notebook that produces the supplementary information. The 0.2.x series should
be refactored so that functions and classes are more logically named and
organized for general use.

Contributions in any form are very welcome.


Documentation
-------------
There is no formal documentation yet, but for now the jupyter notebook in the
tutorials_ folder show basic usage.

.. _tutorials: https://github.com/gjlbeckers-uu/aglcheck/tree/master/tutorials


Installation
------------
The *aglcheck* library requires Python 2.7 or 3.5 or higher, and the packages
*numpy*, *matplotlib*, *yaml*, and *pandas*. I recommend the scientific Python
distribution Anaconda_ for easy installation, although it is not required.

.. _Anaconda: https://www.continuum.io/downloads

I also recommend using Jupyter_ Notebook for interactive data science and
reproducible and documented analyses. The tutorial mentioned above is an
example of its use.

.. _Jupyter: https://jupyter.org/index.html


Copyright and License
---------------------
:copyright: Copyright 2016-2017 by Gabriel Beckers, Utrecht University.
:license: 3-Clause Revised BSD License, see LICENSE.txt for details.


Contact
-------
Gabriel Beckers, Utrecht University, https://www.gbeckers.nl
",2022-08-12
https://github.com/growthcharts/bdsreader,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# bdsreader

<!-- badges: start -->

[![Lifecycle:
experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![R-CMD-check](https://github.com/growthcharts/bdsreader/workflows/R-CMD-check/badge.svg)](https://github.com/growthcharts/bdsreader/actions)
<!-- badges: end -->

The `bdsreader` package is a lightweight package that

-   Reads and parses child data coded according to the Basisdataset JGZ
    protocol;
-   Compares the child data against one of two JSON validation schema;
-   Calculates the child’s D-score from the Van Wiechen Schema (DDI)
    responses;
-   Adds Z-scores for height, weight, head circumference, BMI,
    weight-for-height and D-score;
-   Converts the result into a a list with person-level and time-level
    data.

The `bdsreader` translates child data (incoming via an API request) into
a data object useful for `R` processing. The package is part of Joint
Automatic Measurement and Evaluation System (JAMES) developed by the
Netherlands Organisation for Applied Scientific Research TNO.

## Installation

Install the development version `bdsreader` by

``` r
install.packages(""remotes"")
remotes::install_github(""growthcharts/bdsreader"")
```

There is no CRAN release.

## Example

The following commands illustrate the main use of `bdsreader`.

``` r
library(bdsreader)
fn <- system.file(""examples"", ""maria2.json"", package = ""bdsreader"")
tgt <- read_bds(fn)
timedata(tgt)
#> # A tibble: 11 × 8
#>       age xname yname zname zref                        x     y      z
#>     <dbl> <chr> <chr> <chr> <chr>                   <dbl> <dbl>  <dbl>
#>  1 0.0849 age   hgt   hgt_z nl_2012_hgt_female_27  0.0849 38    -0.158
#>  2 0.167  age   hgt   hgt_z nl_2012_hgt_female_27  0.167  43.5   0.047
#>  3 0      age   wgt   wgt_z nl_2012_wgt_female_27  0       0.99  0.19 
#>  4 0.0849 age   wgt   wgt_z nl_2012_wgt_female_27  0.0849  1.25 -0.203
#>  5 0.167  age   wgt   wgt_z nl_2012_wgt_female_27  0.167   2.1   0.015
#>  6 0.0849 age   hdc   hdc_z nl_2012_hdc_female_27  0.0849 27    -0.709
#>  7 0.167  age   hdc   hdc_z nl_2012_hdc_female_27  0.167  30.5  -0.913
#>  8 0.0849 age   bmi   bmi_z nl_1997_bmi_female_nl  0.0849  8.66 -5.72 
#>  9 0.167  age   bmi   bmi_z nl_1997_bmi_female_nl  0.167  11.1  -3.77 
#> 10 0.0849 hgt   wfh   wfh_z nl_2012_wfh_female_   38       1.25 -0.001
#> 11 0.167  hgt   wfh   wfh_z nl_2012_wfh_female_   43.5     2.1   0.326
```

Column `age` holds decimal age for the measurement. Every row contains a
measurement `yname`, the conditioning variable `xname` and the Z-score
`zname`. The column named `zref` holds the name of the growth reference
(as defined in the `nlreference` package) used to calculate the Z-score.
Columns `y`, `x` and `z` store their values, respectively.

The `persondata()` function extracts the person-level information:

``` r
persondata(tgt)
#> # A tibble: 1 × 16
#>      id name      dob        dobf       dobm       src   dnr   sex     gad    ga
#>   <int> <chr>     <date>     <date>     <date>     <chr> <chr> <chr> <dbl> <dbl>
#> 1    -1 fa308134… 2018-10-11 1995-07-04 1990-12-02 1234  <NA>  fema…   189    27
#> # … with 6 more variables: smo <dbl>, bw <dbl>, hgtm <dbl>, hgtf <dbl>,
#> #   agem <dbl>, etn <chr>
```

The result of `read_bds()` feeds into further data processing in `R`.

## Breakdown in steps

### JSON Input Data

The example file `maria2.json` contains Maria’s data coded in JSON
format according to BDS-schema file
[bds_v2.0.json](https://raw.githubusercontent.com/growthcharts/bdsreader/master/inst/schemas/bds_v2.0.json).
Here’s the contents of the file with the child data:

    {
      ""Format"": ""2.0"",
      ""OrganisatieCode"": 1234,
      ""Referentie"": ""Maria2"",
      ""ClientGegevens"": [
        {
          ""ElementNummer"": 19,
          ""Waarde"": ""2""
        },
        {
          ""ElementNummer"": 20,
          ""Waarde"": ""20181011""
        },
        {
          ""ElementNummer"": 82,
          ""Waarde"": 189
        },
        {
          ""ElementNummer"": 91,
          ""Waarde"": ""2""
        },
        {
          ""ElementNummer"": 110,
          ""Waarde"": 990
        },
        {
          ""ElementNummer"": 238,
          ""Waarde"": 1670
        },
        {
          ""ElementNummer"": 240,
          ""Waarde"": 1900
        },
        {
          ""GenesteElementen"": [
            {
              ""ElementNummer"": 63,
              ""Waarde"": ""19950704""
            },
            {
              ""ElementNummer"": 71
            },
            {
              ""ElementNummer"": 62,
              ""Waarde"": ""01""
            }
          ]
        },
        {
          ""GenesteElementen"": [
            {
              ""ElementNummer"": 63,
              ""Waarde"": ""19901202""
            },
            {
              ""ElementNummer"": 71
            },
            {
              ""ElementNummer"": 62,
              ""Waarde"": ""02""
            }
          ]
        }
      ],
      ""ContactMomenten"": [
        {
          ""Tijdstip"": ""20181011"",
          ""Elementen"": [
            {
              ""ElementNummer"": 245,
              ""Waarde"": 990
            }
          ]
        },
        {
          ""Tijdstip"": ""20181111"",
          ""Elementen"": [
            {
              ""ElementNummer"": 235,
              ""Waarde"": 380
            },
            {
              ""ElementNummer"": 245,
              ""Waarde"": 1250
            },
            {
              ""ElementNummer"": 252,
              ""Waarde"": 270
            }
          ]
        },
        {
          ""Tijdstip"": ""20181211"",
          ""Elementen"": [
            {
              ""ElementNummer"": 235,
              ""Waarde"": 435
            },
            {
              ""ElementNummer"": 245,
              ""Waarde"": 2100
            },
            {
              ""ElementNummer"": 252,
              ""Waarde"": 305
            }
          ]
        }
      ]
    }

JSON is a lightweight format to exchange data between electronic
systems. `""ElementNummer""` fields refer to the numbers defined in the
Basisdataset JGZ, whereas `""Waarde""` fields contain the value. The
element numbers and the value correspond to the Basisdataset JGZ, which
you can find
[here](https://www.ncj.nl/themadossiers/informatisering/basisdataset/documentatie/).

### Read and parse input data

### Validate input data

### Age calculation

### D-score calculation

### Z-score calculation

### Structure of result

## Further information
",2022-08-12
https://github.com/growthcharts/brokenstick,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# brokenstick

<!-- badges: start -->

[![Lifecycle:
stable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html#stable)
[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/brokenstick)](https://cran.r-project.org/package=brokenstick)
[![](https://img.shields.io/badge/github%20version-2.2.0-orange.svg)](https://growthcharts.org/brokenstick/)

<!-- badges: end -->

The broken stick model describes a set of individual curves by a linear
mixed model using second-order linear B-splines. The main use of the
model is to align irregularly observed data to a user-specified grid of
break ages.

All fitting can done in the Z-score scale, so nonlinearities and
irregular data can be treated as separate problems. This package
contains functions for fitting a broken stick model to data, for
exporting the parameters of the model for independent use outside this
package, and for predicting broken stick curves for new data.

## Installation

Install the `brokenstick` package from CRAN as follows:

``` r
install.packages(""brokenstick"")
```

The latest version (revision branch) can be installed from GitHub as
follows:

``` r
install.packages(""remotes"")
remotes::install_github(""growthcharts/brokenstick"")
```

## Overview

The *broken stick model* describes a set of individual curves by a
linear mixed model using linear B-splines. The model can be used

-   to smooth growth curves by a series of connected straight lines;
-   to align irregularly observed curves to a common age grid;
-   to create synthetic curves at a user-specified set of break ages;
-   to estimate the time-to-time correlation matrix;
-   to predict future observations.

The user specifies a set of break ages at which the straight lines
connect. Each individual obtains an estimate at each break age, so the
set of estimates of the individual form a smoothed version of the
observed trajectory.

The main assumptions of the broken stick model are:

-   The trajectory between the break ages follows a straight line, and
    is generally not of particular interest;
-   Broken stick estimates follow a common multivariate normal
    distribution;
-   Missing data are missing at random (MAR);
-   Individuals are exchangeable and uncorrelated.

In order to conform to the assumption of multivariate normality, the
user may fit the broken stick model on suitably transformed data that
yield the standard normal
(![Z](https://latex.codecogs.com/png.image?%5Cdpi%7B110%7D&space;%5Cbg_white&space;Z ""Z""))
scale. Unique feature of the broken stick model are:

-   *Modular*: Issues related to non-linearity of the growth curves in
    the observed scale can be treated separately, i.e., outside the
    broken stick model;
-   *Local*: A given data point will contribute only to the estimates
    corresponding to the closest break ages;
-   *Exportable*: The broken stick model can be exported and reused for
    prediction for new data in alternative computing environments.

The `brokenstick` package contains functions for

-   Fitting the broken stick model to data,
-   Plotting individual trajectories,
-   Predicting broken stick estimates for new data.

## Resources

### Background

1.  I took the name *broken stick* from Ruppert, Wand, and Carroll
    ([2003](#ref-ruppert2003)), page 59-61, but it is actually much
    older.
2.  As far as I know, de Kroon et al. ([2010](#ref-dekroon2010)) is the
    first publication that uses the broken stick model without the
    intercept in a mixed modelling context. See [The Terneuzen birth
    cohort: BMI changes between 2 and 6 years correlate strongest with
    adult
    overweight](https://stefvanbuuren.name/publications/2010%20TBC%20Overweight%20-%20PLoS%20ONE.pdf).
3.  The model was formally defined and extended in [Flexible Imputation
    of Missing Data (second
    edition)](https://stefvanbuuren.name/fimd/sec-rastering.html#sec:brokenstick).
    See van Buuren ([2018](#ref-vanbuuren2018)).
4.  The evaluation by Anderson et al. ([2019](#ref-anderson2019))
    concluded:

> > We recommend the use of the brokenstick model with standardised
> > Z‐score data. Aside from the accuracy of the fit, another key
> > advantage of the brokenstick model is that it is easier to fit and
> > provides easily interpretable estimates of child growth
> > trajectories.

### Instructive materials

-   [Companion site](https://growthcharts.org/brokenstick/) contains
    vignettes and articles that explain the model and the use of the
    software;
-   Paper in preparation: *Broken Stick Model for Irregular Longitudinal
    Data*:
    [html](https://growthcharts.org/brokenstick/articles/manual/manual.html).

### References

<div id=""refs"" class=""references csl-bib-body hanging-indent"">

<div id=""ref-anderson2019"" class=""csl-entry"">

Anderson, C., R. Hafen, O. Sofrygin, L. Ryan, and HBGDki Community.
2019. “Comparing Predictive Abilities of Longitudinal Child Growth
Models.” *Statistics in Medicine* 38 (19): 3555–70.

</div>

<div id=""ref-dekroon2010"" class=""csl-entry"">

de Kroon, M. L. A., C. M. Renders, J. P. van Wouwe, S. van Buuren, and
R. A. Hirasing. 2010. “The Terneuzen Birth Cohort: BMI Changes Between 2
and 6 Years Correlate Strongest with Adult Overweight.” *PloS ONE* 5
(2): e9155.

</div>

<div id=""ref-ruppert2003"" class=""csl-entry"">

Ruppert, D., M. P. Wand, and R. J. Carroll. 2003. *Semiparametric
Regression*. Cambridge: Cambridge University Press.

</div>

<div id=""ref-vanbuuren2018"" class=""csl-entry"">

van Buuren, S. 2018. *Flexible Imputation of Missing Data. 2nd Edition*.
Boca Raton, FL: CRC Press.

</div>

</div>
",2022-08-12
https://github.com/growthcharts/centile,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# centile

<!-- badges: start -->

[![Lifecycle:
experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![CRAN
status](https://www.r-pkg.org/badges/version/centile)](https://CRAN.R-project.org/package=centile)
[![R-CMD-check](https://github.com/growthcharts/centile/workflows/R-CMD-check/badge.svg)](https://github.com/growthcharts/centile/actions)
<!-- badges: end -->

The `centile` package

-   Defines the reference interchange format (RIF), a simple text format
    for exchanging growth references;
-   Provides tools for reading and validating RIF files;
-   Names and loads references as `R` objects;
-   Converts between measurements, Z-scores and centiles;
-   Contains built-in WHO Multicentre Growth Standard for height,
    weight, head circumference and body mass index.

The currently supported distributions are normal (`NO`), Lambda-Mu-Sigma
(`LMS`), Box-Cox Green Cole (`BCCG`), Box-Cox Power Exponential (`BCPE`)
and Box-Cox t (`BCT`).

## Installation

You can install the development version of `centile` with:

``` r
remotes::install_github(""growthcharts/centile"")
```

## Examples
",2022-08-12
https://github.com/growthcharts/chartbox,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# chartbox

<!-- badges: start -->

[![R-CMD-check](https://github.com/growthcharts/chartbox/workflows/R-CMD-check/badge.svg)](https://github.com/growthcharts/chartbox/actions)
<!-- badges: end -->

The `chartbox` package stores empty Dutch growth charts. With the
package you can

-   list the available growth charts;
-   set the color palette;
-   load one of the growth charts for further processing.

## Installation

The following statements will install the `chartbox` package

``` r
install.packages(""remotes"")
remotes::install_github(""growthcharts/chartbox"")
```

## Example

The `load_chart()` function makes the stored growth charts available for
further processing. Here’s an example that writes chart `PJAAN25` to a
PDF file.

``` r
library(chartbox)

# choose and load chart
chartcode <- ""PJAAN25""
g <- load_chart(chartcode)

# set the correct color palette
pop <- chartcatalog::parse_chartcode(chartcode)$population
old_pal <- palette(palettes[pop, ])

# create the pdf
pdf(paste(chartcode, ""pdf"", sep = "".""), height = 29.7/2.54, width = 21/2.54)
grid::grid.draw(g)
dev.off()
#> quartz_off_screen 
#>                 2

# restore palette
palette(old_pal)
```
",2022-08-12
https://github.com/growthcharts/chartcatalog,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# chartcatalog

<!-- badges: start -->

[![R-CMD-check](https://github.com/growthcharts/chartcatalog/workflows/R-CMD-check/badge.svg)](https://github.com/growthcharts/chartcatalog/actions)
<!-- badges: end -->

The `chartcatalog` package contains tools to

-   Lookup available charts in JAMES;
-   Create and parse chart codes;
-   Find outcomes, chart group and growth reference;
-   Obtain viewport number and transformation functions;
-   Lookup break points from the brokenstick model.

## Installation

You can install the development version `chartcatalog` by

``` r
install.packages(""remotes"")
remotes::install_github(""growthcharts/chartcatalog"")
```

There is no release on CRAN.

## Example 1: Available charts in JAMES

The current catalogs holds 388 Dutch charts.

``` r
library(chartcatalog)
head(ynames_lookup)
#>   chartgrp chartcode yname vpn vp                         reference
#> 1   nl2010      DJAA   hdc   3  A clopus::nl2009[[""nl2009.mhdcDS""]]
#> 2   nl2010      DJAA   hgt   4  B clopus::nl2009[[""nl2009.mhgtDS""]]
#> 3   nl2010      DJAA   wgt   5  C   clopus::nl1980[[""nl1980.mwgt""]]
#> 4   nl2010      DJAH   hgt   1  A clopus::nl2009[[""nl2009.mhgtDS""]]
#> 5   nl2010      DJAO   hdc   1  A clopus::nl2009[[""nl2009.mhdcDS""]]
#> 6   nl2010      DJAW   wgt   1  A   clopus::nl1980[[""nl1980.mwgt""]]
#>                   tx            ty seq
#> 1 function(x) x * 12 function(y) y  tr
#> 2 function(x) x * 12 function(y) y  tr
#> 3 function(x) x * 12 function(y) y  tr
#> 4 function(x) x * 12 function(y) y  tr
#> 5 function(x) x * 12 function(y) y  tr
#> 6 function(x) x * 12 function(y) y  tr
length(unique(ynames_lookup$chartcode))
#> [1] 388
```

The charts are subdivided into three groups:

``` r
unique(ynames_lookup$chartgrp)
#> [1] ""nl2010""  ""preterm"" ""who""
```

There are five outcomes, but not every outcome (`yname`) appears in all
chart groups:

``` r
with(ynames_lookup, table(chartgrp, yname))
#>          yname
#> chartgrp  bmi dsc hdc hgt wfh wgt
#>   nl2010   20   4  56  64  40  24
#>   preterm   0  48  48  96   0  96
#>   who       0   0   4   8   4   4
```

## Example 2: Chartcode creating and parsing

A `chartcode` identifies the type of growth chart. The code is a
combination of 4–7 alphanumeric characters. The `create_chartcode()` and
`parse_chartcode()` functions can be used to obtain information from the
codes. For example,

``` r
parse_chartcode(c(""HJAA""))
#> $population
#> [1] ""HS""
#> 
#> $sex
#> [1] ""male""
#> 
#> $design
#> [1] ""A""
#> 
#> $side
#> [1] ""front""
#> 
#> $language
#> [1] ""dutch""
#> 
#> $week
#> [1] """"
```

shows that chart with code `""HJAA""` identifies the front of the chart
for Hindustan boys living in the Netherlands, design A (A4 size, 0-15
months).

## Example 3: Find outcomes, chart group and growth reference

If we have a chart code, we may find its chart group and outcomes as

``` r
get_chartgrp(""PJEAN26"")
#>   PJEAN26 
#> ""preterm""
get_ynames(""PJEAN26"")
#> PJEAN26 PJEAN26 
#>   ""hgt""   ""wgt""
```

We obtain the call to the `clopus` package to the post-natal growth
reference of weight of preterms born at a gestational age of 26 week by

``` r
call <- get_reference_call(""PJEAN26"", yname = ""wgt"")
call
#> [1] ""clopus::preterm[[\""pt2012a.mwgt26\""]]""
```

This call can be stored as a shortcut to the reference. Use
`eval(parse(text = call))` to execute the call. Alternatively, if you
have `clopus` installed, we may obtain the reference directly by

``` r
tb <- get_reference(""PJEAN26"", yname = ""wgt"")
slotNames(tb)
#> [1] ""table"" ""info""
head(data.frame(tb@table@table))
#>        x    L     M     S
#> 1 0.0000 1.09 0.985 0.206
#> 2 0.0027 1.08 0.971 0.206
#> 3 0.0055 1.08 0.960 0.207
#> 4 0.0082 1.07 0.950 0.207
#> 5 0.0110 1.07 0.943 0.208
#> 6 0.0137 1.06 0.937 0.208
```

More details can be found in the `info` slot:

``` r
tb@info
#> An object of class ""referenceInfo""
#> Slot ""n"":
#> [1] 4
#> 
#> Slot ""i"":
#> [1] 2
#> 
#> Slot ""j"":
#> [1] 2
#> 
#> Slot ""code"":
#> [1] ""mpt2012a""
#> 
#> Slot ""country"":
#> [1] ""NL""
#> 
#> Slot ""year"":
#> [1] 2012
#> 
#> Slot ""sex"":
#> [1] ""male""
#> 
#> Slot ""sub"":
#> character(0)
#> 
#> Slot ""yname"":
#> [1] ""wgt""
#> 
#> Slot ""dist"":
#> [1] ""LMS""
#> 
#> Slot ""mnx"":
#> [1] 0
#> 
#> Slot ""mxx"":
#> [1] 4
#> 
#> Slot ""source"":
#> [1] ""Bocca-Tjeertes 2012""
#> 
#> Slot ""public"":
#> character(0)
#> 
#> Slot ""remark"":
#> [1] ""Bemerkung =\t\t\t\t\t\t\t\t\t\t\t\t\t""
#> 
#> Slot ""date"":
#> [1] ""2021-01-27 16:33:21 CET""
```
",2022-08-12
https://github.com/growthcharts/chartplotter,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# chartplotter

<!-- badges: start -->

[![R-CMD-check](https://github.com/growthcharts/chartplotter/workflows/R-CMD-check/badge.svg)](https://github.com/growthcharts/chartplotter/actions)
<!-- badges: end -->

The goal of `chartplotter` is to

-   plot the child’s growth curves on pre-defined charts;
-   plot two (visit) lines indicating a present and a future visit;
-   find matches from donor data similar to the target child;
-   predict the most likely future visit value;
-   represent the inherent uncertainty of the prediction.

## Installation

You need access to `growthcharts/donorloader` in order to install
`chartplotter`.

The following statements will install the `chartplotter` package, as
well as any of its dependencies:

``` r
install.packages(""remotes"")
remotes::install_github(""growthcharts/chartplotter"")
remotes::install_github(""growthcharts/jamesdemodata"")
```

## Example

The main function in `chartplotter` is `process_chart()`.

### Example 1: Plot child’s data onto a growth chart

``` r
library(chartplotter)
library(bdsreader)
library(svglite)
fn <- system.file(""extdata"", ""bds_v2.0"", ""smocc"", ""Laura_S.json"", package = ""jamesdemodata"")
tgt <- bdsreader::read_bds(fn)
svglite::svglite(file = ""figures/chart1.svg"", height = 29.7/2.54, width = 21/2.54)
g <- process_chart(tgt, chartcode = ""NMBA"")
grid::grid.draw(g)
dev.off()
#> quartz_off_screen 
#>                 2
```

<img src=""figures/chart1.svg"" title=""Dutch girls, 0-4 years"" alt=""Dutch girls, 0-4 years"" width=""100%"" style=""display: block; margin: auto;"" />

### Example 2: Predict height at 3y9m when child is 3 months

Suppose the baby is 3 months old, and that we want to predict the future
child’s height at the age of 3y9m. The following examples finds 25
matches to the child, and plots the observed curves of those matches as
grey curves.

The blue line indicates the predicted height at age 3y9m. The variation
between the grey curves at age 3y9m indicates the amount of uncertainty
of the prediction.

``` r
set.seed(61771)
svglite(file = ""figures/chart2.svg"", height = 29.7/2.54, width = 21/2.54)
g <- process_chart(tgt, chartcode = ""NMBA"", dnr = ""2-4"", period = c(0.25, 3.75), 
                   nmatch = 25, show_future = TRUE)
grid::grid.draw(g)
dev.off()
#> quartz_off_screen 
#>                 2
```

<img src=""figures/chart2.svg"" title=""Dutch girls, 0-4 years"" alt=""Dutch girls, 0-4 years"" width=""100%"" style=""display: block; margin: auto;"" />

### Example 3: Predict height at 3y9m when child is 2 years

Same as before, but now using all data up to (but not beyond) the age of
2 years. The variation between the grey curves at age 3y9m is now much
smaller.

``` r
svglite(file = ""figures/chart3.svg"", height = 29.7/2.54, width = 21/2.54)
g <- process_chart(tgt, chartcode = ""NMBA"", dnr = ""2-4"", period = c(2.0, 3.75), 
                   nmatch = 25, show_future = TRUE)
grid::grid.draw(g)
dev.off()
#> quartz_off_screen 
#>                 2
```

<img src=""figures/chart3.svg"" title=""Dutch girls, 0-4 years"" alt=""Dutch girls, 0-4 years"" width=""100%"" style=""display: block; margin: auto;"" />

### Example 4: Square plot of height, chart `NMBH`

``` r
svglite(file = ""figures/chart4.svg"", height = 18/2.54, width = 18/2.54)
g <- process_chart(tgt, chartcode = ""NMBH"", quiet = FALSE, dnr = ""2-4"",
                   period = c(2.0, 3.75), nmatch = 25,
                   show_future = TRUE, show_realized = TRUE)
#> chartcode:  NMBH
grid::grid.draw(g)
dev.off()
#> quartz_off_screen 
#>                 2
```

<img src=""figures/chart4.svg"" title=""Dutch girls, 0-4 years"" alt=""Dutch girls, 0-4 years"" width=""100%"" style=""display: block; margin: auto;"" />
",2022-08-12
https://github.com/growthcharts/curvematching,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# curvematching

<!-- badges: start -->

[![R-CMD-check](https://github.com/growthcharts/curvematching/workflows/R-CMD-check/badge.svg)](https://github.com/growthcharts/curvematching/actions)
<!-- badges: end -->

The goal of `curvematching` is to …

## Installation

You can install the development version from
[GitHub](https://github.com/) with:

``` r
# install.packages(""devtools"")
devtools::install_github(""growthcharts/curvematching"")
```
",2022-08-12
https://github.com/growthcharts/growthscreener,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# growthscreener

<!-- badges: start -->
<!-- badges: end -->

The `growthscreener` package implements tools to evaluate child growth
with respect to Dutch criteria for unusual growth. Application of these
tools helps to identify children that meet criteria for criteria for
referral from youth health care (JGZ) for follow-up with a general
physician or pediatrician.

The current version implements Dutch guidelines for

-   height
-   weight
-   head circumference

## Installation

The following statements will install the `growthscreener` package

``` r
install.packages(""remotes"")
remotes::install_github(""growthcharts/growthscreener"")
```

## Example

Find the advice for a very short girl:

``` r
library(growthscreener)
#> Loading required package: nlreferences

# a very short girl, 4 months old
msgcode <- calculate_advice_hgt(sex = ""female"", bw = 3250, ga = 40, dom1 = 134, y1 = 55)
msgcode
#> [1] 1045
cat(fold(msg(msgcode)))
#> Het advies volgens de JGZ-richtlijn lengtegroei is als volgt: Verwijzen naar
#> huisarts/kinderarts, omdat de lengte SDS < -3 is en het geboortegewicht >= 2500
#> gram is.
```

The height SDS at the age of about 4 months is equal to -3.255, which is
the reason for referral.

## Background

The package implements the following guidelines:

-   **JGZ-Richtlijn Lengtegroei 2019**:
    <https://www.ncj.nl/richtlijnen/alle-richtlijnen/richtlijn/lengtegroei-2019>
-   **JGZ-Richtlijn Overgewicht 2012**:
    <https://www.ncj.nl/richtlijnen/alle-richtlijnen/richtlijn/overgewicht>
-   **JGZ-Richtlijn Ondergewicht 2019**:
    <https://www.ncj.nl/richtlijnen/alle-richtlijnen/richtlijn/ondergewicht-2019>
-   **Beslisboom Hoofdomtrek**: A decision tree for head circumference
    for children below the age of 1 year.

There are 45 different messages for height, 26 messages for weight and
17 messages for head circumference. To see them all:

``` r
messages
```
",2022-08-12
https://github.com/growthcharts/james,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

## Overview

The Joint Automatic Measurement and Evaluation System (JAMES) is a web
service for creating and interpreting charts of child growth and
development. The service aids in monitoring and evaluating childhood
growth. The current version

1.  provides access to high-quality growth charts used by the Dutch
    youth health care;
2.  interchanges data coded according to the Basisdataset JGZ;
3.  screens for abnormal height, weight and head circumference;
4.  converts developmental data into the D-score;
5.  predicts future growth and development.

JAMES is a REST API that runs on a remote host. The following sections
illustrate how a client can make requests to JAMES using R and bash. In
principle, any `HTTP` client will work with JAMES. The document provides
pointers to relevant background information.

## JAMES R Package

The JAMES R package defines the end points of the JAMES REST API.

JAMES is created and maintained by TNO, the Netherlands Organisation for
Applied Scientific Research. Please contact Stef van Buuren
\<stef.vanbuuren at tno.nl> for further information.

## Primary JAMES user functionality

| Verb | API Endpoint stem        | Description                             | Maps to `james` function |
|:-----|:-------------------------|:----------------------------------------|:-------------------------|
| POST | `/data/upload/{dfm}`     | Upload child data                       | `upload_data()`          |
| POST | `/charts/draw/{ffm}`     | Draw child data on growth chart         | `draw_chart()`           |
| POST | `/charts/list/{dfm}`     | List available growth charts            | `list_charts()`          |
| POST | `/charts/validate/{dfm}` | Validate a chart code                   | `validate_chartcode()`   |
| POST | `/screeners/apply/{dfm}` | Apply growth screeners to child data    | `apply_screeners()`      |
| POST | `/screeners/list/{dfm}`  | List available growth screeners         | `list_screeners()`       |
| POST | `/site/request/{dfm}`    | Request personalised site               | `request_site()`         |
| POST | `/blend/request/{sfm}`   | Obtain a blend from multiple end points | `request_blend()`        |
| POST | `/version/{dfm}`         | Obtain version information              | `version()`              |
| GET  | `/site`                  | Display personalised site               |                          |
| GET  | `/{session}/{info}`      | Extract session details                 |                          |
| GET  | `/{2}/{1}/man`           | Consult R help                          | `help({1}_{2})`          |

The table lists the defined API end points and the mapping to each end
point to the corresponding R function.

## Resources

| Description                                                                                            | Version |
|:-------------------------------------------------------------------------------------------------------|:--------|
| [JAMES demo](https://tnochildhealthstatistics.shinyapps.io/james_tryout/)                              | 1.2.0   |
| [Example requests]()                                                                                   | 1.2.0   |
| [JAMES Swagger](https://app.swaggerhub.com/apis-docs/stefvanbuuren/james)                              | 1.2.0   |
| [JSON data schema](https://github.com/growthcharts/bdsreader/blob/master/inst/schemas/bds_v2.0.json)   | 2.0     |
| [Source files](https://github.com/growthcharts/james)                                                  | current |
| [JAMES issue tracker](https://github.com/growthcharts/james/issues)                                    | current |
| [Basisdataset JGZ](https://www.ncj.nl/themadossiers/informatisering/basisdataset/documentatie/?cat=13) | current |
",2022-08-12
https://github.com/growthcharts/jamesclient,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# jamesclient

<!-- badges: start -->

[![R-CMD-check](https://github.com/growthcharts/jamesclient/workflows/R-CMD-check/badge.svg)](https://github.com/growthcharts/jamesclient/actions)
<!-- badges: end -->

The goal of `jamesclient` is to facilitate interaction for `R` users
with the **Joint Automatic Measurement and Evaluation System (JAMES)**.
JAMES is an online resource for creating growth charts and analysing
growth curves.

## Installation

You can install the development version from
[GitHub](https://github.com/) with:

``` r
install.packages(""remotes"")
remotes::install_github(""growthcharts/jamesclient"")
```

## Example

The primary functions are:

| Function             | Description                                     |
|----------------------|-------------------------------------------------|
| `james_post`         | Send POST request to JAMES                      |
| `james_get`          | Send GET request to JAMES                       |
| `inspect_demodata()` | Upload demo child data and download parsed file |

### `james_post()`

Upload BDS data

``` r
library(jamesclient)
fn <- system.file(""extdata"", ""allegrosultum"", ""client3.json"", 
                  package = ""jamesdemodata"", mustWork = TRUE)
host <- ""https://james.groeidiagrammen.nl""
r1 <- james_post(host = host, path = ""data/upload/json"", txt = fn)
r1$parsed
#> $psn
#>   id                                 name        dob       dobf       dobm  src
#> 1 -1 fa308134-069e-49ce-9847-ccdae380ed6f 2018-10-11 1995-07-04 1990-12-02 1234
#>      sex gad ga smo  bw hgtm hgtf agem etn
#> 1 female 189 27   1 990  167  190   27  NL
#> 
#> $xyz
#>       age xname yname zname                  zref       x     y      z
#> 1  0.0849   age   hgt hgt_z nl_2012_hgt_female_27  0.0849 38.00 -0.158
#> 2  0.1670   age   hgt hgt_z nl_2012_hgt_female_27  0.1670 43.50  0.047
#> 3  0.0849   age   wgt wgt_z nl_2012_wgt_female_27  0.0849  1.25 -0.203
#> 4  0.1670   age   wgt wgt_z nl_2012_wgt_female_27  0.1670  2.10  0.015
#> 5  0.0849   age   hdc hdc_z nl_2012_hdc_female_27  0.0849 27.00 -0.709
#> 6  0.1670   age   hdc hdc_z nl_2012_hdc_female_27  0.1670 30.50 -0.913
#> 7  0.0849   age   bmi bmi_z nl_1997_bmi_female_nl  0.0849  8.66 -5.719
#> 8  0.1670   age   bmi bmi_z nl_1997_bmi_female_nl  0.1670 11.10 -3.767
#> 9  0.0849   hgt   wfh wfh_z   nl_2012_wfh_female_ 38.0000  1.25 -0.001
#> 10 0.1670   hgt   wfh wfh_z   nl_2012_wfh_female_ 43.5000  2.10  0.326
#> 11 0.0000   age   wgt wgt_z nl_2012_wgt_female_27  0.0000  0.99  0.190
```

### `james_get()`

Get output from R console (just for checking)

``` r
r2 <- james_get(host = host, path = paste(r1$session, ""console"", sep = ""/""))
cat(r2$parsed, ""\n"")
#> > upload_data(txt = c(""{\""Referentie\"":\""fa308134-069e-49ce-9847-ccdae380ed6f\"",\""OrganisatieCode\"":1234,\""ClientGegevens\"":{\""Elementen\"":[{\""Bdsnummer\"":19,\""Waarde\"":\""2\""},{\""Bdsnummer\"":20,\""Waarde\"":\""20181011\""},{\""Bdsnummer\"":82,\""Waarde\"":\""189\""},{\""Bdsnummer\"":91,\""Waarde\"":\""1\""},{\""Bdsnummer\"":110,\""Waarde\"":\""990\""},{\""Bdsnummer\"":238,\""Waarde\"":\""1670\""},{\""Bdsnummer\"":240,\""Waarde\"":\""1900\""}],\""Groepen\"":[{\""Elementen\"":[{\""Bdsnummer\"":63,\""Waarde\"":\""19950704\""},{\""Bdsnummer\"":71,\""Waarde\"":\""6030\""},{\""Bdsnummer\"":62,\""Waarde\"":\""01\""}]},{\""Elementen\"":[{\""Bdsnummer\"":63,\""Waarde\"":\""19901202\""},{\""Bdsnummer\"":71,\""Waarde\"":\""6030\""},{\""Bdsnummer\"":62,\""Waarde\"":\""02\""}]}]},\""Contactmomenten\"":[{\""Tijdstip\"":\""20181111\"",\""Elementen\"":[{\""Bdsnummer\"":235,\""Waarde\"":\""380\""},{\""Bdsnummer\"":245,\""Waarde\"":\""1250\""},{\""Bdsnummer\"":252,\""Waarde\"":\""270\""}]},{\""Tijdstip\"":\""20181211\"",\""Elementen\"":[{\""Bdsnummer\"":235,\""Waarde\"":\""435\""},{\""Bdsnummer\"":245,\""Waarde\"":\""2100\""},{\""Bdsnummer\"":252,\""Waarde\"":\""305\""}]}]}""))
#> List of length  2 
#> [psn]
#> [xyz]
```

For other end points, see <https://james.groeidiagrammen.nl>.

### `inspect_demodata()`

``` r
library(jamesclient)
data <- inspect_demodata(name = ""Anne_S"", ""smocc"", format = ""1.0"")
data
#> $psn
#>   id   name        dob dobf       dobm src  dnr    sex gad ga smo   bw hgtm
#> 1 -1 Anne S 1989-01-31 <NA> 1961-08-01   0 <NA> female 283 40   0 3300  172
#>   hgtf agem etn
#> 1  183   27  NL
#> 
#> $xyz
#>       age xname yname zname                   zref       x     y      z
#> 1  0.0000   age   hgt hgt_z  nl_1997_hgt_female_nl  0.0000 51.00  0.052
#> 2  0.0986   age   hgt hgt_z  nl_1997_hgt_female_nl  0.0986 54.70  0.145
#> 3  0.1369   age   hgt hgt_z  nl_1997_hgt_female_nl  0.1369 56.00  0.114
#> 4  0.2327   age   hgt hgt_z  nl_1997_hgt_female_nl  0.2327 59.50  0.206
#> 5  0.5010   age   hgt hgt_z  nl_1997_hgt_female_nl  0.5010 68.00  0.661
#> 6  0.7885   age   hgt hgt_z  nl_1997_hgt_female_nl  0.7885 73.00  0.498
#> 7  0.9610   age   hgt hgt_z  nl_1997_hgt_female_nl  0.9610 75.50  0.375
#> 8  1.2485   age   hgt hgt_z  nl_1997_hgt_female_nl  1.2485 80.00  0.434
#> 9  1.5140   age   hgt hgt_z  nl_1997_hgt_female_nl  1.5140 83.50  0.449
#> 10 1.9740   age   hgt hgt_z  nl_1997_hgt_female_nl  1.9740 89.50  0.731
#> 11 0.0000   age   wgt wgt_z  nl_1997_wgt_female_nl  0.0000  3.30 -0.105
#> 12 0.0986   age   wgt wgt_z  nl_1997_wgt_female_nl  0.0986  4.10 -0.280
#> 13 0.1369   age   wgt wgt_z  nl_1997_wgt_female_nl  0.1369  4.52 -0.123
#> 14 0.2327   age   wgt wgt_z  nl_1997_wgt_female_nl  0.2327  5.64  0.350
#> 15 0.5010   age   wgt wgt_z  nl_1997_wgt_female_nl  0.5010  7.95  0.723
#> 16 0.7885   age   wgt wgt_z  nl_1997_wgt_female_nl  0.7885  9.46  0.714
#> 17 0.9610   age   wgt wgt_z  nl_1997_wgt_female_nl  0.9610 10.01  0.552
#> 18 1.2485   age   wgt wgt_z  nl_1997_wgt_female_nl  1.2485 11.35  0.827
#> 19 1.5140   age   wgt wgt_z  nl_1997_wgt_female_nl  1.5140 12.01  0.711
#> 20 1.9740   age   wgt wgt_z  nl_1997_wgt_female_nl  1.9740 13.34  0.747
#> 21 0.0986   age   hdc hdc_z  nl_1997_hdc_female_nl  0.0986 37.60  0.461
#> 22 0.1369   age   hdc hdc_z  nl_1997_hdc_female_nl  0.1369 38.30  0.398
#> 23 0.2327   age   hdc hdc_z  nl_1997_hdc_female_nl  0.2327 40.40  0.655
#> 24 0.5010   age   hdc hdc_z  nl_1997_hdc_female_nl  0.5010 44.40  1.134
#> 25 0.7885   age   hdc hdc_z  nl_1997_hdc_female_nl  0.7885 46.00  0.829
#> 26 0.9610   age   hdc hdc_z  nl_1997_hdc_female_nl  0.9610 47.00  0.929
#> 27 1.2485   age   hdc hdc_z  nl_1997_hdc_female_nl  1.2485 48.00  0.896
#> 28 1.5140   age   hdc hdc_z  nl_1997_hdc_female_nl  1.5140 48.50  0.801
#> 29 1.9740   age   hdc hdc_z  nl_1997_hdc_female_nl  1.9740 50.10  1.354
#> 30 0.0000   age   bmi bmi_z  nl_1997_bmi_female_nl  0.0000 12.69  0.160
#> 31 0.0986   age   bmi bmi_z  nl_1997_bmi_female_nl  0.0986 13.70 -0.476
#> 32 0.1369   age   bmi bmi_z  nl_1997_bmi_female_nl  0.1369 14.41 -0.295
#> 33 0.2327   age   bmi bmi_z  nl_1997_bmi_female_nl  0.2327 15.93  0.214
#> 34 0.5010   age   bmi bmi_z  nl_1997_bmi_female_nl  0.5010 17.19  0.352
#> 35 0.7885   age   bmi bmi_z  nl_1997_bmi_female_nl  0.7885 17.75  0.590
#> 36 0.9610   age   bmi bmi_z  nl_1997_bmi_female_nl  0.9610 17.56  0.514
#> 37 1.2485   age   bmi bmi_z  nl_1997_bmi_female_nl  1.2485 17.73  0.824
#> 38 1.5140   age   bmi bmi_z  nl_1997_bmi_female_nl  1.5140 17.23  0.641
#> 39 1.9740   age   bmi bmi_z  nl_1997_bmi_female_nl  1.9740 16.65  0.436
#> 40 0.0986   age   dsc dsc_z  nl_2014_dsc_female_40  0.0986 14.69  0.190
#> 41 0.1369   age   dsc dsc_z  nl_2014_dsc_female_40  0.1369 16.72 -0.094
#> 42 0.2327   age   dsc dsc_z  nl_2014_dsc_female_40  0.2327 20.82 -0.743
#> 43 0.5010   age   dsc dsc_z  nl_2014_dsc_female_40  0.5010 35.30 -0.002
#> 44 0.7885   age   dsc dsc_z  nl_2014_dsc_female_40  0.7885 41.52 -0.806
#> 45 0.9610   age   dsc dsc_z  nl_2014_dsc_female_40  0.9610 47.59 -0.157
#> 46 1.2485   age   dsc dsc_z  nl_2014_dsc_female_40  1.2485 56.21  0.844
#> 47 1.5140   age   dsc dsc_z  nl_2014_dsc_female_40  1.5140 59.75  0.654
#> 48 1.9740   age   dsc dsc_z  nl_2014_dsc_female_40  1.9740 64.21  0.248
#> 49 0.0000   hgt   wfh wfh_z nl_1997_wfh_female_nla 51.0000  3.30 -0.858
#> 50 0.0986   hgt   wfh wfh_z nl_1997_wfh_female_nla 54.7000  4.10 -0.780
#> 51 0.1369   hgt   wfh wfh_z nl_1997_wfh_female_nla 56.0000  4.52 -0.419
#> 52 0.2327   hgt   wfh wfh_z nl_1997_wfh_female_nla 59.5000  5.64  0.215
#> 53 0.5010   hgt   wfh wfh_z nl_1997_wfh_female_nla 68.0000  7.95  0.326
#> 54 0.7885   hgt   wfh wfh_z nl_1997_wfh_female_nla 73.0000  9.46  0.655
#> 55 0.9610   hgt   wfh wfh_z nl_1997_wfh_female_nla 75.5000 10.01  0.584
#> 56 1.2485   hgt   wfh wfh_z nl_1997_wfh_female_nla 80.0000 11.35  0.851
#> 57 1.5140   hgt   wfh wfh_z nl_1997_wfh_female_nla 83.5000 12.01  0.644
#> 58 1.9740   hgt   wfh wfh_z nl_1997_wfh_female_nla 89.5000 13.34  0.464
```

## Some older functions

Everything can be done with `james_post()` and `james_get()`. The
functions below are not needed anymore, and will be deprecated in the
future.

### `upload_txt()`

Upload BDS data and create a tibble on the server:

``` r
library(jamesclient)
fn <- file.path(path.package(""jamesclient""), ""testdata"", ""client3.json"")
r1 <- upload_txt(fn)
browseURL(get_url(r1, ""return""))
```

### `request_chart()`

Make a combined upload and automatic chartcode choice:

``` r
r2 <- request_chart(fn, chartcode = ""PJAHN27"")
browseURL(get_url(r2, ""svg""))
```

## Removed functions

| Function         | Description              | Alternative    |
|------------------|--------------------------|----------------|
| `request_site()` | Create personalised site | `james_post()` |
| `upload_bds()`   | Upload and parse data    | `james_post()` |
",2022-08-12
https://github.com/growthcharts/jamesdemo,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# jamesdemo

The `jamesdemo` package contains a simple Shiny app that shows site
functionality of the **Joint Automatic Measurement and Evaluation System
(JAMES)**. The API endpoint `request_site` as defined by
[james](https://github.com/growthcharts/james) returns an URL that
points to a site with personalised child charts. The `jamesdemo` app
shows the site for a set of demo children. The
[jamesdemodata](https://github.com/growthcharts/jamesdemodata) package
stores the data of these children.

## Installation

The following statements will install the `jamesdemo` package

``` r
install.packages(""remotes"")
remotes::install_github(""growthcharts/jamesdemo"")
```

## Example

Within RStudio, view the website locally as follows:

``` r
library(jamesdemo)
go()
```

The app does not run in the internal RStudio viewer. Click on button
`Open in browser` to see it in action.

## Online version

You can spare yourself the trouble of installing the package, and visit
`JAMES tryout` at
<https://tnochildhealthstatistics.shinyapps.io/james_tryout/>.

## Some guidelines using the app

Interaction within the app should explain itself, but a few things may
not be obvious at first.

![Control panel in
jamesdemo](https://raw.githubusercontent.com/growthcharts/jamesdemo/master/inst/figures/JAMES_tryout.png?raw=true)

There are two control bars at the left. The leftmost bar with **Groep**,
**Naam kind** and **Server** belongs to the `jamesdemo` Shiny app and
mimics the external client with a database with the children’s data. The
**Server** menu allows setting one of three servers:

-   **JAMES**: the production server. This should always work, and is
    the default;
-   **localhost**: the local server. For development. Works only if you
    are running the app locally and if the relevant JAMES container is
    running locally under docker.

Everything within the panel GROEIDIAGRAMMEN is produced by JAMES. The
middle bar with **Groei** contains interactive controls managed by the
JAMES API, in particular by function `james::request_site()`. The right
hand side figure, with the chart, is responsive to changes made in the
middle bar and is also part of JAMES.

For developers: You may request the entire site in your application (as
shown here), but it is also possible to just call individual elements,
and build the user interaction yourself.

## Resources

-   [james](https://github.com/growthcharts/james)
-   [jamesdemodata](https://github.com/growthcharts/jamesdemodata)
",2022-08-12
https://github.com/growthcharts/jamesdemodata,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# jamesdemodata

<!-- badges: start -->

[![R-CMD-check](https://github.com/growthcharts/jamesdemodata/workflows/R-CMD-check/badge.svg)](https://github.com/growthcharts/jamesdemodata/actions)
<!-- badges: end -->

The `jamesdemodata` is a lightweight repo with demo and test children.
The goal of the package is to test the **Joint Automatic Measurement and
Evaluation System (JAMES)**. JAMES is an **experimental** online
resource for creating and analysing growth charts.

## Installation

The following statements will install the `jamesdemodata` package

``` r
install.packages(""remotes"")
remotes::install_github(""growthcharts/jamesdemodata"")
```

## Example

Test- en demo data are stored in the `inst/extdata` directory. In order
to generate a file name of the SMOCC child Laura S use

``` r
fn <- system.file(""extdata"", ""bds_v2.0"", ""smocc"", ""Laura_S.json"", package = ""jamesdemodata"")
```

Read the data into R using the `bdsreader` package:

``` r
library(bdsreader)
tgt <- read_bds(fn)
head(tgt)
persondata(tgt)
```

See <https://github.com/growthcharts/bdsreader> for installation notes
of `bdsreader`.
",2022-08-12
https://github.com/growthcharts/jamesdocs,"
## jamesdocs

The `jamesdocs` repository contains documentation for the **Joint Anthropometric Measurement and Evaluation System (JAMES)**, an **experimental** online resource for creating growth charts.

See [JAMES](https://github.com/growthcharts/james) for more information.

## About

**Work in progress**. Direct suggestions and inquiries to Stef van Buuren (stef.vanbuuren at tno.nl), <https://stefvanbuuren.name>, <https://github.com/stefvanbuuren>.

",2022-08-12
https://github.com/growthcharts/nlreferences,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# nlreferences

<!-- badges: start -->

[![R-CMD-check](https://github.com/growthcharts/nlreferences/workflows/R-CMD-check/badge.svg)](https://github.com/growthcharts/nlreferences/actions)
<!-- badges: end -->

The nlreferences package provides Dutch reference values.

## Installation

You can install the development version from
[GitHub](https://github.com/) with:

``` r
# install.packages(""devtools"")
devtools::install_github(""growthcharts/nlreferences"")
```
",2022-08-12
https://github.com/gspeed0689/Benelux-Sentinel2-RGBI-NDVI,"## Benelux-Sentinel2-RGBI-NDVI
Preprocessing script for easy creation of RGB &amp; Near Infrared and NDVI JPEG-2000 Sentinel 2 imagery files from .SAFE  

## Purpose  

The purpose of this code repository is to accompany the Geo-Yoda data repository of Benelux (Belgium, Netherlands, Luxembourg) Sentinel 2 satellite imagery preprocessed for easy use from the .SAFE format that the European Space Agency (ESA) Developed. The dataset consists of JPEG-2000 (JP2) multi-band image files containing the red, green, blue, and near-infrared (RGBI) bands, and pre-processed NDVI single-band JPEG-2000 image files. The initial data is collected from the Sentinel data hub, extracted to directories from the .SAFE zip files, and using the `rasterio` library it takes the individual band JP2 files and combines them into a single multi-band image file.   

## NDVI Calculation  

The NDVI calculation used is as follows.   

![NDVI Equation](https://github.com/gspeed0689/Benelux-Sentinel2-RGBI-NDVI/raw/main/NDVI_Equation.PNG)

The `Infrared - Red / Infrared + Red` is a standard NDVI calculation, multiplying by 2^15 is to fill the signed 16-bit number space `(2^16 - (2^16)/2)`. The `floor`function is to return the values from floats to integer values.   

NDVI is stored in a separate file due to the nature of the data and the JP2 file format; the optical data (RGBI) data is stored as unsigned integers (`0 - (2^16 - 1)`), and NDVI requires signed number formats. The JP2 driver in `rasterio` does not have a floating point encoder, so the values must be stored as integers, and thus the 2^15 multiplier, and the floor function, so we can then fit the NDVI results into a signed 16-bit integer encoding.   

## Script Usage

The notebook folder is the working IPython notebooks I originally created to develope the script and is there for reference only.   

The Sentinel2_RGBI_NDVI.py file is the entirely self contained script.   

Basic use:  

`python3 Sentinel2_RGBI_NDVI.py -f /path/to/folder [-rgbi] [-ndvi]`  

Add `-ndvi` to tell the script to generate NDVI JP2 files.   
Add `-rgbi` to tell the script to generate Red, Green, Blue, Near-Infrared JP2 files.   

## Current version 0.2.0  
  
## Future versions   

0.3 - Type hints  
1.0 - Rewritten in an object oriented class based way.     
1.1 - File Name Parsing for custom file name conventions, aim to be like `%yyyy-%mm-%dd` style.     
",2022-08-12
https://github.com/gspeed0689/KML-Heading-Removal,"# KML-Heading-Removal
A 2014 script I wrote to write an integer of 0 to the `<heading>` tag of KML documents created by ArcMap.

Sometimes ArcMap at the time I wrote this script would produce heading values in a KML file that were unwanted, so this script will open a KML file and set all of the `<heading>` tags to a node value of `0`

**Usage**
`python kmlRemoveHeading.py -f {filename}.kml`
",2022-08-12
https://github.com/gspeed0689/photo-management-2022,"# Photo Management 2022
Photo management scripts for 2022

This is a personal project to automate the import, conversion, and organization of my photography. 

***It is not in a working state at the moment.***

Some features:

* Import via command line from the SD card folder
* Import photos only from today, yesterday, or an ISO 8601 date `YYYY-MM-DD`
* Include a uid in the file name to track exports later
* Multiple uid generation methods
  * secrets.token_urlsafe
  * hashlib.md5
  * hashlib.sha256
  * uuid.uuid4()
* Rename files to avoid duplicate file names  
  * I have two of the same model cameras and there is no way to rename the file prefix in camera. 
  * Will rename to `serial_uid_originalname.ext`
* Convert to DNG
* Create a batch file to convert videos with ffmpeg
* Convert videos using subprocess
* Set affinity for video subprocess conversion (Windows Processor Affinity)
* Run only one affinity enabled process at a time",2022-08-12
https://github.com/gspeed0689/PiCalculationsPy,"# PiCalculationsPy
**Practice code for calculating pi using Python, RabbitMQ, and IPythonNotebooks.**

Pi day with the other day, and I was interested in quickly writing different python scripts to implement the various ways of calculating pi. The basic methods were very easy to write, and are in the notebook `Different Ways of Calculating Pi.ipynb`.

This seemed really easy, and I wanted to take it a step further, so I decided to add a multiprocessing through a message passing intermediary aspect to calculating pi.
The script `rabbitmq-GLS.py` is my first use of a message queue and runs the Gregory-Leibniz series for calculating pi, and it makes use of a RabbitMQ server on the local machine.
`run_rabbitmq_GLS.py` is an easy script to spawn all of the different processing nodes for running the calculation. 
This script also uses the `decimal` library to have more decimal points in the calculation than the `float` type would provide. 
",2022-08-12
https://github.com/gspeed0689/pod-assistant,"# pod-assistant
Old python utility to assist with making a Bentley Pod Creator conversion project file. 

This Python 2.7 utility was made when I was a graduate student to solve an issue with the Bentley Pod Creator. The issue with the program was that when trying to add point cloud files to the conversion software, it would not add all of the files, and subsequent file adding porcesses would still fail to add all of the files. This python utility would create a Bentley Pod Creator compliant XML file with all of the desired point cloud files, the Bentley Pod Creator software would then convert all of the files. 

**Usage**

In its current state there are only two command line arguments active, `-d` for directory, and `-t` for type of point cloud. `-d` is the directory of all of the point cloud tiles, and `-t` is the type of point cloud, either `las` or `laz`. When running the script, it will list all of the files in the directory of the `-t` type specified, and write a `.pbp` xml file to that same directory.  

**Future Fixes**

Had Bentley not fixed their software, I could have extended the script to use all of the options available in the software. I had already populated the command line argument parser with all of the other options available in the software. 
",2022-08-12
https://github.com/gspeed0689/target-sphere-labels,"# target-sphere-labels
An older python script to create randomized code labels to be wrapped around laser scanning sphere targets for identifying individual locations of the target. 

Terrestrial laser scanning uses spherical targets to help register data.
For individual placement tracking purposes I wanted a script to randomly generate lables to be attached to the tripods the sphere targets are placed on, and thus this script was born. 
In Microsoft Word I created a template document and saved it as an `xml` document file compatible with Word.
This script reads the template file, replaces the text using proper `xml` document object model manipulation with the `xml.dom.minidom` python library.
The script will automatically call Word with the `subprocess` library and also automatically print the randomly generated codes document to the system's default printer. 

**Usage**

Place all of the provided files in the same directory, and edit the python file with a path for your Word executable, the path to the template `SphereTargetTemplate.xml` file, and a path for a temporary directory. The three variables to change are all at the top of the python file after the imports. You should then be able to run `python sphere_targets.py` as many times as you like and it will print six 3 letter code labels for attaching as labels to scanning sphere targets. 
",2022-08-12
https://github.com/gspeed0689/various,"# various
Random IPYNBs and Python files

This is a collection of little things I've written to practice python, some are useful, some are silly, some are things. 

## eXtended Unique IDentifier (XUID)

Do you think to yourself that UUID doesn't provide enough random unique values to fit your application, or storing things like time and MAC addresses is too useful, and you don't want to be as crazy as AWS with internal 420-bit URNs, then have I got a package for you. 
XUID uses the whole alphanumeric space to create a unique string that you can use to uniquely identify all of your digital objects.

## Dynamic Salt

I read an article on how to allegedly store passwords, as in don't store the password but store a salted hash of the password, and compare a salted hash of the current password attempt to the one in the database. 
So I tried implementing something like that in python, but with different salt for different characters. 
",2022-08-12
https://github.com/haddocking/3D-DART,"# 3D-DART

Standalone version of 3D-DART (originally developed by [Marc van Dijk](https://github.com/marcvdijk)) for Python 2.x series.

## Installation

* We recommend to use a virtual environment to install 3D-DART:

```
virtualenv --python=python2.7 dart
cd dart
source bin/activate
```

* Install [numpy](https://numpy.org/) library for Python 2.x:

```
pip install numpy
```

* Clone this repository:

```bash
git clone https://github.com/haddocking/3D-DART.git
```

* Create [X3DNA](http://x3dna.org/) software folder structure:

```bash
cd 3D-DART/software
mkdir X3DNA-linux X3DNA-mac
```

* Download from [http://x3dna.org/](http://x3dna.org/) your specific architecture version and place it `X3DNA-linux` if GNU/Linux or `X3DNA-mac` if macOS. For example, `X3DNA-mac` folder should contain:

```
[3D-DART/software/X3DNA-mac]$ ls
BASEPARS FIBER    bin
```

and more in deep:

```
BASEPARS/:
ATOMIC           Atomic_G.pdb     Atomic_U.pdb     Block_BP.alc     Pxyz.dat         RNA_BASES        col_mname.dat    misc_3dna.par    ps_image.par     trans_pep.alc
Atomic_A.pdb     Atomic_P.pdb     Atomic_i.pdb     Block_R.alc      PxyzH.dat        baselist.dat     fig_image.par    my_header.r3d    raster3d.par     trans_pep.pdb
Atomic_C.pdb     Atomic_T.pdb     BLOCK            Block_Y.alc      README           col_chain.dat    help3dna.dat     ndb_raster3d.par rotz90

FIBER/:
Data   Str02  Str05  Str08  Str11  Str14  Str17  Str20  Str23  Str26  Str29  Str32  Str35  Str38  Str41  Str44  Str47  Str50  Str53
README Str03  Str06  Str09  Str12  Str15  Str18  Str21  Str24  Str27  Str30  Str33  Str36  Str39  Str42  Str45  Str48  Str51  Str54
Str01  Str04  Str07  Str10  Str13  Str16  Str19  Str22  Str25  Str28  Str31  Str34  Str37  Str40  Str43  Str46  Str49  Str52  Str55

bin/:
EnergyPDNA.exe anyhelix       cehs           dcmnfile       fiber          get_part       nmr_ensemble   pdb2img        regular_dna    std_base
alc2img        block_atom     comb_str       del_ms         find_pair      manalyze       nmr_strs       r3d_atom       rotate_mol     step_hel
analyze        blocview       cp_std         ex_str         frame_mol      mstack2img     o1p_o2p        rebuild        stack2img
```

* Test running the main script, you should get a similar output to this:

```bash
cd /path/to/3D-DART
./RunDART.py
```

```
--> Performing System Checks:
   * Python version is: 2.7.1
   * Could not import Numeric package trying NumPy
   * Importing NumPy package succesfull
--> Your current working directory is: /path/to/3D-DART
--------------------------------------------------------------------------------------------------------------
Welcome to 3D-DART version 1.2   Thu Apr  2 12:03:14 2020
--------------------------------------------------------------------------------------------------------------
--> Parsing command-line arguments:
Wrong command line

```

If you see the message above, then 3D-DART is ready.


## Example

```bash
cd /path/to/3D-DART
./RunDART.py -w NAensemblebuild -f example/struct_*.pdb
```


## Output description

A brief description of the directories:

| Folder  | Description  |
|---|---|
| jobnr1-FileSelector  | Contains the files you used as input. |
| jobnr2-BuildNucleicAcids  | Generates a canonical starting structure of the DNA with the desired sequence. |
| jobnr3-X3DNAAnalyze | Runs a 3DNA analysis over the structure generated in job 2 to obtain initial starting parameters for modeling. |
| jobnr4-ModelNucleicAcids | The actual modeling step. Models are represented as base-pair(step) parameter files. |
| jobnr5-BuildNucleicAcids | Converts the parameter files from job 4 into PDB structure files. |
| jobnr6-X3DNAanalyze | Runs a 3DNA analysis over the modeled structures. |
| jobnr7-NABendAnalyze | Runs a global bend analysis over the modeled structures. |
| jobnr8-PDBeditor | Makes some last-minute changes to the PDB files. This directory contains the final structures. |

In the main directory you will find the log file of your job named 'dart.out'
In case you encounter unexpected errors please look into the log file.

## Reference

If you use 3D-DART please cite:

M. van Dijk and A.M.J.J. Bonvin (2009) ""3D-DART: a DNA structure modelling server"", Nucl. Acids Res.
37 (Web Server Issue):W235-W239, [doi:10.1093/nar/gkp287](https://doi.org/10.1093/nar/gkp287)

",2022-08-12
https://github.com/haddocking/3D-DART-server,"# 3D-DART-server

3D-DART web server repository. Docker is needed in order to build the 3D-DART server container and to execute it. It is based in the Ubuntu 18.04 Docker image (with 32bit support needed for the version of X3DNA software used by 3D-DART) and with Apache2 and Python 2.7.

**Please note that 3D-DART server is legacy software and many input data errors are not properly checked/validated.**

## 1. Setup

### 1.1. Build Docker container

**Note:** if using git on Windows, please make sure **automatic conversion of EOL is set to `false`** to avoid mixing `\r` and `\n` end of lines: `git config --global core.autocrlf false`.

```bash
cd 3D-DART-server
docker build -t 3d-dart .
```

### 1.2. Run 3D-DART Docker container

```bash
docker run --name 3ddart -p 80:80 -i -t 3d-dart
```

### 1.3. Stop 3D-DART Docker container

From the terminal, Ctrl+C.

In case of re-running the container, previous exited containers must be cleaned:

```bash
docker rm $(docker ps --filter ""status=exited"" -q)
```

And then again:

```bash
docker run --name 3ddart -p 80:80 -i -t 3d-dart
```

## 2. How to use the server

### 2.1. Web interface

Once the docker container is running, 3D-DART server will be available from the url [http://127.0.0.1](http://127.0.0.1):

![DARTCustombuild](img/DARTcustombuild.png)

This is the default interface, `DARTCustombuild`. There are several examples of this form pre-filled in the following urls:

* [http://127.0.0.1/3DDART/html/Example1.html](http://127.0.0.1/3DDART/html/Example1.html)
* [http://127.0.0.1/3DDART/html/Example2.html](http://127.0.0.1/3DDART/html/Example2.html)
* [http://127.0.0.1/3DDART/html/Example3.html](http://127.0.0.1/3DDART/html/Example3.html)
* [http://127.0.0.1/3DDART/html/Example4.html](http://127.0.0.1/3DDART/html/Example4.html)

For example, this is Example1.html:

![Example1](img/Example1.png)

Once input data is filled and `Submit` button clicked, you will be redirected to the results page:

![DARTResult](img/DARTresult.png)

Server results are also available through this link: [http://127.0.0.1/3DDART/server/results/](http://127.0.0.1/3DDART/server/results/).

In case of server logic error, temporary error pages are generated at: [http://127.0.0.1/3DDART/error/](http://127.0.0.1/3DDART/error/).

### 2.2. Command line

First, get a bash terminal from the Docker container:

```bash
docker exec -it 3ddart bash
```

You will find the server logic in `/var/www/html/3DDART` path. `RunDART.py` might be called from any location:

```bash
root@ccfe5d663254:/# cd /var/www/html/3DDART
root@ccfe5d663254:/var/www/html/3DDART# ls
cgi  error  html  index.html  server  software
root@ccfe5d663254:/var/www/html/3DDART# RunDART.py 
--> Performing System Checks:
   * Python version is: 2.7.1
   * Could not import Numeric package try numpy
   * Importing numpy package succesfull
--> Your current working directory is: /var/www/html/3DDART
--------------------------------------------------------------------------------------------------------------
Welcome to 3D-DART version 1.2   Fri Aug 28 09:26:19 2020
--------------------------------------------------------------------------------------------------------------
```

For more information about how to use `RunDART.py` please refer to the standalone version of [3D-DART](https://github.com/haddocking/3D-DART).
",2022-08-12
https://github.com/haddocking/benchmark-tools,"# `benchmark-tools` for HADDOCK v2.4+

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

This is a benchmarking framework for HADDOCK v2.4+, it aims to reduce code duplication and development overlap by centralizing our benchmark effots into a single tool.

It will read a configuration file and setup your runs, customize `run.cns` parameters with the custom parameters and execute the simulations.

## Installation

```text
$ git clone https://github.com/haddocking/benchmark-tools.git
$ cd benchmark-tools
$ python setup.py install
$ haddock_bm -h
usage: haddock_bm [-h] [--force] [-v] config_file

Run a Haddock Benchmark

positional arguments:
  config_file    Configuration file, toml format

optional arguments:
  -h, --help     show this help message and exit
  --force        DEV only, forcefully removeinitiated runs
  -v, --version  show version
```

## Configuration

To execute the benchmark tools you need a configuration file as below:

```toml
# ==============================
# This general section is obligatory and must contain the following keys
[ general ]
# Location of HADDOCK
haddock_path = '/Users/rodrigo/repos/haddock'

# Since HADDOCK v2.4 runs on python2, we also need to point out its location
python2 = '/usr/bin/python2'

# Location of your prepared dataset
dataset_path = '/Users/rodrigo/repos/BM5-clean/HADDOCK-ready'

# We will automatically detect what is the receptor and the ligand
#  inside your dataset folder, but they need the match the suffixes below
receptor_suffix = '_r_u.pdb'
ligand_suffix = '_l_u.pdb'
# ==============================

# Here you will define the scenarios to benchmark,
#  each section must be named scenario_N
# Each parameter inside the scenario corresponds to
#  a parameter inside run.cns, except run_name and ambig_tbl
#  that are used to setup the run
[ scenario_1 ]
run_name = 'true-interface'
ambig_tbl = 'ambig.tbl'
noecv = true

[ scenario_2 ]
run_name = 'CM'
noecv = true
cmrest = true
cmtight = true
# ==============================
```

Create this file with whatever editor you prefer and save it in the location of your benchmark as benchmark_config.toml

## Execution example

```text
cd benchmark-tools/example
haddock_bm scenarios.toml
```
",2022-08-12
https://github.com/haddocking/binding-affinity-benchmark,"## Protein-protein binding affinity benchmark


![affinity-benchmark](docs/7cei.png)


The design of an ideal scoring function for protein−protein docking that would also predict the binding affinity of a complex is one of the challenges in structural proteomics. Such a scoring function would open the route to in silico, large-scale annotation and prediction of complete interactomes.


Here we present a protein−protein binding affinity benchmark consisting of binding constants (Kd’s) for 81 complexes. This benchmark was used to assess the performance of nine commonly used scoring algorithms along with a free-energy prediction algorithm in their ability to predicting binding affinities. Our results reveal a poor correlation between binding affinity and scores for all algorithms tested. However, the diversity and validity of the benchmark is highlighted when binding affinity data are categorized according to the methodology by which they were determined. By further classifying the complexes into low, medium and high affinity groups, significant correlations emerge, some of which are retained after dividing the data into more classes, showing the robustness of these correlations.


Despite this, accurate prediction of binding affinity remains outside our reach due to the large associated standard deviations of the average score within each group. Therefore, improvements of existing scoring functions or design of new consensus tools will be required for accurate prediction of the binding affinity of a given protein−protein complex.


In a collaborative effort with the groups of Prof. Janin (Université Paris-Sud), Dr. P. Bates (Cancer Research UK, London) and Prof. Z. Weng (University of Massachusetts Medical School, Worcester) to extend this benchmark, we have discovered a number of discrepancies corresponding to some of the reported values and entries in our previously published benchmark.


Accordingly, only 46 of the 81 reported binding affinity data can be considered fully accurate. We have reanalyzed the accuracy of the various scoring functions on this subset and, although correlations are slightly improved (sqrt(R)<0.3), current functions still do not hold any predictive capacity.

![New-correlations](docs/New_correlations.png)

The list of 41 complexes can be found in the [Corrected_Benchmark1.0.pdf](Corrected_Benchmark1.0.pdf) file.

We have developed the protein-protein binding affinity benchmark to be of general use to the docking community for the development of the scoring functions. We welcome all suggestions aimed at improving or expanding the benchmark.

* * *

### Version history

* 23-03-2010 | The initial version of protein-protein affinity benchmark and the correlations of scoring functions to affinities.

* 03-11-2010 | Benchmark version 1.0: Major updates. A community-wide effort is on the loose, reviewing, extending and annotating the benchmark: 46/81 complexes are considered fully accurate for benchmarking studies (either their affinity data or their 3D structures). PDB IDs of the 46 complexes along with their binding affinities - **This is the current version**

* 05-11-2010 | Reanalyzing the accuracy of scoring functions in binding affinity prediction: We have re-evaluated if scoring functions can predict the affinities of protein-protein complexes, considering 46 high-quality complexes present in our updated benchmark. Although improvement of the correlations are evident, scoring is still limited in predicting binding affinities. The New_correlation figure provided in this repo illustrates those correlations.

* * * 

### Reference

When using the protein-protein binding affinity benchmark or discussing binding prediction of protein-protein complexes, please cite using the following reference:

* P. Kastritis and **A.M.J.J. Bonvin**
[Are scoring functions in protein-protein docking ready to predict interactomes? Clues from a novel binding affinity benchmark.](https://doi.org/doi:10.1021/pr9009854)
_J. Proteome Research_, *9*, 2216-2225 (2010).  See also the <a href=""https://doi.org/doi:10.1021/pr101118t"">published correction</a>

* * * 

### Related publications

* T Vreven+, I.H. Moal+, A. Vangone+, B.G. Pierce, P.L. Kastritis, M. Torchala, R. Chaleil, B. Jiménez-García, P.A. Bates#, Juan Fernandez-Recio#, **A.M.J.J. Bonvin**# and Z. Weng#.
[Updates to the integrated protein-protein interaction benchmarks: Docking benchmark version 5 and affinity benchmark version 2](https://doi.org/doi:10.1016/j.jmb.2015.07.016).
_J. Mol. Biol._ *19*, 3031-3041 (2015).

* A Vangone and **A.M.J.J. Bonvin**.
[Contacts-based prediction of binding affinity in protein-protein complexes](http://elifesciences.org/content/4/e07454).
_eLife_, *4*, e07454 (2015).

* P.L. Kastritis, J.P.G.L.M Rodrigues, G.E. Folkers, R. Boelens and **A.M.J.J. Bonvin**.
[Proteins feel more than they see: Fine-tuning of binding affinity by properties of the non-interacting surface.](https://doi.org/10.1016/j.jmb.2014.04.017) 
_J. Mol. Biol._ *426*, 2632-2652 (2014).

* P.L. Kastritis, J.P.G.L.M. Rodrigues and **A.M.J.J. Bonvin**
[HADDOCK2P2I: A robust biophysical model for predicting the binding affinity of protein-protein interaction inhibitors](http://pubs.acs.org/doi/abs/10.1021/ci4005332)
_J. Chem. Info. Model._ *54*, 826-836 (2014).

* P.L. Kastritis and **A.M.J.J. Bonvin**
[Molecular origins of binding affinity: Seeking the Archimedean point.](https://doi.org/doi:10.1016/j.sbi.2013.07.001)
_Curr. Opin. Struct. Biol._, *23*, 868-877 (2013).

* P.L. Kastritis and **A.M.J.J. Bonvin**
[On the binding affinity of macromolecular interactions: daring to ask why proteins interact](https://doi.org/doi:10.1098/rsif.2012.0835)
_J. R. Soc. Interface_, *10*, doi: 10.1098/rsif.2012.0835 (2013).

* P.L. Kastritis, I.H. Moal, H. Hwang, Z. Weng, P.A. Bates, **A.M.J.J. Bonvin** and J. Janin
[A structure-based benchmark for protein-protein binding affinity.](https://doi.org/doi:10.1002/pro.580)
_Prot. Sci._, *20*, 482-41 (2011).


",2022-08-12
https://github.com/haddocking/bioexcel-IO,"# Background
The vast network of the interactome involves hundreds of thousands of protein-protein and other biomolecular complex interactions. 
Malfunctions in this network are responsible for a plethora of diseases, which highlights the importance of understanding how it works on a deep level. 
Adding structural information to such an intricate network comes with two challenges; the generation of hundreds of thousands of independent docking runs and an all-vs-all docking of the network components for further analysis. 
This task will generate a gargantuan number of files and will only be possible by taking advantage of exascale computing resources both for processing and I/O handling. 
This benchmark is related to our BioExcel project on high throughput modelling of interactomes.

# Description

Here we present a I/O benchmark consisting of 1,000,000 files – docking models in PDB format (plain text file) - generated by HADDOCK.
Each of these files contains in its header various energetic components that have been calculated by HADDOCK. 

```REMARK FILENAME=""1ATN_1061.pdb0""
REMARK ===============================================================
REMARK HADDOCK run for 1ATN
REMARK initial structure: 1ATN_1.pdb
REMARK final NOE weights: unambig 50 amb: 50
REMARK ===============================================================
REMARK            total,bonds,angles,improper,dihe,vdw,elec,air,cdih,coup,rdcs,vean,dani,xpcs,rg
REMARK energies: 642.296, 0, 0, 0, 0, 4.80171, -0.512644, 638.007, 0, 0, 0, 0, 0, 0, 0
REMARK ===============================================================
REMARK            bonds,angles,impropers,dihe,air,cdih,coup,rdcs,vean,dani,xpcs
REMARK rms-dev.: 0,0,0,0,1.07224,0,0, 0, 0, 0, 0
REMARK ===============================================================
REMARK               air,cdih,coup,rdcs,vean,dani,xpcs
REMARK               >0.3,>5,>1,>0,>5,>0.2,>0.2
REMARK violations.: 8, 0, 0, 0, 0, 0, 0
REMARK ===============================================================
REMARK                        CVpartition#,violations,rms
REMARK AIRs cross-validation: 0, 0, 0
REMARK ===============================================================
REMARK NCS energy: 0
REMARK ===============================================================
REMARK Symmetry energy: 0
REMARK ===============================================================
REMARK Membrane restraining energy: 0
REMARK ===============================================================
REMARK Local cross-correlation:  0.0000
REMARK ===============================================================
REMARK Desolvation energy: 2.18009
REMARK Internal energy free molecules: 17670.2
REMARK Internal energy complex: 17670.2
REMARK Binding energy: 6.46916
REMARK ===============================================================
REMARK buried surface area: 1319.21
REMARK ===============================================================
REMARK water - chain_1: 0 0 0
REMARK water - chain_2: 0 0 0
REMARK ===============================================================
REMARK water - water: 0 0 0
REMARK ===============================================================
REMARK DATE:25-Dec-2018  19:07:53       created by user: enm009
```

The benchmark is composed of the results of applying HADDOCK to the cases of the Protein-Protein Docking Benchmark v5 (BM5), which is a well-accepted benchmark in the protein docking community.   Due to the nature of the BM5 benchmark, the PDB files are different in length as they represent different molecular structures.  Files are placed in a single directory and named according to the BM5 they originated from, docking procedure used, docking stage and model number, example; 1BVN_ti5-it1_167.pdb and 1AK4_cm-it0_1601.pdb.

The HADDOCK score itself is a linear combination of the extracted energetic components, weighted according to its stage (rigid-body, semi-flexible or water).
For more information about the scoring, please refer to the [HADDOCK Manual](http://www.bonvinlab.org/software/haddock2.2/scoring/).

# Download

The datasets (4 of those containing each 250k models) can be downloaded from [DOI:10.5281/zenodo.3271425](https://doi.org/10.5281/zenodo.3271425)


This script can be used for downloading with streamed unpacking:

```
for n in 01 02 03 04 ; do 
  (curl https://zenodo.org/record/3271425/files/dataset-$n.tar.gz?download=1 | tar zxf -) & done
```

The above can be sensitive to network connectivity issues, for more reliable download try:

```
curl --retry 3 -fO https://zenodo.org/record/3271425/files/dataset-0[1-4].tar.gz?download=1

md5sum -c - <<EOF
e81b134c8ddd035985a63301e07ac7ed  dataset-01.tar.gz
689efdee5a1374f0f1ac1ef52beffe39  dataset-02.tar.gz
de9db417622dc43472da7ac88954d2ef  dataset-03.tar.gz
174c60b6b28205b21256326f6b580f95  dataset-04.tar.gz
EOF

for x in dataset-*.tar.gz ; do tar zxf $x ; done
```


# Requirements

* Python 3.6.x
* ~ 80 Gb (Compressed, 20Gb each)
* ~ 375 Gb (Uncompressed, 93Gb each)

# Usage

Make sure the datasets are unpacked and create first a file list containing all models to be analysed.
For example to analyse 250k models:

```bash 
$ i=250000 ; find dataset-*/. -type f | head -n $i > dataset-250k.file
```
And to analyse 1 million models:

```bash 
$ i=1000000 ; find dataset-*/. -type f | head -n $i > dataset-1M.file
```


Execute main code:

```bash
$ python3 calc-batch-hs.py dataset-250k.file
```


# Scenario
 
 Processing these files quickly without straining the system will require extremely efficient I/O solutions.
In order to evaluate this scenario we created a python script named `calc-batch-hs.py` that executes the following tasks:
1. Extract the energy terms from the header of the files
2. Calculate the HADDOCK score and 
3. Write a sorted list which represents the final ranking. 

***

This benchmark was created by Rodrigo Honorato and Alexandre Bonvin @ Utrecht University // [BonvinLab](http://www.bonvinlab.org)
",2022-08-12
https://github.com/haddocking/BM5-clean,"## BM5-clean

Docking benchmark 5 (BM5) - cleaned and ready to use for HADDOCK

[![DOI](https://zenodo.org/badge/162272657.svg)](https://zenodo.org/badge/latestdoi/162272657)

This is the docking and binding affinity benchmark described in:

T Vreven, I.H. Moal, A. Vangone, B.G. Pierce, P.L. Kastritis, M. Torchala, R. Chaleil, 
B. Jimenez-Garcia, P.A. Bates, Juan Fernandez-Recio, A.M.J.J. Bonvin and Z. Weng.  
**Updates to the integrated protein-protein interaction benchmarks: Docking benchmark version 5 and affinity benchmark version 2**. <BR>
_J. Mol. Biol._ *19*, 3031-3041 (2015).  
<https://doi.org/doi:10.1016/j.jmb.2015.07.016>

The repository contains the following information:

# HADDOCK-ready 

The directory contains HADDOCK-ready files for each entry of BM5.
Each sub-directory (one per complex) contains the following files:

* `XXX_r_u.pdb` : Unbound receptor PDB
* `XXX_l_u.pdb` : Unbound ligand PDB
* `XXX_r_b-matched.pdb` : Matched bound receptor PDB
* `XXX_l_b-matched.pdb` : Matched bound ligang PDB
* `XXX_r_u_cg.pdb` : Martini v2 coarse-grain models for the receptor proteins
* `XXX_l_u_cg.pdb` : Martini v2 coarse-grain models for the ligand proteins

And for each PDB there is an associated `.info` file providing statistics of the PDB content

Various distance restraints files are present:

* `ambig.tbl` : Ambiguous interaction restraints based on the true interface measured with a 3.9A cutoff
* `restraint-bodies.tbl` : If present, contains a list of distance restraints to keep unconnected bodies together 
* `XXX_*.tbl` : If present, contains a list of distance restraints to keep the ligand in place in the structure
* `hbonds.tbl` : the combination of the bodies and ligand distance restraints (used in HADDOCK)

And if there is a co-factor or ligand in the structure:

* `ligand.param` : the ligand parameter file as generated by PRODRG
* `ligand.top` : the ligand topology file as generated by PRODRG

Further each sub-directory contains an `ana_scripts` directory containing analysis scripts:

* `target.pdb` : the reference, matched complex 
* `target-unbound.pdb` : the unbound complex built by superimposing the unbound structures onto the reference complex
* `target.contacts5` : intermolecular contacts at 5A cutoff used for calculating the fraction of native contacts
* `target.izone` : the interface definition for i-RMSD calculations with ProFit (derived from all residue contacts at 10A)
* `target.izoneA` : same as `target.izone` but for chainA only
* `target.izoneB` : same as `target.izone` but for chainB only
* `target.lzone` : the zone definition for l-RMSD calculations with ProFit 
* `i-rmsd_to_xray.csh`: csh script to calculate i-RMSDs with ProFit from HADDOCK `file.nam` files
* `l-rmsd_to_xray.csh`: csh script to calculate l-RMSDs with ProFit from HADDOCK `file.nam` files
* `fraction-native.csh` "" csh script to calculate the fraction of native contacts from HADDOCK `file.nam` files
* `cluster-fnat.csh` : a script that generate cluster stats including RMSD and Fnat values
* `run_all.csh` : a csh script that runs the complete analysis of all three stages of HADDOCK

Note that the paths in the various analysis scripts must be adapted to your directory structure.
This can be done by running the `scripts/setup-ana_scripts.csh` script with as argument the directory name of all entries

Finally, the `HADDOCK-ready` directory also contains pre-calculated i-RMSD values for the superimposed unbound structures onto the reference complex and for each separate interface:

* `i-RMSD.dat` : Interface RMSD unbound superimposed versus reference, sorted in the order of the directory listing
* `i-RMSD-sorted.dat` : Interface RMSD unbound superimposed versus reference, sorted from small to large
* `i-RMSD_r.dat` : Interface RMSD of the unbound receptor interface versus reference, sorted in the order of the directory listing
* `i-RMSD_r-sorted.dat` : Interface RMSD of the unbound receptor interface superimposed versus reference, sorted from small to large
* `i-RMSD_l.dat` : Interface RMSD of the unbound ligand interface versus reference, sorted in the order of the directory listing
* `i-RMSD_l-sorted.dat` : Interface RMSD of the unbound ligand interface superimposed versus reference, sorted from small to large

Other sub-directories:

* `scripts` : directory containing various scripts used for generating restraint files, initial analysis and automation of running HADDOCK. Refer to the README file in that directory for details
* `data` : reference `run.cns` and patch files to setup HADDOCK runs for various scenarios



# structures-matched

This directory contains the matched PDB files for all entries of the benchmark.
All structures (bound or unbound) consist of a unique chain (A for the receptor, B for the ligand)
with non overlapping numbering.

The bound forms have been matched to the unbound, meaning that they have the same residue numbering
and only contain residues matching residues in the unbound forms.

For each entry XXX the following files are present:

* `XXX_r_u.pdb` : Unbound receptor PDB
* `XXX_l_u.pdb` : Unbound ligand PDB
* `XXX_r_b-matched.pdb` : Matched bound receptor PDB
* `XXX_l_b-matched.pdb` : Matched bound ligang PDB

And for each PDB there is an associated `.info` file providing statistics of the PDB content


# structures-orig

This directory contains the original PDB files for all entries of the benchmark as downloaded
from [https://zlab.umassmed.edu/benchmark/](https://zlab.umassmed.edu/benchmark/)

And for each PDB there is an associated `.info` file providing statistics of the PDB content


# scripts

A few basic `csh` scripts used to prepare the matched PDBs. 

Manual intervention and checking was however required in several instances
",2022-08-12
https://github.com/haddocking/Capri51-Target183,"# Capri Round 51 (COVID19 thematic) Target 183

Selected models for this target are available at `/selection`


## Requirements
* Python 3x + Pandas
* [haddock-tools](https://github.com/haddocking/haddock-tools)
* [pdb-tools](https://github.com/haddocking/pdb-tools)
* [HADDOCK v2.4 Webserver File Interface](https://bianca.science.uu.nl/haddock2.4/submit_file) to replicate the results

***
## Information

```
Capri target description:
EXOSC8 is a non-catalytic component of the RNA exosome complex (9 subunits). PDB:2NN6 seems to contain the whole exosome. Nsp8-EXOSC2, Nsp8-EXOSC3 and Nsp8-EXOSC5 are also part of the list of Virus-Host interactions with 80 % template coverage. Predictors may need to consider interactions with these subunits as well. Note that Q13868 (EXOSC2) and Q9NPD3 (EXOS4) do not show up as interactors of Nsp7 and Nsp12 (which form a complex with NSsp18) in Gordon et al. dataset.
```

This could be interpreted as:
```
Nsp8 -EXOSC2 ✅
Nsp8 -EXOSC3 ✅
Nsp8 -EXOSC5 ✅
Nsp7 -EXOSC2 🚫
Nsp7 -EXOS4  🚫
```
***

## Approach

Dock the Nsp7/8 dimer against the full Exosome and then filter out conformations in which Nsp7 interacts with EXOSC2 and EXOSC4. Use the remaining conformations for a contact analysis and take the result of this contact analysis as restraints for another round of docking now using the hexadecamer of Nsp7/8 which was part of T184.

***

## Input

1. Mol A - Nsp7/Nsp8
    The PDB [6m5i](https://www.rcsb.org/structure/6M5I) is Nsp7/Nsp8 complex but the structure has a very bad quality and it’s also incomplete, it's a perfect match with Nsp8. We run molecular refinement to improve its quality - [Nsp7/Nsp8 refinement](runs/nsp7_8-refinement.hson)

2. Mol B - Exosome

    The PDB [2nn6](https://www.rcsb.org/structure/2NN6) has the whole exosome but the structure is also not of good quality. Refine it as well - [Exosome refinement](runs/exosome-refinement.json)


## Processing

To use it in HADDOCK the Top1 of each of the refinement run was renumbered/rechained using [pdb-tools](http://github.com/haddocking/pdb-tools) and its solvent accessible surface calculated with [haddock-tools/calc-accessibility.py](http://github.com/haddocking/haddock-tools)

```
# Renumbered residue mapping
NSP8 A  1-115
NSP7 B  116-196

EXOSC9  A
EXOSC4  B   304-537
EXOSC8  C   538-807
EXOSC5  D   808-1012
EXOSC7  E
EXOSC6  F
EXOSC3  G   1511-1746
EXOSC2  H   1747-1996
EXOSC1  I
```
```
$ python haddock-tools/calc-accessibility.py input/nsp8_A-nsp7_B.pdb
$ python haddock-tools/calc-accessibility.py input/exosome.pdb
```

### Get Accessible residues of Nsp8

Use some simple python scripting to find the intersection between all accessible residues in the dimer and those ones belonging only to Nsp8.


```python
$ python
>>> nsp8 = list(range(1,115))
>>> all_accessible = [1,2,3,4,5,6,7,9,10,13,14,18,20,21,23,25,28,29,32,35,36,37,42,47,48,49,51,58,60,63,64,67,68,69,70,72,75,77,79,81,82,86,87,88,89,90,92,93,94,95,97,98,99,100,102,103,105,107,113,115,116,117,119,120,123,126,130,133,136,138,139,140,141,142,145,146,148,149,152,153,156,158,159,160,161,162,165,178,179,182,184,185,188,189,191,192,193,194,195,196]
>>> set(nsp8).intersection(all_accessible)
{1, 2, 3, 4, 5, 6, 7, 9, 10, 13, 14, 18, 20, 21, 23, 25, 28, 29, 32, 35, 36, 37, 42, 47, 48, 49, 51, 58, 60, 63, 64, 67, 68, 69, 70, 72, 75, 77, 79, 81, 82, 86, 87, 88, 89, 90, 92, 93, 94, 95, 97, 98, 99, 100, 102, 103, 105, 107, 113}
```


### Get Accessible residues of EXOSC2/3/4/8

Same for EXOSC2/3/4/8:
```python
$ python
>>> exosc2 = list(range(1747,1996))
>>> exosc3 = list(range(1511,1746))
>>> exosc5 = list(range(808,1012))
>>> exosc8 = list(range(538,807))
>>> all_accessible = [3,8,11,12,15,16,19,20,21,25,26,28,31,34,36,42,43,52,60,62,64,65,67,68,69,70,72,75,77,89,92,93,94,96,97,98,99,111,114,117,118,119,120,123,125,126,127,164,166,168,169,170,171,173,175,176,177,178,180,181,182,184,188,200,201,203,218,227,228,229,238,241,243,244,245,249,252,253,256,259,260,263,264,266,267,270,271,273,274,275,277,278,279,280,281,282,286,287,290,291,292,293,294,297,299,301,303,305,306,307,310,311,312,314,316,317,320,322,324,329,341,356,357,358,359,360,362,363,364,365,379,380,381,383,384,386,387,388,389,390,391,393,405,410,411,413,414,427,450,463,464,471,472,475,476,477,478,480,489,500,502,504,505,507,508,511,512,515,518,519,522,523,525,526,527,530,533,534,536,537,538,539,540,541,542,543,544,547,550,551,552,553,556,557,559,560,562,565,566,567,569,574,577,586,597,598,599,601,602,603,605,606,607,609,611,623,625,627,628,629,645,648,651,653,654,659,660,661,693,695,698,700,702,703,704,705,706,707,709,710,711,713,714,715,716,717,719,721,722,733,744,747,748,750,759,760,762,775,777,778,781,784,785,787,788,791,792,795,798,799,801,802,803,805,806,807,808,810,819,822,834,847,848,849,850,852,854,861,863,865,866,867,871,877,880,881,887,889,902,907,926,938,939,940,941,942,947,948,949,952,953,955,957,965,966,975,977,981,984,985,988,989,991,992,995,999,1002,1006,1007,1010,1011,1013,1014,1016,1019,1020,1024,1029,1036,1039,1042,1044,1046,1048,1049,1051,1062,1063,1073,1075,1076,1078,1079,1080,1081,1083,1088,1090,1092,1099,1100,1101,1102,1103,1104,1105,1107,1108,1112,1118,1119,1122,1125,1129,1130,1136,1137,1138,1139,1172,1175,1177,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1196,1197,1198,1209,1210,1218,1219,1222,1225,1235,1237,1245,1246,1252,1255,1256,1259,1263,1265,1266,1267,1268,1270,1271,1274,1277,1278,1279,1281,1282,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1297,1299,1301,1306,1309,1317,1318,1327,1329,1330,1331,1332,1333,1335,1337,1339,1348,1349,1351,1352,1353,1354,1356,1357,1358,1360,1375,1377,1378,1380,1381,1398,1429,1430,1431,1432,1433,1434,1435,1442,1444,1445,1446,1450,1460,1470,1471,1472,1473,1475,1476,1479,1482,1485,1486,1489,1493,1494,1497,1501,1504,1505,1506,1507,1508,1509,1510,1511,1512,1514,1515,1516,1517,1519,1520,1527,1529,1531,1532,1533,1534,1536,1539,1542,1544,1546,1556,1558,1560,1561,1563,1564,1565,1566,1567,1568,1570,1578,1583,1584,1585,1593,1594,1596,1597,1598,1601,1605,1615,1618,1619,1620,1621,1622,1623,1625,1626,1627,1629,1630,1631,1637,1639,1643,1644,1654,1655,1656,1657,1658,1659,1660,1662,1665,1666,1667,1672,1674,1680,1684,1685,1687,1689,1690,1693,1694,1696,1697,1699,1713,1714,1716,1717,1720,1723,1726,1727,1730,1732,1733,1734,1735,1737,1738,1742,1745,1746,1747,1753,1754,1756,1758,1759,1760,1762,1763,1765,1769,1770,1771,1772,1773,1783,1784,1785,1786,1787,1788,1792,1799,1801,1802,1803,1812,1814,1815,1816,1817,1821,1823,1825,1831,1832,1833,1836,1838,1839,1842,1843,1846,1855,1856,1858,1859,1860,1861,1864,1868,1869,1870,1871,1882,1886,1889,1890,1891,1892,1895,1897,1899,1901,1917,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1949,1951,1953,1956,1960,1964,1967,1968,1970,1971,1972,1973,1975,1976,1978,1981,1982,1985,1988,1989,1990,1992,1993,1994,1995,1996,1997,2003,2004,2005,2008,2009,2010,2012,2014,2015,2017,2021,2022,2023,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2047,2048,2049,2050,2051,2053,2054,2055,2056,2057,2058,2059,2060,2061,2063,2065,2068,2072,2073,2075,2080,2082,2083,2084,2086,2089,2090,2091,2093,2097,2099,2100,2104,2105,2106,2111,2114,2116,2122,2125,2128,2129,2130,2131,2139,2140,2141,2145,2146,2148,2150,2152,2154,2156,2157,2158,2159,2160,2162,2166,2167,2169,2171,2173,2176]
>>> selected_res = []
>>> selected_res += list(set(exosc2).intersection(all_accessible))
>>> selected_res += list(set(exosc3).intersection(all_accessible))
>>> selected_res += list(set(exosc5).intersection(all_accessible))
>>> selected_res += list(set(exosc8).intersection(all_accessible))
>>> selected_res
[1747, 1753, 1754, 1756, 1758, 1759, 1760, 1762, 1763, 1765, 1769, 1770, 1771, 1772, 1773, 1783, 1784, 1785, 1786, 1787, 1788, 1792, 1799, 1801, 1802, 1803, 1812, 1814, 1815, 1816, 1817, 1821, 1823, 1825, 1831, 1832, 1833, 1836, 1838, 1839, 1842, 1843, 1846, 1855, 1856, 1858, 1859, 1860, 1861, 1864, 1868, 1869, 1870, 1871, 1882, 1886, 1889, 1890, 1891, 1892, 1895, 1897, 1899, 1901, 1917, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1949, 1951, 1953, 1956, 1960, 1964, 1967, 1968, 1970, 1971, 1972, 1973, 1975, 1976, 1978, 1981, 1982, 1985, 1988, 1989, 1990, 1992, 1993, 1994, 1995, 1536, 1539, 1542, 1544, 1546, 1556, 1558, 1560, 1561, 1563, 1564, 1565, 1566, 1567, 1568, 1570, 1578, 1583, 1584, 1585, 1593, 1594, 1596, 1597, 1598, 1601, 1605, 1615, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1626, 1627, 1629, 1630, 1631, 1637, 1639, 1643, 1644, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1662, 1665, 1666, 1667, 1672, 1674, 1680, 1684, 1685, 1687, 1689, 1690, 1693, 1694, 1696, 1697, 1699, 1713, 1714, 1716, 1717, 1720, 1723, 1726, 1727, 1730, 1732, 1733, 1734, 1735, 1737, 1738, 1742, 1745, 1511, 1512, 1514, 1515, 1516, 1517, 1519, 1520, 1527, 1529, 1531, 1532, 1533, 1534, 902, 907, 926, 808, 810, 938, 939, 940, 941, 942, 819, 947, 948, 822, 949, 952, 953, 955, 957, 834, 965, 966, 847, 848, 849, 850, 975, 852, 977, 854, 981, 984, 985, 988, 861, 989, 863, 991, 865, 866, 867, 992, 995, 871, 999, 1002, 877, 1006, 1007, 880, 881, 1010, 1011, 887, 889, 538, 539, 540, 541, 542, 543, 544, 547, 550, 551, 552, 553, 556, 557, 559, 560, 562, 565, 566, 567, 569, 574, 577, 586, 597, 598, 599, 601, 602, 603, 605, 606, 607, 609, 611, 623, 625, 627, 628, 629, 645, 648, 651, 653, 654, 659, 660, 661, 693, 695, 698, 700, 702, 703, 704, 705, 706, 707, 709, 710, 711, 713, 714, 715, 716, 717, 719, 721, 722, 733, 744, 747, 748, 750, 759, 760, 762, 775, 777, 778, 781, 784, 785, 787, 788, 791, 792, 795, 798, 799, 801, 802, 803, 805, 806]
```

## Docking stage 1

* MolA = Nsp8/7 dimer - Solvent accessible active
* MolB = EXOSC2/3/5/8 - Surface passive

Using ""Optimize for Bioinformatics prediction"" option

* [Docking parameters](runs/28513-nsp8-surf-act-exosc2_3_5-passive_ncvpart.json)

_Submit it via HADDOCK's file interface and download/uncompress in `runs/`_

## Analysis stage 1

1. Filter out Nsp7+EXOSC2/EXOS4 contacts 

This script will first calculate the contacts of each single model present inside the `it0` directory of the simulation using [haddock-tools/contact-chainID](http://github.com/haddocking/haddock-tools), this will generate many `.contacts` file inside the directory. Then for each model a function will check how many contacts are part of the ""forbidden contacts"" (lower than 20%), those being nsp7+exosc2/4. A text file will be written containing the filtered models.

```
$ python scripts/filter-contacts.py -h
usage: filter-contacts.py [-h] [--np NP] [--cutoff CUTOFF] run_directory contact_exec

positional arguments:
  run_directory    file.nam generated by HADDOCK
  contact_exec     Compiled contact script

optional arguments:
  -h, --help       show this help message and exit
  --np NP          Number of processors to use
  --cutoff CUTOFF  Cutoff of forbidden contacts allowed in the PDB to be filtered, float between 0 and 1

$ python scripts/filter-contacts.py runs/28513-nsp8-surf-act-exosc2_3_5-passive_ncvpart ~/repos/haddock-tools/contact-chainID --np 8
```

2. Run contact analysis

We then rank the selected models by their HADDOCK-score and extract residues that were observed to be in contact for a specific subset. Here we are using the Top10 models since Haddock did a good job of enriching the desired contacts.


```
$ python scripts/contact_analysis.py -h
usage: contact_analysis.py [-h] [--top TOP] file_list input_pdblist

positional arguments:
  file_list      File containing the Haddock-scores for each PDB
  input_pdblist  Filtered PDB List containing the full path of the PDB

optional arguments:
  -h, --help     show this help message and exit
  --top TOP      After ranking, how many models should be considered when counting contacts, default=100
$ python scripts/contact_analysis.py runs/28513-nsp8-surf-act-exosc2_3_5-passive_ncvpart/structures/it0/file.list filtered-pdbs.list
```

The contact analysis shows that basically all of Nsp8 makes contact with the Exosome, however some of the Exosome residues have been filtered out, from the initial 258 to 99.

```python
$ python
>>> filt_exosome = [1527,1545,1546,1547,1548,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,547,548,1574,551,552,553,559,2105,1618,1619,1620,1621,1622,1623,1626,1627,606,2145,2146,1637,2152,2153,2154,2156,2158,2160,1653,2166,1655,1656,1657,1658,1659,2167,1660,1661,1654,2169,2171,533,1666,651,653,654,1680,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1693,1694,1696,1697,1698,1699,1700,175,1713,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1741,1742,1745,721,1747,1756,1759,1760,1761,1762,1763,1765,1769,1772,1782,1783,1784,1785,1786,1787,760,775,776,1801,1802,1803,777,778,781,780,784,290,292,293,295,296,297,298,299,300,301,302,1853,1854,1855,1856,1857,1858,1859,1868,1869,1870,1871,1872,1889,1890,1892,1895,1896,1897,1899,1919,1920,410,411,999,938,939,1002,1003,953,955,1994,1995,992,993,995,996,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1006,1010,1007,1009,1012,1011,1521,1528,1529,1530,1531,1532,1533,1534,1526]
>>> exosc8 = list(range(538,807))
>>> sele_exosc8 = list(set(exosc8).intersection(filt_exosome))
>>> sele_exosc8.sort()
>>> sele_exosc8
[547, 548, 551, 552, 553, 559, 606, 651, 653, 654, 721, 760, 775, 776, 777, 778, 780, 781, 784]
```

Visualize this in PyMol with:
```
$ pymol input/exosome.pdb
PyMol> color white
PyMol> sele sas, resid 1747+1753+1754+1756+1758+1759+1760+1762+1763+1765+1769+1770+1771+1772+1773+1783+1784+1785+1786+1787+1788+1792+1799+1801+1802+1803+1812+1814+1815+1816+1817+1821+1823+1825+1831+1832+1833+1836+1838+1839+1842+1843+1846+1855+1856+1858+1859+1860+1861+1864+1868+1869+1870+1871+1882+1886+1889+1890+1891+1892+1895+1897+1899+1901+1917+1919+1920+1921+1922+1923+1924+1925+1926+1927+1928+1929+1930+1931+1932+1949+1951+1953+1956+1960+1964+1967+1968+1970+1971+1972+1973+1975+1976+1978+1981+1982+1985+1988+1989+1990+1992+1993+1994+1995+1536+1539+1542+1544+1546+1556+1558+1560+1561+1563+1564+1565+1566+1567+1568+1570+1578+1583+1584+1585+1593+1594+1596+1597+1598+1601+1605+1615+1618+1619+1620+1621+1622+1623+1625+1626+1627+1629+1630+1631+1637+1639+1643+1644+1654+1655+1656+1657+1658+1659+1660+1662+1665+1666+1667+1672+1674+1680+1684+1685+1687+1689+1690+1693+1694+1696+1697+1699+1713+1714+1716+1717+1720+1723+1726+1727+1730+1732+1733+1734+1735+1737+1738+1742+1745+1511+1512+1514+1515+1516+1517+1519+1520+1527+1529+1531+1532+1533+1534+902+907+926+808+810+938+939+940+941+942+819+947+948+822+949+952+953+955+957+834+965+966+847+848+849+850+975+852+977+854+981+984+985+988+861+989+863+991+865+866+867+992+995+871+999+1002+877+1006+1007+880+881+1010+1011+887+889+538+539+540+541+542+543+544+547+550+551+552+553+556+557+559+560+562+565+566+567+569+574+577+586+597+598+599+601+602+603+605+606+607+609+611+623+625+627+628+629+645+648+651+653+654+659+660+661+693+695+698+700+702+703+704+705+706+707+709+710+711+713+714+715+716+717+719+721+722+733+744+747+748+750+759+760+762+775+777+778+781+784+785+787+788+791+792+795+798+799+801+802+803+805+806
PyMol> sele exosc8_con, resid 547+548+551+552+553+559+606+651+653+654+721+760+775+776+777+778+780+781+784
PyMol> color red, sas
PyMol> color magenta, exosc8_con
PyMol> show sticks, exosc8_con
```

<!-- This residues correspond to `16, 17, 20, 22, 75, 120, 122, 123, 190, 229, 230, 244, 245, 246, 247, 249, 250, 253` in the original PDB -->


## Final docking


Run another docking using the filtered residues of the EXOSC8 and Nsp8
* MolA = Nsp8 - Hexadecamer - All surface solvent accessible passive
* MolB = EXOSC8 - 2nn6 (renumbered) - Active: 585,586,589,591,644,689,691,692,759,798,799,813,814,815,816,818,819,822

https://bianca.science.uu.nl/haddock2.4/run/1524990235/31391-nsp8-hexadeca-exosome

Again using ""Optimize for Bioinformatics prediction"" option

* [Docking parameter](runs/final-docking.json)

```
Hexadecamer Nsp8 solvent accessible:
2304, 2306, 2309, 2310, 2313, 2317, 2328, 2329, 2330, 2332, 2341, 2344, 2345, 2348, 2349, 2350, 2353, 2356, 2358, 2360, 2362, 2363, 2368, 2370, 2371, 2373, 2375, 2376, 2378, 2379, 2380, 2381, 2383, 2384, 2386, 2285, 2286, 2287, 2290, 2294, 2301, 2302

Full exosome target residues
585,586,589,591,644,689,691,692,759,798,799,813,814,815,816,818,819,822
```

## Selection

From the run below we select the Top 10 clusters, alternating between the best model of each cluster and the second, then we fill the selection to 100 models by adding conformations based on their single-structure ranking.

The following script will:

Read the cluster information, rank the clusters and identify to which cluster each structure belongs to. For prosperity, it prepares a dataframe that can easily be parsed with pandas, example: 
```python
>>> df = pd.DataFrame(data, columns=['pdb', 'single_structure_ranking', 'overall_cluster_ranking', 'internal_cluster_ranking'])
>>> top1_top10_cluster = df[(df['overall_cluster_ranking'] <= 5) & (df['internal_cluster_ranking'] == 1)]['pdb']
```


After the models have been selected, the script then does sequence alignment between each of the models and the template to create a reference numbering dictionary so that the final submission matches the provided template. The renumbered models are made into an ensemble and prepared for submission using pdb-tools's `pdb_mkensemble` and `pdb_tidy`

```
$ python scripts/prepare_submission.py -h
usage: prepare_submission.py [-h] run_path template

positional arguments:
  run_path    Location of the run
  template    Template provided by CAPRI

optional arguments:
  -h, --help  show this help message and exit

$ python prepare_submission.py capri_51_183.brk runs/31349-nsp8-hexadeca-exosome
```

*** 
",2022-08-12
https://github.com/haddocking/CASP-CAPRI-T70-tutorial,"Our information-driven docking approach [HADDOCK](http://www.bonvinlab.org/software/haddock2.2) is a consistent top predictor and scorer since the start of its participation in the [CAPRI](http://www.ebi.ac.uk/msd-srv/capri) community-wide experiment. This sustained performance is due, in part, to its ability to integrate experimental data and/or bioinformatics information into the modelling process, and also to the overall robustness of the scoring function used to assess and rank the predictions. 

This tutorial will demonstrate the use of HADDOCK for predicting target70 of the CASP-CAPRI experiment. This target was given to the CAPRI community as a tetramer, but there has been discussions whether the biological unit is a dimer or a tetramer. We will use this target to illustrate the ab-initio docking mode of HADDOCK, using a combination of [center-of-mass restraints](http://www.bonvinlab.org/software/haddock2.2/run/#disre) to bring the subunits together and [symmetry restraints](http://www.bonvinlab.org/software/haddock2.2/run/#sym) to define the symmetry of the assembly.

The tutorial is described on our web page at: [http://www.bonvinlab.org/education/HADDOCK-CASP-CAPRI-T70/](http://www.bonvinlab.org/education/HADDOCK-CASP-CAPRI-T70/)


#Citation#
This tutorial is citable. 

For this use the following DOI:
[![DOI](https://zenodo.org/badge/21589/haddocking/CASP-CAPRI-T70-tutorial.svg)](https://zenodo.org/badge/latestdoi/21589/haddocking/CASP-CAPRI-T70-tutorial)
",2022-08-12
https://github.com/haddocking/cport,"# CPORT: Consensus Prediction Of interface Residues in Transient complexes

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![python lint](https://github.com/haddocking/cport/actions/workflows/lint.yml/badge.svg)](https://github.com/haddocking/cport/actions/workflows/lint.yml)
[![unittests](https://github.com/haddocking/cport/actions/workflows/unittests.yml/badge.svg)](https://github.com/haddocking/cport/actions/workflows/unittests.yml)
[![Codacy Badge](https://app.codacy.com/project/badge/Grade/c4dd6c7da85847e1832b9668beb6de31)](https://www.codacy.com/gh/haddocking/cport/dashboard?utm_source=github.com&utm_medium=referral&utm_content=haddocking/cport&utm_campaign=Badge_Grade)
[![Codacy Badge](https://app.codacy.com/project/badge/Coverage/c4dd6c7da85847e1832b9668beb6de31)](https://www.codacy.com/gh/haddocking/cport/dashboard?utm_source=github.com&utm_medium=referral&utm_content=haddocking/cport&utm_campaign=Badge_Coverage)

[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/6074/badge)](https://bestpractices.coreinfrastructure.org/projects/6074)
[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8B%20%20%E2%97%8B%20%20%E2%97%8F-orange)](https://fair-software.eu)

## DISCLAIMER

This repository contains an **ongoing development** of the next version of CPORT, and as such it is not stable yet. The codebase might change drastically in the future.

For the current stable version, please access the web service at [https://alcazar.science.uu.nl/services/CPORT](https://alcazar.science.uu.nl/services/CPORT/).

Please also refer to the original publication:

- de Vries, S. J. & Bonvin, A. M. J. J. [CPORT: A Consensus Interface Predictor and Its Performance in Prediction-Driven Docking with HADDOCK](https://doi.org/10.1371/journal.pone.0017695). PLoS ONE vol. 6 e17695 (2011).

---

## Predictors

| Server                                                           | Status | Implemented |
| ---------------------------------------------------------------- | ------ | ----------- |
| [WHISCY](https://wenmr.science.uu.nl/whiscy/)                    | 🔴     |             |
| [SCRIBER](http://biomine.cs.vcu.edu/servers/SCRIBER/)            | 🟢     | ✔️          |
| [ISPRED4](https://ispred4.biocomp.unibo.it/ispred/default/index) | 🟢     | ✔️          |
| [SPPIDER](https://sppider.cchmc.org)                             | 🟢     | ✔️          |
| [meta-PPISP](https://pipe.rcc.fsu.edu/meta-ppisp.html)           | 🟢     | ✔️          |
| [PredUs2](http://honig.c2b2.columbia.edu/predus)                 | 🟠     |             |
| [Cons-PPISP](https://pipe.rcc.fsu.edu/ppisp.html)                | 🟢     | ✔️          |
| [PredictProtein](https://predictprotein.org)                     | 🟢     | ✔️          |
| [PSIVER](https://mizuguchilab.org/PSIVER/)                       | 🟢     | ✔️          |
| [CSM-Potential](http://biosig.unimelb.edu.au/csm_potential/)     | 🟢     | ✔️          |
| [ScanNet](http://bioinfo3d.cs.tau.ac.il/ScanNet/index_real.html) | 🟢     | ✔️          |

## Installation

Please refer to [INSTALL.md](INSTALL.md) for installation instructions.

## Example

```text
cport path/to/file/1PPE.pdb E
```

## How to contribute

Please have a look at [CONTRIBUTE](CONTRIBUTING.md)

## How to get Support

If you encounter a bug or would like to request a feature, please open an ISSUE or a PR.
",2022-08-12
https://github.com/haddocking/cyclic-peptides,"

# Cyclic-peptide-docking-benchmark

[![DOI](https://zenodo.org/badge/433746674.svg)](https://zenodo.org/badge/latestdoi/433746674)

![This is an image](https://github.com/haddocking/cyclic-peptides/blob/main/cyclic_peptide_banner.png)

This is the docking benchmark described in the manuscript:
_A Cyclisation and Docking Protocol for Cyclic Peptide Modeling using HADDOCK2.4_
Vicky Charitou, Siri Camee van Keulen, Alexandre M.J.J. Bonvin

DOI: https://doi.org/10.1021/acs.jctc.2c00075


## 1. Content 
In this repository you can find the Dataset directory which contains:

* 30 `XXXX_complex` folders listed according to their PDB ID containing the files used for docking
* 30 `XXXX_peptide` folders listed according to the complex PDB ID containing:
	* Input peptide conformations for Step 1
	* Output peptide conformations for Step 3 (50 structures)
* `setup-analysis_example.csh` A script to run the Fnat (and i-RMSD) analysis 

### 1.1. Each `XXXX_complex` folder contains: 

* HADDOCK-ready files:
	* `XXX_r_b.pdb` Bound receptor PDB with renumbered atoms and renamed chain ID
	* `XXX_r_u.pdb` Unbound receptor PDB with renumbered atoms and renamed chain ID
	* `XXX_peptide_XXXw.pdb` Bound ligand PDB with renumbered atoms and renamed chain ID
	* `ensemble_pdb.list` List of all ligand PDBs that are included in the ensemble
		
* Distance restraints file:
	* `ambig.tbl` Ambiguous interaction restraints
	* `hbonds.tbl` Unambiguous restraints

* An analysis directory named `ana_script` that contains:
	* `target.pdb` Reference complex structure with renumbered atoms and renamed chainID that matches HADDOCK output
	* `cluster-fnat.csh` `fraction-native.csh` `i-rmsd_to_xray.csh` `l-rmsd_to_xray.csh` `run_all-no-it0.csh` `make-target-files.csh` `run_all.csh` `run_all-dockQ.csh` `run_dockQ.csh` All required scripts for the analysis
	* `target.contacts10` `target.izoneA` `target.contacts5` `target.izoneB` `target.izone` `target.lzone` All required files for the analysis: 

### 1.2. Each `XXXX_peptide` folder contains:

* HADDOCK-ready files:
	* `peptide_beta.pdb` Pymol generated PDB with ligand in a beta-sheet conformation
	* `peptide_polypro.pdb` Pymol generated PDB with ligand in a polyproline conformation
	* `pdb.list` List of ligand PDBs for step2 of cyclisation
	* `XXX_peptide_XXXw.pdb` Unbound ligand PDB (HADDOCK output from step2 of cyclisation)
	* `step3_pdb.list` List of ligand PDBs for step3 of cyclisation

*  Distance restraints file:
	* `unambig.tbl` Unambiguous cyclisation restrains

## 2. Docking results 
Results of the 50STR_COMB docking protocol can be found on SBGrid: [https://data.sbgrid.org/dataset/912](https://data.sbgrid.org/dataset/912)
",2022-08-12
https://github.com/haddocking/D3R-tools,"# Intro

This is a collection of scripts that the HADDOCK team made use
of during D3R Grand Challenges 2 and 3. In addition to the scripts
we also provide the data that was used to train our ligand-based
binding affinity prediction method for GC3.

# Data

Under `data/GC3/CatS`
Under `data/GC3/Kinase/[JAK2-SC2/vEGFR2/p38a]`

All of these directories contain three files:

+ TRAIN.matrix This is symmetrix square matrix that contains all-vs-all Atom-Pair
based training set similarities. It is used to train the model.
+ TEST.matrix This is an assymetric matrix that contains the Atom-Pair based simi-
larities prediction set similarities. It is used for the ranking of the models that
was submitted for the competition.
+ IC50_train.csv This is the file that lists the training set binding afinities.

The matrix files have been compressed to cut down on the file size.

All three files are in CSV format. The IC50 file has a header as well.

# Code

Under `code`

`train.m` and `test.m`

These two scripts are used to train the SVM and carry out the predictions respectively.

`calc_atom-pair.R`

This is an R script that accepts two SDF formatted files as arguments and creates
an Atom Pair-based similarity matrix. It depends on the `ChemmineR` package.

Under `code/renaming`

This is a collection of scripts that can be used to rename PDB files of ligands.
The motivation for writing this was that different types of software had different
ways of naming the atoms that make up a ligand. Some name all atoms sequentially,
some sequentially by aotm type, some randomly. However, we needed the ligand atoms
to have consistent naming to allow for structural calculations to take place
succesfully.

The master script is the `rename.sh` bash script and as long as every other script
in the folder is in the same working directory or in the $PATH it should work as
advertised. The other scripts are:

`getListOfCommonAtoms.R`

This is an R script that calculates the Maximum Common Substructure (MCS) between
two molecules. If the two molecules are identical (they differ only in the way their
atoms are name) it should identify the entire molecule as being part of the MCS. It
prints an atom index mapping that is used by the python script below to do the actual
renaming.

`rename_atoms.py`

Python code that reads the output file of the R script above and renames the file
that is provided as an argument according to the other file that is provided as an
argument.

`oe-convert.py`

This is a Python OpenEye script that converts files between various formats often
encountered during chemoinformatics workflows. It is distributed with all OpenEye
installations and can also be obtained
[online](https://docs.eyesopen.com/toolkits/python/oechemtk/molreadwrite.html#readwritecompressedfiles.py)
",2022-08-12
https://github.com/haddocking/DACUM,"# DACUM
**D**atabase of binding **A**ffinity **C**hange **U**pon **M**utations in protein complexes

The DACUM is a cleaned subset of the SKEMPI database (Moal and Fernández-Recio, 2012), which contains 1872 non-redundanted, single point mutations with additional information on the experimental methods used to measure the binding affinity. 

## Format
**1. SKEMPI_ID:** The column is a combination of columns “Protein” and “Mutation_PDB” from SKEMPI database (see below).

**2. Protein:** (SKEMPI) The PDB entry for the complex, followed by the chain identifiers for the two subunits. The first chain(s) correspond to protein 1 (column 16) and the second chain(s) correspond to protein 2 (column 17). Here is the [Protein Data Bank](http://www.rcsb.org/pdb/home/home.do).

**3. Mutation_PDB:** (SKEMPI) The mutation corresponding to the residue numbering found in the Protein Data Bank. The first character is the one letter amino acid code for the original residue, the second character is the chain identifier, the third to penultimate characters indicate the residue number, followed by the residue insertion code where applicable, and the final character indicates the mutant amino acid. Where multiple mutations are present, they are separated by commas.

**4. Mutation_cleaned:** (SKEMPI) The mutation corresponding to the residue numbering in the 'cleaned' pdb files provided by SKEMPI, in the same format as for column 3 ""Mutation_PDB"". Here are the [cleaned PDB files]( http://life.bsc.es/pid/mutation_database/SKEMPI_pdbs.tar.gz).

**5. Location:** (SKEMPI) The location of the mutation in or away from the binding site, i.e. COR, RIM, SUP, INT and SUR, as defined in (Levy, 2010).

**6. Position:** The position of single point mutation, i.e. interface (ITF) or non-interface (NIF) mutation, is defined based on the column 5 “Location”. The mutation located on COR, RIM or SUP region is defined as interface mutation, while the one on INT or SUR region as non-interface mutation.

**7. Kd_mut (M):** (SKEMPI) The dissociation constant of the mutant form.

**8. Kd_wt (M):** (SKEMPI) The dissociation constant of the wild-type form, or form in the PDB structure.

**9. dG_mut (kcal mol^(-1)):** The binding free energy of the mutant form. dG_mut = RTln(Kd_mut), R the ideal gas constant (0.0019872 kcal K^(-1) mol^(-1)), T the temperature (column 12) in Kelvin.  

**10. dG_wt (kcal mol^(-1)):** The binding free energy of the wild-type form, or form in the PDB structure. dG_wt = RTln(Kd_wt).

**11. ddG (kcal mol^(-1)):** The binding affinity change on mutation. ddG = dG_mut - dG_wt = RTln(Kd_mut/Kd_wt).

**12. Temperature (K):** (SKEMPI) The temperature at which the experiment was performed.

**13. Method:** The abbreviation of experimental measurement method for each entry. The corresponding full names of methods are listed in the following table of Experimental measurement methods.

**14. Hold_out_type:** (SKEMPI) Some of the complexes are classified as protease-inhibitor (PI), or antibody-antigen (AB). This classification was introduced to aid in the cross-validation of empirical models trained using the data in the SKEMPI database. so that proteins of a similar type can be simultaneously held out during a cross-validation.

**15. Hold_out_proteins:** (SKEMPI) This column contains the PDB identifiers (column 2) and/or hold-out types (column 14) for all the protein complexes which should be excluded from the training when cross-validating an empirical model trained on this data, so as to avoid contaminating the training set with information pertaining to the binding site being evaluated.

**16. Protein 1:** (SKEMPI) This is the name of the protein which corresponds to the first chain(s) given in column 2 ""Protein"".

**17. Protein 2:** (SKEMPI) This is the name of the protein which corresponds to the second chain(s) given in column 2 ""Protein"".

**18. kon_mut (M^(-1)s^(-1)):** (SKEMPI) The association rate for the mutant protein, where available.

**19. kon_wt (M^(-1)s^(-1)):** (SKEMPI) The association rate for the wild-type protein or protein in the crystal structure, where available.

**20. koff_mut (s^(-1)):** (SKEMPI) The dissociation rate for the mutant protein, where available.

**21. koff_wt (s^(-1)):** (SKEMPI) The dissociation rate for the wild-type protein or protein in the crystal structure, where available.

**22. dH_mut (kcal mol^(-1)):** (SKEMPI) The enthalpy of association for the mutant protein, where available.

**23. dH_wt (kcal mol^(-1)):** (SKEMPI) The enthalpy of association for the wild-type protein or protein in the crystal structure, where available.

**24. dS_mut (cal mol^(-1) K^(-1)):** (SKEMPI) The entropy of association for the mutant protein, where available.

**25. dS_wt (cal mol^(-1) K^(-1)):** (SKEMPI) The entropy of association for the wild-type protein or protein in the crystal structure, where available.

**26. Reference:** The column is extracted from the “Reference” in SKEMPI database. The PubMed ID is given where available, otherwise the whole reference is provided.

**27. Notes:** (SKEMPI) Notes regarding the entry.

**28. Notes_method:** Notes regarding the column 13 ""Method"".


## Experimental measurement methods
| Abbreviation | Full name                                  |
|--------------|--------------------------------------------|
| ITC          | Isothermal Titration Calorimetry           |
| SPR          | Surface Plasmon Resonance                  |
| FL           | Fluorescence                               |
| SP           | Spectroscopy                               |
| SFFL         | Stopped-Flow Fluorescence                  |
| SFSP         | Stopped-Flow Spectroscopy                  |
| IAFL         | Inhibition Assay Fluorescence              |
| IASP         | Inhibition Assay Spectroscopy              |
| IAGE         | Inhibition Assay Gelelectrophoresis        |
| IARA         | Inhibition Assay Radioactivity             |
| RA           | Radioactivity                              |
| CSPRIA       | Competition Solid-Phase Radio-Immune Assay |
| ELFA         | Enzyme-Linked Functional Assay             |
| ELISA        | Enzyme-Linked Immunosorbent Assay          |
| EMSA         | Electrophoretic Mobility Shift Assay       |

## Citation
Cunliang Geng, Anna Vangone and Alexandre M.J.J. Bonvin, [Exploring the interplay between experimental methods and the performance of predictors of binding affinity change upon mutations in protein complexes. ][1] _Protein Engineering, Design, and Selection_, 29, 291-299 (2016).


## Reference
- Moal, I. H. & Fernández-Recio, J. SKEMPI: a Structural Kinetic and Energetic database of Mutant Protein Interactions and its use in empirical models. Bioinformatics 28, 2600–2607 (2012).
- Levy, E. D. A Simple Definition of Structural Regions in Proteins and Its Use in Analyzing Interface Evolution. Journal of Molecular Biology 403, 660–670 (2010).

[1]: https://doi.org/10.1093/protein/gzw020
",2022-08-12
https://github.com/haddocking/disvis,"# DisVis

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1037197.svg)](https://doi.org/10.5281/zenodo.1037197)

## About DisVis

**DisVis** is a Python package and command line tool to visualize and quantify 
the accessible interaction space of distance restrained binary biomolecular complexes.
It performs a full and systematic 6 dimensional search of the three translational
and rotational degrees of freedom to determine the number of complexes consistent
with the restraints. In addition, it outputs the percentage of restraints being violated
and a density that represents the center-of-mass position of the scanning chain corresponding 
to the highest number of consistent restraints at every position in space.


## Requirements

* Python2.7
* NumPy

Optional for faster CPU version

* FFTW3
* pyFFTW

For GPU hardware acceleration the following packages are also required

* OpenCL1.1+
* [pyopencl](https://github.com/pyopencl/pyopencl)
* [clFFT](https://github.com/clMathLibraries/clFFT)
* [gpyfft](https://github.com/geggo/gpyfft)

Recommended for installation

* [git](https://git-scm.com/download)
* [pip](https://pip.pypa.io/en/latest/installing.html)
* Cython


## Installation

If the requirements are met, **DisVis** can be installed by opening a terminal
and typing

    git clone https://github.com/haddocking/disvis.git
    cd disvis
    python setup.py install

The last command might require administrator priviliges for system wide installation.
After installation, the command line tool *disvis* should be at your disposal.

If you are starting from a clean system, then read the installation instructions 
below to prepare your particular operating system.
The INSTALLATION.md file contains instructions for accessing GPU acceleration on MacOSX.


### Linux

First install git and check whether the Python header files and the Python
package manager *pip*, are available by typing

    sudo apt-get install git python-dev python-pip

If you are working on a Fedora system, replace *apt-get* with *yum*.
The final step to prepare you system is installing the Python dependencies

    sudo pip install numpy cython

Wait untill the installation is finished.
Your system is now ready. Follow the general instructions above to install **DisVis**.


### MacOSX

First install [*git*](https://git-scm.com/download) for MacOSX (this might already be present on your system)
by either installing it from the website or using *brew*

    brew install git

Next, install [*pip*](https://pip.pypa.io/en/latest/installing.html), 
the official Python package manager. 
Either follow the link and install *pip* using
their installation instructions or open a terminal and type

    sudo easy_install pip

The final step to prepare your system, is installing the Python dependencies.
In a terminal, type

    sudo pip install numpy cython

Wait till the compilation and installation is finished. 
Your system is now properly prepared. Follow the general instructions above to install **DisVis**.


### Windows

First install [*git*](https://git-scm.com/download) for Windows, as it also comes
with a handy *bash* shell.

For Windows it easiest to install a Python distribution with NumPy and Cython
(and many other) packages included, such as [Anaconda](https://continuum.io/downloads).
Follow the installation instructions on their website.

Next open a *bash* shell that was shipped with *git*. Follow the general instructions
above to install **DisVis**.


## Usage

The general pattern to invoke *disvis* is

    disvis <pdb1> <pdb2> <distance-restraints-file>

where `<pdb1>` is the fixed chain, `<pdb2>` is the scanning chain and 
`<distance-restraints-file>` is a text-file
containing the distance restraints in the following format

     <chainid 1> <resid 1> <atomname 1> <chainid 2> <resid 2> <atomname 2> <mindis> <maxdis>

As an example
    
    A 18 CA F 27 CB 10.0 20.0

This puts a distance restraint between the CA-atom of residue 18 of 
chain A of pdb1 and the CB-atom of residue 27 of chain F of pdb2 that 
should be longer than or equal to 10A and smaller than or equal to 20A.
Comments can be added by starting the line with the pound sign (#) and empty
lines are ignored.


### Options

To get a help screen with available options and explanation type
            
    disvis --help

Some examples to get you started. To perform a 5.27 degree rotational search and store the results
in the directory *results/*

    disvis 1wcm_A.pdb 1wcm_E.pdb restraints.dat -a 5.27 -d results

Note that the directory is created if it doesn't exist.

To perform a 9.72 degree rotational search with 16 processors and a voxel spacing of 2A

    disvis O14250.pdb Q9UT97.pdb restraints.dat -a 9.72 -p 16 -vs 2

To offload computations to the GPU (if the requirements are met) and increase
the maximum allowed volume of clashes as well as the minimum required volume of
interaction, and set the interaction radius to 2A

    disvis 1wcm_A.pdb 1wcm_E.pdb restraints.dat -g -cv 6000 -iv 7000 -ir 2

To perform a grid occupancy analysis for complexes consistent with at least N
restraints, include the `-oa` or `--occupancy-analysis` flag to the command. By
default, the analysis is only performed on complexes consistent with
either *all* and *all - 1* restraints. If you want to perform an occupancy
analysis for complexes consistent with a lower number of restraints, use the
`-ic` or `--interaction-restraints-cutoff` option, followed by the minimum
number of required consistent restraints. Thus the command

    disvis 1wcm_A.pdb 1wcm_E.pdb restraints.dat -oa -ic 5

will make an occupancy grid for complexes consistent with at least 5
restraints, and higher.

Finally, to perform an interaction analysis to determine which residues
are most likely at the interface, the `-is` or `--interaction-selection`
options can be used with an additional file giving the residue numbers of the
receptor and ligand for which the interaction analysis will be performed. The
input file consists of two lines, where the two lines are a space separated
sequence of residue numbers for the receptor and ligand, respectively. As the
interaction analysis is particularly expensive, only complexes consistent with
*all*, or *all - 1* are considered. To lower the barrier, the `-ic` option can
again be used. Also limit the selected residues to only the solvent accessible
ones. A simple example for the selection-file

    1 2 3 4
    101 302 888

This will select residue numbers 1, 2, 3, and 4 for the receptor, and 101, 302,
and 888 for the ligand.


### Output

Standard *disvis* outputs 5 files:

* *accessible_complexes.out*: a text file containing the number of complexes
  consistent with a number of restraints. 
* *violations.out*: a text file showing how often a specific restraint is
  violated for each number of consistent restraints. This helps in identifying
  which restraint is most likely a false-positive if any.
* *accessible_interaction_space.mrc*: a density file in MRC format. The density
  represents the center of mass of the scanning chain conforming to the maximum
  found consistent restraints at every position in space. The density can be
  inspected by opening it together with the fixed chain in a molecular viewer
  (UCSF Chimera is recommended for its easier manipulation of density
  data, but also PyMol works).
* *z-score.out*: a text file giving the Z-score for each restraint. The higher
  the score, the more likely the restraint is a false-positive. Z-scores above
  1.0 are explicitly mentioned in the output.
* *disvis.log*: a log file showing all the parameters used, together with date
  and time indications.

If an occupancy and/or interaction analysis was requested, *disvis* also
outputs the following files:

* *occupancy_N.mrc*: a volume file giving a normalized indication of how
  frequent a grid point is occupied by the ligand, where *N* indicates the minimal
  required number of consistent restraints that resulted in the occupancy grid.
* *[receptor|ligand]_interactions.txt*: a text file containing the average
  number of interactions per complex each selected residue made, for both the
  receptor and ligand molecule. Each column denotes the minimal number of
  consistent restraints of each complex for which interactions were counted.


Licensing
---------

If this software was useful to your research please cite us

**Van Zundert, G.C.P. and Bonvin, A.M.J.J.** (2015) DisVis: Visualizing and
quantifying accessible interaction space of distance restrained biomolecular complexes.
*Bioinformatics 31*, 3222-3224.

Apache License Version 2.0

The elements.py module is licenced under the MIT License, Copyright Christoph
Gohlke.


## Tested platforms

| Operating System| CPU single | CPU multi | GPU |
| --------------- | ---------- | --------- | --- |
|Linux            | Yes        | Yes       | Yes |
|MacOSX           | Yes        | Yes       | No  |
|Windows          | Yes        | Fail      | No  |

The GPU version has been tested on:
NVIDIA GeForce GTX 680 and AMD Radeon HD 7730M for Linux
",2022-08-12
https://github.com/haddocking/DNA-ligand-benchmark,"# DNA-ligand-benchmark
DNA - ligand docking benchmark and associated scripts
",2022-08-12
https://github.com/haddocking/EDES,"# EDES (Ensemble-Docking with Enhanced-sampling of pocket Shape)
#
#### - The files refer to the work available [here](https://www.biorxiv.org/content/early/2018/10/03/434092);
#### - The overall idea behind the method, applied in the context of protein-ligand binding, can be found in this [tutorial](http://www.bonvinlab.org/education/biomolecular-simulations-2018/Metadynamics_tutorial/) used for the [BioExcel Summer School 2018](https://bioexcel.eu/services/training/summerschool2018/).
#
Collection of key scripts to generate the CIPs variables and to run the multi-step cluster analysis.

##### Script to define the CIPs variables: ""divide_into_lists.tcl"".
This script is found within the metadynamics_scripts folder. It is based on the [orient and la1.0 VMD libraries](https://www.ks.uiuc.edu/Research/vmd/script_library/scripts/orient/) and it only requires the knowledge of the residues lining the binding site in order to generate the list of residues for each of the three CIP variable.
##### USAGE:
Open your protein with VMD, then run the script from the Tk VMD console by typing:
```sh
source divide_into_lists.tcl
```
Please be sure to edit, before running the script, the list of residues lining the binding site.
You should put your selection in place of ""--> INSERT HERE RESIDUES OF THE BINDING SITE <--"".


In our example (file 1JEJ.pdb within the ""example"" folder), the binding site is ""resid 18 213 238 240 188 189 191 195 214 237 261 267 269 272"". The output will look like the following one:

```sh
plane xy resIDs
Z_1: 18 213 214 237 238 240 272
Z_2: 188 189 191 195 261 267 269

List Z_1 serials:  {133 134 135 136} {1727 1728 1729 1730} {1738 1739 1740 1741} {1927 1928 1929 1930} {1936 1937 1938 1939} {1951 1952 1953 1954} {2207 2208 2209 2210}
List Z_2 serials:  {1524 1525 1526 1527} {1528 1529 1530 1531} {1545 1546 1547 1548} {1575 1576 1577 1578} {2105 2106 2107 2108} {2160 2161 2162 2163} {2175 2176 2177 2178}

plane xz resIDs
Y_1: 18 240 261 267 269 272
Y_2: 188 189 191 195 213 214 237 238

List Y_1 serials:  {133 134 135 136} {1951 1952 1953 1954} {2105 2106 2107 2108} {2160 2161 2162 2163} {2175 2176 2177 2178} {2207 2208 2209 2210}
List Y_2 serials:  {1524 1525 1526 1527} {1528 1529 1530 1531} {1545 1546 1547 1548} {1575 1576 1577 1578} {1727 1728 1729 1730} {1738 1739 1740 1741} {1927 1928 1929 1930} {1936 1937 1938 1939}

plane yz resIDs
X_1: 188 195 213 214 237 238 267 269 272
X_2: 18 189 191 240 261

List X_1 serials:  {1524 1525 1526 1527} {1575 1576 1577 1578} {1727 1728 1729 1730} {1738 1739 1740 1741} {1927 1928 1929 1930} {1936 1937 1938 1939} {2160 2161 2162 2163} {2175 2176 2177 2178} {2207 2208 2209 2210}
List X_2 serials:  {133 134 135 136} {1528 1529 1530 1531} {1545 1546 1547 1548} {1951 1952 1953 1954} {2105 2106 2107 2108}
```

Where you can see the resIDs and serials associated to each list for all the three CIP variables.

Once you have the serial numbers, you can use them to define the collective variables as in the example files plumed-common.dat and plumed.[0-3].dat you can find in the example folder.

##### Script to perform the multi-step cluster analysis: ""R_clustering_cut.sh"".
This script is found within the ""clustering_scripts"" folder.
  It requires two input files: ""index_RoGBS.dat"" and ""CVS_clustering.dat"" (two example files are in the folder ""example""; please ensure to remove the first three lines from the CVS_clustering.dat file before using the scripts!).

##### USAGE:
-Check the formatting of the files needed (use the example files to check it);

-Copy the files in the current working directory (they need to be in the same directory of the R_clustering_cut.sh file);

-Check if you have the required packages for R (they are listed in the files ""hclust_4cvs_template.R"" and ""k-means-head.R"");

-Run ""./R_clustering_cut.sh"" without arguments to check the suggested width for the RoG-windows;
If you run the script in the same directory of the example files given here, you should be able to see an output like this:

```sh
[N] = N° of desired clusters (e.g. N=50)
[Sigma] = Width of each (RoG) window
Usage: ./R_clustering_cut.sh [N] [Sigma]
The number of Windows has been set to 10 - according to their width, the number of structures contained into each window will be different!

RoG_max - RoG_min = 1.481319
So, the suggested Sigma is given by 1.481319/10=
0.148
```

-Run the script to perform the cluster analysis (e.g. ./R_clustering_cut.sh 50 0.291).
After the calculation has finished, you will see a file named: ""matrix_k-means-init-centers.dat"" containing the index of the selected frames.


",2022-08-12
https://github.com/haddocking/fandas,"# FANDAS

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![unittests](https://github.com/haddocking/fandas/actions/workflows/unittests.yml/badge.svg)](https://github.com/haddocking/fandas/actions/workflows/unittests.yml)
[![codecov](https://codecov.io/gh/haddocking/fandas/branch/main/graph/badge.svg?token=RKLO54L6RY)](https://codecov.io/gh/haddocking/fandas)
[![Codacy Badge](https://app.codacy.com/project/badge/Grade/20d8aa34d5fa4922be588b5800d2e097)](https://www.codacy.com/gh/haddocking/fandas/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=haddocking/fandas&amp;utm_campaign=Badge_Grade)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![python lint](https://github.com/haddocking/fandas/actions/workflows/lint.yml/badge.svg)](https://github.com/haddocking/fandas/actions/workflows/lint.yml)

<img src=""imgs/logo.png"" width=""800"">

## ATTENTION

The publication of [FANDAS](https://doi.org/10.1007/978-1-4939-7386-6_6) refers to its **original release** available at https://github.com/siddarthnarasimhan/FANDAS_2.0.

The present code contains changes made to the original code base and might not reflect the description given in its original publication.

**We are currently working on extending this live version with examples, detailed user documentation and explaination of each term used in the input file.**

## Introduction

**F**ast **AN**lysis of multidimensional NMR **DA**ta **S**ets (**FANDAS**) is an analysis tool built on Python to predict peaks in multidimensional NMR experiments on proteins.

**FANDAS** accepts a variety of inputs, with the protein sequence being the only minimum required input. The output generated by FANDAS can be opened using the NMR visualization software SPARKY.

## Installation

```text
git clone https://github.com/haddocking/fandas.git
cd fandas
python setup.py install
fandas -h
```

## Example

```text
cd example
fandas input.toml
```



## Citation

* Narasimhan, S., Mance, D., Pinto, C., Weingarth, M., Bonvin, A. M. J. J. & Baldus, M. [Rapid Prediction of Multi-dimensional NMR Data Sets Using FANDAS](https://doi.org/10.1007/978-1-4939-7386-6_6). Methods in Molecular Biology 111–132 (2017). doi:10.1007/978-1-4939-7386-6_6
",2022-08-12
https://github.com/haddocking/fcc,"FCC Clustering Algorithm
========================

*Fraction of Common Contacts Clustering Algorithm for Protein Models from Structure Prediction Methods*

About FCC
---------

Structure prediction methods generate a large number of models of which only a fraction matches the biologically relevant structure. To identify this (near-)native model, we often employ clustering 
algorithms, based on the assumption that, in the energy landscape of every biomolecule, its native state lies in a wide basin neighboring other structurally similar states. RMSD-based clustering, the 
current method of choice, is inadequate for large multi-molecular complexes, particularly when their components are symmetric. We developed a novel clustering strategy that is based on a very 
efficient similarity measure - the fraction of common contacts. The outcome of this calculation is a number between 0 and 1, which corresponds to the fraction of residue pairs that are present in 
both the reference and the mobile complex.

Advantages of FCC clustering vs. RMSD-based clustering:
* 100-times faster on average.
* Handles symmetry by consider complexes as entities instead of collections of chains.
* Does not require atom equivalence (clusters mutants, missing loops, etc).
* Handles any molecule type (protein, DNA, RNA, carbohydrates, lipids, ligands, etc).
* Allows multiple levels of ""resolution"": chain-chain contacts, residue-residue contacts, residue-atom contacts, etc.

How to Cite
-----------
Rodrigues JPGLM, Trellet M, Schmitz C, Kastritis P, Karaca E, Melquiond ASJ, Bonvin AMJJ. 
[Clustering biomolecular complexes by residue contacts similarity.] [1]
Proteins: Structure, Function, and Bioinformatics 2012;80(7):1810–1817.

Requirements
------------

* Python 2.6+
* C/C++ Compiler

Installation
------------

Navigate to the src/ folder and issue 'make' to compile the contact programs.
Edit the Makefile if necessary (e.g. different compiler, optimization level).

Usage
------------

All scripts produce usage documentation if called without any arguments. Further,
the '-h' option produces (for Python scripts) a more detailed help with descriptions
of all available options.

For most cases, the following setup is enough:

    # Make a file list with all your PDB files
    ls *pdb > pdb.list
    
    # Ensure all PDB models have segID identifiers
    # Convert chainIDs to segIDs if necessary using scripts/pdb_chainxseg.py
    for pdb in $( cat pdb.list ); do pdb_chainxseg.py $pdb > temp; mv temp $pdb; done

    # Generate contact files for all PDB files in pdb.list
    # using 4 cores on this machine.
    python2.6 make_contacts.py -f pdb.list -n 4

    # Create a file listing the names of the contact files
    # Use file.list to maintain order in the cluster output
    sed -e 's/pdb/contacts/' pdb.list | sed -e '/^$/d' > pdb.contacts

    # Calculate the similarity matrix
    python2.6 calc_fcc_matrix.py -f pdb.contacts -o fcc_matrix.out

    # Cluster the similarity matrix using a threshold of 0.75 (75% contacts in common)
    python2.6 cluster_fcc.py fcc_matrix.out 0.75 -o clusters_0.75.out

    # Use ppretty_clusters.py to output meaningful names instead of model indexes
    python2.6 ppretty_clusters.py clusters_0.75.out pdb.list

Authors
------

João Rodrigues

Mikael Trellet

Adrien Melquiond

Christophe Schmitz

Ezgi Karaca

Panagiotis Kastritis

[Alexandre Bonvin] [2]

[1]: http://www.ncbi.nlm.nih.gov/pubmed/22489062 ""FCC @ Pubmed""
[2]: http://nmr.chem.uu.nl/~abonvin ""Alexandre Bonvin's Homepage""
",2022-08-12
https://github.com/haddocking/glycan-docking-benchmark,"## Glycan-docking-benchmark

The repository contains the following information:

# HADDOCK-ready

Directory composed by sub-directories corresponding to each complex of the glycan benchmark. 


Each sub-directory contains HADDOCK-ready files:

* 'XXXX_r_b.pdb' : Bound receptor PDB
* 'XXXX_l_b.pdb' : Bound ligand PDB
* 'XXXX_r_u.pdb' : Unbound receptor PDB
* 'XXXX_l_u.pdb' : Unbound ligand PDB

The original file:
* 'XXXX_ref.pdb' : Original complex structure PDB

An info file:
* 'XXXX_info.txt' : Associated information file for each PDB complex containing the glycan name and its residue names

And the distance restraints file:

* 'true_interface.tbl' : Ambiguous interactions restraints based on true-interface restraints


Each sub-directory contains an analysis file containing the following files:

* 'XXXX_analysis.pdb' : Processed reference complex PDB file to make it ready for analysis with ProFit (v3.3)


The HADDOCK-ready directory also includes pre-calculated RMSD values for the superimposed unbound, free ligand structures onto the bound ligand structures, respectively for each ligand:
* 'bb-RMSD_l.dat' : Unbound carbohydrate backbone atoms superimposed versus bound carbohydrate backbone atoms
* 'bb-RMSD_l-sorted.dat' : Unbound carbohydrate backbone atoms superimposed versus bound carbohydrate backbone atoms, sorted from small to large
",2022-08-12
https://github.com/haddocking/HADDOCK-antibody-antigen,"# HADDOCK-antibody-antigen

[![License](https://img.shields.io/badge/License-Apache%202.0-yellowgreen.svg)](https://opensource.org/licenses/Apache-2.0)
[![unittests](https://github.com/haddocking/HADDOCK-antibody-antigen/actions/workflows/main.yml/badge.svg)](https://github.com/haddocking/HADDOCK-antibody-antigen/actions/workflows/main.yml)
[![DOI](https://zenodo.org/badge/241584375.svg)](https://zenodo.org/badge/latestdoi/241584375)

Here we provide the code to run the antibody protocol of **HADDOCK** by using the residues belonging to the _hypervariable_ (**HV**) loops.
We use [ANARCI](http://opig.stats.ox.ac.uk/webapps/newsabdab/sabpred/anarci/) _[Dunbar, J. et al (2016). Nucleic Acids Res. 44. W474-W478]_ to renumber the antibody according to the Chothia numbering scheme and to identify the HV loops.

## Installation

### Conda

The easiest way is using [Conda](https://docs.conda.io/en/latest/miniconda.html).

```bash
git clone https://github.com/haddocking/HADDOCK-antibody-antigen.git
cd HADDOCK-antibody-antigen

# Create conda environment from environment.yaml file:
conda env create -f environment.yaml
conda activate haddock-antibody
```

If you are still using Python 2, please, consider using [older version](https://github.com/haddocking/HADDOCK-antibody-antigen/releases/tag/2020-first-release)

## Usage

### As separate scripts

```bash
conda activate haddock-antibody

# Renumber antibody with the Chothia scheme
python ImmunoPDB.py -i 4G6K.pdb -o 4G6K_ch.pdb --scheme c --fvonly --rename --splitscfv

# Format the antibody in order to fit the HADDOCK format requirements
# and extract the HV loop residues and save them into a file
python ab_haddock_format.py 4G6K_ch.pdb 4G6K-HADDOCK.pdb A > active.txt

# Add END and TER statements to the .pdb file
pdb_tidy 4G6K-HADDOCK.pdb > oo; mv oo 4G6K-HADDOCK.pdb
```

### As one command

For the convenience all three commands can be run as one command with:

```bash
conda activate haddock-antibody

python run.py --pdb 4G6K.pdb
```

It is also possible to process a whole folder with pdb files as well as subfolders with only one command:

```bash
conda activate haddock-antibody

python run.py --pdb folder_with_pdbs
```
",2022-08-12
https://github.com/haddocking/HADDOCK-binding-sites-tutorial,"## The online version of this tutorial can be found at: [http://bonvinlab.org/education/HADDOCK-binding-sites](http://bonvinlab.org/education/HADDOCK-binding-sites)

## Introduction

In this tutorial we will use HADDOCK in its ab-initio mode to try to identify putative binding sites for small ligands on a protein receptor, 
using as example the multidrug exporter AcrB, described in the following publication:

[Drug export pathway of multidrug exporter AcrB revealed by DARPin inhibitors](https://doi.org/doi:10.1371/journal.pbio.0050007).<br>
Sennhauser G, Amstutz P, Briand C, Storchenegger O, Grütter MG<BR>
*PLoS Biol.* **5** e7 (2007)

ABSTRACT:
*""The multidrug exporter AcrB is the inner membrane component of the AcrAB-TolC drug efflux system in Escherichia coli and is responsible for the resistance of this organism to a wide range of drugs. ... The three subunits of AcrB are locked in different conformations revealing distinct channels in each subunit. There seems to be remote conformational coupling between the channel access, exit, and the putative proton-translocation site, explaining how the proton motive force is used for drug export. Thus our structure suggests a transport pathway not through the central pore but through the identified channels in the individual subunits, which greatly advances our understanding of the multidrug export mechanism.""*

<figure align=""center"">
    <img src=""./AcrB.png"">
</figure>

This tutorial consists of the following sections:

* [Introduction](#introduction)
* [Setup](#setup)
* [Inspecting the content of the tutorial](#inspecting-the-content-of-the-tutorial)
* [Preparing PDB files of the receptor for docking](#preparing-pdb-files-of-the-receptor-for-docking)
* [Preparing PDB files of the ligands for docking](#preparing-pdb-files-of-the-ligands-for-docking)
* [Ab-initio surface-based docking with HADDOCK](#ab-initio-surface-based-docking-with-haddock)
* [First analysis of the results](#first-analysis-of-the-results)
* [Statistical contact analysis](#statistical-contact-analysis)
* [Identifying a binding pocket from the contact statistics](#identifying-a-binding-pocket-from-the-contact-statistics)
* [Setting up a new docking run targeting the identified binding pocket](#setting-up-a-new-docking-run-targeting-the-identified-binding-pocket)
* [Analysis of the targeted docking results](#analysis-of-the-targeted-docking-results)

In the first part of this tutorial you will learn to clean and manipulate PDB files in preparation for docking. Then we will setup an ab-initio docking run in HADDOCK using surface restraints randomly selected from all accessible residues in order to sample the entire surface of the receptor (the so-called *[surface contact restraints](http://www.bonvinlab.org/software/haddock2.2/run/#disre)* in HADDOCK). A statistical analysis of the docking models in terms of most contacted residues will then be performed to identify and visualize putative binding sites. Finally, the results from this statistical analysis will be used to setup a protein-ligand docking run targeting the predicted binding sites.

For this tutorial we will make use of the H[ADDOCK2.2 webserver](http://haddock.science.uu.nl/services/HADDOCK2.2).
A description of our web server can be found in the following publications:

* G.C.P van Zundert, J.P.G.L.M. Rodrigues, M. Trellet, C. Schmitz, P.L. Kastritis, E. Karaca, A.S.J. Melquiond, M. van Dijk, S.J. de Vries and  A.M.J.J. Bonvin.
[The HADDOCK2.2 webserver: User-friendly integrative modeling of biomolecular complexes](https://doi.org/doi:10.1016/j.jmb.2015.09.014).
_J. Mol. Biol._, *428*, 720-725 (2015).

* S.J. de Vries, M. van Dijk and A.M.J.J. Bonvin.
[The HADDOCK web server for data-driven biomolecular docking.](http://www.nature.com/nprot/journal/v5/n5/abs/nprot.2010.32.html)
_Nature Protocols_, *5*, 883-897 (2010).  Download the final author version <a href=""http://igitur-archive.library.uu.nl/chem/2011-0314-200252/UUindex.html"">here</a>.


Throughout the tutorial, coloured text will be used to refer to questions or 
instructions, Linux and/or Pymol commands.

<a class=""prompt prompt-question"">This is a question prompt: try answering 
it!</a>
<a class=""prompt prompt-info"">This an instruction prompt: follow it!</a>
<a class=""prompt prompt-pymol"">This is a Pymol prompt: write this in the 
Pymol command line prompt!</a>
<a class=""prompt prompt-cmd"">This is a Linux prompt: insert the commands in the 
terminal!</a>


<hr>
## Setup

In order to run this tutorial you will need to have [Pymol][link-pymol] installed.
You can of course use instead your favorite structure viewer, but the visualization commands described here are for Pymol.

Further you should install our [PDB-tools][link-pdb-tools], or clone it from the command line:

<a class=""prompt prompt-cmd"">
    git clone https://github.com/haddocking/pdb-tools
</a>

Make sure that the pdb-tools directory is in your search path. For this go into the ```pdb-tools``` directory and then if working under ```tsch``` type:

<a class=""prompt prompt-cmd"">
    set path=($path \`pwd\`)
</a>

And for ```bash```:

<a class=""prompt prompt-cmd"">
    export PATH=${PATH}:\`pwd\`
</a>

Download then the data to run this tutorial from our GitHub
data repository [here][link-data] or clone it from the command line:

<a class=""prompt prompt-cmd"">
    git clone https://github.com/haddocking/HADDOCK-binding-sites-tutorial
</a>

Alternatively, if you do not have git installed, simply go the above web address and download the zip archive.
You will also need to compile a few provided programs for post-analysis.
For this go into the ```ana_scripts``` directory of the cloned directory and type ```make``` 
(we are here assuming a tcsh or csh shell):

<a class=""prompt prompt-cmd"">
cd HADDOCK-binding-sites-tutorial/ana_scripts<BR>
make<BR>
source setup.csh<BR>
cd ..</a>

**Note**: This is defining some environment variable which we will use in the following. Repeat this step and the above step about defining the ```path``` everytime you open a new terminal window.

If you don't want to wait with the docking runs to complete in order to proceed with the analysis (see section about *[Preparing PDB files for docking](#preparing-pdb-files-of-the-receptor-for-docking)* section below), you can already download pre-calculated runs using the script provided into the runs directory:

<a class=""prompt prompt-cmd"">cd runs<BR>./download-run-data.csh<BR>cd ..</a>

This will download two reduced docking runs, one for the random sampling of the surface and one for the targeted protein-ligand docking  (about 450MB of compressed data).

Or to download two full docking runs, one for the random sampling of the surface and one for the targeted protein-ligand docking  (about 6GB of compressed data).:

<a class=""prompt prompt-cmd"">cd runs<BR>./download-run-data-full.csh<BR>cd ..</a>



<hr>
## Inspecting the content of the tutorial

Let us first inspect the various files provided with this tutorial. 
You will see three directories and one file:

* **HADDOCK-runfiles**: this directory contains the reference HADDOCK parameter files for various docking runs described in this tutorial. These can be used to reproduce the docking using the file [upload interface](http://haddock.science.uu.nl/services/HADDOCK2.2/haddockserver-file.html) of the HADDOCK server.

* **ana_scripts**: this directory contains various analysis scripts to analyse the results of the docking, including the statistical contact analysis.

* **pdbs**: this directory contains various PDB files which will be used during this tutorial

* **pdbs-processed**: this directory contains processed, cleaned PDB files (See the *Preparing PDB files for docking* section below), ready for docking 

* **runs**: this directory contains scripts that allows you to download pre-calculated docking runs.


<hr>
## Preparing PDB files of the receptor for docking

One requirement of HADDOCK is that there should not be any overlap in residue numbering. The structure of the apo form of our target receptor, the multidrug efflux pump [AcrB](http://www.uniprot.org/uniprot/P31224) from Escherichia coli, is available from the Protein Data Bank under PDB ID [2J8S](http://www.ebi.ac.uk/pdbe/entry/search/index?text:2J8S). You can download it directly from the PDB using the ```pdb_fetch.py``` script from our ```pdb-tools``` utilities:

<a class=""prompt prompt-cmd"">
  pdb_fetch.py 2J8S >2J8S.pdb
</a>

The file is also provided in the ```pdbs``` directory.

Let's first inspect this structure using Pymol (or your favorite viewer):

<a class=""prompt prompt-cmd"">
pymol 2J8S.pdb
</a>

And in pymol type at the prompt level:

<a class=""prompt prompt-pymol"">
show cartoon<br>
util.cbc
</a>

Take some time to inspect the 3D structure. Each chain should have a different color.

<a class=""prompt prompt-question"">
How many chains can you identify?
</a>

If you look at the desciption of this structure on the [PDB website](http://www.ebi.ac.uk/pdbe/entry/search/index?text:2J8S), it states *""DRUG EXPORT PATHWAY OF MULTIDRUG EXPORTER ACRB REVEALED BY DARPIN INHIBITORS""*. 
You should be able to identify the two darpins (they have chainIDs D and E in the structure). Let's remove them in pymol:

<a class=""prompt prompt-pymol"">
select chain D+E<br>
remove sele
</a>

Now you only see the mutlidrug exporter. It consists of chain A,B and C and is the system which we will further use for docking, but we have to make sure first that there is no overlap in numbering.
For this we will work at the terminal level and use our ```pbd-tools``` utilities. Quit first pymol.

Let's first find out what are the first and last residue numbers of the various chains, to check if there is any overlap in numbering:

<a class=""prompt prompt-cmd"">
pdb_selchain.py -A 2J8S.pdb \| grep \' CA \' \| grep ATOM \| head -5<BR>
pdb_selchain.py -A 2J8S.pdb \| grep \' CA \' \| grep ATOM \| tail -5<BR>
</a>
<a class=""prompt prompt-cmd"">
pdb_selchain.py -B 2J8S.pdb \| grep \' CA \' \| grep ATOM \| head -5<BR>
pdb_selchain.py -B 2J8S.pdb \| grep \' CA \' \| grep ATOM \| tail -5<BR>
</a>
<a class=""prompt prompt-cmd"">
pdb_selchain.py -C 2J8S.pdb \| grep \' CA \' \| grep ATOM \| head -5<BR>
pdb_selchain.py -C 2J8S.pdb \| grep \' CA \' \| grep ATOM \| tail -5<BR>
</a>

Inspecting the results of those commands reveals that we are indeed dealing with overlapping numbering: all three chains start at residue 1.
For use in HADDOCK we have thus to renumber chain B and C. In order to easily match the residue numbers between chains it is advisable to shift the numbering by a round number, e.g. in this case since we have more than 1000 amino acids we can shift chain B and C by 2000 and 4000, respectively. We will use again our ```pdb-tools``` utilities to create a renumbered, clean PDB file (also removing all hetero atoms in the process by selection only ATOM recoords):

<a class=""prompt prompt-cmd"">
pdb_selchain.py -A 2J8S.pdb \| grep ATOM \> 2J8S-renumbered.pdb<BR>
echo TER \>\> 2J8S-renumbered.pdb<BR>
pdb_selchain.py -B 2J8S.pdb \| grep ATOM \| pdb_reres.py -2001 \>\> 2J8S-renumbered.pdb<BR>
echo TER \>\> 2J8S-renumbered.pdb<BR>
pdb_selchain.py -C 2J8S.pdb \| grep ATOM \| pdb_reres.py -4001 \>\> 2J8S-renumbered.pdb<BR>
echo END \>\> 2J8S-renumbered.pdb<BR>
</a>

The PDB file of our receptor should now be ready for docking. You can also check the file format with:

<a class=""prompt prompt-cmd"">
  pdb_format.py 2J8S-renumbered.pdb
</a>

This will report formatting issues.


<hr>
## Preparing PDB files of the ligands for docking

Several small molecules are known to bind to this receptor, among which [rifampicin](http://www.ebi.ac.uk/pdbe/entry/pdb/3aod/bound/RFP) and [minocycline](http://www.ebi.ac.uk/pdbe/entry/pdb/3aod/bound/MIY). A crystal structure of the complex with both ligands is also available from the PDB website ([PBD entry 3AOD](http://www.ebi.ac.uk/pdbe/entry/pdb/3aod)). Those ligands are binding to [two different sites](http://www.ebi.ac.uk/pdbe/entry/pdb/3aod/ligands) on the receptor. 

For docking we need coordinates of those ligands in PDB format with line starting with HETATM.
After downloading the corresponding PDB entry [3AOD](http://www.ebi.ac.uk/pdbe/entry/pdb/3aod) extract the ligands from it with the following commands:

For rifampicin (called RFP in the PDB file):

<a class=""prompt prompt-cmd"">
  grep RFP 3AOD.pdb \|grep HETATM \> rifampicin.pdb<BR>
  echo END \>\> rifampicin.pdb<BR>
</a>

For minocycline (called MIY in the PDB file):

<a class=""prompt prompt-cmd"">
  grep MIY 3AOD.pdb \|grep HETATM \> minocycline.pdb<BR>
  echo END \>\> minocycline.pdb<BR>
</a>



<hr>
## Ab-initio surface-based docking with HADDOCK

We will launch here a docking run using the apo form of the receptor (the renumbered PDB we just prepared) and rifampicin as potential ligand.
For this we will make use of the [guru interface](http://haddock.science.uu.nl/services/HADDOCK2.2/haddockserver-guru.html) of the HADDOCK web server, which does require guru level access (provided with course credentials if given to you, otherwise register to the server and request this access level):

<a class=""prompt prompt-info"">
http://haddock.science.uu.nl/services/HADDOCK2.2/haddockserver-guru.html
</a>

**Note:** The blue bars on the server can be folded/unfolded by clicking on the arrow on the right

* **Step1:** Define a name for your docking run, e.g. *AcrB-rifampicin-surface*.

* **Step2:** Input the protein PDB file. For this unfold the **Molecule definition menu**.

<a class=""prompt prompt-info"">
First molecule: where is the structure provided? -> ""I am submitting it""
</a>
<a class=""prompt prompt-info"">
Which chain to be used? -> All (to select all three chains)
</a>
<a class=""prompt prompt-info"">
PDB structure to submit -> Browse and select 2J8S-renumbered.pdb
</a>
<a class=""prompt prompt-info"">
Segment ID to use during docking -> A
</a>
<a class=""prompt prompt-info"">
The N-terminus of your protein is positively charged -> uncheck the box if needed
</a>
<a class=""prompt prompt-info"">
The C-terminus of your protein is negatively charged -> uncheck the box if needed
</a>

(Our structure might not be the real fragment used for crystallisation - better to have uncharged termini)

* **Step 3.** Input the ligand PDB file. For this unfold the **Molecule definition menu**.

<a class=""prompt prompt-info"">
Second molecule: where is the structure provided? -> ""I am submitting it""
</a>
<a class=""prompt prompt-info"">
Which chain to be used? -> All
</a>
<a class=""prompt prompt-info"">
PDB structure to submit -> Browse and select rifampicin.pdb
</a>
<a class=""prompt prompt-info"">
Segment ID to use during docking -> B
</a>

* **Step 4:** Turn on random surface restraints. For this unfold the **Distance restraints menu**

<a class=""prompt prompt-info"">
Define randomly ambiguous interaction restraints from accessible residues -> Check the box
</a>

* **Step 5:** Since we are doing ab-initio docking we do need to increase the sampling. For this unfold the **Sampling parameters menu**:

<a class=""prompt prompt-info"">
Number of structures for rigid body docking -> 10000
</a>
<a class=""prompt prompt-info"">
Number of structures for semi-flexible refinement -> 400
</a>
<a class=""prompt prompt-info"">
Number of structures for the explicit solvent refinement -> 400
</a>


**Note:** If you use course credentials, these numbers will be reduced to 500/50/50 to save computing time and get back results faster. You can also manually decrease those numbers and download instead a full pre-calculated run for analysis (see setup above).


* **Step 6:** Change the clustering settings since we are dealing with a small molecule. For this unfold the **Clustering parameter menu**:

<a class=""prompt prompt-info"">
Clustering method (RMSD or Fraction of Common Contacts (FCC)) -> RMSD
</a>
<a class=""prompt prompt-info"">
RMSD Cutoff for clustering (Recommended: 7.5A for RMSD, 0.75 for FCC) -> 2.0
</a>


* **Step 7:** Apply some ligand-specific scoring setting. For this unfold the **Scoring parameter menu**:

Our recommended HADDOCK score settings for small ligands docking are the following:

<pre>
     HADDOCKscore-it0   = 1.0 Evdw + 1.0 Eelec + 1.0 Edesol + 0.01 Eair - 0.01 BSA
     
     HADDOCKscore-it1   = 1.0 Evdw + 1.0 Eelec + 1.0 Edesol +  0.1 Eair - 0.01 BSA

     HADDOCKscore-water = 1.0 Evdw + 0.1 Eelec + 1.0 Edesol +  0.1 Eair
</pre>

This differs from the defaults setting (defined for protein-protein complexes). We recommend to change two weights for protein-ligand docking:

<a class=""prompt prompt-info"">
Evdw 1 -> 1.0
</a>

<a class=""prompt prompt-info"">
Eelec 3 -> 0.1
</a>


* **Step 8:** Apply some ligand-specific protocol setting. For this unfold the **Advanced sampling parameter menu**:

<a class=""prompt prompt-info"">
initial temperature for second TAD cooling step with flexible side-chain at the inferface -> 500
</a>

<a class=""prompt prompt-info"">
initial temperature for third TAD cooling step with fully flexible interface -> 300
</a>

<a class=""prompt prompt-info"">
number of MD steps for rigid body high temperature TAD -> 0
</a>

<a class=""prompt prompt-info"">
number of MD steps during first rigid body cooling stage -> 0
</a>


* **Step 8:** You are ready to submit! Enter your username and password (or the course credentials provided to you). Remember that for this interface you do need guru access.


<hr>
## First analysis of the results

Once your run has completed (this can take quite some time considering the size of the receptor) you will be presented with a result page showing the cluster statistics and some graphical representation of the data. Such an example output page can be found [here](http://haddock.science.uu.nl/services/HADDOCK2.2/Files/E2A-HPr-demo/index.html). 

Instead, you can also use the precalculated run. Simply unpack the archive (see the [Setup](#setup) section for downloading the archives), go into the directory and open with your favorite web browser the index.html file to view the results page.

Considering the size of the receptor we are targeting, at this stage it is rather unlikey that any sensible results will be obtained. If you performed the docking with course credentials, most likely the run will have completed but the minimum number of structures per cluster will have automatically reduced to 2 or even 1 in order to produce a result page. If 1, then the clusters reported on the web page will correspond to the top10 ranked models.

You can download the full run as a gzipped tar archive and inspect the results. Copy for this the link provided in the result page and download the archive with:

<a class=""prompt prompt-linux"">
curl -L -O \<link\>
</a>
or
<a class=""prompt prompt-linux"">
wget \<link\>
</a>

Unpack the gzip file with:

<a class=""prompt prompt-linux"">
tar xzf \<archive\>.tgz
</a>

**Note:** You can also view a result page from a downloaded pre-calculated docking run. For this go into the ```runs``` directory and then download the runs using:

<a class=""prompt prompt-linux"">
./download-run-data.csh
</a>

This will download two docking runs performed under course settings (i.e. reduced number of models), for a total of about 450MB of compressed data. Unpack the runs (using ```tar xfz <archive>.tgz```). 
Those contain the same html result page that the server would be returning. To view those, open in your favourite browser the `index.html` file provided in the run directory.


If you want to inspect some of the docking models, change directly to ```runname/structures/it1/water/analysis```. In that directory you will find the models numbered according to their HADDOCK ranking, e.g. ```complexfit_1.pdb, complexfit_2.pdb, ...```
You can for example inspect the first 10 with pymol, comparing them to the reference complex 3AOD:

<a class=""prompt prompt-cmd"">
pymol complexfit_[1-9].pdb complexfit_10.pdb $WDIR/3AOD-renumbered.pdb
</a>

This will load the top 10 models and a renumbered reference crystal structure into pymol (make sure to have run the [Setup](#setup) described above). In case WDIR is not defined or you are not running under a linux-like environment you can find the reference structure into the *ana_scripts* directory. 

**Note:** The chain nomenclature between the 3AOD and the 2J8S structures differ. Chain C,A,B of 3AOD actually correspond to chain A,B,C of 2J8S. The renumbered ```3AOD-renumbered.pdb``` file has been renumbered in such a way that the chains now match.

In pymol you can type the following to superimpose the models and change the style:

<a class=""prompt prompt-pymol"">
align 3AOD-renumbered, complexfit_1<br>
zoom vis<br>
show ribbon<br>
hide lines<br>
util.cbc<BR>
select resn MIY+RFP<BR>
show sphere, sele<BR>
color orange, sele<BR>
select 3AOD-renumbered and resn MIY+RFP<BR>
color red, sele<BR>
</a>

<a class=""prompt prompt-question"">
Can you find any ligand (in orange) close to the position of any of the two ligands in the reference crystal structure (in red)?
</a>
<a class=""prompt prompt-question"">
Also consider that the receptor consists of three identical chains, but in slighly difference conformations. There might thus be symmetry-related binding site.
</a>



<hr>
## Statistical contact analysis

We will now perform a statistical analysis of all residues making contacts with the ligand at the rigid body docking stage. For this, go first into the ```runname/structures/it0``` directory.
We first calculate all intermolecular contacts within 5Å for all models with the following command:

<a class=""prompt prompt-cmd"">
\$WDIR/contacts-analysis.csh \`cat file.nam\`
</a>

This creates a ```contacts``` directory containing the list of intermolecular atomic contacts for each model.
Then we simply count the number of times a residue is contacted:

<a class=""prompt prompt-cmd"">
\$WDIR/contacts-statistics.csh \`cat file.nam\`
</a>

**Note:** If you want to only analyze for example the top 1000 ranked models use instead:

<a class=""prompt prompt-cmd"">
\$WDIR/contacts-statistics.csh \`head -1000 file.nam\`
</a>


The script generates for each chain a sorted list of residue with their number of contacts (on a residue basis). E.g. for chain A:

<pre>
   6 700 A
   6 693 A
   6 692 A
   6 4429 A
   6 2226 A
   5 965 A
   5 811 A
   5 708 A
   5 532 A
...
</pre>

We can encode the contacts statistics in the B-factor field of a PDB file to allow for visualisation. For this we should use a PDB file with chainIDs.
Go into our example run directory, i.e. the run we downloaded from the HADDOCK server called ```AcrB-rifampicin-surface```. 
First let's put back the chainID information in one of the starting model taken from the ```begin``` directory and set all B-factors to 1:

<a class=""prompt prompt-cmd"">
pdb_segxchain.py begin/protein1.pdb \| pdb_b.py -1 \> AcrB_contacts.pdb
</a>

And then we will use this PDB file, together with the contacts statistics file just created in ```structures/it0``` to encode the contacts into the b-factor column of the PDB file with the following command:

<a class=""prompt prompt-cmd"">
\$WDIR/encode-contacts.csh structures/it0/Acontacts.lis AcrB_contacts.pdb
</a>

The result is a new PDB file called ```AcrB_contacts.pdb``` which can now be visualized in Pymol (we also load here the reference structure).
If you have performed the analysis on a full run, use the ```AcrB_contacts.pdb``` file you just created. Otherwise, in order to get more significant results, 
use instead the model provided in the ```AcrB-rifampicin-surface-full``` directory in which you will find the pre-computed data from an analysis 
of 10000 rigid body docking (it0 models). The corresponding full run archive can be downloaded using the ```download-run-data-full.csh``` script 
(but beware it is a large amount of data >10GB when unpacked).

<a class=""prompt prompt-cmd"">
pymol AcrB_contacts.pdb \$WDIR/3AOD-renumbered.pdb
</a>

And then in Pymol type:

<a class=""prompt prompt-pymol"">
align 3AOD-renumbered, AcrB_contacts<br>
zoom vis<br>
hide lines<br>
show mesh, AcrB_contacts<BR>
spectrum b, blue_white_red, minimum=1, maximum=100<BR>
select resn MIY+RFP<BR>
show sphere, sele<BR>
color green, sele<BR>
</a>

The above commands will display the surface of the molecule colored according to the contact frequency (from red 100 to blue 1).
Also visible are the two ligands in the reference structure of the complex.

**Note:** If you want to put more emphasis on the most contacted regions, change the minimum value in the above command to for example 50.

<a class=""prompt prompt-question"">
Inspect the surface: Are there any highly contacted regions close to the actual binding sites?
</a>

Remember here that the receptor consists of three identical chains. For the docking we renumbered the chain to avoid overlap and gave them a unique segid (A).
You can distinguish the various chains by the corresponding resisude numbering:

* Chain A starts at residue number 1
* Chain B starts at residue number 2001
* Chain C starts at residue number 4001.

<a class=""prompt prompt-question"">
With this knowledgle at hand, try to identify in which chain we find a binding pocket highlighted by the most contacted residues from our analysis.
</a>

The original paper by [Sennhauser *et al*](https://doi.org/doi:10.1371/journal.pbio.0050007) indicates that the three chains are locked in different conformations.
They report in particular that chain B has the largest channel opening in their structure:

*""The three AcrB subunits are bound in three different conformations, revealing three distinct channels ([**Figure 3**](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0050007#pbio-0050007-g003)). The width of these channels is sufficient for the passage of typical AcrB substrates. In subunit A, a channel is observed, extending from the external depression through the large periplasmic domain reaching almost the central funnel at the top of the protein ([**Figure 4A**](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0050007#pbio-0050007-g004)). Here the side chains of residues Gln124, Gln125, and Tyr758 form a gate, closing the channel and therefore preventing direct access to the central funnel. ... A similar channel, although a little wider, is present in subunit B ([**Figure 4B**](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0050007#pbio-0050007-g004)). In addition, the channel is open not only to the periplasm but also to the membrane bilayer at the periphery of the TM domain. In subunit C, the channel entrances are closed due to movements of PC2 and PN1 ([**Figure 4C**](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0050007#pbio-0050007-g004)).""*

<a class=""prompt prompt-question"">
To which one of the three channels/subunit do we observe preferential contacts on docking models?.
</a>
<a class=""prompt prompt-question"">
Is this consistent with the observations by Sennhauser *et al*?.
</a>

<a class=""prompt prompt-info"">
**Note:** You can repeat the same analysis for a full docking run if you are interested. For this download the full run archives using the download-run-data-full.csh script in the runs directory (about 6GB of compressed data!). 
</a>
<a class=""prompt prompt-info"">
But instead you also only look at the encoded contacts statistics in the PDB file provided in the runs directory: AcrB-rifampicin-surface-full-contacts.pdb.
This files contains the results of the contacts analysis of 10000 rigid body docking models.
</a>

<a class=""prompt prompt-cmd"">
pymol AcrB-rifampicin-surface-full-contacts.pdb \$WDIR/3AOD-renumbered.pdb
</a>

And then in Pymol type:

<a class=""prompt prompt-pymol"">
align 3AOD-renumbered, AcrB-rifampicin-surface-full-contacts<br>
zoom vis<br>
hide lines<br>
show mesh, AcrB-rifampicin-surface-full-contacts<BR>
spectrum b, blue_white_red, minimum=1, maximum=100<BR>
select resn MIY+RFP<BR>
show sphere, sele<BR>
color green, sele<BR>
</a>


<hr>
## Identifying a binding pocket from the contact statistics

We will now make use of the contact statistics obtained previously to target a specific binding site in a new docking run. We should make use of statistics obtained from a full docking run.
In the previous section we have identified two preferred binding pockets in what should be chain A or chain B of the receptor. Chain B has a residue numbering which starts at 2001. In the renumbered 3AOD structure, rifampicin is found to bind to chain A.

We can extract the most contacted residue for chain B from the file containing the contacts statistics provided in the ```runs``` directory and called ```AcrB-rifampicin-surface-full-contacts.lis```
So first change directory to the ```runs``` dir.

Let us first see how many residues are sampled in chain B and what are the highest and lowest number of contacts. The following command will give the total number of residues in chain A contacted by the ligand out of the 10000 models analysed:

<a class=""prompt prompt-cmd"">
awk \'$2>2000 && $2<4000\' AcrB-rifampicin-surface-full-contacts.lis \|wc -l
</a>

Calculate what would correspond to the top 10% of the most contacted residues.

The most contacted residues are found at the head of this file:

<a class=""prompt prompt-cmd"">
awk \'$2>2000 && $2<4000\' AcrB-rifampicin-surface-full-contacts.lis \|head -5
</a>

<details style=""background-color:#DAE4E7""><summary><b>See solution:</b>
</summary>
<pre>
 198 2717 A
 171 2566 A
 166 2830 A
 158 2715 A
 154 2029 A
</pre>
</details>
<br>


And the less often contacted residues at the bottom:

<a class=""prompt prompt-cmd"">
awk \'$2>2000 && $2<4000\' AcrB-rifampicin-surface-full-contacts.lis \|tail -5
</a>

<details style=""background-color:#DAE4E7""><summary><b>See solution:</b>
</summary>
<pre>
   1 2379 A
   1 2351 A
   1 2329 A
   1 2200 A
   1 2132 A
</pre>
</details>
<br>

Let us now extract the list of the top 10% most contacted residues (change the value in the head statement if required) and store it to a new file:

<a class=""prompt prompt-cmd"">
awk \'$2>2000 && $2<4000\' AcrB-rifampicin-surface-full-contacts.lis \| head -n 82 > AcrB-rifampicin-surface-full-contacts-top10.lis
</a>

We can now encode this information in a PDB file to visualize the defined binding site:

<a class=""prompt prompt-cmd"">
pdb_b.py -1 $WDIR/../pdbs/2J8S-renumbered.pdb \|pdb_chain.py -A > AcrB-rifampicin-surface-full-contacts-top10.pdb<BR>
$WDIR/encode-contacts.csh AcrB-rifampicin-surface-full-contacts-top10.lis AcrB-rifampicin-surface-full-contacts-top10.pdb<BR>
</a>

<a class=""prompt prompt-info"">
Load the resulting model in pymol and type:
</a>

<a class=""prompt prompt-pymol"">
zoom vis<br>
hide lines<br>
show cartoon<br>
show mesh<br>
spectrum b, blue_white_red, minimum=1, maximum=100<br>
</a>

You should now be looking only at highly contacted regions of chain B. There is a clear binding pocket visible in the chainB corresponding to the entrance of the channel as described by [Sennhauser *et al*](https://.doi.org/doi:10.1371/journal.pbio.0050007) (see [Figure 3](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0050007#pbio-0050007-g003)). This is the left view in the figure below. A second highly contacted region seems to be in the inside of the trimer (right picture below).

<figure align=""center"">
    <img src=""./AcrB-rifampicin-contacts.png"">
</figure>

Try to figure out which residue numbers are lining the binding pocket shown on the left figure. 
From an analyis in Pymol it looks like these are mainly located between residue 2550 and 2725. 
We can extract their number from the top 10 contact statistics file with:

<a class=""prompt prompt-cmd"">
awk \'$2>2550 && $2<2725\' AcrB-rifampicin-surface-full-contacts-top10.lis \| awk \'{printf \""%s, \"", $2}\'
</a>

<details style=""background-color:#DAE4E7""><summary><b>See solution:</b>
</summary>
<pre>
2717, 2566, 2715, 2722, 2676, 2562, 2677, 2678, 2645, 2561, 2580, 2690, 2694, 2662, 2579, 2674, 2675, 2689, 2718, 2664, 2560, 2667, 2564, 2693, 2716, 2700, 2666, 2563, 2565, 2554, 2577, 2601
</pre>
</details>
<br>
Save this list since we will need it to setup the targeted docking run.



<hr>
## Setting up a new docking run targeting the identified binding pocket

We will now setup a second docking run targeting specifically the identified binding pocket on chain B. 
For our targeted ligand docking protocol, we will first create two sets of restraints which we will use at different stages of the docking:

1. For the rigid-body docking, we will first define the entire binding pocket on the receptor as active and the ligand as active too. 
This will ensure that the ligand is properly drawn inside the binding pocket.

2. For the subsequent flexible refinement stages, we define the binding pocket only as passive and the ligand as active. 
This ensures that the ligand can explore the binding pocket.

In order to create those two restraint files, use the HADDOCK server tool to generate AIR restraints: [http://haddock.science.uu.nl/services/GenTBL/](http://haddock.science.uu.nl/services/GenTBL/) (unfold the *Residue selection* menu):

<a class=""prompt prompt-info"">
Selection 1: Active residues (directly involved in the interaction) -> enter here the list of residues defining the binding site (see above)
</a>
<a class=""prompt prompt-info"">
Selection 2: Active residues (directly involved in the interaction) -> enter here the residue number of our ligand rifampicin (2002)
</a>
<a class=""prompt prompt-info"">
Click on submit and save the resulting page, naming it AcrB-rifampicin-act-act.tbl
</a>

**Note:** This works best with Firefox. Currently when using Chrome, saving as text writes the wrong info to file. In that case copy the content of the page and paste it in a text file.

**Note:** Avoid Safari for the time being - it is giving problems (we are working on it).

Now repeat the above steps, but this time entering the list of residues for the binding pocket into the passive residue list.
Save the resulting restraint file as AcrB-rifampicin-pass-act.tbl

**Note:** Two pre-generated distance restraints files are available in the `runs` directory:
<pre>
    AcrB-rifampicin-act-act.tbl
    AcrB-rifampicin-pass-act.tbl
</pre>

The number of distance restraints defined in those file can be obtained by counting the number of times that an ```assign``` statement is found in the file, e.g.:

<a class=""prompt prompt-cmd"">
grep -i assign AcrB-rifampicin-act-act.tbl \| wc -l
</a>

<a class=""prompt prompt-question"">
Compare the two generated files: what are the differences? How many restraints are defined in each?
</a>


**Note:** A description of the restraints format can be found in Box 4 of our Nature Protocol 2010 server paper:

* S.J. de Vries, M. van Dijk and A.M.J.J. Bonvin.
[The HADDOCK web server for data-driven biomolecular docking.](http://www.nature.com/nprot/journal/v5/n5/abs/nprot.2010.32.html)
_Nature Protocols_, *5*, 883-897 (2010).  Download the final author version <a href=""http://igitur-archive.library.uu.nl/chem/2011-0314-200252/UUindex.html"">here</a>.



We have now all the required information to setup our targeted docking run. We will again make use of the [guru interface](http://haddock.science.uu.nl/services/HADDOCK2.2/haddockserver-guru.html) of the HADDOCK web server, which does require guru level access (provided with course credentials if given to you, otherwise register to the server and request this access level):

<a class=""prompt prompt-info"">
http://haddock.science.uu.nl/services/HADDOCK2.2/haddockserver-guru.html
</a>

**Note:** The blue bars on the server can be folded/unfolded by clicking on the arrow on the right

* **Step1:** Define a name for your docking run, e.g. *AcrB-rifampicin-pocket*.

* **Step2:** Input the protein PDB file. For this unfold the **Molecule definition menu**.

<a class=""prompt prompt-info"">
First molecule: where is the structure provided? -> ""I am submitting it""
</a>
<a class=""prompt prompt-info"">
Which chain to be used? -> All (to select all three chains)
</a>
<a class=""prompt prompt-info"">
PDB structure to submit -> Browse and select 2J8S-renumbered.pdb
</a>
<a class=""prompt prompt-info"">
Segment ID to use during docking -> A
</a>
<a class=""prompt prompt-info"">
The N-terminus of your protein is positively charged -> uncheck the box if needed
</a>
<a class=""prompt prompt-info"">
The C-terminus of your protein is negatively charged -> uncheck the box if needed
</a>

(Our structure might not be the real fragment used for crystallisation - better to have uncharged termini)

* **Step 3.** Input the ligand PDB file. For this unfold the **Molecule definition menu**.

<a class=""prompt prompt-info"">
Second molecule: where is the structure provided? -> ""I am submitting it""
</a>
<a class=""prompt prompt-info"">
Which chain to be used? -> All
</a>
<a class=""prompt prompt-info"">
PDB structure to submit -> Browse and select rifampicin.pdb
</a>
<a class=""prompt prompt-info"">
Segment ID to use during docking -> B
</a>

* **Step 4:** Input the restraint files for docking. For this unfold the **Distance restraints menu**

<a class=""prompt prompt-info"">
Instead of specifying active and passive residues, you can supply a HADDOCK restraints TBL file (ambiguous restraints) -> Upload here the AcrB-rifampicin-act-act.tbl
</a>
<a class=""prompt prompt-info"">
You can supply a HADDOCK restraints TBL file with restraints that will always be enforced (unambiguous restraints) -> Upload here the AcrB-rifampicin-pass-act.tbl
</a>

* **Step 5:** Change the clustering settings since we are dealing with a small molecule. For this unfold the **Clustering parameter menu**:

<a class=""prompt prompt-info"">
Clustering method (RMSD or Fraction of Common Contacts (FCC)) -> RMSD
</a>
<a class=""prompt prompt-info"">
RMSD Cutoff for clustering (Recommended: 7.5A for RMSD, 0.75 for FCC) -> 2.0
</a>

* **Step 6:** Define when to use each of the two restraint files we are uploading: For this unfold the **Restraints energy constants menu**"", and in that menu unfold the **Energy constants for ambiguous restraints** menu. 

<a class=""prompt prompt-info"">
Last iteration (0-2) -> 0 (this defines that the ambiguous restraints (the act-act file) will only be used in iteration 0 (rigid-body docking)
</a>

* **Step 7:** Apply some ligand-specific scoring setting. For this unfold the **Scoring parameter menu**:

Our recommended HADDOCK score settings for small ligands docking are the following:

<pre>
     HADDOCKscore-it0   = 1.0 Evdw + 1.0 Eelec + 1.0 Edesol + 0.01 Eair - 0.01 BSA
     
     HADDOCKscore-it1   = 1.0 Evdw + 1.0 Eelec + 1.0 Edesol +  0.1 Eair - 0.01 BSA

     HADDOCKscore-water = 1.0 Evdw + 0.1 Eelec + 1.0 Edesol +  0.1 Eair
</pre>

This differs from the defaults setting (defined for protein-protein complexes). We recommend to change two weights for protein-ligand docking:

<a class=""prompt prompt-info"">
Evdw 1 -> 1.0
</a>

<a class=""prompt prompt-info"">
Eelec 3 -> 0.1
</a>


* **Step 8:** Apply some ligand-specific setting. For this unfold the **Advanced sampling parameter menu**:

<a class=""prompt prompt-info"">
initial temperature for second TAD cooling step with flexible side-chain at the inferface -> 500
</a>

<a class=""prompt prompt-info"">
initial temperature for third TAD cooling step with fully flexible interface -> 300
</a>

<a class=""prompt prompt-info"">
number of MD steps for rigid body high temperature TAD -> 0
</a>

<a class=""prompt prompt-info"">
number of MD steps during first rigid body cooling stage -> 0
</a>


* **Step 8:** You are ready to submit! Enter your username and password (or the course credentials provided to you). Remember that for this interface you do need guru access.

Again, pre-calculated runs are provided if you have executed the ```download-run-data.csh``` and/or ```download-run-data-full.csh``` scripts provided in the ```runs``` directory (see [Setup](#setup) section).


<hr>
## Analysis of the targeted docking results

Once your run has completed  you will be presented with a result page showing the cluster statistics and some graphical representation of the data.
Instead, you can also use the precalculated run. Simply unpack the archive, go into the directory and open with your favorite web browser the index.html file to view the results page.

<a class=""prompt prompt-question"">
How many clusters have been generated?
</a>
<a class=""prompt prompt-question"">
What is the score difference between the various clusters? Is the top one significantly better in score than the next one?
</a>

<a class=""prompt prompt-info"">
Visualize and compare the various clusters (use all what you have learned about Pymol in this tutorial to visualize the binding site).
</a>

You can also compare the orientation of the ligand in our models with the orientation of the same ligand in the crystal structure with rifampicin bound in chain C (remember that chain C of that structure corresponds to chain A in the nomemclature of Sennhauser et al.) ([3AOD](http://www.ebi.ac.uk/pdbe/entry/pdb/3aod/bound/RFP)), corresponding to a channel slightly narrower than for chain A (our current chain B in Sennhauser). Or simply use the renumbered 3AOD structure provided in the ```pdbs``` directory called ```3AOD-renumbered-BCA.pdb``` to compare the structures in Pymol. In this renumbered structure, we changed the chain IDs such as that the chain binding rifampicin corresponds to chain B of 2J8S which we targeted.

<a class=""prompt prompt-cmd"">
pymol cluster*_1.pdb $WDIR/../pdbs/3AOD-renumbered-BCA.pdb
</a>

<a class=""prompt prompt-pymol"">
select refe, 3AOD-renumbered-BCA<br>
select cluster*<br>
alignto refe, method=align, cycles=0<br>
zoom vis<br>
show ribbon<br>
hide lines<br>
util.cbc<BR>
select resn MIY+RFP<BR>
show sphere, sele<BR>
color orange, sele<BR>
select 3AOD-renumbered-BCA and resn MIY+RFP<BR>
color red, sele<BR>
</a>


**Note:** You should realize that the crystal structure has a limited resolution (3.3Å) and its quality is also limited (see the ""[Experiments and Validation](http://www.ebi.ac.uk/pdbe/entry/pdb/3aod/experiment)"" page provided by the PDBe for this structure). In general for modelling purposes, it might also be worth considering the recalculated structure from [PDB_REDO](http://www.cmbi.ru.nl/pdb_redo/), the database of updated and optimized X-ray structure models.

<hr>
## Congratulations!

You have completed this tutorial. If you have any questions or 
suggestions, feel free to contact us via email or by submitting an issue in the 
appropriate [Github repository][link-data] or asking a question through our [support center](http://ask.bioexcel.eu).

[link-pymol]: http://www.pymol.org/ ""Pymol""
[link-data]: https://github.com/haddocking/HADDOCK-binding-sites-tutorial ""HADDOCK binding site tutorial data""
[link-pdb-tools]: https://github.com/haddocking/pdb-tools ""PDB tools""
",2022-08-12
https://github.com/haddocking/haddock-pilot,"# HADDOCK pilot machinery for running on HPC systems


### Introduction and concept

This repository contains simple example of a pilot machinery for running HADDOCK on a HPC system, using multiple nodes.
One pilot is started on each node, which runs as long as there is work to be done.

The pilot monitors the `01-TODO` directory for gzipped tar archives (`.tgz`) which are ready to execute HADDOCK runs.
When work is found, the pilot will move the archive to the `02-RUNNING` directory, touch in that directory a corresponding `.tgz.process` file to tell other pilots that this particular run is already being handled. The pilot then copies and unpacks the HADDOCK run into the `/tmp` directory and executes HADDOCK, running HADDOCK in node mode (i.e. the HADDOCK python process is started locally on the node, and the queue command defined in `run.cns` is simply `csh` (a version of HADDOCK configured to this end should thus be used). To use a full node, the number of queues in `run.cns` should be set to the number of available cores/threads on the node.

All computations thus occur locally on the node as the run directory should have been setup with relative paths (i.e. `./`).
Upon completion of the HADDOCK run, the pilot will archive the run, move it to the `04-RESULTS` directory and delete the local directory under `/tmp`. 
The input archive is moved from `02-RUNNING` to `03-DONE` and the `.process` file is deleted.

The pilot will then look for the next workload.
When no workloads are present anymore the pilot will stop.


### Relevant scripts


* `setup-run-node.csh`: An example script to prepare a docking run and modify parameters in `run.cns`. Edit it an adapt it to your needs. The script is currently configured to run the BM5 examples provided in the `DATA` directory, using true interface restraints defined at 5A. Call this script with as argument a list of complexes to pre-process (the structure of the complex directory should follow that of the HADDOCK-ready BM5 repository). E.g.: `./setup-run-node.csh DATA/1AY7`

* `haddock-pilot.csh`: The simple pilot script that will process the workload in alphabetical order as present in the `01-TODO` directory

* `haddock-pilot-size.csh`: The simple pilot script that will process the workload starting with the largest archives present in the `01-TODO` directory

* `run-mpi-haddock.sh`: An example script to manually launch the pilots on a set of nodes using `mpirun`. It expects a `hostfile` containing the list of nodes. The number of nodes given as argument to `-np` should match the number of nodes in the `hostfile`

* `run-pbs-haddock.sh`: An example script to launch the pilots using `mpirun` via the PBS batch system. The queue (`-q` argument), number of nodes (`-l nodes`) and number of processes per node (`ppn`) should be adapted to your needs. The hostfile for `mpirun` will be automatically created based on the allocated nodes.

* `run-slurm-haddock.sh`: An example script to launch the pilots using `srun` via the SLURM batch system. The number of nodes and processes (`-N` and `-n`) should match the number of requested nodes. The number of processes per node (`-c`) should match the number of cores/threads per node (in case of ful node allocation) and match the number of queues defined in `run.cns`. This script should be executed on the master nodes. The `srun` SLRUM command will allocate the nodes and start the pilots.


### Directory structure


The repository contains four directories:

* `01-TODO`: This is the directory where the ready to run archives should be placed. The pilots will monitor this directory.

* `02-RUNNING`: Directory containing the currently running jobs

* `03-DONE`: Directory containing the completed jobs (the ready to run archives)

* `04-RESULTS`: Directory containg the completed results archives

* `DATA`: Example data directory containing 8 complexes taken from the Docking Benchmark5, ranging in size from 184 to 211 residues.

",2022-08-12
https://github.com/haddocking/haddock-tools,"
 haddock-tools
================================================

Set of useful HADDOCK utility scripts, which requires Python 3.7+.

About
---------
This is a collection of scripts useful for pre- and post-processing and analysis for HADDOCK runs.
Requests for new scripts will be taken into consideration, depending on the effort and general usability of the script.


Installation
------------
Download the zip archive or clone the repository with git. This last is the recommended option as it
is then extremely simple to get updates.

```bash
# To download
git clone https://github.com/haddocking/haddock-tools

# To compile the executables
cd haddock-tools
make

# To update
cd haddock-tools && git pull origin master
```

Scripts
------------


## Restraints-related

#### passive_from_active.py
A python script to obtain a list of passive residues providing a PDB file and a list of active residues.
This will automatically calculate a list of surface residues from the PDB to filter out buried residues except if
a surface list is provided.
By default, neighbors of the active residues are searched within 6.5 Angstroms and surface residues are residues whose
relative side chain accessibility or main chain accessibility is above 15%.

Requirements:
* [FreeSASA](https://freesasa.github.io/)

`pip install freesasa`
* [BioPython](http://biopython.org/)

`pip install biopython`

Usage:
```bash
./passive_from_active.py [-h] [-c CHAIN_ID] [-s SURFACE_LIST]
                              pdb_file active_list

positional arguments:
  pdb_file              PDB file
  active_list           List of active residues IDs (int) separated by commas

optional arguments:
  -h, --help            show this help message and exit
  -c CHAIN_ID, --chain-id CHAIN_ID
                        Chain id to be used in the PDB file (default: All)
  -s SURFACE_LIST, --surface-list SURFACE_LIST
                        List of surface residues IDs (int) separated by commas
```

#### active-passive_to_ambig.py
A python script to create ambiguous interaction restraints for use in HADDOCK based on list of active and passive residues (refer to the [HADDOCK software page](http://www.bonvinlab.org/software/haddock2.2/haddock.html) for more information)

Usage:
```bash
     ./active-passive_to_ambig.py <active-passive-file1> <active-passive-file2>
```

where <active-passive-file> is a file consisting of two space-delimited lines with
the first line active residues numbers and the second line passive residue numbers. One file per input structure should thus be provided.


#### restrain_bodies.py
A python script to creates distance restraints to lock several chains together.
Useful to avoid unnatural flexibility or movement due to
sequence/numbering gaps during the refinement stage of HADDOCK.

Usage:
```bash
./restrain_bodies.py [-h] [--exclude EXCLUDE [EXCLUDE ...]] [--verbose] structures [structures ...]

  positional arguments:
    structures            PDB structures to restraint

  optional arguments:
    -h, --help            show this help message and exit
    --exclude EXCLUDE [EXCLUDE ...], -e EXCLUDE [EXCLUDE ...] Chains to exclude from the calculation
    --verbose, -v
```

#### restrain_ligand.py
Calculates distances between neighboring residues of a ligand molecule and produces a set of
unambiguous distance restraints for HADDOCK to keep it in place during semi-flexible refinement.
Produces, at most, one restraint per ligand atom.

Usage:
```bash
./restrain_ligand.py [-h] -l LIGAND [-p] pdbf

positional arguments:
  pdbf                  PDB file

optional arguments:
  -h, --help            show this help message and exit
  -l LIGAND, --ligand LIGAND
                        Ligand residue name
  -p, --pml             Write Pymol file with restraints
```

#### haddock_tbl_validation
The validate_tbl.py script in that directoy will check the correctness of your restraints (CNS format) for HADDOCK.

Usage:
```bash
usage: python validate_tbl.py [-h] [--pcs] file

This script validates a restraint file (*.tbl).

positional arguments:
  file        TBL file to be validated

  optional arguments:
    -h, --help  show this help message and exit
    --pcs       PCS mode
```

### calc-accessibility.py

```bash
$ python3 haddock-CSB-tools/calc-accessibility.py -h                                                                                                                                                                                                               [17:06:52]
usage: calc-accessibility.py [-h] [--log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}] [--cutoff CUTOFF] pdb_input

positional arguments:
  pdb_input             PDB structure

optional arguments:
  -h, --help            show this help message and exit
  --log-level {DEBUG,INFO,WARNING,ERROR,CRITICAL}
  --cutoff CUTOFF       Relative cutoff for sidechain accessibility

$ python3 haddock-CSB-tools/calc-accessibility.py complex_1w.pdb --cutoff 0.4                                                                                                                                                                                      [17:10:51]
02/11/2020 17:10:57 L157 INFO - Calculate accessibility...
02/11/2020 17:10:57 L228 INFO - Chain: A - 115 residues
02/11/2020 17:10:57 L228 INFO - Chain: B - 81 residues
02/11/2020 17:10:57 L234 INFO - Applying cutoff to side_chain_rel - 0.4
02/11/2020 17:10:57 L244 INFO - Chain A - 82,83,84,85,86,87,88,90,91,94,95,98,99,102,104,106,109,113,116,117,118,122,128,129,130,132,139,141,144,145,148,149,150,151,153,156,158,160,162,163,167,168,169,170,171,173,174,175,176,178,179,180,181,183,184,186,188,194,196
02/11/2020 17:10:57 L244 INFO - Chain B - 1,2,4,5,8,11,12,15,18,21,23,24,25,26,27,30,31,33,34,37,38,41,43,44,45,46,47,50,63,64,67,69,70,73,74,76,77,78,79,80,81
```

### create_cif.py

Converts the `cluster*.pdb` files in a run directory to [IHM mmCIF](https://mmcif.wwpdb.org/dictionaries/mmcif_ihm.dic/Index/) format

Warning: Limited functionally, still work in progress! Tested for hetero-complexes with ambig restraints.

Needs `ihm` and `biopython`, install it with
```bash
$ pip install ihm --install-option=""--without-ext""
$ pip install biopython
```

```
$ python3 create_cif.py -h
usage: create_cif.py [-h] run_directory

positional arguments:
  run_directory  Location of the uncompressed run, ex:
                 /home/rodrigo/runs/47498-protein-protein

optional arguments:
  -h, --help     show this help message and exit


$ python3 haddock-CSB-tools/create_cif.py ~/projects/cif_parser/47518-cif
[23/02/2021 13:45:25] INFO Converting the cluster*.pdb structures to .cif
[23/02/2021 13:45:25] INFO Looking for models in /Users/rodrigo/projects/cif_parser/47518-cif
[23/02/2021 13:45:25] INFO Found 4 structures
[23/02/2021 13:45:25] INFO Looking for the tblfile field in /Users/rodrigo/projects/cif_parser/47518-cif/job_params.json
[23/02/2021 13:45:25] INFO tblfile field found, extracting information
[23/02/2021 13:45:25] INFO Converting /Users/rodrigo/projects/cif_parser/47518-cif/cluster1_1.pdb
[23/02/2021 13:45:26] INFO Saving as cluster1_1.cif
[23/02/2021 13:45:26] INFO Converting /Users/rodrigo/projects/cif_parser/47518-cif/cluster1_2.pdb
[23/02/2021 13:45:26] INFO Saving as cluster1_2.cif
[23/02/2021 13:45:27] INFO Converting /Users/rodrigo/projects/cif_parser/47518-cif/cluster1_3.pdb
[23/02/2021 13:45:27] INFO Saving as cluster1_3.cif
[23/02/2021 13:45:28] INFO Converting /Users/rodrigo/projects/cif_parser/47518-cif/cluster1_4.pdb
[23/02/2021 13:45:29] INFO Saving as cluster1_4.cif
```

------------
## PDB-related

#### contact-segid
A c++ program to calculate all heavy atom interchain contacts (where the chain identification is taken from the segid) within a given distance cutoff in Angstrom.

Usage:
```bash
   contact-segid <pdb file> <cutoff>
```

#### contact-chainID
A c++ program to calculate all heavy atom interchain contacts (where the chain identification is taken from the chainID) within a given distance cutoff in Angstrom.

Usage:
```bash
   contact-chainID <pdb file> <cutoff>
```

#### molprobity.py
A python script to predict the protonation state of Histidine residues for HADDOCK. It uses molprobity for this, calling the [Reduce](http://kinemage.biochem.duke.edu/software/reduce.php) software which should in the path.

Usage:
```bash
    ./molprobity.py <PDBfile>
```

Example:
```bash
./molprobity.py 1F3G.pdb
## Executing Reduce to assign histidine protonation states
## Input PDB: 1F3G.pdb
HIS ( 90 )	-->	HISD
HIS ( 75 )	-->	HISE
```
An optimized file is also written to disk, in this example it would be called ```1F3G_optimized.pdb```.


#### pdb_blank_chain
Simple perl script to remove the chainID from a PDB file

Usage:
```bash
    pdb_blank_chain inputfile > outputfile
```

#### pdb_blank_segid
Simple perl script to remove the segid from a PDB file

Usage:
```bash
    pdb_blank_segid inputfile > outputfile
```

#### pdb_blank_chain-segid
Simple perl script to remove both the chainID and segid from a PDB file

Usage:
```bash
    pdb_blank_chain-segid inputfile > outputfile
```

#### pdb_chain-to-segid
Simple perl script to copy the chainID to the segid in a PDB file

Usage:
```bash
    pdb_chain-to-segid inputfile > outputfile
```

#### pdb_segid-to-chain
Simple perl script to copy the segid to the chainID in a PDB file

Usage:
```bash
    pdb_segid-to-chain inputfile > outputfile
```

#### pdb_chain-segid
Simple perl script to copy the chainID to segid in case the latter is empty (or vice-verse) in a PDB file

Usage:
```bash
    pdb_chain-segid inputfile > outputfile
```

#### pdb_setchain
Simple perl script to set the chainID in a PDB file

Usage:
```bash
     pdb_setchain -v CHAIN=chainID inputfile > outputfile
```

#### joinpdb
Simple perl script to concatenate separate single structure PDB files into a multi-model PDB file.
Usage:
```bash
     joinpdb  -o outputfile  [inputfiles]

    where inputfiles are a list of PDB files to be concatenated
```

#### pdb_mutate.py
A python script to mutate residues for HADDOCK. A mutation list file is used as input, and the output is/are corresponding PDB file(s) of mutant(s). The format of mutation in the mutation list file is ""PDBid ChainID ResidueID ResidueNameWT ResidueNameMut"".


Usage:
```bash
    ./pdb_mutate.py <mutation list file>
```

Example:
```bash
./pdb_mutate.py mut_1A22.list

## In  mut_1A22.list, the residue 14, 18 and 21 in chain A will be mutated to ALA:
## 1A22.pdb A 14 MET ALA
## 1A22.pdb A 18 HIS ALA
## 1A22.pdb A 21 HIS ALA
```

#### pdb_strict_format.py
A python script to check format of PDB files with respect to HADDOCK format rules. A PDB file is used as input, and the output is a console message if an error or a warning is triggered by a bad formmated line. The script uses wwPDB format guidelines [wwwPDB guidelines](http://www.wwpdb.org/documentation/file-format-content/format33/sect9.html) and check resid against a list of known ligands and amino-acids recognized by HADDOCK.


Usage:
```bash
./pdb_strict_format.py [-h] [-nc] pdb

This script validates a PDB file (*.pdb).

positional arguments:
  pdb                PDB file

optional arguments:
  -h, --help         show this help message and exit
  -nc, --no_chainid  Ignore empty chain ids
```

#### param_to_json.py
A python script to transform a haddockparam.web file into a JSON structure. It is possible to use it as a class and then access
extra functions like: `change_value(key, value)` ; `update(subdict_to_replace)` ; `dump_keys()` ; `get_value(key)` ; `write_json()`

Usage:
```bash
./param_to_json.py [-h] [-o OUTPUT] [-g GET] [-e [EXAMPLE]] web

This script parses a HADDOCK parameter file (*.web) and transforms it to JSON
format. It also allows to change a parameter of the haddockparam.web

positional arguments:
  web                   HADDOCK parameter file

optional arguments:
  -h, --help            show this help message and exit
  -o OUTPUT, --output OUTPUT
                        Path of JSON output file
  -g GET, --get GET     Get value of a particular parameter
  -e [EXAMPLE], --example [EXAMPLE]
                        Print an example
```

#### `renumber_model.py`

A python script to match chains of a model to a given reference. The numbering relationship is obtained via sequence alignment with BLOSUM62 matrix. This is only indicated for complexes with high similarity.

This script supports multi-chain complexes but expects the chains to match sequentially between the `reference` and the `model`.

```text
Ref  Model
A   A
B   B
C   C
```

Usage:

```text
$ python renumber_model.py example_data/renumber_model/ref.pdb example_data/renumber_model/to_refine.BL00010001.pdb

 [2022-07-13 16:29:05,492 renumber_model:L211 INFO] Getting sequence numbering relationship via BLOSUM62 alignment
 [2022-07-13 16:29:05,525 renumber_model:L114 DEBUG] Writing alignment to blosum62_A.aln
 [2022-07-13 16:29:05,526 renumber_model:L151 DEBUG] Sequence identity between chain A of example_data/ref.pdb and chain A of example_data/to_refine.BL00010001.pdb is 100.00%
 [2022-07-13 16:29:05,527 renumber_model:L114 DEBUG] Writing alignment to blosum62_C.aln
 [2022-07-13 16:29:05,528 renumber_model:L151 DEBUG] Sequence identity between chain C of example_data/ref.pdb and chain C of example_data/to_refine.BL00010001.pdb is 100.00%
 [2022-07-13 16:29:05,529 renumber_model:L114 DEBUG] Writing alignment to blosum62_D.aln
 [2022-07-13 16:29:05,529 renumber_model:L151 DEBUG] Sequence identity between chain D of example_data/ref.pdb and chain D of example_data/to_refine.BL00010001.pdb is 98.18%
 [2022-07-13 16:29:05,531 renumber_model:L114 DEBUG] Writing alignment to blosum62_E.aln
 [2022-07-13 16:29:05,531 renumber_model:L151 DEBUG] Sequence identity between chain E of example_data/ref.pdb and chain E of example_data/to_refine.BL00010001.pdb is 95.50%
 [2022-07-13 16:29:05,531 renumber_model:L213 INFO] Renumbering model according to numbering relationship
 [2022-07-13 16:29:05,531 renumber_model:L178 INFO] Renumbered model name: to_refine.BL00010001_renumbered.pdb
 [2022-07-13 16:29:05,539 renumber_model:L199 WARNING] Ignored residues [43, 82, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202] in model's chain D
 [2022-07-13 16:29:05,539 renumber_model:L199 WARNING] Ignored residues [17, 42, 77, 80, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237] in model's chain E
 [2022-07-13 16:29:05,539 renumber_model:L216 INFO] Renumbering complete
 [2022-07-13 16:29:05,540 renumber_model:L217 INFO] DO NOT trust this renumbering blindly!
 [2022-07-13 16:29:05,540 renumber_model:L218 INFO] Check the .aln files for more information
```

A `_renumbered.pdb` file is created in the same directory as the input file together with multiple `blosum62_ChainID.aln` files.

License
---------

Apache Licence 2.0
",2022-08-12
https://github.com/haddocking/haddock24-json-editor,"# README

# haddock-json-editor

A simple gui to help you change your molecule json files.

# about

This is meant as an offline/intermediary solution to editing molecule json files. This will be made available via web interface in an upcoming haddock update hopefully soon. You can use this to change the PDB and/or active/passive residue.

# requirements:

- A working python 3 environment with pip. 
- The haddock-logo and molecule images are required to be in the same map as the gui.py script.

 
# for windows users:

open cmd, navigate to your map of choice. If you already have git you can skip the first step

Just copy pasta the commands below into your terminal. Say Y if it asks you a Y/n question. 


```
pip install git
```

```
git clone https://github.com/haddocking/haddock24-json-editor
```

```
pip install -r requirements.txt
```

```
gui.py 
```

# For linux users:

open terminal, navigate to your map of choice. If you already have git you can skip the first step

Just copy pasta the commands below into your terminal. Say Y if it asks you a Y/n question.

sidenote: some versions of linux have issues with tkinter and/or python, while perfectly solvable this involves different steps with package managers depending on the destro. Switch to windows if you don't know how to fix these issues.

```
sudo apt-get install git
```

```
git clone https://github.com/haddocking/haddock24-json-editor
```

```
pip3 install -r requirements.txt
```

```
python3 gui.py 
```

# Usage for editing your json file

Once you have executed the script you should be presented with the following gui.

![ExplainPicture1](https://user-images.githubusercontent.com/39408360/140933003-5df5a5a4-5993-4975-be4c-c0b7c6e6fcbb.png)

By clicking on the top button a file opener will appear and you can navigate to the folder where the json file is you wish to edit.

![ExplainPicture3](https://user-images.githubusercontent.com/39408360/140933013-0f11fbe0-fdde-4a99-b104-ba2a608f1e2a.png)

Upon succesfully finding and opening your json file you should be prompted with the following screen. For each molecule a button will be generated.

![ExplainPicture4](https://user-images.githubusercontent.com/39408360/140933015-ce99d1d0-719e-44b4-ac12-b22397b6ab09.png)

Click on the molecule of your choice you wish to edit. You will then be taken to the following screen. For now we will replace a pdb.

![ExplainPicture5](https://user-images.githubusercontent.com/39408360/140933017-a761ef3b-1f0c-48d5-9ac1-cc06868cae7a.png)

Clicking on replace pdb will open up a file browser again, use this to open up your .pdb file.

![ExplainPicture6](https://user-images.githubusercontent.com/39408360/140933019-be82f155-9f9d-422b-98ef-0efeb2fb79a1.png)

If all went well, you should recieve the following popup.

![ExplainPicture7](https://user-images.githubusercontent.com/39408360/140933020-7f8663b6-69ea-480b-812e-a4b77203e1a5.png)

Afterwards you have replaced your pdb file you will automaticly be taken back to the molecule selection screen

![ExplainPicture8](https://user-images.githubusercontent.com/39408360/140933022-b582112b-c987-4543-b81d-039174aea3e8.png)

For editing a residue of a molecule, select the molecule. But this time click edit active/passive residue (they work the same).
You will be presented with the following screen

![ExplainPicture9](https://user-images.githubusercontent.com/39408360/140933024-3831f988-009b-4891-b144-3cdcb0aad1a0.png)

Editing a residue you can do by clicking on the text box and using your keyboard.
If the residue for some reason is very large and you need to change it with another very large residue, you can use doubleclick on the textbox and then ctrl-c ctrl-v (copy paste) your new residue in. 

![ExplainPicture10](https://user-images.githubusercontent.com/39408360/140936507-3916a13d-8570-466d-a26d-8c5b5a5ae006.png)

You will then be taken back to the molecule selection screen, having done our changes we now save the new file.

![ExplainPicture11](https://user-images.githubusercontent.com/39408360/140937729-5b6215d8-f92a-4be9-b602-fa198b071041.png)

Save your file and done!

# Updates/desires/requests

If you would like more, maybe different options, feel free to add requests below 
",2022-08-12
https://github.com/haddocking/haddock2mmcif,"# `haddock2mmcif`

🚧 Attention 🚧

This code is under development and subject to change. Please report any bugs by opening an issue in the repository.

* * *

Encode information from a HADDOCK run into a `.cif` to be deposited in PDB-Dev.

## Currently the follow information is encoded in the `.cif`

- Models are represented as:

    1. Whole structure as rigid
    2. Interface as flexible, defined by the `flcut` parmameter in `run.cns`

- Top4 models from each clusters within `ModelGroup`, ranked by their top4 HADDOCK score
- Restraints as `Dataset` represented as `DerivedDistanceRestraint`

    1. Ambiguous, Active/Passive as `ResidueFeature` with probability defined by `ncvpart` in `run.cns` read from `ambig.tbl`
    2. Unambiguous, the `ResidueFeature` are read from `unambig.tbl`


## To-be-implemented

- HADDOCK-score

* * *

## Installation

```
$ git clone https://github.com/haddocking/haddock2mmcif
$ cd haddock2mmcif
$ python setup.py develop
$ cd tools
$ g++ -o contact-chainID contact-chainid.cpp
$ cd ..
```

## Usage

The input for `haddock2mmcif` is the folder of a run-directory, either downloaded from the web server or executed locally.

```
$ cd example_data
$ tar zxvf 6269-E2A-HPR.tgz
$ haddock2mmcif --output example.cif E2A-HPR/
```
",2022-08-12
https://github.com/haddocking/haddock3,"Welcome to the HADDOCK3-Beta version.

The `main` branch represents the latest state of HADDOCK v3. Currently,
stable beta version.

[![unit tests](https://github.com/haddocking/haddock3/workflows/tests/badge.svg?branch=main)](https://github.com/haddocking/haddock3/actions?workflow=tests)
[![build](https://github.com/haddocking/haddock3/workflows/build/badge.svg?branch=main)](https://github.com/haddocking/haddock3/actions?workflow=build)
[![docs](https://github.com/haddocking/haddock3/workflows/docs/badge.svg?branch=main)](https://github.com/haddocking/haddock3/actions?workflow=docs)

* * *

# HADDOCK3

## 1. Installation

To install HADDOCK3 follow the instructions in the [INSTALL](docs/INSTALL.md)
file.

## 2. Documentation

HADDOCK3-beta documentation is hosted online at https://www.bonvinlab.org/haddock3/.
The documentation is rendered and update at every commit to the `main` branch. So,
you will always find the latest version in the link above.

If you want to compile the HADDOCK3 documentation pages locally (offline),
install HADDOCK3 and activate the `haddock3` python environment as explained in
the [installation instructions](docs/INSTALL.md). Then, in your terminal
window, run:

```bash
tox -e docs
```

*Ignore any warning messages.* After, use your favorite browser to open the
file `haddock3-docs/index.html`. This will open a local webpage with the
complete HADDOCK3 documentation. Exactly the same you will find online.
Navigate around, enjoy, and contribute.

## 3. Examples

### 3.1. Basic scoring of an ensemble of 5 structures:

In the `examples/` folder you find several examples for you to test and
learn HADDOCK3. Additional information is in the documentation pages.

```bash
cd examples/scoring/
haddock3 emscoring-test.cfg
```

## 4. Contribute

If you want to contribute to HADDOCK3's development, read the
[CONTRIBUTING](CONTRIBUTING.md) file for instructions.

## 5. Keep in contact and support us

HADDOCK3 is an academic project supported by various grants, including the EU
BioExcel Center of Excellence for Computational Biomolecular Research. HADDOCK3
is fully open-source and free to download. If you clone this repository and use
HADDOCK3 for your research, please support us by signing [this Google
form][googleform] if you have not yet done so. This will allow us contact you
when needed for HADDOCK3-related issues, and also provide us a mean to
demonstrate impact when reporting for grants.

[googleform]: https://docs.google.com/forms/d/e/1FAIpQLScDcd0rWtuzJ_4nftkDAHoLVwr1IAVwNJGhbaZdTYZ4vWu25w/viewform
",2022-08-12
https://github.com/haddocking/haddock_param_tools,"[![Build Status](https://travis-ci.com/mtrellet/haddock_param_tools.svg?branch=master)](https://travis-ci.com/mtrellet/haddock_param_tools)
[![Build status](https://ci.appveyor.com/api/projects/status/fol5butsr9nd236x?svg=true)](https://ci.appveyor.com/project/mtrellet/haddock-param-tools)
[![codecov](https://codecov.io/gh/mtrellet/haddock_param_tools/branch/master/graph/badge.svg)](https://codecov.io/gh/mtrellet/haddock_param_tools)
[![Documentation Status](https://readthedocs.org/projects/haddock-param-tools/badge/?version=latest)](https://haddock-param-tools.readthedocs.io/en/latest/?badge=latest)

# haddock_param_tools
Python scripts and API to manipulate HADDOCK2.4 parameter files (*.json).

This repository is split between scripts that can be used at the command-line level
and a API allowing to use those functions in any python script.

Tested with:

    - Python3.6 
    - Python3.7

# Scripts

The scripts contained in the [scripts](scripts/) directory can be used as commands in any terminal.
They accept both file path or stream from the standard output as input. This allows for piping them
one after each other in order to perform different operations at once.

## Summary

```bash
$> python haddock_param_summary.py job_params.json
it0	1000
it1	20
itw	20
Partner1: Protein
Partner2: Protein
clust_meth: FCC
clust_cutoff: 0.6
```

## Validate a parameter file

```bash
$> python haddock_param_validate.py job_params.json
WARNING: No partner detected
ERROR: Wrong type for parameter 'amb_cool2'
...
```

## Get input PDB files

```bash
$> python haddock_param_extract_pdb.py job_params.json
partner1.pdb created
partner2.pdb created
```

## Replace a parameter

```bash
$> python haddock_param_replace.py amb_cool1 20 job_params.json
{
 'amb_cool1': 20,
 'amb_cool2': 50,
 ...
}
```

# API

This [API](param_to_json) allows access to most operations on a parameter file at the python level. 
It is based on a HADDOCKParam object that encapsulates the JSON dictionary and exposes different 
information and operations to edit/change the parameters. Any change can be dumped to a new parameter 
file using the `dumper` function.

## Documentation

See more information here: https://haddock-param-tools.readthedocs.io/

## Installation

- Build

```bash
$> python setup.py build
```
- Optional - Test
```bash
$> python setup.py test
```

- Install
```bash
$> python setup.py install
```

## Usage

### Load a parameter file

```python
from param_to_json import HADDOCKParam

params = HADDOCKParam()
params.load('test/input/prot-prot-em.json')
print(params.nb_partners)
```

# License

Apache (see [LICENSE](LICENSE))
",2022-08-12
https://github.com/haddocking/iSee,"# iSEE

## About iSEE
iSEE is a computational predictor of binding affinity changes upon single point mutations (∆∆G) [1]. It is based on a random forest model that makes use of **i**nterface **S**tructure, **E**volution and **E**nergy-based features.

This repository provides the iSEE predictor that you can use to do ∆∆G prediction and to identify the effects of mutations on your protein complexes. Besides, we provide all features data used in the iSEE project [1].

The HADDOCK-refined models of both wild type and mutant complexes as well as the PSSM files used to construct the features can be found in the SBGrid data repository ([doi:10.15785/SBGRID/520](https://data.sbgrid.org/dataset/520)).

Please cite the article [1] if you use iSEE.


## Requirements
1. Download and install [R](https://www.r-project.org/). After installing, type command `R` or `RScript` in your terminal, they should be available now.
2. Install R packages [randomForest](https://cran.r-project.org/web/packages/randomForest/) and [caret](https://cran.r-project.org/web/packages/caret/)
```r
# Enter R enviroment
R

# Install R packages
> install.packages(c(""caret"", ""randomForest""))

# Quit R environment
> q()
```


## Usage
To use the iSEE ∆∆G predictor at the command line type:

	Rscript run_iSEE.R  <features data file>

- `iSEE.model` is the iSEE ∆∆G predictor, which is a binary file and executed by the script `run_iSEE.R`. Note the two files must be in the same directory.
- Users need to provide the `<features data file>` with rigorous format as shown in the example file `features_examples.tsv`. Headers must be present and a tab should be used as column separator.


#### Example:

	Rscript run_iSEE.R features_examples.tsv

## Features data

The directory `isee_features_data` contains all features data used in this work[1].

Features based on the top 1 HADDOCK refined model:
- `features_training_dataset_top1.tsv`: Features used as training dataset. The `iSEE.model` we provide here is trained on these data.
- `features_NM_test_dataset_top1.tsv`: The independent NM test dataset of Benedix et al. Nature Methods 2009.
- `features_S487_test_dataset_top1.tsv`: The independent SKEMPI2 S487 test
  dataset.
- `features_MDM2-p53_test_dataset_top1.tsv`: The independent MDM2-p53 test dataset.
- `features_MDM2-p53_full_mutations_top1.tsv`: Features used for MDM2-p53 full mutations.

Average features based on the top 4 HADDOCK refined models:
- `features_training_dataset_top4.tsv`
- `features_NM_test_dataset_top4.tsv`
- `features_S487_test_dataset_top4.tsv`
- `features_MDM2-p53_test_dataset_top4.tsv`
- `features_MDM2-p53_full_mutations_top4.tsv`


## Reference
1. C. Geng, A. Vangone, G.E. Folkers, L.C. Xue and A.M.J.J. Bonvin. [iSEE: Interface Structure, Evolution and Energy-based machine learning predictor of binding affinity changes upon mutations.](https://doi.org/10.1002/prot.25630) Proteins: Struc. Funct. & Bioinformatics Advanced Online Publication (2018).
",2022-08-12
https://github.com/haddocking/MD-scoring,"# MD-scoring
Script and associated data for MD-based scoring of HADDOCK clusters

This repository is part of a protocol based on molecular dynamics (MD) simulations would allow to distinguish native from non-native models to complement scoring functions used in docking. First, models for 25 protein-protein complexes were generated using HADDOCK. Next, MD simulations complemented with machine learning were used to discriminate between native and non-native complexes based on a combination of metrics reporting on the stability of the initial models. Here one can access used scripts, jupyter notebooks of machine learning models and extracted features which served as input for machine learning.


## MD trajectories
MD trajectories of 100 ns for 25 protein-protein complexes (8 HADDOCK models x 2 replicas, reference crystal structure in 4 replicas per model) are available at [zenodo](http://doi.org/10.5281/zenodo.4629895)

## Obtaining complex properties -> features as input for ML classifiers
All trajectories were analyzed by scripts in the `gmx_scripts` directory 

## Features
All properties were extracted into features .csv files. Feature files for different training and validation sets are in the `features` directory. 

## RF classifiers
* First, the most fitting(accurate) ML classifier was selected. This was done comparing 9 different classifiers from the Scikit-learn library. The jupyter notebook used for this purpose is `Classifier selection`.
* To optimize the most accurate classifier (Random Forest in our case) two runs of optimization were run (Random and Grid Search). Both of these procedures are shown in the `Parameters_search` notebook.
* To estimate the accuracy of the Random Forest classifier we first tested it the training set using K-fold cross-validation (100x) in `Cross_validaiton`.
* To test the classifier on an external validations set(s) the`External_validation` notebook was used.",2022-08-12
https://github.com/haddocking/MemCplxDB,"# MemCplxDB

[![DOI](https://zenodo.org/badge/141016280.svg)](https://zenodo.org/badge/latestdoi/141016280)

Table of Contents

* [Introduction](#introduction)
* [Repository Structure](#repository-structure)
* [Benchmark entries](#benchmark-entries)
  * [Dimeric complexes](#dimeric-complexes)
  * [Trimeric complexes](#trimeric-complexes)
* [Analysis](#analysis)
  * [HADDOCK terms](#haddock-terms)
  * [RMSD terms](#rmsd-terms)
* [Reference](#reference)

## Introduction

Membrane protein complexes docking benchmark. This repository contains the
entries of the Membrane Protein docking benchmark (V1.0). It also contains
[ProFit](http://www.bioinf.org.uk/software/profit/) analysis scripts for
the calculation of Interface- and Ligand-RMSD (I-/L-RMSD) values for all
the entries of the benchmark. We have generated docking decoys for all the
entries of the benchmark with the [HADDOCK](http://milou.science.uu.nl/services/HADDOCK2.2/)
webserver. In addition to I- and L-RMSDs we have calculated van der Waals,
Electrostatic and Desolvation energies and the Buried Surface Area (BSA)
for all decoys of all complexes.

## Repository Structure

The repository structure is the following:

    |   LICENSE
    |   README.md
    |   analysis
    |   \   random_restraints
    |   |   \   1k4c.txt
    |   |   |   1m56.txt
    |   |   /   ...
    |   |   true_interface
    |   |   \   1k4c.txt
    |   |   |   1m56.txt
    |   /   /   ...
    |   structures
    |   \   1k4c
    |   |   \   1k4c_bound.pdb
    |   |   |   1k4c_unbound.pdb
    |   |   |   1k4c_unbound_A.pdb
    |   |   |   1k4c_unbound_C.pdb
    |   |   |   AC.irmsd
    |   |   |   AC.izone
    |   |   /   AC.lzone
    |   |   1m56
    |   |   \   1m56_bound.pdb
    |   |   |   1m56_unbound.pdb
    |   |   |   1m56_unbound_AB.pdb
    |   |   |   1m56_unbound_CD.pdb
    |   |   |   AB.irmsd
    |   |   |   AB.izone
    |   |   /   AB.lzone

The [structures](structures) folder contains all the structure files in PDB format. Both
reference (bound) and unbound structures are included. All structures have been
modified so that chain identifiers and residue numbering is consistent.

Every subfolder in the [structures](structures) folder (eg [1k4c](structures/1k4c)) contains
the following files:

* `*_bound.pdb` -> The reference structure for a given complex.
* `*_unbound_*.pdb`-> The unbound structures for a given complex.
* `*_unbound.pdb` -> The unbound structures optimally and independently superimposed on the reference chains.
* `*.irmsd` -> The backbond I-RMSD (in Å) of the unbound complex relative to the reference structure.
* `*.izone` -> [ProFit](http://www.bioinf.org.uk/software/profit/)-compatible I-RMSD calculation script.
* `*.lzone` -> [ProFit](http://www.bioinf.org.uk/software/profit/)-compatible L-RMSD calculation script.

The two subfolders in the [analysis](analysis) folder contain the results of the docking
runs for two docking scenarios. One using only random restraints to drive the docking
([random_restraints](analysis/random_restraints)) and one using restraints derived from
the interface of the bound complex ([true_interface](analysis/true_interface)). Both folders
contain 37 space-separated text files that are the results of the analysis of the docking
run of a given complex.

## Benchmark entries

The two tables below list the dimeric and trimeric entries respectively. The second column is the PDB
id of the bound complex, the second and third (second through fourth for the trimeric table) list the
origin of the unbound structures for that complex. The complexes that appear more than once in the
benchmark are idenitified by the presence of an identifier in the complex_id field (eg 2r6g-TM).

The _Category_ column refers to the nature of the complex. MS stands for complexes whose interface lies
along the membrane-soluble region, TM stands for complexes whose interface lies within the membrane,
Both stands for complexes whose interface lies in both membrane and soluble regions, Buried stands for
complexes where one of the partners is buried in a transmembrane β-barrel and AB stands for complexes
where one of the partners is an antibody-like domain.

The _Composition_ column refers to the origin of the unbound structures of a complex. BB stands for
Bound-Bound and means all partners have been extracted from the bound complex, UB stands for Bound-Unbound
and means at least one of the partners has been extracted from an alternative structure and UU stands
for Unbound-Unbound and means none of the partners originate in the bound complex.

The _Difficulty_ column reflects the difficulty of a given complex with Bound cases having an I-RMSD
of 0, Easy cases having an I-RMSD between 0 and 1Å, Intermediate cases having an I-RMSD between 1 and 2Å
and Hard cases having an I-RMSD greater than 2Å.

### Dimeric complexes

| complex | complex_pdb_id | Unbound PDB id 1 | Unbound PDB id 2 | Category | Composition |  Difficulty  | Interface RMSD [Å] |
| ------: | -------------- | ---------------- | ---------------- | -------- | ----------- | -----------  | -----------------: |
| 1       | 2bg9           | 2bg9_ADE         | 2bg9_BC          | Both     | BB          | Bound        | 0                  |
| 2       | 2bs2           | 2bs2_AB          | 2bs2_CD          | MS       | BB          | Bound        | 0                  |
| 3       | 2r6g-TM        | 2r6g_F           | 2r6g_G           | TM       | BB          | Bound        | 0                  |
| 4       | 2vpz           | 2vpz_AB          | 2vpz_CD          | MS       | BB          | Bound        | 0                  |
| 5       | 4hg6           | 4hg6_A           | 4hg6_B           | TM       | BB          | Bound        | 0                  |
| 6       | 4huq-TM        | 4huq_S           | 4huq_T           | TM       | BB          | Bound        | 0                  |
| 7       | 4huq-TM-A      | 4huq_ST          | 4huq_A           | MS       | BB          | Bound        | 0                  |
| 8       | 4huq-TM-B      | 4huq_ST          | 4huq_B           | MS       | BB          | Bound        | 0                  |
| 9       | 5a63-BC        | 5a63_B           | 5a63_C           | TM       | BB          | Bound        | 0                  |
| 10      | 2hdi           | 2hdi_A           | 1cii_A           | Buried   | UB          | Easy         | 0.361              |
| 11      | 4j3o           | 4j3o_D           | 3bfq_FG          | Buried   | UB          | Easy         | 0.392              |
| 12      | 1m56           | 2gsm_AB          | 1m56_CD          | TM       | UB          | Easy         | 0.572              |
| 13      | 1k4c           | 1k4c_A           | 1j95_ABCD        | MS       | UU          | Easy         | 0.638              |
| 14      | 3x29           | 3x29_A           | 2quo_A           | MS       | UB          | Easy         | 0.673              |
| 15      | 2k9j           | 2rmz_A           | 2k1a_A           | TM       | UU          | Easy         | 0.678              |
| 16      | 2r6g-TM-peri   | 2r6g_FG          | 1jw4_A           | MS       | UB          | Easy         | 0.716              |
| 17      | 2gsk           | 2guf_A           | 1u07_A           | MS       | UU          | Easy         | 0.86               |
| 18      | 5aww           | 5aww_YG          | 5aww_E           | TM       | UB          | Easy         | 0.868              |
| 19      | 2zxe-AG        | 2zxe_A           | 2zxe_G           | TM       | UB          | Easy         | 0.919              |
| 20      | 2zxe-AB        | 2zxe_A           | 2zxe_B           | TM       | UB          | Easy         | 0.94               |
| 21      | 3wxw           | 3wxw_A           | 1vfa_HL          | AB       | UB          | Easy         | 0.982              |
| 22      | 3hd7           | 3hd7_A           | 3hd7_B           | TM       | UU          | Intermediate | 1.024              |
| 23      | 3csl           | 3csl_A           | 1b2v_A           | MS       | UB          | Intermediate | 1.065              |
| 24      | 2ks1           | 2n2a_A           | 2m0b_A           | TM       | UU          | Intermediate | 1.158              |
| 25      | 5d0o           | 5d0o_A           | 2yhc_A           | MS       | UB          | Intermediate | 1.182              |
| 26      | 5a63-AC        | 5a63_A           | 5a63_C           | TM       | UB          | Intermediate | 1.218              |
| 27      | 3p0g           | 2rh1_A           | 4unu_A           | AB       | UU          | Intermediate | 1.26               |
| 28      | 2r6g-TM-cyto   | 2r6g_FG          | 1q12_AB          | MS       | UB          | Intermediate | 1.363              |
| 29      | 3o0r           | 3o0r_B           | 3o0r_C           | AB       | UB          | Intermediate | 1.445              |
| 30      | 5fxb           | 5fxb_AB          | 1ttf_A           | AB       | UB          | Intermediate | 1.475              |
| 31      | 4q35           | 4q35_A           | 4nhr_A           | Buried   | UB          | Hard         | 2.061              |
| 32      | 4m48           | 4m48_A           | 4dvb_HL          | AB       | UB          | Hard         | 2.335              |
| 33      | 2hi7           | 1ti1_A           | 2k73_A           | MS       | UU          | Hard         | 2.588              |
| 34      | 1ots           | 1kpk_AB          | 4nzu_HL          | AB       | UU          | Hard         | 3                  |
| 35      | 3v8x           | 3v8x_A           | 4x1b_A           | MS       | UB          | Hard         | 3.422              |

### Trimeric complexes

| complex | complex_pdb_id | Unbound PDB id 1 | Unbound PDB id 2 | Unbound PDB id 3 | Category | Composition |  Difficulty  | Interface RMSD [Å] |
| ------: | -------------- | ---------------- | ---------------- | ---------------- | -------- | ----------- |  ----------  | -----------------: |
| 1       | 2j8s           | 3w9h_A           | 3w9h_B           | 3w9h_C           | Both     | UUU         | Easy         | 0.648              |
| 2       | 4fz0           | 2qts_A           | 2qts_B           | 2qts_C           | Both     | UUU         | Intermediate | 1.18               |

## Analysis

All the RMSD values have been calculated with [ProFit](http://www.bioinf.org.uk/software/profit/) and
all energy values along with Surface area by [HADDOCK](http://milou.science.uu.nl/services/HADDOCK2.2/).

The following is an example of the results of the analysis for dimeric complex.

    $ head 1k4c.txt
    complex i-RMSD l-RMSD HS Eair Evdw Eelec Edesolv BSA stage pdb rank difficulty
    1 0.959 2.102 -35.7645987 417.16 9.01013 -14.8071 -12.3443 1287.49 it0 1k4c 705 Easy
    2 13.601 30.105 -10.0395301 1298.94 -1.60801 -4.26846 -6.80819 1193.62 it0 1k4c 6360 Easy
    3 10.638 26.939 -25.8562489 1185.26 4.58911 -5.10454 -19.5606 1308.96 it0 1k4c 3083 Easy
    4 6.757 20.617 -26.756805 990.199 12.5825 -9.88172 -11.7789 1512.4 it0 1k4c 2916 Easy
    5 0.934 4.333 -31.571787 602.385 24.9723 -16.4337 -8.06686 1334.48 it0 1k4c 2137 Easy
    6 14.473 29.379 -2.988335 1385.01 26.7835 -1.33664 -4.52393 1124.57 it0 1k4c 8595 Easy
    7 14.348 29.247 -4.985707 1253.87 20.8563 -0.97943 -5.58424 1116.93 it0 1k4c 8029 Easy
    8 10.826 26.383 -25.9056835 1206.93 -2.13535 -4.64953 -19.672 1363.21 it0 1k4c 3074 Easy
    9 6.213 27.452 -10.763576 1411.04 24.4844 -2.1838 -14.0344 890.062 it0 1k4c 6111 Easy

and for a trimeric one

    complex i-RMSD l-RMSD l-RMSD-sd HS Eair Evdw Eelec Edesolv BSA stage pdb rank difficulty
    1 46.268 75.6035 1.8335 1790.72757 102074 718.818 -13.7373 7.13569 1901.6 it0 4fz0 8305 Intermediate
    2 33.500 51.8455 1.1275 401.32159 38540.4 248.141 -9.81252 -51.7452 3421.41 it0 4fz0 393 Intermediate
    3 36.232 58.523 17.595 673.51548 50304 375.559 53.4633 5.52919 3188.66 it0 4fz0 3527 Intermediate
    4 47.686 62.758 6.711 636.45832 53861.1 356.872 -10.6338 31.3379 3474.35 it0 4fz0 3109 Intermediate
    5 53.633 89.111 14.586 1552.34581 117606 412.29 -7.01079 6.8906 1238.89 it0 4fz0 7804 Intermediate
    6 18.942 41.166 0.153 663.585429 53746.8 388.635 12.9086 -0.648221 2777.93 it0 4fz0 3401 Intermediate
    7 38.178 61.3475 1.0615 579.84936 46221.8 172.666 42.2576 15.0654 3910.15 it0 4fz0 2494 Intermediate
    8 34.095 62.5555 16.3635 833.98877 66967.1 250.811 5.05806 34.3765 3304.69 it0 4fz0 5141 Intermediate
    9 49.997 91.598 8.271 2210.46723 89319.8 419.985 -7.88672 -13.6433 2024.06 it0 4fz0 8940 Intermediate

### HADDOCK terms

The values that have been calculated with HADDOCK are the Eair, Evdw, Eelec, Edesolv and BSA terms.
HS stands for HADDOCK-SCORE which is a weighted sum of the pervious terms. Eair refers to the
restraint energy, Evdw and Eelec are the non-bonded terms and are calculated with the OPLS force
field, Edesolv stands for desolvation energy and BSA stands for Buried Surface Area.

The terms that are related to HADDOCK but are not included in the scoring of complexes are stage,
and rank. Stage refers to one of three stages of a HADDOCK run:

* it0 - Rigid-body energy minisation
* it1 - Simulated annealing in torsional space
* itw - Flexible refinement in explicit solvent

Rank refers to the ranking of that particular structure by the HADDOCK scoring function.

### RMSD terms

The two RMSD terms are i-RMSD and l-RMSD. They stand for interface and ligand RMSD respectively. For
the I-RMSD calculation we have calculated the RMSD of the backbone atoms of the interface. The interface
is defined as the set of residues whose atoms are within 10Å of any atom of a partner. For the L-RMSD
calculation we have superimposed the receptors (defined as the biggest partner) using all backbone atoms
and calculated the displacement of the backbone atoms of the ligand (defined as the smallest partner).
For the trimers, we have calculated the displacement of both partners (with regards to the first) and
report their mean and standard deviation.

### ProFit Instructions

Those interested in reproducing the RMSD calculations can do so with the following ProFit commands

Using complex `1m56` as an example.

For the I-RMSD calculations:

`profit -f AB.izone 1m56_bound.pdb 1m56_unbound.pdb`

If ProFit is correctly installed on your system you should see the following output:

                  PPPPP                FFFFFF ii   tt
                  PP  PP               FF          tt
                  PP  PP rrrrr   oooo  FF     ii  ttttt
                  PPPPP  rr  rr oo  oo FFFF   ii   tt
                  PP     rr     oo  oo FF     ii   tt
                  PP     rr     oo  oo FF     ii   tt
                  PP     rr      oooo  FF      ii   ttt

                      Protein Least Squares Fitting

                               Version 3.1

      Copyright (c) Dr. Andrew C.R. Martin, SciTech Software 1992-2009
              Copyright (c) Dr. Craig T. Porter, UCL 2008-2009

     Reading reference structure...
     Reading mobile structure...

     Starting script: 'AB.izone'
     Fitting structures...
     RMS: 0.572
     Finished script: 'AB.izone'

The line starting with RMS contains the I-RMSD value.

For the L-RMSD calculations:

`profit -f AB.lzone 1m56_bound.pdb 1m56_unbound.pdb`

Which should print:

                  PPPPP                FFFFFF ii   tt
                  PP  PP               FF          tt
                  PP  PP rrrrr   oooo  FF     ii  ttttt
                  PPPPP  rr  rr oo  oo FFFF   ii   tt
                  PP     rr     oo  oo FF     ii   tt
                  PP     rr     oo  oo FF     ii   tt
                  PP     rr      oooo  FF      ii   ttt

                      Protein Least Squares Fitting

                               Version 3.1

      Copyright (c) Dr. Andrew C.R. Martin, SciTech Software 1992-2009
              Copyright (c) Dr. Craig T. Porter, UCL 2008-2009

     Reading reference structure...
     Reading mobile structure...

     Starting script: 'AB.lzone'
     Fitting structures...
     RMS: 0.560
     RMS: 0.000
     Finished script: 'AB.lzone'

In this case two RMSD values are calculated and printed to the terminal. The first is the RMSD
of the backbone atoms of the receptor (in this case chain A). The second is the L-RMSD value.
In this case it is 0 as chain B has been extracted from the bound complex for case `1m56`.

## Reference

When using this membrane complexes benchmark in any publication, please cite:

* P.I. Koukos, I. Faro, C.W. van Noort and A.M.J.J. Bonvin. [A membrane protein complex docking benchmark](https://doi.org/10.1016/j.jmb.2018.11.005). _J. Mol. Biol._ Advanced Online Publication (2018).
",2022-08-12
https://github.com/haddocking/molmod-education,# molmod-education,2022-08-12
https://github.com/haddocking/MolMod-scripts,"# MolMod-scripts

This is the script for the exercise sessions of the bachelor course Molecular Modelling as taught at the Utrecht University.

# Scripts

* `EM_and_MD.py`: Intended to be send to the students
* `EM_and_MD_answers_wc2.py`: Gives the solution for the Steepest Descent method, can be send to the students after exercise session 2.
* `EM_and_MD_answers_wc3.py`: Gives the solution for both the Steepest Descent and Verlet methods, can be send to the students after exercise session 3.

# Intended use in the MolMod course

* Exercise session 2: Download the script and implement the Steepest Descent method at line 109.
* Exercise session 3: Check answers from last session and implement the Verlet method at line 152.
* Exercise session 4:
    * Check answers from last session.
    * Exercise 1: Toggle the boolean at line 58 from `True` to `False`.
    * Exercise 2: Toggle the boolean at line 59 from `False` to `True`.
    * Exercise 3: Uncomment the 'straightforward' implementation in function `LJ2` line 269 and change the powers.
",2022-08-12
https://github.com/haddocking/pdb-tools,"# pdb-tools

[![PyPI version](https://badge.fury.io/py/pdb-tools.svg)](https://pypi.python.org/pypi/pdb-tools)
[![PyPi Downloads](https://img.shields.io/pypi/dm/pdb-tools.svg)](https://pypi.python.org/pypi/pdb-tools)
[![tests](https://github.com/haddocking/pdb-tools/workflows/ci/badge.svg?branch=master)](https://github.com/haddocking/pdb-tools/actions?workflow=ci)
[![DOI](https://zenodo.org/badge/27217350.svg)](https://doi.org/10.12688/f1000research.17456.1)

A swiss army knife for manipulating and editing PDB files.


## Looking for the _other_ pdb-tools?
The Harms lab maintains a set of tools also called `pdbtools`, which perform a
slightly different set of functions. You can find them [here](https://github.com/harmslab/pdbtools).


## About
Manipulating PDB files is often painful. Extracting a particular chain or set of
residues, renumbering residues, splitting or merging models and chains, or just
ensuring the file is conforming to the PDB specifications are examples of tasks
that can be done using any decent parsing library or graphical interface. These,
however, almost always require 1) scripting knowledge, 2) time, and 3) installing
one or more programs.

`pdb-tools` were designed to be a swiss-knife for the PDB format. They have no
external dependencies, besides obviously the [Python programming language](http://www.python.org).
They are the descendant of a set of old FORTRAN77 programs that had the 
particular advantage of working with streams, i.e. the output of one script 
could be piped into another. Since FORTRAN77 is a pain too, I rewrote them in
Python and added a few more utilities. 

The philosophy of the scripts is simple: one script, one task. If you want to 
do two things, pipe the scripts together. Requests for new scripts will be taken
into consideration - use the Issues button or write them yourself and create a
Pull Request.


## Installation Instructions
`pdb-tools` are available on PyPi and can be installed though `pip`. This is the
recommended way as it makes updating/uninstalling rather simple:
```bash
pip install pdb-tools
```

Because we use semantic versioning in the development of `pdb-tools`, every bugfix
or new feature results in a new version of the software that is automatically published
on PyPI. As such, there is no difference between the code on github and the latest version
you can install with `pip`. To update your installation to the latest version of the code
run:
```bash
pip install --upgrade pdb-tools
```

## What can I do with them?
The names of the tools should be self-explanatory. Their command-line interface
is also pretty consistent. Therefore, here is a couple of examples to get you
started:

* Downloading a structure
   ```bash
   pdb_fetch 1brs > 1brs.pdb  # 6 chains
   pdb_fetch -biounit 1brs > 1brs.pdb  # 2 chains
   ```

* Renumbering a structure
   ```bash
   pdb_reres -1 1ctf.pdb > 1ctf_renumbered.pdb
   ```

* Selecting chain(s)
   ```bash
   pdb_selchain -A 1brs.pdb > 1brs_A.pdb
   pdb_selchain -A,D 1brs.pdb > 1brs_AD.pdb
   ```

* Deleting hydrogens
   ```bash
   pdb_delelem -H 1brs.pdb > 1brs_noH.pdb
   ```

* Selecting backbone atoms
   ```bash
   pdb_selatom -CA,C,N,O 1brs.pdb > 1brs_bb.pdb
   ```

* Selecting chains, removing HETATM, and producing a valid PDB file
  ```bash
  pdb_selchain -A,D 1brs.pdb | pdb_delhetatm | pdb_tidy > 1brs_AD_noHET.pdb
  ```

*Note: On Windows the tools will have the `.exe` extension.*


## What _can't_ I do with them?
Operations that involve coordinates or numerical calculations are usually not in
the scope of `pdb-tools`. Use a proper library for that, it will be much faster
and scalable. Also, although we provide mmCIF<->PDB converters, we do not support
large mmCIF files with more than 99999 atoms, or 9999 residues in a single chain.
Our tools will complain if you try using them on such a molecule. 


## Citation
We finally decided to write up a small publication describing the tools. If you
used them in a project that is going to be published, please cite us:

```
Rodrigues JPGLM, Teixeira JMC, Trellet M and Bonvin AMJJ.
pdb-tools: a swiss army knife for molecular structures. 
F1000Research 2018, 7:1961 (https://doi.org/10.12688/f1000research.17456.1) 
```

If you use a reference manager that supports BibTex, use this record:
```
@Article{ 10.12688/f1000research.17456.1,
AUTHOR = { Rodrigues, JPGLM and Teixeira, JMC and Trellet, M and Bonvin, AMJJ},
TITLE = {pdb-tools: a swiss army knife for molecular structures [version 1; peer review: 2 approved]
},
JOURNAL = {F1000Research},
VOLUME = {7},
YEAR = {2018},
NUMBER = {1961},
DOI = {10.12688/f1000research.17456.1}
}
```

## Requirements
`pdb-tools` should run on Python 2.7+ and Python 3.x. We test on Python 2.7, 3.6,
and 3.7. There are no dependencies.


## Installing from Source
Download the zip archive or clone the repository with git. We recommend the `git`
approach since it makes updating the tools extremely simple.

```bash
# To download
git clone https://github.com/haddocking/pdb-tools
cd pdb-tools

# To update
git pull origin master

# To install
python setup.py install
```

## Contributing
If you want to contribute to the development of `pdb-tools`, provide a bug fix,
or a new tools, read our `CONTRIBUTING` instructions [here](https://github.com/haddocking/pdb-tools/blob/master/CONTRIBUTING.md).

## License
`pdb-tools` are open-source and licensed under the Apache License, version 2.0.
For details, see the LICENSE file.
",2022-08-12
https://github.com/haddocking/physiological_homodimers_benchmark,"# Physiological protein homodimers Benchmark

## Structure

* `pdbs`: folder containing for a given version (i.e. `v2`) all the PDB structures ready.
* `prediction`: prediction files for each of the methods benchmarked.

### Files

* `pdbs/v2/complex.list`: list of all complexes in the benchmark
* `pdbs/v2/physiological.list`: all complexes classified as physiological
* `pdbs/v2/nonphysiological.list`: all complexes classified as non-physiological
* `pdbs/v2/error.list`: Errors found on PDB structures which have been manually curated.
* `pdbs/v2/pdb_chains.list`: PDB IDs and chains IDs for each complex.

## Methods

### PRODIGY-CRYSTAL


The classification into physiological assemblies and crystal contacts is based on the contact-based PRODIGY-CRYSTAL method, described online [here](https://bianca.science.uu.nl/prodigy/method#heading_c_three) and published in:

* K. Elez, A.M.J.J. Bonvin* and A. Vangone. 
[Distinguishing crystallographic from biological interfaces in protein complexes: Role of intermolecular contacts and energetics for classification](https://doi.org/10.1186/s12859-018-2414-9). BMC Bioinformatics, 19 (Suppl 15), 438 (2018).

PRODIGY-CRYSTAL is available as a [web server](https://bianca.science.uu.nl/prodigy/):

* B. Jiménez-García, K. Elez, P.I. Koukos, A.M.J.J. Bonvin and A. Vangone. 
[PRODIGY-crystal: a web-tool for classification of biological interfaces in protein complexes](https://doi.org/10.1093/bioinformatics/btz437). Bioinformatics, 35, 4821–4823 (2019).

and the code is available from our [GitHub repository](https://github.com/haddocking/prodigy-cryst).


#### Performance on this data set:

```
Size of complex list: 1677
Size of Physiological list: 836
Size of Non Physiological list: 841
Predicted as BIO and Physiological: 539
Predicted as BIO and Non-Physiological: 409
Predicted as CRYSTAL and Physiological: 297
Predicted as CRYSTAL and Non-Physiological: 432

Success rate: 57.9%
```

#### Performance on this data set excluding overlapping with the MANY dataset:

(See `prediction/prodigy-crystal/overlap.list`)

```
Size of complex list: 1546
Size of Physiological list: 836
Size of Non Physiological list: 841
Predicted as BIO and Physiological: 457
Predicted as BIO and Non-Physiological: 401
Predicted as CRYSTAL and Physiological: 278
Predicted as CRYSTAL and Non-Physiological: 410

Success rate: 56.08%
```

### Deeprank 

The [Deeprank](https://github.com/DeepRank/deeprank) classification is based on a 3D Convolution Neural Network (CNN) that has been pre-trained on the [MANY dataset](https://pubmed.ncbi.nlm.nih.gov/25326082/), which consists of 2828 biological interfaces and 2911 crystal ones, only using PSSM features. The MANY dataset was devided into a training (80%) and a validation set (20%), while maintaining the balance between positive and negative data. The training dataset was augmented by randomly rotating each complex 30 times.

29 complexes have been discarded from the dataset classification due to :
- the lack of pssm matrix 40 complexes)
- Others - See prediction/deeprank/data/README_*_dataset (15 complexes)

#### Performance on this data set:

```
Size of complex list: 1677
Size of complex list processed : 1622
Size of Physiological list: 836
Size of Non Physiological list: 841
Predicted as BIO and Physiological: 642
Predicted as BIO and Non-Physiological: 222
Predicted as CRYSTAL and Physiological: 187
Predicted as CRYSTAL and Non-Physiological: 571

Success rate (considering unprocessed data): 72.332%
Success rate (omitting unprocessed data): 74.784%
```

#### Performance on this data set excluding overlapping with the MANY dataset:

```
Size of complex list: 1551
Size of complex list processed : 1497
Size of Physiological list: 735
Size of Non Physiological list: 816
Predicted as BIO and Physiological: 551
Predicted as BIO and Non-Physiological: 220
Predicted as CRYSTAL and Physiological: 178
Predicted as CRYSTAL and Non-Physiological: 548

Success rate (considering unprocessed data): 70.858%
Success rate (omitting unprocessed data): 73.413%
```

(See `prediction/deeprank/deeprank_prediction_v2.list`)
",2022-08-12
https://github.com/haddocking/powerfit,"# PowerFit

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1037228.svg)](https://doi.org/10.5281/zenodo.1037228)

## About PowerFit

PowerFit is a Python package and simple command-line program to automatically
fit high-resolution atomic structures in cryo-EM densities. To this end it
performs a full-exhaustive 6-dimensional cross-correlation search between the
atomic structure and the density. It takes as input an atomic structure in
PDB-format and a cryo-EM density with its resolution; and outputs positions and
rotations of the atomic structure corresponding to high correlation values.
PowerFit uses the local cross-correlation function as its base score. The
score can optionally be enhanced by a Laplace pre-filter and/or a core-weighted
version to minimize overlapping densities from neighboring subunits. It can
further be hardware-accelerated by leveraging multi-core CPU machines out of
the box or by GPU via the OpenCL framework. PowerFit is Free Software and has
been succesfully installed and used on Linux and MacOSX machines.


## Requirements

Minimal requirements for the CPU version:

* Python2.7
* NumPy 1.8+
* SciPy
* GCC (or another C-compiler)

Optional requirement for faster CPU version:

* FFTW3
* pyFFTW 0.10+

To offload computations to the GPU the following is also required

* OpenCL1.1+
* pyopencl
* clFFT
* gpyfft

Recommended for installation

* git
* pip


## Installation

If you already have fulfilled the requirements, the installation should be as
easy as opening up a shell and typing

    git clone https://github.com/haddocking/powerfit.git
    cd powerfit
    sudo python setup.py install

If you are starting from a clean system, follow the instructions for your
particular operating system as described below, they should get you up and
running in no time.


### Linux 

Linux systems usually already include a Python2.7 distribution. First make
sure the Python header files, NumPy, SciPy, and *git* are available by
opening up a terminal and typing for Debian and Ubuntu systems

    sudo apt-get install python-dev python-numpy python-scipy git

If you are working on Fedora, this should be replaced by 

    sudo yum install python-devel numpy scipy git

Sit back and wait till the compilation and installation is finished. Your
system is now prepared, follow the general instructions above to install
**PowerFit**.


### MacOSX

First install [*git*](https://git-scm.com/download) by following the
instructions on their website, or using a package manager such as *brew*

    brew install git

Next install [*pip*](https://pip.pypa.io/en/latest/installing.html), the
Python package manager, by following the installation instructions on the
website or open a terminal and type 

    sudo easy_install pip

Next, install NumPy and SciPy by typing

    sudo pip install numpy scipy

Wait for the installation to finish. Follow the general instructions above to
install **PowerFit**.

Installing pyFFTW for faster CPU version can be done as follows using *brew*

    brew install fftw
    sudo pip install pyfftw


### Windows

First install *git* for Windows, as it comes with a handy bash shell. Go to
[git-scm](https://git-scm.com/download/), download *git* and install it. Next,
install a Python distribution with NumPy and Scipy included such as
[Anaconda](http://continuum.io/downloads). After installation, open up the
bash shell shipped with *git* and follow the general instructions written
above.


## Usage

After installing PowerFit the command line tool *powerfit* should be at your
disposal. The general pattern to invoke *powerfit* is

    powerfit <map> <resolution> <pdb>

where `<map>` is a density map in CCP4 or MRC-format, `<resolution>`  is the
resolution of the map in &aring;ngstrom, and `<pdb>` is an atomic model in the
PDB-format. This performs a 10&deg; rotational search using the local
cross-correlation score on a single CPU-core. During the search, *powerfit*
will update you about the progress of the search if you are using it
interactively in the shell.


### Options

First, to see all options and their descriptions type

    powerfit --help

The information should explain all options decently. In addtion, here are some
examples for common operations.

To perform a search with an approximate 24&deg; rotational sampling interval

    powerfit <map> <resolution> <pdb> -a 24

To use multiple CPU cores with laplace pre-filter and 5&deg; rotational
interval

    powerfit <map> <resolution> <pdb> -p 4 -l -a 5

To off-load computations to the GPU and use the core-weighted scoring function
and write out the top 15 solutions

    powerfit <map> <resolution> <pdb> -g -cw -n 15

Note that all options can be combined except for the `-g` and `-p` flag:
calculations are either performed on the CPU or GPU.


### Output

When the search is finished, several output files are created

* *fit_N.pdb*: the top *N* best fits.
* *solutions.out*: all the non-redundant solutions found, ordered by their
correlation score. The first column shows the rank, column 2 the correlation
score, column 3 and 4 the Fisher z-score and the number of standard deviations
(see N. Volkmann 2009, and Van Zundert and Bonvin 2016); column 5 to 7 are the
x, y and z coordinate of the center of the chain; column 8 to 17 are the
rotation matrix values.
* *lcc.mrc*: a cross-correlation map, showing at each grid position the highest
correlation score found during the rotational search.
* *powerfit.log*: a log file, including the input parameters with date and
timing information.


## Creating an image-pyramid

The use of multi-scale image pyramids can signicantly increase the speed of
fitting. PowerFit comes with a script to quickly build a pyramid called
`image-pyramid`. The calling signature of the script is

    image-pyramid <map> <resolution> <target-resolutions ...>

where `<map` is the original cryo-EM data, `<resolution` is the original
resolution, and `<target-resolutions>` is a sequence of resolutions for the
resulting maps. The following example will create an image-pyramid with
resolutions of 12, 13 and 20 angstrom

    image-pyramid EMD-1884/1884.map 9.8 12 13 20

To see the other options type

    image-pyramid --help


## Licensing

If this software was useful to your research, please cite us

**G.C.P. van Zundert and A.M.J.J. Bonvin**. 
Fast and sensitive rigid-body fitting into cryo-EM density maps with PowerFit.
*AIMS Biophysics* 2, 73-87 (2015).


For the use of image-pyramids and reliability measures for fitting, please cite

**G.C.P van Zundert and A.M.J.J. Bonvin**. 
Defining the limits and reliability of rigid-body fitting in cryo-EM maps using
multi-scale image pyramids. 
*J. Struct. Biol.* 195, 252-258 (2016).

Apache License Version 2.0

The elements.py module is licensed under MIT License (see header).
Copyright (c) 2005-2015, Christoph Gohlke



## Tested platforms

| Operating System| CPU single | CPU multi | GPU |
| --------------- | ---------- | --------- | --- |
|Linux            | Yes        | Yes       | Yes |
|MacOSX           | Yes        | Yes       | Yes |
|Windows          | Yes        | Fail      | No  |

The GPU version has been tested on:
* NVIDIA GeForce GTX 680 and AMD Radeon HD 7730M for Linux
* NVIDIA GeForce GTX 775M for MacOSX 10.10
",2022-08-12
https://github.com/haddocking/powerfit-tutorial,"About
=====

Repository holding all the data to follow the PowerFit tutorial, provided by
the [Bonvin Lab][link-bonvinlab].


Requirements
============

* [PowerFit][link-powerfit]
* [UCSF Chimera][link-chimera]
* C++-compiler

Compile the contact-chainID.cpp file as follows

    g++ contact-chainID.cpp -o contact-chainID


Licence
=======

<a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/"">
    <img alt=""Creative Commons License"" style=""border-width:0""
         src=""https://i.creativecommons.org/l/by/4.0/88x31.png"" />
</a>
<br />

This work is licensed under a 
<a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/"">
    Creative Commons Attribution 4.0 International License
</a>.


Citation
========

This tutorial is citable.

For this used the following DOI: [![DOI](https://zenodo.org/badge/21589/haddocking/powerfit-tutorial.svg)](https://zenodo.org/badge/latestdoi/21589/haddocking/powerfit-tutorial)

Copyright © Gydo van Zundert, Alexandre Bonvin.


[link-bonvinlab]: http://www.bonvinlab.org/education/powerfit-tutorial ""Bonvin Lab""
[link-powerfit]: https://github.com/haddocking/powerfit ""PowerFit""
[link-chimera]: https://www.cgl.ucsf.edu/chimera/ ""UCSF Chimera""
",2022-08-12
https://github.com/haddocking/prodigy,"# PRODIGY / Binding Affinity Prediction

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1193244.svg)](https://doi.org/10.5281/zenodo.1193244)

Collection of scripts to predict binding affinity values
for protein-protein complexes from atomic structures.

The online version of PRODIGY predictor can be found here:
* [PRODIGY](https://wenmr.science.uu.nl/prodigy/)

Details of the binding affinity predictor implemented in PRODIGY can be found here:
* [Contacts-based model](http://www.ncbi.nlm.nih.gov/pubmed/26193119)

# Requirements

Python 3

# Installation

```
git clone http://github.com/haddocking/prodigy
cd prodigy
pip install .

# Have fun!
```

# Usage

```bash
prodigy <pdb file> [--selection <chain1><chain2>]
```

To get a list of all the possible options.
```bash
prodigy --help 
```

# Information about dependencies
The scripts rely on [Biopython](www.biopython.org) to validate the PDB structures and calculate
interatomic distances. [freesasa](https://github.com/mittinatten/freesasa), with the parameter
set used in NACCESS ([Chothia, 1976](http://www.ncbi.nlm.nih.gov/pubmed/994183)), is also
required for calculating the buried surface area.

**DISCLAIMER**: given the different software to calculate solvent accessiblity, predicted
values might differ (very slightly) from those published in the reference implementations.
The correlation of the actual atomic accessibilities is over 0.99, so we expect these
differences to be very minor.

To install and use the scripts, just clone the git repository or download the tarball zip
archive. Make sure `freesasa` and Biopython are accessible to the Python scripts
through the appropriate environment variables ($PYTHONPATH).

# License
These utilities are open-source and licensed under the Apache License 2.0. For more information
read the LICENSE file.

# Citing us
If our predictive model or any scripts are useful to you, consider citing them in your
publications:

**Xue L, Rodrigues J, Kastritis P, Bonvin A.M.J.J, Vangone A.**: PRODIGY: a web server for predicting the binding affinity of protein-protein complexes. *Bioinformatics* (2016) ([link](http://bioinformatics.oxfordjournals.org/content/early/2016/08/27/bioinformatics.btw514))

**Anna Vangone and Alexandre M.J.J. Bonvin**: Contacts-based prediction of binding affinity in protein-protein complexes. *eLife*, e07454 (2015) ([link](http://www.ncbi.nlm.nih.gov/pubmed/26193119))

**Panagiotis L. Kastritis , João P.G.L.M. Rodrigues, Gert E. Folkers, Rolf Boelens, Alexandre M.J.J. Bonvin**: Proteins Feel More Than They See: Fine-Tuning of Binding Affinity by Properties of the Non-Interacting Surface. *Journal of Molecular Biology*, 14, 2632–2652 (2014). ([link](http://www.ncbi.nlm.nih.gov/pubmed/24768922))

# Contact
For questions about PRODIGY usage, please contact the team at: prodigy.bonvinlab@gmail.com
",2022-08-12
https://github.com/haddocking/prodigy-cryst,"[![unittests py36](https://github.com/haddocking/prodigy-cryst/actions/workflows/py36.yml/badge.svg)](https://github.com/haddocking/prodigy-cryst/actions/workflows/py36.yml)
[![codecov](https://codecov.io/gh/haddocking/prodigy-cryst/branch/master/graph/badge.svg?token=KCGiAqKRnu)](https://codecov.io/gh/haddocking/prodigy-cryst)

# Interface Classifier  
Collection of scripts to predict whether an interface in a protein-protein 
complex is biological or crystallographic from its atomic coordinates.

## Quick & Dirty Installation
```bash
git clone http://github.com/biopython/biopython.git
cd biopython
sudo python setup.py install # Alternatively, install locally but fix $PYTHONPATH

wget https://github.com/mittinatten/freesasa/releases/download/1.0/freesasa-1.0.tar.gz
tar -xzvf freesasa-1.0.tar.gz
cd freesasa-1.0
./configure && make && make install

pip3 install scikit-learn==0.22

git clone http://github.com/haddocking/interface-classifier

# Edit the config.py to setup the paths to the freesasa binary and radii files

# Have fun!
```

## Usage

```bash
python interface_classifier.py <pdb file> [--selection <chain1><chain2>]
```

Type --help to get a list of all the possible options of the script.

## Dependencies  
* The scripts rely on [Biopython](www.biopython.org) to validate the PDB structures and calculate interatomic distances.
* [freesasa](https://github.com/mittinatten/freesasa), with the parameter set used in NACCESS ([Chothia,1976](http://www.ncbi.nlm.nih.gov/pubmed/994183)), is also required for calculating the buried surface area. Both 2.x and 1.x version series are supported.
* [scikit-learn](https://github.com/scikit-learn/scikit-learn) for Python 3 is necessary to load and use the classifier.

To install and use the scripts, just clone the git repository or download the tarball zip
archive. Make sure `freesasa`, Biopython and scikit-learn are accessible to the Python scripts
through the appropriate environment variables ($PYTHONPATH).

## License  
These utilities are open-source and licensed under the Apache License 2.0. For more information
read the LICENSE file.

",2022-08-12
https://github.com/haddocking/prodigy-lig,"[![DOI](https://zenodo.org/badge/118625187.svg)](https://zenodo.org/badge/latestdoi/118625187)

Table of Contents
=================

   * [Introduction](#introduction)
   * [Installation](#installation)
   * [Quick start](#quick-start)
   * [Usage](#usage)
      * [Command line arguments](#command-line-arguments)
         * [Required arguments](#required-arguments)
            * [Input file](#input-file)
            * [Chain specification](#chain-specification)
         * [Optional arguments](#optional-arguments)
            * [Electrostatics Energy](#electrostatics-energy)
            * [Distance cutoff](#distance-cutoff)
            * [Processed output file](#processed-output-file)
            * [Verbosity](#verbosity)
            * [Version](#version)
      * [I don't like the command line.](#i-dont-like-the-command-line)
   * [Licensing](#licensing)
   * [Citing us](#citing-us)
   * [Contact](#contact)

# Introduction

PRODIGY-LIG (PROtein binDIng enerGY prediction - LIGands) is a structure-based
method for the prediction of binding affinity in protein-small ligand (such as
drugs or metabolites) complexes.

Here is reported a collection of scripts what you can download and use to predict
the binding addinity of your complex.

If you are not familiar with scripting, the online version 
of PRODIGY-LIG can be found here:
* [PRODIGY-LIG](https://nestor.science.uu.nl/prodigy/lig)

# Installation

prodigy_lig is [python 3](https://www.python.org/) code and it depends on
[biopython](http://biopython.org/).

From the command line terminal run the following command:

```sh
pip install git+https://github.com/haddocking/prodigy-lig
```

If you are installing to a system-wide location (e.g. `/usr/bin`) you might
need elevated privileges.

`prodigy_lig` should now be accessible from the command line.

# Quick start

You can fetch a structure from the PDB and start using the code immediately.
The following two lines should do the trick:

```
wget https://files.rcsb.org/download/1A0Q.pdb
prodigy_lig.py -c H,L H:HEP -i 1A0Q.pdb
```

If all goes well this will produce the following two lines of output.

```
Job name        DGprediction (low refinement) (Kcal/mol)
1a0q    -8.49
```

You can read more about the usage of prodigy_lig in the next section

# Usage

Running `prodigy_lig -h` will list some basic information about the code.

```
usage: prodigy_lig [-h] -c CHAINS CHAINS -i INPUT_FILE [-e ELECTROSTATICS]
                   [-d DISTANCE_CUTOFF] [-o] [-v] [-V]

Calculate the Binding Affinity score using the PRODIGY-LIG model

prodigy_lig dependes on biopython for the structure manipulations
and only requires a single structure file (in mmCIF or PDB format)
as input.

prodigy_lig is licensed under the Apache License 2.0 included in
the LICENSE file of this repository or at the following URL

https://github.com/haddocking/prodigy-lig/blob/master/LICENSE

If you use prodigy_lig in your research please cite the following
papers:

1. to be submitted
2. https://doi.org/10.1007/s10822-017-0049-y

optional arguments:
  -h, --help            show this help message and exit
  -c CHAINS CHAINS, --chains CHAINS CHAINS
                        Which chains to use. Expects two sets of arguments.
                        The first set refers to the protein selection and
                        multiple chains can be specified by separating the
                        chain identifiers with commas. The second set refers
                        to the ligand and requires one chain and the residue
                        identifier of the ligand. A typical use case could be
                        the following: prodigy_lig.py -c A,B A:LIG
  -i INPUT_FILE, --input_file INPUT_FILE
                        This is the PDB/mmcif file for which the score will be
                        calculated.
  -e ELECTROSTATICS, --electrostatics ELECTROSTATICS
                        This is the electrostatics energy as calculated during
                        the water refinement stage of HADDOCK.
  -d DISTANCE_CUTOFF, --distance_cutoff DISTANCE_CUTOFF
                        This is the distance cutoff for the Atomic Contacts
                        (def = 10.5Å).
  -o, --output_file     Store the processed file. The filename will be the
                        name of the input with -processed appended just before
                        the file ending.
  -v, --verbose         Include the calculated contact counts in the output.
  -V, --version         Print the version and exit.

Authors: Panagiotis Koukos, Anna Vangone, Joerg Schaarschmidt
```

For all the examples mentioned below the following two caveats apply:

**prodigy_lig only considers contacts between the ligand and residues of the
protein**. Any cofactors, ions or solvent molecules that might be present,
are not included in the distances that are calculated.

Additionaly, prodigy_lig **only works on single model structures**. That means
that if your structure of interest contains multiple models (e.g. NMR conformers)
only the first model will be used for the calculations.

## Command line arguments

The command line arguments belong to one of two categories; required and optional.
The two required arguments are the input structure file and the specification of
the chain and residue identifiers of the interactors.

### Required arguments

#### Input file

The input structure file can be specified with the `-i` flag (or `--input_file`)
and it should be the path to the input PDB/mmCIF file.

#### Chain specification

The `-c` flag (or `--chains`) must be used to specify the chain identifiers for
the interactors, and for the ligand the residue identifier as well. The first
argument allows to specify the protein chains to be used in the analysis. Multiple
chains can be specified by comma separating them. The second argument needs to 
specify the chain and residue identifier of the ligand of interest separated by 
a colon (`:`).

For the next examples we will be using the structure from the quickstart
[section](#quick-start), 1A0Q.

 If you didn't do so before you can fetch the PDB file with the following line of
 code from the command line.

 ```
 wget https://files.rcsb.org/download/1A0Q.pdb
 ```

Or by simply pointing your browser [here](https://files.rcsb.org/download/1A0Q.pdb)

After examining the file with your favourite molecular viewer you will note that
this is a Fab fragment with a small molecule ligand embedded between the heavy and
light chains. The chain identifiers of the heavy and light chains are H and L, and
the small molecule of interest has the residue identifier `HEP` and is part of chain
H.

The simplest analysis we can do is to include both chains

```
prodigy_lig -c H,L H:HEP -i 1A0Q.pdb
```

which produces the following output.

```
Job name        DGprediction (low refinement) (Kcal/mol)
1a0q    -8.49
```

In this case, atomic distances from the ligand to residues of both chains have
been calculated.

If we wanted to only include the heavy chain (`H`) in the analysis we would use
this command instead.


```
prodigy_lig -c H H:HEP -i 1A0Q.pdb
```

which would produce this output

```
Job name        DGprediction (low refinement) (Kcal/mol)
1a0q    -6.69
```

### Optional arguments

The first two optional arguments will affect your results, whereas the latter ones
will only impact the formatting and content of your output.

#### Electrostatics Energy

In addition to the contact information prodigy_lig can make use of the electrostatic
energy of the interaction as well. This refers to the intermolecular electrostatic
energy as calculated by [HADDOCK](https://wenmr.science.uu.nl/haddock2.4).

If you know this energy because you have refined your complex through HADDOCK then
you can specify this using the `-e` flag (`--electrostatics`). Additionaly, if your
PDB file is coming from HADDOCK, `prodigy_lig` will automatically extract the relevant
information from the input file and make use of it.

#### Distance cutoff

This is the cutoff used when calculating the atomic contacts. By default it has a
value of 10.5 Angstrom, which was identified when training the model. You can modify
this with the `-d` flag (`--distance_cutoff`).

#### Processed output file

If you would like a copy of the structure that was used to calculate the atomic
contacts you can use the `-o` flag (`--output_file`). This might be useful if you
have chosen to exclude some chains from the analysis. The file will be created in
the current working directory and its filename will be `input_name-processed.pdb`.

#### Verbosity

If you specify the `-v` flag (`--verbose`), in addition to the DG values / scores,
the output will include the contact counts and, if defined, electrostatics as well.
Once again using the same example as above.

```
prodigy_lig -c H,L H:HEP -i 1A0Q.pdb -v
```

Will produce the following output

| Job name | DGprediction (low refinement) (Kcal/mol) | CC | CN | CO | CX | NN | NO | NX | OO | OX | XX
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1A0Q | -8.49 | 1838 | 589 | 1010 | 124 | 27 | 175 | 30 | 132 | 24 | 0

#### Version

Finally you can use the `-V` flag (`--version`) to get the version of the code.

## I don't like the command line.

Too bad. Luckily for you, prodigy_lig has also been implemented in a freely accessible
[webserver](https://bianca.science.uu.nl/prodigy/lig).

# Licensing

prodigy_lig is licensed under [Apache 2.0](LICENSE).

# Citing us

If our predictive model or any scripts are useful to you, consider citing them in your
publications:

**Vangone A, Schaarschmidt J, Koukos P, Geng C, Citro N, Trellet M, Xue L, Bonvin A.**: Large-scale
prediction of binding affinity in protein-small ligand complexes: the PRODIGY-LIG web server. *Bioinformatics*,
([link](https://doi.org/10.1093/bioinformatics/bty816))

**Kurkcuoglu Z, Koukos P, Citro N, Trellet M, Rodrigues J, Moreira I, Roel-Touris J,
Melquiond A, Geng C, Schaarschmidt J, Xue L, Vangone A, Bonvin AMJJ.**: Performance
of HADDOCK and a simple contact-based protein-ligand binding affinity predictor in
the D3R Grand Challenge 2. *J Comput Aided Mol Des* 32(1):175-185 (2017). ([link](https://link.springer.com/article/10.1007/s10822-017-0049-y))

# Contact

Issues? Bug reports? Suggestions? Feel free to
[submit](https://github.com/haddocking/prodigy-lig/issues) an issue. Alternatively,
you can reach us at prodigy.bonvinlab@gmail.com.
",2022-08-12
https://github.com/haddocking/Prot-DNABenchmark,"## Prot-DNA Docking Benchmark

[![DOI](https://zenodo.org/badge/263383097.svg)](https://zenodo.org/badge/latestdoi/263383097)

![PROTDNABenchmark](docs/Protein-DNA-benchmark.png)

### Introduction

Biomolecular docking aims at predicting the structure of a complex given the three dimensional structures of its components. Although much improvement has been made in the field of protein-protein docking, in the case of protein-DNA complexes, however, progress lags behind. The scarcity of information for proper identification of interaction surfaces on DNA and its inherent flexibility have hampered the development of effective docking methods. To facilitate the development of effective protein-DNA docking methods a set of well-defined test cases that form a common ground for development, validation and comparison of docking methods is necessary.

We present here a protein-DNA docking benchmark containing 47 unbound-unbound test cases, of which 13 are classified as easy cases, 22 as intermediate cases and 12 as difficult cases. The latter show considerable structural rearrangement upon complex formation. DNA-specific modifications such as flipped out bases and base modifications are included. The benchmark covers all major types of DNA binding proteins according to the classifications of Luscombe et al., Genome Biol 2010. The variety in test cases make this non-redundant benchmark a useful tool for comparison and further development of protein-DNA docking methods.

We developed the protein-DNA benchmark to be of general use to the docking community. We welcome all suggestions aimed at improving or expanding the benchmark.

* * *

### Current version

The current version of the protein-DNA benchmark is 1.2

* * *

### Version history

* 01-01-2008 | The original protein-DNA benchmark version 1.0 release.

* 14-08-2008 | Benchmark version 1.1: Minor updates. The bound to unbound residue mapping file (profit.dat) has been redesigned to make it more flexible in use. The PDB structure file that represents the complex reconstructed from the unbound processed components after superimposition contained more than one instance of the same complex coordinate set. This has been fixed.

* 22-09-2009 | Benchmark version 1.2: Small fixes in residue mapping for two entries. For two entries (1EYU,1RVA) the unbound protein was composed off two distinct subunits. These have now been separated into individual pdb files and all other files have been adjusted accordingly.

* * *

### Citation

When using the protein-DNA docking benchmark please cite using the following reference:

* M. van Dijk and A.M.J.J. Bonvin
[A protein-DNA docking benchmark.](https://doi.org/doi:10.1093/nar/gkn386)
_Nucl. Acids Res._ (2008), *36*, e88, doi: 10.1093/nar/gkn386.

* * *

### Data organization


The benchmark in organized in directory structure, containing one directory per complex corresponding to the PDB ID of the reference structure.

Each directory contains the following files:

* _X.pdb_ and _Y.pdb_ :	The unmodified RCSB PDB files for the structure of the complex and the structure of the unbound protein
								
* _X_bound-prot(x).pdb_ : The processed PDB file of the bound protein (extracted from the complex) 

* _X_bound-DNA.pdb_ : The processed PDB file of the bound DNA (extracted from the complex)
								
* _Y_unbound-prot(x).pdb_ : Structure files of the unbound protein, processed. In case of an NMR ensemble the file is separated into its individual models.
								
* _DNA_unbound.pdb_ : 3DNA generated unbound canonical BDNA representation.

* _X_complex.pdb_ :	PDB file of the bound complex reconstructed from the individual processed bound structures.
								
* _X_ubcomplex.pdb_ : Structure file of the complex reconstructed from the unbound processed components after superimposition using all CA and P atoms.
								
* _interface_fit.dat_ : Data file containing the residue zones for all unbound components that have CA atoms at the interface

* _interface_fit.pdb_ : PDB file of the unbound protein after superimposition on the bound structure using CA atoms.

* _contacts.dat_ : Text file listing all intermolecular contacts in the bound complex.

* _contacts_ub.dat_ : Text file listing all intermolecular contacts in the bound complex (unbound re-numbered).
								
* _alignment.dat_ : Text file of a Needleman-Wunsch sequence alignment between bound and unbound protein sequences.
							
* _profit.dat_ : Text file containing ProFit structure fitting data for automatic structure fitting using ProFit


**Note** that the processed PDB files have been modified to avoid any overlap in residue numbering (e.g. shifting the numnber of the second DNA strand). Also the DNA nomenclature follows a three letter code (ADE, CYT, GUA, THY) compatible with [HADDOCK2.2](https://www.bonvinlab.org/software/haddock2.2).  For use in [HADDOCK2.4](https://www.bonvinlab.org/software/haddock2.2), the base names must be reverted to a single letter code (A,C,G,T).

",2022-08-12
https://github.com/haddocking/protein-peptide-docking_1CZY,"# Protein-peptide-docking_1CZY

This repository contains all necessary information and scripts to do protein-peptide docking in HADDOCK, using the complex 1CZY as an example, which is the Supplementary Information for paper[reference]. For the usage of these files and scripts, please check the content of the paper[reference]. 

*Note that the provided scripts are in principle generic and can be used for other systems as well.*

This repository contains following folders and files:

## pdb_files

  - reference_complex.pdb, the crystal structure of the protein-peptide complex used as reference, which is chain A and D of 1CZY.
  - protein.pdb, the crystal structure of unbound protein used for docking, which is chain C of 1CZZ.
  - peptide_alpha.pdb, the alpha-helix conformation of peptide.
  - peptide_polypro.pdb, the polyproline-II conformation of peptide.
  - peptide_extended.pdb, the extended conformation of peptide.
  - protein_active_residues.dat, the list of active residues of protein.
  - peptide_passive_residues.dat, the list of passive residues of peptide.
  - protein_histidine_states.dat, the list of charged states of Histidines in protein.
  - structures.list, the list of 33 conformations or structures of peptide used for docking.

  ### MD_conformations
    - md_1.pdb ... md_30.pdb, the 30 structures of peptide from MD simulations.


## MD-Parameter_Files

  - vacuum.mdp, MD parameters for energy minimization in vacuum.
  - ions.mdp, MD parameters for energy minimization in the presence of solvent and ions.
  - nvt.mdp, MD parameters for energy minimization in constant volume conditions.
  - npt.mdp, MD parameters for energy minimization in constant pressure conditions.
  - unrestrained.mdp, MD parameters for energy minimization without any restraints.
  - production.mdp, MD parameters for final production MD simulation.


## docking
  - restraints.tbl, the AIRs file.
  - new.html, the file to start a new HADDOCK project.


## scripts-PyMOL
  -build_seq.py, PyMOL script to build protein structure from sequence.


## scripts-MD
  - automd.sh, bash script to prepare files and perform MD simulations using GROMACS.
  - dpca.sh, bash script to do dihedral PCA using GROMACS.


## scripts-HADDOCK
  - molprobity.py, python script to define charge states of histidines.


# How to Cite
C. Geng, S. Narasimhan, João P.G.L.M. Rodrigues and Alexandre M.J.J. Bonvin, Information-driven, ensemble flexible peptide docking using HADDOCK, Modeling of peptide-protein interactions, Methods in Molecular Biology series (Springer Press), 2016

",2022-08-12
https://github.com/haddocking/refine-EM-complexes,"## refine-EM-complexes

Repository containing the data set, analysis results and scripts associated with our manuscript on:

_Interface Refinement of Low-to-Medium Resolution Cryo-EM Complexes using HADDOCK2.4_

To be published.

***

### Content

* ***RAW_DATA*** contains CSV files used for the analysis of the protocols, generated from the top 4 models.  
* ***analysis*** contains a description of the used analysis scripts and the jupiter notebooks used to analyse the obtained data.  
* ***dataset*** contains the reference models which are used as dataset.  
* ***pymol_scripts*** contains the pymolscripts used to generate the clash figures for the manuscript.  
* ***top4_structures*** contains the top 4 models from each refinement protocol.
",2022-08-12
https://github.com/haddocking/samplex,"SAMPLEX
=======

## Smooth Automated Mapping of Proteins from Listed EXtreme (SAMPLEX) ###

## Authors: Mickael Krzeminski 

## Version: 1.0, August 2009

## Reference:

When using SAMPLEX cite:

* M. Krzeminski, K. Loth, R. Boelens and A.M.J.J. Bonvin SAMPLEX: Automatic mapping of perturbed and unperturbed regions of proteins and complexes. BMC Bioinformatics, 11, 51 (2010).


The activity of proteins within the cell is characterized by their motions, flexibility, interactions or even the particularly intriguing case of partially unfolded states. In the last two cases, a part of the protein is affected either by binding or unfolding and the detection of the respective perturbed and unperturbed region(s) is a fundamental part of the structural characterization of these states. This can be achieved by comparing experimental data of the same protein in two different states (bound/unbound, folded/unfolded). For instance, measurements of chemical shift perturbations (CSPs) from NMR 1H-15N HSQC experiments gives an excellent opportunity to discriminate both moieties.


SAMPLEX is an innovative, automatic and unbiased method to distinguish perturbed and unperturbed regions in a protein existing in two distinct states (folded/partially unfolded, bound/unbound). The SAMPLEX program takes as input a set of data and the corresponding three-dimensional structure and returns the confidence for each residue to be in a perturbed or unperturbed state. Its performance is demonstrated for different applications including the prediction of disordered regions in partially unfolded proteins and of interacting regions in protein complexes.


The method is not restricted to NMR data, but is generic and can be applied to a wide variety of information. It requires as input a 3D structure of the protein, and a data table containing residue numbers with their associated property to be classified.



##Usage:##
```
	python samplex_1.0.py [-ng -f save.samp | -f save.samp]
```


Requirements
------------

   SAMPLEX only requires:
		1. A structure file
		2. A data file

An example is provided with all distribution. Please, always test it the first time you
use SAMPLEX.


	1. Structure file

   As the algorithm that governs SAMPLEX is based on the three dimension structure of
the bio-molecule, the program first need the structure file, which specifies the ensemble
of files that contain the atomic coordinates of the molecule. In case where there is only
one file, please DO NOT specify this pdb file, but write in the structure file the location
of this single file. Of note, the ensemble of specified files must contain the same atoms
(NMR ensemble for instance).


	2. Data file

   The data file contains the perturbation information that SAMPLEX will treat.
The perturbation data file must look like the following:
			7	0.2911
			11	0.0712
			20	0.1101
			21	0.0810
			27	0.2310
			...

The values that are specified depends on the data type and the way the user interpret them. For
instance, a convenient experiment in NMR is to measure the 1H-15N HSQC spectrum of the same protein in
two different states. Once the peak are attributed for each amide group, the chemical shift pertur-
bations can be estimated with: sqrt[pow(DHN, 2)+pow(DN/6.515, 2) (Farmer, 1996; Mulder et al., 1999)



Interface
----------

   SAMPLEX is before all of graphical usage. It uses Python language and requires some crucial
libraries to be run. In case where these libraries are not present in the normal installation
directory of Python, the program will complain and invit you to use a non-graphical interface.


	1. Graphical interface (GI)

   The GI is by default run. With the option -f, the user can load directly a previously saved
file to gain time, instead of filling all requiered fields, as following:
```
	samplex_1.0.py -f save.samp
```
You can also run SAMPLEX without specifying any save file. SAMPLEX will automatically invit you
to load one. You can ignore and cancel this proposition if you want to start from scratch.
In case where some problems are met while reading the save file, SAMPLEX will use some values by
default.


   The GI is split into two parts: the general information and the advanced option. In the former
one, you have to specify the working directory, where will be created the output files, and where
SAMPLEX will first look at to find other files (if their location are specified relatively). Then,
the location of the structure file and the experimental file must be specified.

The advanced options gives the user the possibility to specify the confidence range that makes a
residue considered as perturbed, non-perturbed or intermediate. Moreover, the progression of calculations
can be visualized with PyMol. This options must be checked to be run. In PyMol, for each run, the
unperturbed, perturbed and intermediate regions are colored in blue, red and purple, respectively.


			
	2. Non-Graphical interface (NGI)

   In the NGI, the user must provide a correct saved file. An example of this file is provided with
the distribution. If an error is encountered in the save file, the program will abandon and stop.
The user will have to check the specified save file manually before re-running SAMPLEX.
In this NGI, no PyMol interface will be run.








Known problems
--------------

Obviously, many different platforms trigger problems on some of them. The following part list
the possible errors you can meet and how to fix them.

1. I got a message similar to ""samplex_1.0.py: Command not found."" upon running SAMPLEX. Why ?
  This can be due to the pathway python has been installed on your computer. We assume for a Linux
distribution for instance that your /usr/bin/env file can locate it. If it is not the case, replace
the very first line of the script by the correct location, as following:
```
    #!/usr/bin/env python	-->	#!/usr/bin/python
```
		alternatively simply call samplex with python as:
```
    python samplex_1.0.py
```
The other reason can come that you are not in the directory where is located the main Python script
of SAMPLEX. In such a case, change directory or specify either a CORRECT absolute or relative pathway.

2. Samplex stops with some strange error message.
   This is often related to the graphical interface. You can run samplex in its non-graphical mode by adding the --ng option
For this edit first the paramter file to define all files and paramters and then call samplex with:
```
   samplex_1.0.py -ng -f save.samp
```
",2022-08-12
https://github.com/haddocking/SARS-COV2-NTD-SIA-modelling,"# MODELLING OF THE BINDING OF VARIOUS SIALIC ACID-CONTAINING OLIGOSACCHARIDES TO THE NTD DOMAIN OF THE SPIKE PROTEIN OF SARS-COV2

[![DOI](https://zenodo.org/badge/312301680.svg)](https://zenodo.org/badge/latestdoi/312301680)

This dataset is related to the following submitted manuscript:

**Cryptic SARS-CoV2-spike-with-sugar interactions revealed by ‘universal-STD’**

_Charles J. Buchanan<sup>2</sup>, Ben Gaunt<sup>1</sup>, Peter J. Harrison<sup>3</sup>, Audrey Le Bas<sup>3</sup>, Aziz Khan<sup>2</sup>, Andrew M. Giltrap<sup>1,2</sup>, Philip N. Ward<sup>3</sup>, Maud Dumoux<sup>1</sup>, Sergio Daga<sup>5</sup>, Nicola Picchiotti<sup>8,9</sup>, Margherita Baldassarri<sup>5</sup>, Elisa Benetti<sup>7</sup>, Chiara Fallerini<sup>5</sup>, Francesca Fava<sup>5,6</sup>, Annarita Giliberti<sup>5</sup>, Panagiotis I. Koukos<sup>11</sup>, Abirami Lakshminarayanan<sup>2</sup>, Xiaochao Xue<sup>2,4</sup>, Virgínia Casablancas-Antràs<sup>2</sup>, Timothy D.W. Claridge<sup>2</sup>, Alexandre M.J.J Bonvin<sup>11</sup>, Quentin Sattentau<sup>4</sup>, Simone Furini<sup>7</sup>, Marco Gori<sup>8,10</sup>, Jiangdong Huo<sup>1,3</sup>, Raymond J. Owens<sup>1,3</sup>, Alessandra Renieri<sup>5,6</sup>, GEN-COVID Multicenter Study, James H. Naismith<sup>*1,3</sup>, Andrew Baldwin<sup>*2</sup>, Benjamin G. Davis<sup>*1,2</sup>_

1. The Rosalind Franklin Institute, Harwell Science & Innovation Campus, OX11 0FA, UK.
2. Department of Chemistry, University of Oxford, Oxford, OX1 3TA
3. Division of Structural Biology, University of Oxford, The Wellcome Centre for Human Genetics, Headington, Oxford, OX3 7BN, UK.
4. Sir William Dunn School of Pathology, South Parks Road, Oxford, UK.
5. Medical Genetics, University of Siena, Siena, Italy
6. Azienda Ospedaliera Universitaria Senese, Siena, Italy
7. Department of Medical Biotechnologies, University of Siena, Siena, Italy
8. Department of Information Engineering and Mathematics, University of Siena, Italy
9. Department of Mathematics, University of Pavia, Pavia, Italy
10. Université Côte d’Azur, Inria, CNRS, I3S, Maasai
11. Bijvoet Centre for Biomolecular Research, Faculty of Science, Utrecht University, Netherlands.

## Modelling of the N-terminal domain of SARS-CoV-2

We modelled the structure of the N-terminal domain (NTD) on Protein Data Bank (PDB) [1] entry 7c2l [2] since it provided significantly better coverage of the area of interest when compared to the majority of the templates available on the PDB as of July 15th 2020. The models were created with Modeller [3], using the ‘automodel’ protocol without refining the ‘loop’. We generated 10 models and ranked them by their DOPE score [4], selecting the top 5 for ensemble docking.

## Docking

The docking was performedwith version 2.4 of the HADDOCK webserver [5, 6]. The binding site on NTD was defined by comparison with PDB entry 6q06 [7], a complex of MERS-CoV spike protein and 2,3-sialyl-N-acetyl-lactosamine. The binding site could not directly be mapped because of conformational differences between the NTDs of MERS-CoV and SARS-CoV-2, but by inspection a region with similar properties (aromatics, methyl groups and positively charged residues) could be identified. We defined in HADDOCK the sialic acid as ‘active’ and residues 18, 19, 20, 21, 22, 68, 76, 77, 78, 79, 244, 254, 255, 256, 258 and 259 of NTD as ‘passive’, meaning the sialic acid needs to make contact with at least one of the NTD residues but there is no penalty if it doesn’t contact all of them, thus allowing the compound to freely explore the binding pocket. Since only one restraint was used we disabled the random removal of restraints. Following our small molecule docking recommended settings  [7] we skipped the ‘hot’ parts of the semi-flexible simulated annealing protocol (‘initiosteps’ and ‘cool1_steps’ set to 0) and also lowered the starting temperature of the last two substages to 500 and 300K, respectively (‘tadinit2_t’ and ‘tadinit3_t’ to 500 and 300, respectively). Clustering was performed based on ‘RMSD’ with a distance cut-off of 2Å and the scoring function was modified to:

    HADDOCKscore = 1.0*E_vdW+0.1*E_elec+1.0*E_desol+0.1*E_AIR

All other settings were kept to their default values.

## Data files in this archive

Docked models are provided for the following systems:

- SIA
- SIA-23-GAL-14-BGC
- SIA-23-GAL-14-BGC-seed (same as previous, but different random seed)
- SIA-23-GAL-14-BGC-ens (same as 2nd, but using an ensemble of 5 conformations for the oligosaccharide)
- SIA-26-GAL-14-BGC-ens (using an ensemble of 5 conformations for the oligosaccharide)

Each directory contains:

    - complex_XXXw.pdb                      : models corresponding to the final, refined HADDOCK models
    - file.list                             : list of PDB filenames including the HADDOCK score, ranked based on the HADDOCK score)
    - file.nam_clustX and file.list_clustX  : list of PDB filenames belonging to a specific cluster X)
    - clusters_haddock-sorted.stat_best4    : file containing the cluster statistics (HADDOCK score and various energetics terms)
    - structures_haddock-sorted.stat        : file containing the single models statistics (HADDOCK score and various energetics terms)

Self-contained HADDOCK json files allowing to repeat the docking throught the submit_file interface of the HADDOCK2.4 server are provided in the [HADDOCK-server-files directory](HADDOCK-server-files).

[1]: http://dx.doi.org/10.1093/nar/28.1.235
[2]: http://dx.doi.org/10.1126/science.abc6952
[3]: https://doi.org/10.1016/S1357-4310(95)91170-7
[4]: https://dx.doi.org/10.1110%2Fps.062416606
[5]: https://doi.org/10.1021/ja026939x
[6]: https://doi.org/10.1016/j.jmb.2015.09.014
[7]: https://doi.org/10.1038/s41594-019-0334-7
[8]: https://doi.org/10.1007/s10822-018-0148-4
",2022-08-12
https://github.com/haddocking/shape-restrained-haddocking,"# Shape restrained haddocking

## Introduction

This repository contains all the code and data associated with our publication
titled:

""Shape-restrained modelling of protein-small molecule complexes with HADDOCK""

immediately available online as a preprint and peer-reviewed publication.

A secondary addendum to the publication is the sbgrid deposition which in addition
to the contents of this repository also contains all the models that were generated
for the benchmarking of these protocols and are impossible to include in GitHub due
to repository size limitations.

We have made our best efforts to organise the data as intuitively as possible so as
to make our code and data as assesible to as many interested parties as possible. An
explanation of the various folders and files in this repository follows.

## Data layout overview [[^](#shape-restrained-haddocking)]

This is the top-level git directory

```sh
LICENSE
README.md
code/
conformers/
data/
manuscript/
restraints/
results/
runs/
scripts/
setup/
shapes/
structures/
templates/
toppar/
```

## Details [[^](#shape-restrained-haddocking)]

`LICENSE` contains the license under which we are making our code and data available.
`README.md` is this document.

### Code [[^](#shape-restrained-haddocking)]

---

This folder contains general-purpose scripts that can be applied beyond the
specific confines of this project. In most cases the name of the code should
suffice in terms of explanation as to what it does. In case it doesn't though,
most of these files are very well documented.

The contents of the folder are:

```sh
add_atom_features.py
add_his_protonation.py
calc_mcs.py*
dist_to_izone.py*
generate_conformers.py*
get_similar_by_ligand.py*
get_similar_by_protein.py*
ligdist.py*
pdb_element.py*
pharm2D_Tc.py
plot_helpers.R
restrain_ion.py*
```

In summary, `add_atom_features.py` can be used to define the features on which the
pharmacophore-based protocol depends on, `add_his_protonation.py` makes use of the
output of [`molprobity`](http://molprobity.biochem.duke.edu/) to set the protonation
state of HIS residues (this was only used for the pharmacophore-based protocol),
`calc_mcs.py` calculates the Maximum Common Substructure-based similarity between
a single reference and multiple template compounds using the Tanimoto and Tversky
coefficients (the latter was used for the shape-based protocol), `dist_to_izone.py`
converts atom-based distances as they are calculated by the `ligdist.py` script to
the format that is accepted [ProFit](http://www.bioinf.org.uk/software/profit/),
`generate_conformers.py` generates 3D conformers while accepting multiple arguments
that determine the process that is used for the torsional sampling, `get_similar_by_ligand.py`
and `get_similar_by_protein.py` were used to fetch data from the [RCSB](https://www.rcsb.org/)
while making use of the REST- and graphql-based APIs, `ligdist.py` calculates distances
between a compound and its cognate receptor, `pdb_element.py` is a modified version of
the repsective tool from the [PDB-tools suite](https://www.bonvinlab.org/pdb-tools/)
used for the analysis of the results, `pharm2D_Tc.py` calculates the Tanimoto
coeeficient between compounds using the aforementioned pharmacophore fingerprints,
`plot_helpers.R` is a `ggplot`-based library of graphs and plots and `restrain_ion.py`
produces tbl-formatted (accepted by HADDOCK) restraints that maintain the correct
geometry for ions and their coordinating sidechains throughout the simulation.

Details regarding the software requirements for this project can be found in a
[later](#software-requirements-) section.

### Conformers [[^](#shape-restrained-haddocking)]

---

This is where individual PDB files for each target compound can be found. The
conformers have been generated with RDKit using the settings described in the
paper. There are at most 50 PDB files per directory as that is the number of
conformers that was found to provide the best balance of docking results and
sampling efficiency. Files are grouped by target.

### Data [[^](#shape-restrained-haddocking)]

---

This folder hosts two files, one per protocol and they are appropriately named as to
reflect the protocol to which they correspond. The file for the shape protocol is in
JSON format because of the presence of the `low_sim` templates whereas the file for
the pharm protocol is in csv format. These files contain all the data pertaining to
the targets, for example, template PDB ids, RMSD of the binding site, etc...

### Manuscript [[^](#shape-restrained-haddocking)]

---

This folder contains all the code, data and analysis that was undertaken in preparation
for the submission of the manuscript. Additionally it contains all the figures of the
main text as well as the tables and figures of the SI.

### Restraints [[^](#shape-restrained-haddocking)]

---

This is where the restraints used during the two protocols can be found. This is
broken down by protocol and grouped by target.

### Results [[^](#shape-restrained-haddocking)]

---

This is where the results for both protocols are stored in 5-column, space-separated
txt files with columns 1-5 corresponding to target, stage, model number, IL-RMSD
and model rank, respectively,
broken down by protocol and grouped by target.

### Runs [[^](#shape-restrained-haddocking)]

---

This folder is not version-controlled since it would be impossible to have
the run folders under git and is therefore absent from the repository. It is the
default location for the runs created by the scripts under `scripts`.

### Scripts [[^](#shape-restrained-haddocking)]

---

Broken down by protocol, this is where the scripts that orchestrate setting up,
starting and analysing runs can be found. The names should be self-explanatory.
They all assume that the runs are in the aforementioned `runs` folder and the
run directories have already been created. The crontab that is running everything
looks something like this:

```sh
$ crontab -l
*/5 * * * * cd .../runs; ../scripts/shape/start.sh >> .start.log 2>> .start.err
*/1 * * * * cd .../runs; ../scripts/shape/analyse.sh >> .analyse.log 2>> .analyse.err
1,16,31,46 * * * * cd .../runs; ../scripts/shape/check.sh &> .check.log
```

where `...` is the path to the aforementioned `runs` directory.

### Setup [[^](#shape-restrained-haddocking)]

---

Broken down by protocol, this is where the `run.param` and `run.cns` used by the
scripts under `scripts` are located. These files control the protocol that is being
benchmarked.

### Shapes [[^](#shape-restrained-haddocking)]

---

Broken down by protocol, this is where the shapes extracted from the PDB files of
the templates can be found. For the shape protocol, some targets also contain the
low_sim shapes. **The shapes are in the coordinate system of their respective
template receptor, and by extension the reference as well.**

### Structures [[^](#shape-restrained-haddocking)]

---

These are the reference structures of the benchmark. In addition to the reference
complex each target folder also contains the reference compound, the original complex
as it was downloaded from the PDB and a README file detailing all the modifications
that were made in preparing the structure for docking/analysis.

### Templates [[^](#shape-restrained-haddocking)]

---

Broken down by protocol, this is where the template receptors and compounds (in
separate files) can be found. **The templates have been superimposed on their
respective reference receptor.**

### Toppar [[^](#shape-restrained-haddocking)]

---

This is where the topologies and parameters for all target compounds can
be found.

## Software requirements [[^](#shape-restrained-haddocking)]

Regarding the software side of things, the code that can be found in [scripts](scripts)
only depends on bash meaning there are no dependencies. The code under [code](code) is
mostly python and does have a few dependencies. Namely:

```sh
requests
pandas
rdkit
openbabel
```

along with a reasonably recent version of Python. This code and all work on this
project was performed with Python 3.8 installed managed by conda. Conda was chosen
as it is the preferred way to install RDKit. All of the aforementioned modules can
be found on the conda-forge channel and installed in one go with a command that looks
like this:

`conda install -c conda-forge requests pandas rkdit openbabel`
",2022-08-12
https://github.com/haddocking/vmbuilder,"## Bonvin Lab VM Builder Environment

This repository contains the Vagrantfiles, provisioners, and assets required to build the several VMs managed by the lab (educational, tutorials, etc).

### What is this repository and why should you learn how to use it?
If you want to have a .ova file to distribute to students, this is where you should start looking. The repository is organized for simplicity:

```
vmbuilder/
    assets/
    provisioning/
    vagrantfiles/
    builder.sh
    README.md
```

* *assets*: contains several scripts accessible to the machines and the provisioners (e.g. bashrc). Use this to store files that you want copied to the machine. If the files are extremely specific to the machine, e.g. some software binary, consider creating an external repository and cloning that repository via a provisioner. See the `molmod` machine Vagrantfile and provisioners for a good example.
* *provisioning*: contains the provisioners themselves. The Vagrantfile should be able to access them directly. Keep them simple and modular, to encourage reutilization.
* *vagrantfiles*: contains the Vagrantfiles. It would be good to follow a naming convention, such as `Vagrantfile.<box>` where `<box>` is either the box name or a shorthand name.
* *builder.sh*: automated builder script. Use it *after* you have tested and debugged your machine to generate a distributable `.ova` Virtualbox appliance.
* *README.md*: this file.

### Software Requirements
* [VirtualBox 5.0+](https://www.virtualbox.org/)
* [Vagrant](https://www.vagrantup.com/)
* [Git](https://git-scm.com/)

### How to package my box?
After you have tested and debugged your image, i.e. you've run `vagrant up` and all your provisioners run perfectly, copy-paste your Vagrantfile, provisioners, and assets to the right folders. Then, just call the `builder.sh` accordingly:

```bash
  ./builder vagrantfile/Vagrantfile.dummy dummy # creates dummy.ova
```

That simple. If you want to complicate things, do so on your own fork first. You're welcome to submit pull requests to change the builder script as well, but bear in mind that the script must be able to generate **all** machines, not just yours.

#### Shared Folders
Vagrant automatically sets up the folder containing the VagrantFile as a shared folder and
mounts it under `/vagrant` inside the image. This is extremely handy to develop scripts and
whatnot, so use it and abuse it. If you need more shared folders, read the VagrantFile and
the Documentation; there is also an easy way of setting them up automatically.

#### Vagrant Basic Usage
For a complete list of the available commands and their descriptions, read the [Vagrant Documentation](http://docs.vagrantup.com/v2/).

This VM works just like any other VirtualBox image. Opening the VirtualBox application after
running `vagrant up` will show the newly created machine. Starting with version 5.0, VirtualBox
allows connections to a running (headless) machine, but any version will allow any sort of
control over the machine, e.g. stopping, deleting, exporting.

Importantly, when running `vagrant up` for the first time, what is called the Vagrant box,
the machine image and hard drive, will be downloaded to specific folder. This will be copied
to wherever necessary to create a new machine. So, even if all machines are destroyed, there
will still be a copy of this image lying around.

Nonetheless, if you prefer to use Vagrant to manage the machine:
* `vagrant ssh`: connects to the machine via SSH.
* `vagrant reload (--provision)`: restarts the machine (and re-runs the provisioners).
* `vagrant suspend`: saves the current machine state and stops execution.
* `vagrant resume`: restarts a machine from the last saved state.
* `vagrant halt`: shutdown the machine.
* `vagrant destroy`: shutdown and _delete_ the machine.
* `vagrant box list`: list all the installed boxes (e.g. courseVM).
* `vagrant box remove <box name>`: removes a particular box permanently.
",2022-08-12
https://github.com/haddocking/whiscy,"![WHISCY](media/whiscy_logo.png)

<hr>

Table of Contents
=================

  * [WHat Information does Surface Conservation Yield?](#what-information-does-surface-conservation-yield)
  * [How does WHISCY work?](#how-does-whiscy-work)
  * [1. Installation](#1-installation)
  * [2. WHISCY setup](#2-whiscy-setup)
  * [3. WHISCY prediction](#3-whiscy-prediction)

## WHat Information does Surface Conservation Yield?

WHISCY is a program to predict protein-protein interfaces. It is primarily based on conservation, but it also takes into account structural information. A sequence alignment is used to calculate a prediction score for each surface residue of your protein.

This repository contains the Python 3 implementation of the original code developed by [Sjoerd de Vries](https://scholar.google.de/citations?user=fpjNl3wAAAAJ&hl=en) and originally published in [2006](https://doi.org/doi:10.1002/prot.20842):

> *De Vries SJ, van Dijk ADJ, and Bonvin AMJJ*<br>
> WHISCY: What information does surface conservation yield? Application to data-driven docking.<br>
> *Proteins: Struc. Funct. & Bioinformatics*; 2006, **63**(3): 479-489.


## How does WHISCY work?

WHISCY requires a protein structure and a sequence alignment. First, it identifies a master sequence, the sequence that best matches the structure.

The sequence distance (amount of mutation) between the master sequence and all sequences is estimated. This determines the amount of expected mutation.

Then, for each residue, the expected mutation is compared with the observed mutation. Less change than expected means conservation, translated into a positive WHISCY score.

<div style=""text-align:center;"">
    <img src=""media/compare.jpg""/>
</div>

Next, the interface propensity is taken into account.

Phenylalanines, for example, are likely to be in a protein-protein interface, so all phenylalanines receive a higher score. Lysines are much less likely to be in a protein-protein interface, so lysines receive a lower score.

Finally, all scores are smoothed over the surface of the protein structure.

Interfaces often form patches, so that neighbours of interface residues often are interface residues, too. The smoothing means that the scores of these neighbours are taken into account.

<div style=""text-align:center;"">
    <img src=""media/interface.jpg""/>
</div>



## 1. Installation

WHISCY needs the following software to be installed:

* **Python 3** (tested in version 3.6.6)
* **Python 3 libraries**: numpy (1.15.0), nose (1.3.7), biopython (1.71)
* **GNU CC compiler** (gcc, tested on version 6.4.0)
* **FreeSASA** (tested on version 2.0.3)
* **MUSCLE** (only if you want WHISCY to do automotically the multiple sequence alignment for you, tested on version 3.8.31)

Software version is indicative except for Python, which has to be from the 3.6.x series, and freesasa from the series 2.x and above.

### 1.1. Installation in macOS

#### 1.1.1. Python3, GCC and libraries
Using [Macports](https://www.macports.org/), you can install Python 3 and the necessary libraries, GCC and git:

```bash
sudo port install gcc6 git python36 py36-biopython py36-numpy py36-nose
```

Similar installation should be possible using [Homebrew](https://brew.sh/) instead of Macports.

If you already have `python3` and `pip3` installed, it is completely OK to install `bio`, `numpy`and `nose` libraries using `pip3`.


#### 1.1.2. MUSCLE
To install MUSCLE, go to [the official download site](https://www.drive5.com/muscle/downloads.htm), download the Mac OS X version suitable for your architecture (32 or 64bit) and follow the instructions provided by the authors.

#### 1.1.3. FreeSASA
Go to the [FreeSASA main site](https://freesasa.github.io/) and follow the `Quick-start guide`. We don't need the Python bindings as we will be calling freesasa binary from command line.

**Make sure freesasa binary is in your path:**

```bash
$ freesasa --version
FreeSASA 2.0.3
License: MIT <http://opensource.org/licenses/MIT>
If you use this program for research, please cite:
  Simon Mitternacht (2016) FreeSASA: An open source C
  library for solvent accessible surface area calculations.
  F1000Research 5:189.

Report bugs to <https://github.com/mittinatten/freesasa/issues>
Home page: <http://freesasa.github.io>
```

#### 1.1.4. WHISCY
Then, the next step is to clone thise repository:

```bash
git clone https://github.com/haddocking/whiscy.git
cd whiscy
pwd
```

With `pwd`, you will get the directory where you have cloned `whiscy`. Please, copy that directory path because you will need to specify it in your `.bashrc` or `.bash_profile` file.

Edit your `.bashrc` or `.bash_profile` and add the following lines:

```bash
# Whiscy
export WHISCY_PATH=/PATH/TO/WHISCY
export PYTHONPATH=$PYTHONPATH:${WHISCY_PATH}
export WHISCY_BIN=${WHISCY_PATH}/whiscy.py
export PATH=$PATH:${WHISCY_PATH}
```

You have to change `/PATH/TO/WHISCY` according to the directory pointed by the `pwd` command.

Now, we compile the `protdist` software: 

```bash
cd $WHISCY_PATH
cd bin/protdist
./compile.sh
./protdist 
```

If we see an output like this:

```
Too few arguments for this modified version of PROTDIST
Usage: protdist <infile> <outfile>
```
everything is ready.

There is only one final step where we tell WHISCY about where to find MUSCLE binary. Edit `$WHISCY_PATH/etc/local.json`:

```json
{
  ""ALIGN"": {
    ""MUSCLE_BIN"": ""/path/to/bin/muscle/muscle3.8.31_i86darwin64""
  },
  ""CUTOFF"": {
    ""sa_pred_cutoff"": 15.0,
    ""sa_act_cutoff"": 40.0,
    ""air_cutoff"": 0.18,
    ""air_dist_cutoff"": 6.5
  },
  ""AIR"": {
    ""air_pro_percentage"": 10.0,
    ""air_wm_pro_or"": 98.52,
    ""air_wm_whis_or"": 0.370515,
    ""air_wm_pro_and"": 55.42,
    ""air_wm_whis_and"": 0.106667
  }
}
```

Change the `MUSCLE_BIN` variable to the correct path of your MUSCLE binary.


### 1.2. Installation in GNU/Linux

In Debian/Ubuntu flavours use `apt`:

```bash
sudo apt-get install python3 python3-numpy python3-nose2 python3-biopython gcc-4.6 git-all
```

For the next steps, see the macOS instructions which also apply: [1.1.2](#112-muscle), [1.1.3](#113-freesasa) and [1.1.4](#114-whiscy).


## 2. WHISCY setup

WHISCY needs of some initial data in order to do the prediction. For that purpose, a script called `whiscy_setup.py` is provided:

```bash
$ whiscy_setup.py 
usage: whiscy_setup [-h] pdb_file_name chain_id
whiscy_setup: error: the following arguments are required: pdb_file_name, chain_id
```

The parameters of this script are `pdb_file_name` and `chain_id`. While `pdb_file_name` can be a pdb file (for example `1ppe.pdb`) or a [PDB code](https://www.rcsb.org/) (`1ppe`), `chain_id` is a character (upper or minor case) indicating the chain of the molecule to use for the prediction.

For example, if we are instered in predicting the chain E of the 1PPE complex:

```bash
$ whiscy_setup.py 1ppe e
Downloading PDB structure '1ppe'...
whiscy_setup [INFO] PDB structure with chain E saved to 1ppe_E.pdb
whiscy_setup [INFO] Atom accessibility calculated to 1ppe_E.rsa
whiscy_setup [INFO] Surface and buried residues calculated
whiscy_setup [INFO] Downloading HSSP alignment...
whiscy_setup [INFO] HSSP alignment stored to 1ppe.hssp
whiscy_setup [INFO] HSSP file converted to PHYLIP format
whiscy_setup [INFO] Protdist calculated
whiscy_setup [INFO] Conversion table file generated
whiscy_setup [INFO] Whiscy setup finished
```

`whiscy_setup.py` first checks if the PDB file or PDB structure contains the chain indicated, then tries to download from the [HSSP database](https://swift.cmbi.umcn.nl/gv/hssp/) the PDB complex MSA alignment. If this step fails, the script will try a different approach based on 1) [NBCI Blastp](https://blast.ncbi.nlm.nih.gov/Blast.cgi) and then 2) a multiple sequence alignment of the `blastp` results using `MUSCLE`.

Note that `whiscy_setup.py` requires of internet access in order to gather the relevant files.

### whiscy_setup.py output

`whiscy_setup.py` generates a set of files needed for the prediction step with `whiscy.py`. Here it is a list of the generated files in our 1ppe complex [example](example/):

| File name                                 |    Explanation                                                 |
| ----------------------------------------- |----------------------------------------------------------------|
| [1ppe.hssp](example/1ppe.hssp)           | Multiple sequence alignment download from the HSSP database     |
| [1ppe.hssp.bz2](example/1ppe.hssp.bz2)   | HSSP MSA file compressed                                        |
| [1ppe.pdb](example/1ppe.pdb)             | PDB file download from the Protein Data Bank                    |
| [1ppe_E.pdb](example/1ppe_E.pdb)         | 1ppe.pdb parsed to select only the given `chain_id`             |
| [1ppe_E.rsa](example/1ppe_E.rsa)         | SASA output of `freesasa` in `NACCESS` format of `1ppe_E.pdb` file|
| [1ppe_E.fasta](example/1ppe_E.fasta)     | Sequence of 1ppe_E.pdb. Alternative residues have been removed  |
| [1ppe_E.phylseq](example/1ppe_E.phylseq) | MSA file translated from HSSP to PHYLIP format                  |
| [1ppe_E.conv](example/1ppe_E.conv)       | PDB residue numeration to FASTA sequence numeration             | 
| [1ppe_E.out](example/1ppe_E.out)         | Output of the `protdist` software on 1ppe_E.pdb                 | 
| [1ppe_E.sur](example/1ppe_E.sur)         | >15 % surface residue list according to `sa_pred_cutoff` cutoff | 
| [1ppe_E.suract](example/1ppe_E.suract)   | >40 % surface residue list according to `sa_act_cutoff` cutoff  | 
| [1ppe_E.lac](example/1ppe_E.lac)         | 0-15 % accessible residue list                                  | 


## 3. WHISCY prediction

Running the main `whiscy.py` script without parameters will give you a guess of the required files for WHISCY in order to predict the interface residues of your protein:

```bash
$ whiscy.py
usage: whiscy [-h] [-o output_file]
              surface_list conversion_table alignment_file distance_file
whiscy: error: the following arguments are required: surface_list, conversion_table, alignment_file, distance_file
```

WHISCY needs of four input files:

* `surface_list` which is a list of residues in the interface. Tipically comes with `.sur`extension, for example [1ppe_E.sur](example/1ppe_E.sur).
* `conversion_table`, the file representing the mapping of the PDB file residue numeration into the FASTA sequence numeration, tipically with `.conv` extension and for example, [1ppe_E.conv](example/1ppe_E.conv).
* `alignment_file` is the MSA file in PHYLIP format, `.phylseq` extension, [1ppe_E.phylseq](example/1ppe_E.phylseq).
* `distance_file` is the output of protdist software, in our example with extension `.out`: [1ppe_E.out](example/1ppe_E.out).

If we try the input from our example, `whiscy_setup.py` runs with the protein [1PPE](https://www.rcsb.org/structure/1ppe) and chain `E`:

```bash
$ whiscy.py 1ppe_E.sur 1ppe_E.conv 1ppe_E.phylseq 1ppe_E.out 
whiscy [INFO] Parsing surface list...
whiscy [INFO] Loading conversion table...
whiscy [INFO] Converting...
whiscy [INFO] Initializing score calculation...
whiscy [INFO] Calculating scores...
whiscy [INFO] Subtracting average value ...
whiscy [INFO] Sorting scores...
whiscy [INFO] Writing scores...
0.95155   G19 
0.95070  G219 
0.91582  G133 
0.90942   H57 
0.87666   P28 
0.84307   G18 
0.82484  K107 
0.80778  L123 
0.75653  P173 
...
-1.24180  K222 
-1.31572  G187 
-1.41471  G203 
-1.43181  G193 
-1.54083  L185 
-1.64713  Y184 
-2.41928  C232 
-2.49188  C191 
-2.67467  W237 
-2.77188  W215 


""My God, so much I like to drink Scotch that sometimes I think my name is Igor Stra-whiskey.""
  -  Igor Stravinsky
```

There is also the possibility of writing the WHISCY scores to a file if we use the `-o` flag:

```bash
$ whiscy.py 1ppe_E.sur 1ppe_E.conv 1ppe_E.phylseq 1ppe_E.out -o 1ppe_E.cons
whiscy [INFO] Parsing surface list...
whiscy [INFO] Loading conversion table...
whiscy [INFO] Converting...
whiscy [INFO] Initializing score calculation...
whiscy [INFO] Calculating scores...
whiscy [INFO] Subtracting average value ...
whiscy [INFO] Sorting scores...
whiscy [INFO] Writing scores...
whiscy [INFO] Prediction written to 1ppe_E.cons

""My God, so much I like to drink Scotch that sometimes I think my name is Igor Stra-whiskey.""
  -  Igor Stravinsky
```

The prediction in this case will be saved to the [1ppe_E.cons](example/1ppe_E.cons) file.

### 3.1. WHISCY server-like prediction

To mimic the WHISCY server behavior using interface propensities and surface smoothing, there is a BASH script in the WHISCY home directory. You can execute it like in this example:

```bash
whiscy_protocol.sh 1ppe_E
```

After a few seconds, there will be a new `.pscons` file with the predicted residues in the interface sorted by their WHISCY score:

```bash
$ head 1ppe_E.pscons 
 0.61467   I73
 0.58322   G78
 0.56459   V75
 0.55906   F82
 0.49307   Q81
 0.48432   L114
 0.47943   E80
 0.46539   Y39
 0.45669   V76
 0.43158   N72
```

As stated in the original WHISCY publication, residues are predicted to be in the interface if the WHISCY score is higher than 0.180, corresponding to a 29.4% of sensitivity.


### 3.2. Show WHISCY predictions

There is a Python3 script in the `bin` directory called `whiscy2bfactor.py` in charge of mapping the WHISCY interface residues prediction into the B-factor column of the PDB file.

An usage example:

```bash
$ cd ${WHISCY_PATH}/example
$ ../bin/whiscy2bfactor.py 1ppe_E.pdb 1ppe_E_whiscy.pdb 1ppe_E.pscons
1ppe_E_whiscy.pdb PDB file with WHISCY scores in B-factor column has been created
```

We can use any molecular visualization software to depict our molecule using the B-factor column over the surface (example from [UCSF Chimera](https://www.cgl.ucsf.edu/chimera/) using Cyan-Maroon scale for predicted/not-predicted):

<div style=""text-align:center;"">
    <img src=""media/1ppe_whiscy.png""/>
</div>
",2022-08-12
https://github.com/hamediut/GeoWGAN-GP,"# GeoWGAN-GP
Code for training a WGAN-GP to reconstruct two-dimensional backscatter electron microscopy images of geological rock samples. More details can be found in **Quantifying complex microstructures of earth material: Reconstructing higher-order spatial correlations using deep generative adversarial networks**.

Authors: [Hamed Amiri](https://www.researchgate.net/profile/Hamed-Amiri-10), [Ivan Vasconcelos](https://www.uu.nl/medewerkers/IPiresdeVasconcelos), [Yang Jiao](https://isearch.asu.edu/profile/1970397), [Pei-En Chen](https://www.researchgate.net/profile/Pei-En-Chen-2), [Oliver Plümper](https://www.uu.nl/staff/OPlumper)

Original data and data for reproducing figures in the publication can be found in Yoda repository of Utrecht University available on:
% here the link to Yoda.

## Data
Large segmented backscattered electron (BSE) images of both meta-igneous and serpentinte samples can be found in the Original_images folder. These images are used to create training images of size 128 by 128 pixels (default image size).
Give the path to one of these images upon running training code (train.py).

## How to run the code?
* To run the code for training WGAN-GP type: train.py  -ip "" path to the large segmented image in the Original_images folder"".
* Other paramter is '-project_name' which is Meta-igneous by default but you can define your own name. it will create a folder where your training images will be saved.
* Default image size for reconstruction is 128 by 128 pixels. if you want to train WGAN-GP on larger images you should pass argumnt '-isize' which is set to 128 by default.
Networks' architectures will be automatically updated to create images of this size. Note that isize should be a multliple 16.

## Remarks
* MSE between two point correlation of real and reconstructed images are used as a accuracy metric and also for saving the models. By default, when mse is less than 5e-7, model is saved in a new folder called ""checkpoints"" in which there will be a subfolder to save the best model with smallest mse.
* Two-point correlation and autoscaled correlations plots calculated from real and reconstructed images are also save in output folder.
* Both mse and two-point correlation functions are calculated on 128 images (=batch size) and average values are considered.

## Requirements
Following libraries and versions have been used in this repository.
* Python 3.8.12
* PyTorch 1.10.12
* Numpy 1.19.5
* Numba 0.54.0
* Pandas 1.3.5
* Scikit-image 0.18.3
* Tifffile 2021.11.2
",2022-08-12
https://github.com/hanneoberman/ADS_thesis_template,"# Simulation template for ADS thesis projects 2022

The file `simulation.R` contains the simulation script, to be completed by the students.

Good luck!
-Hanne
",2022-08-12
https://github.com/hanneoberman/convergence,"# Non-convergence in iterative imputation

A simulation study to evaluate non-convergence in the MICE algorithm.

## Abstract

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

## Contents

* [Manuscript.pdf](.\convergence\Manuscript.pdf)
* [Simulation.R](.\convergence\Simulation.R)
* [Manuscript/](.\convergence\Manuscript)
  * [Manuscript.Rmd](.\convergence\Manuscript\Manuscript.Rmd)
  * [references.bib](.\convergence\references.bib)
* [Simulation/](.\convergence\Simulation)
  * [DGM.R](.\convergence\Simulation\DGM.R)
  * [methods.R](.\convergence\Simulation\methods.R)
  * [performance.R](.\convergence\Simulation\performance.R)
  * [utils.R](.\convergence\Simulation\utils.R)
",2022-08-12
https://github.com/hanneoberman/CV,"# CV
CV to be updated. Template from the `vitae` R package.
",2022-08-12
https://github.com/hanneoberman/hanneoberman,"## Hi, I'm Hanne! 
A statistician interested in data visualization, interdisciplinarity, and open science. 
<br/><br/>
![Twitterbanner2](https://user-images.githubusercontent.com/38891540/150965992-bb00a8a2-a331-4ca4-88d0-8eb742ed2102.jpg)
<br/><br/>
H. I. Oberman, MSc | she/her | PhD Candidate | Utrecht University
<br/><br/>
*⇇ look overthere for some links to get in touch!* <br/><br/>


<!--
**hanneoberman/hanneoberman** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- 🔭 I’m currently working on ...
- 🌱 I’m currently learning ...
- 👯 I’m looking to collaborate on ...
- 🤔 I’m looking for help with ...
- 💬 Ask me about ...
- 📫 How to reach me: ...
- 😄 Pronouns: ...
- ⚡ Fun fact: ...
-->
",2022-08-12
https://github.com/hanneoberman/litmice,"# litmice

Repository for a systematic review that I'm currently conducting, investigating how the imputation package {mice} is used and reported in applied research literature.
",2022-08-12
https://github.com/hanneoberman/meta-equi,"# meta-equi
stage tu/e
",2022-08-12
https://github.com/hanneoberman/MIMP,"# MIMP
Missing Data in Practice summer school
",2022-08-12
https://github.com/hanneoberman/MissingThePoint,"# MissingThePoint
This repository contains the improved version of my MSc thesis manuscript 'Missing The Point: Non-Convergence in Iterative Imputation Algorithms'. If you are looking for the original version, or the one presented at the ICML workshop ARTEMISS, please see https://github.com/hanneoberman/MScThesis instead.
",2022-08-12
https://github.com/hanneoberman/MLRP,"This repository serves as working directory for the course 'Markup Languages and Reproducible Programming' (Utrecht University). It contains the following folders:

- `Simulation_study` | Small simulation study to show proficiency with Markdown.

- `Shiny_app` | Elementary Shiny app to explore functionalities and lay foundation for `ShinyMICE` thesis project.

And a file explaining where to find which documents for grading the coursework (`Dear_Gerko.Rmd`).


Note. *This repository is only a showcase for educational purposes. The Shiny app is **not** a finished product.*

-Hanne",2022-08-12
https://github.com/hanneoberman/multilevelmice,"# Imputation of Incomplete Multilevel Data with `mice`

Tutorial paper on imputing incomplete multilevel data with `mice`. Including methods for ignorable and non-ignorable missingness.

Contributors: Johanna Munoz, Valentijn de Jong, Gerko Vink, Thomas Debray, and Hanne Oberman.",2022-08-12
https://github.com/hanneoberman/presentations,"# Presentations

Some of my research talks and lecture slides. See [hanneoberman.github.io/presentations](https://hanneoberman.github.io/presentations).

## 2021

- Guest lecture MSc Methodology and Statistics, Utrecht University, *Some Shiny Stuff: building (web)apps with the `R` package `shiny`*. Slides [here](https://hanneoberman.github.io/presentations/2021/shiny/static/shiny_guest_lecture_static.html).
",2022-08-12
https://github.com/hanneoberman/projects,"# projects
Just some projects I'm working on. Mostly related to my PhD thesis.
",2022-08-12
https://github.com/hanneoberman/publications,"# Publications

Overview of my research output (incl. pre-prints and conference papers).

## 2021

- Liu, D., **Oberman, H. I.**, Muñoz, J., Hoogland, J., & Debray, T. P. A. (2021). *Quality control, data cleaning, imputation* [pre-print]. To appear in *Clinical applications of artificial intelligence in real-world data*. DOI: arxiv.org/abs/2110.15877. Download [here](https://hanneoberman.github.io/publications/2021/quality_control_data_cleaning_imputation.pdf).

- **Oberman, H. I.** (2021). *Visualizing uncertainty due to missing data* [conference paper]. IEEE VIS, VisComm workshop. DOI: doi.org/10.31219/osf.io/ahtfy. Download [here](https://hanneoberman.github.io/publications/2021/visualizing_uncertainty_due_to_missing_data.pdf).


## 2020

- **Oberman, H. I.**, Buuren, S. van, & Vink, G. (2020). *Missing the Point: Non-Convergence in Iterative Imputation Algorithms* [conference paper]. ICML, ARTEMISS workshop. DOI: arxiv.org/abs/2110.11951v1. Download [here](https://hanneoberman.github.io/publications/2020/missing_the_point.pdf).
",2022-08-12
https://github.com/hanneoberman/quarto,"# quarto
test website with quarto
",2022-08-12
https://github.com/hanneoberman/real-time-missing,"# An evaluation of ‘real-time’ missing data handling in machine learning and prevailing statistical models

A collaboration between UMCU and UU to work on real-time missing data methods for incomplete prediction model data of individual patients. Simulation study and paper by Steven W.J. Nijman, Hanne I. Oberman, Gerko Vink, Thomas P.A. Debray, and Maarten van Smeden.

## Abstract
The need to solve for missing values in real-time is unique to the application of prediction models. The topic of real-time imputation is underrepresented in the literature. In this study, we aim to evaluate various real-time strategies to handle the pervasive problem of missing data when using clinical data. We aim to evaluate the influence of built-in missing data handling mechanisms on prediction accuracy and compare it with existing real-time imputation methods (e.g., joint modeling imputation). We evaluate the effect of various missing data handling methods under specific missing data circumstances as would occur in medical practice.

## Repository
This Github repository contains all of the `R` code required to reproduce our simulation study. More specifically, running the file `run_simulation.R` from the folder 'Functions' would yield the results for all but one method. We used high performance computing for the method 'random forest with surrogate splits'. Results for this method can be obtained with the files `submit_hpc_simulation.sh` in combination with `run_hpc_simulation.R` from the 'Functions' folder and the list of datasets generated by running `run_simulation.R`. Subsequently, running `analyze_results.R` from the same folder will summarize the results across simulation repetitions and produce the estimates presented in the manuscript.

The repository is structured as follows:
- '**Manuscript.docx**' contains the most recent version of the paper
- '**Data**' will contain the data generated by running `run_simulation.R`
- '**Functions**' contains the simulation scripts
- '**Results**' will contain the results and estimates generated by running `analyze_results.R`
- '**Visualizations**' contains the visualization scripts 

",2022-08-12
https://github.com/hanneoberman/review_missing_data,"# review_missing_data
Missing data methods literature review as part of a chapter on 'Quality control, data cleaning, imputation' with Thomas Debray UMCU, and Dawei Liu (Biogen) 
",2022-08-12
https://github.com/hanneoberman/svy,"# svy
Survey Data Analysis
",2022-08-12
https://github.com/hanneoberman/viscomm2021,"# viscomm2021
Presentation at IEEE VIS conference VisComm workshop, see https://virtual.ieeevis.org/year/2021/session_w-viscomm.html
",2022-08-12
https://github.com/hecklab/glyco-peptidome,"# glyco-peptidome

These scripts where used in the data analysis of the described paper below. These scripts combined all search output data from Byonic and then profiled the phosphorylation and O-glycosylation PSMs along the protein back bone of β-casein. Please see the published paper for links to the data uploaded to MassIVE. 

**Monitoring β-casein phosphorylation and O-glycosylation over lactation reveals distinct differences between the proteome and endogenous peptidome**

*Kelly A. Dingess<sup>1,2</sup>, Inge Gazi<sup>1,2</sup>, Henk van den Toorn<sup>1,2</sup>, Marko Mank<sup>3</sup>, Bernd Stahl<sup>3,4</sup>, Karli R. Reiding<sup>1,2\*</sup>, Albert J.R. Heck<sup>1,2\*</sup>*
	
1. Biomolecular Mass Spectrometry and Proteomics, Bijvoet Center for Biomolecular Research and Utrecht Institute for Pharmaceutical Sciences, University of Utrecht, Padualaan 8, Utrecht 3584 CH, The Netherlands
2. Netherlands Proteomics Center, Padualaan 8, Utrecht 3584 CH, The Netherlands
3. Danone Nutricia Research, Uppsalalaan 12, 3584 CT Utrecht, The Netherlands
4. Chemical Biology & Drug Discovery, Utrecht Institute for Pharmaceutical Sciences, University of Utrecht, Universiteitsweg 99, 3584 CG Utrecht, The Netherlands


\*Corresponding authors: Karli R. Reiding, E-mail: k.r.reiding@uu.nl; Albert J. R. Heck, E-mail: a.j.r.heck@uu.nl

Keywords: Human milk, Mass Spectrometry, O-Glycosylation, Peptidomics, Antimicrobial peptides 

------

# Instructions for use

First, the Byonic output files from the article must be downloaded and combined using the python script. The script will read all \*.xlsx files from the current directory, and outputs a file called ""Combined.xlsx"".
The script uses the **pandas** package.

Second, run the jupyter notebook to go through the analysis done in the article, reading in the Combined.xlsx file. This notebook uses the R kernel, and it requires the **tidyverse, cowplot, scales and readxl** packages.
",2022-08-12
https://github.com/hecklab/pLink-results-analysis,"# pLink-results-analysis

These R scripts and functions were used in the data processing of the described paper below. The notebook reads in pLink output files which are deposited on Pride.

The sctipts and functions reformat the data into flat tables that resemble an output one can get from the XlinkX node of proteome discoverer.\
The scripts clean the data and perform some processing which were useful in this project.

Some of the processing steps might not be suitable for some data sets. The functions are meant to be reused while the notebook should serve as a guide and an example.

**Characterization of protein complexes in extracellular vesicles by intact extracellular vesicle crosslinking mass spectrometry (iEVXL)**

Julia Bauzá-Martinez<sup>1,2</sup>, Gad Armony<sup>1,2</sup>, Matti F. Pronker<sup>1,2</sup>, Wei Wu<sup>1,2,3*</sup>

<sup>1</sup> **Biomolecular Mass Spectrometry and Proteomics**, Bijvoet Center for Biomolecular Research and Utrecht Institute for Pharmaceutical Sciences, Utrecht University, Padualaan 8, 3584 CH Utrecht, The Netherlands\
<sup>2</sup> **Netherlands Proteomics Centre**, Padualaan 8, 3584 CH Utrecht, The Netherlands\
<sup>3</sup> **Singapore Immunology Network (SIgN)**, Agency for Science, Technology and Research (A*STAR), Singapore, Singapore.

\*correspondence: Wei Wu, wu_wei@immunol.a-star.edu.sg or w.wu1@uu.nl  


## Instructions for use

### Environment
The code is embedded in a jupyter notebook which can be executed in several enviourments, in jupyter lab for example.\
Make sure that an R kernel is available to run the notebook.
The following packages need to be installed in R:
* tidyverse
* docstring (for rendring function documentation)

### Obtain the files
To obtain the functions, notebook, and data, use git to download ('clone') the files, or simply download the files as a zip file from the web interface. The URL for cloning, or the link to download the zip, are availabe under the green ""Code"" button above the file listing.


### Notes
* The frunctions assume that the protein names in the fasta file used to search in pLink, are in the uniprot format (sp|ACCESSION|NAME), while contaminants are not (do not start with sp| ).
* The renumber functions assumes that the fasta file used for the search in pLink was modified to remove signal peptides. These functions readjust the numers to keep the uniprot numbering in place.",2022-08-12
https://github.com/hecklab/Protein-arginine-phosphorylation,"# Protein-arginine-phosphorylation

These scripts where used in the data analysis of the described paper below. These scripts read in all necessary MQ output files which are deposited on ProteomeXchange with the accession number PXD026981.

The scripts analyze the stability of synthetic pArg, fragmentation characteristics as well as retention time.

Please cite the following publication when using the respective scripts. 
https://doi.org/10.1016/j.mcpro.2022.100232 

**Widespread arginine phosphorylation in Staphylococcus aureus**

Nadine Prust<sup>1,2</sup>, Pieter C. van Breugel<sup>1,2</sup> and Simone Lemeer<sup>1,2,#</sup>

<sup>1</sup>**Biomolecular Mass Spectrometry and Proteomics**, Bijvoet Center for Biomolecular Research and Utrecht Institute for Pharmaceutical Sciences, Utrecht University, Padualaan 8, 3584 CH Utrecht, The Netherlands
<sup>2</sup>**Netherlands Proteomics Center**, Padualaan 8, 3584 CH Utrecht, The Netherlands

*<sup>#</sup>Correspondence to: Simone Lemeer, s.m.lemeer@uu.nl*


## Instructions for use

### Environment
The code is embedded in RMarkdown documents, one per analysis. RStudio (https://rstudio.com, no affiliation) is a convenient environment for ""knitting"" these documents, to create HTML or PDF output. The following packages need to be installed in R:

* tidyverse
* ggpubr
* reshape2
* stringr
* colorspace
* ggforce
* RColorbrewer
* VennDiagram
* psych

### Obtain the markdown documents
To obtain these documents, use git (available in RStudio as well) to download ('clone') the documents, or simply download the files as a zip file. The URL for cloning, or the link to download the zip, are availabe under the green ""Code"" button above the file listing.
We have added a project file for convenience, so you can double-click it to open the project in RStudio.

### Obtain the data
No data (MaxQuant output .txt files) is available in this repository, it needs to be downloaded from the Pride archive.
From there, obtain the MQ_output_txt.zip file and extract it in the same directory as the markdown documents. The scripts will locate the required .txt file in the subfolder and load all necessary libraries. 
Second, open the .Rmd file you are interested in and 'knit' the document. The scripts will generate the plots used in the above mentioned paper. 
",2022-08-12
https://github.com/higher-bridge/copying-task-uu,"# copying-task-uu
Copying task for Utrecht University

# Environment installation
Run `conda env create --name erc --file environment.yml` in the anaconda prompt",2022-08-12
https://github.com/higher-bridge/project_forrest,"# project_forrest

This repository contains the code and outcome measures for the following manuscript (link will be added once manuscript becomes available):

**Seeing the Forrest through the trees: Oculomotor movement is linked to heart rate**
Alex J. Hoogerbrugge, Christoph Strauch, Zoril A. Oláh, Edwin S. Dalmaijer, Tanja C.W. Nijboer, Stefan Van der Stigchel


### Procedure
#### Data
- Since this project uses data from a publicly available source, it is not included in the repository.
- Links to the raw data and their descriptions can be retrieved from https://studyforrest.org (specifically [Hanke et al., 2016](https://www.nature.com/articles/sdata201692) for the publication and [OpenNeuro](https://openneuro.org/datasets/ds000113/versions/1.3.0) for the direct link to the data) and should be placed in a subfolder of the ```data``` folder: ```data/eyetracking``` or ```data/heartrate``` respectively.
- Raw data takes the form of ```sub-02_ses-movie_func_sub-02_ses-movie_task-movie_run-1_recording-eyegaze_physio.tsv``` for eyetracking, and ```sub-02_ses-movie_func_sub-02_ses-movie_task-movie_run-1_recording-cardresp_physio.tsv``` for pulse oximetry. Discard all other files.
- The current version of the code looks at characters 5 and 6 to determine participant ID. Data is also sorted based on filename, so ensure that there is some numbering in the filenames for the multiple runs per participant (as is the case in the original naming scheme).
#### Reproducibility
- A seed value of 42 is set for numpy in ```constants.py```.
- All relevant output values, matching those in the manuscript, should be included in the repository. ```EXP_[0/1]_RED[0/1]``` indicate whether feature explosion and dimensionality reduction were applied to achieve this outcome measure (0 = False, 1 = True).
- I have attempted to include the correct environment to exactly reproduce the results. Install with [Anaconda](https://www.anaconda.com/): ```conda install --name ENV_NAME --file environment.yml``` or ```conda env install --name ENV_NAME --file environment.yml```.
- I run an M1 CPU using Rosetta2, so behaviour should be similar to Intel CPU's, but have not been able to test this extensively.
#### Code
- The ```N_JOBS``` variable in ```constants.py``` sets the number of CPU threads the code will use. Be sure to check whether this is set to a suitable value for your CPU. **Note that setting a different number of jobs will affect  outcomes.**
- After raw data has been placed in the correct folders, run ```src/main_dataloader.py```. This will perform initial pre-processing and fixation/heartrate extraction. This can take a while to run. 
    - Any static variables can be changed in ```constants.py```. 
- Then, run ```src/main_pipeline.py```. This has been designed to run separately from the dataloader, so that the dataloader only needs to be run once. The main pipeline can also take some time to run (depending on ```N_JOBS```, ```HYPERPARAMETER_SAMPLES``` and ```SEARCH_ITERATIONS```). 
    - It's possible to comment and uncomment sections of code if you're only looking to reproduce certain analyses.
    - Again, static variables can be changed in ```constants.py```. 


### Reproducibility in case of failed environment build
It may happen that the conda environment cannot be installed. Below are listed the most important packages and their versions (as described in environment.yml). Please note, however, that differing operating systems, system variables, or hardware may still result in differing outcomes. Nonetheless, due to the procedure of repeating all models where randomness is applied, results should still be fairly similar to those reported in the manuscript.

conda:
- python==3.8.10
- numpy==1.20.3
- pandas==1.3.1
- scikit-learn==0.24.2
- scipy==1.6.2
- matplotlib==3.4.2
- seaborn==0.11.1

pip:
- heartpy==1.2.7
- statsmodels==0.12.2
",2022-08-12
https://github.com/hnunner/hibernate-tryouts,"hibernate-tryouts
=================

Test project for using hibernate in OSGi projects
",2022-08-12
https://github.com/hnunner/ipojo-creationstrategy,"ipojo-creationstrategy
======================

Creation strategies used by iPOJO service injections.
",2022-08-12
https://github.com/hnunner/ipojo-tryouts,"ipojo-tryouts
=============

This project contains different use cases and examples of using iPOJO in the OSGi environment.
",2022-08-12
https://github.com/hnunner/karaf-db-tests,"karaf-db-tests
==============

General tests for databases in Karaf OSGi runtime.
",2022-08-12
https://github.com/hnunner/karaf-debug,"karaf-debug
===========

Shell script to start karaf in debug mode.  

TODO
----
- Edit karaf path.
",2022-08-12
https://github.com/hnunner/maven-kar-archive,"maven-kar-archive
=================

Example how to create karaf kar archives using maven.  

Deployment
----------
* Copy the generated .kar file into [apache-karaf-x.x.x]/deploy folder

CAUTION
-------
Deployment might result in Stacktrace:

    java.lang.VerifyError: Expecting a stackmap frame at branch target 77 in method ...

**Background:**
* http://chrononsystems.com/blog/java-7-design-flaw-leads-to-huge-backward-step-for-the-jvm
* https://issues.apache.org/jira/browse/FELIX-3568  

**Solution:**  
* add ""*export set JAVA_OPTS=""-XX:-UseSplitVerifier""*"" to your .bashrc
* alternative - http://mail-archives.apache.org/mod_mbox/felix-users/201308.mbox/%3C1148955CCC8F2E47AFCE7A1A54C4AB3F01EDB940@sol-rz-ex04.solon.loc%3E
",2022-08-12
https://github.com/hnunner/nidm-simulation,"# Networking during Infectious Diseases Model (NIDM) Simulator

Welcome to the NIDM Simulator project. This project is a stand-alone Java program of the **N**etworking during **I**nfectious **D**iseases **M**odel (Nunner, Buskens, & Kretzschmar, 2021) to simulate the dynamic interplay of social network formation and infectious diseases. The project comes in two versions:
 1. A _GUI_ version that provides an easy to use graphical user interface, allows to play around with parameter settings, and produces a visualization and detailed real-time statistics of the resulting networks. This is suitable to investigate small networks with 50 agents max and test the immediate effects of different parameter combinations.
 2. A _Data Generator_ version that allows to produce large data with many different parameter combinations.

## Installing the NIDM Simulator
The project is a stand-alone software application that can be used in two ways:

### 1. Downloading the project and executing the provided jar-file
This is the easier way to use the simulator and suffices if you want to use the _GUI_‚ version only.
 1. [Make sure Java Runtime Environment (JRE) is installed](https://www.baeldung.com/java-check-is-installed).
 2. [Dowload the JRE](https://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html) and install if necessary.
 3. Download the NIDM project as zip-file by clicking the __Clone or download__ button at the top this page.
 4. Unzip the downloaded file to extract the project into a folder.
 5. Execute the jar-file by double clicking: path.to.unzipped.folder/executables/nidm-`version.number`.jar

### 2. Cloning the project and running it as a Maven project
This way allows to run both the _GUI_ and the _Data Generator_ versions. Further, the code can be changed to realize new features. The following steps are necessary to run the code in the Eclipse IDE:
 1. Download and install the latest version of [Eclipse](https://www.eclipse.org/downloads/).
 2. Clone (or download) the NIDM repository from this page.
 3. Start Eclipse.
 4. In the menu bar of Eclipse go to: _File_ - _Import..._
 5. In the _Import_ window navigate to: _Maven_ - _Existing Maven Projects_ and click __Next__
 6. In the _Import Maven Projects_ window click on __Browse__ and go to the root folder of the cloned (or downloaded) repository. Note: This folder needs to contain a file named _pom.xml_.
 7. Click __Finish__
 8. In the _Package Explorer_ window on the left of Eclipse navigate to: _nidm-simulation_ - _src/main/java_ - _nl.uu.socnetid.nidm.mains_
 9. The location contains two files (_CIDMDataGenerator.java_, _UserInterface.java_) which can be started by right clicking on them and selecting: _Run as_ - _Java application_.

__Note:__ Depending on the settings in _src/main/resources/config.properties_ the _Data Generator_ may generate large amounts of data taking a very long time to finish.

## Using the _GUI_ version of the NIDM Simulator
When starting the _GUI_ version of the NIDM Simulator (by double clicking path.to.unzipped.folder/executables/nidm-`version.number`.jar) two windows appear. The main window allows to define the parameter settings, control the simulation, export network data, and displays the network simulations. The second window shows detailed statistics for the whole network (_Global Stats_) and for a single agent (_Agent Stats_) that can be selected by clicking on the corresponding node in the white network area once agents have been added.

### Running a simulation
Running a minimal simulation requires the following steps:
 1. Define the model and its parameters in the _Model_ tab:
    1. Select the type of model (currently only the _CIDM_ model is available).
    2. Set the parameters to values of interest. The initial settings provide a scenario with high social benefits, risk avoiding agents, and severe infections.
 2. Add agents and start the simulation in the _Simulation_ tab:
    1. Add agents by clicking the __Add agent__ button at the top of the window multiple times.
    2. Start the simulation by clicking the __Start__ button at the bottom of the window.
    3. Infect an agent by either clicking on the __Infect random agent__ button, or activating _On node click:_ - _Toggle infection_ and clicking on a network node.

### Exporting data ###
The simulator allows static and dynamic network exports.

#### Static network exports ####
Static network exports use the data of the network at the time the export is created. In order to ensure that the network structure does not change, it is advised to push the __Pause__ button on the _Simulation_ tab first. Three data types are available:
 * _GEXF_: A detailed graph representation optimized for [Gephi](https://gephi.org/).
 * _Edge List_: A list of all connections between nodes. This file is a plain text file and has no ending by default.
 * _Adjacency Matrix_: A matrix of all nodes with a _1_ at the intersection of two nodes if a tie exists and _0_ otherwise. This file is a plain text file and has no ending by default.

Exporting static networks is the same for all static network types:
 1. Run a simulation as described above.
 2. Pause the simulation by pushing the __Pause__ button on the _Simulation_ tab.
 3. Select the preferred data type in the _Export_ tab. In case of _GEXF_ make sure to select _Network type:_ - _Static_.
 4. Push the __Export__ button and select a folder and file to store the data in.

#### Dynamic network exports ####
Dynamic network exports track detailed network data over the course of time. This format is only supported by the _GEXF_ export type. In order to track the creation and progression of a dynamic network, please follow these steps:
 1. Reset the simulation if necessary.
    1. Pause the simulation if it is running by pushing the __Pause__ button on the _Simulation_ tab.
    2. Remove all agents and ties by pushing the __Clear all__ button on the _Simulation_ tab.
 2. Select _GEXF_ as export type on the _Export_ tab.
 3. Select _Network type:_ - _Dynamic_ on the _Export_ tab.
 4. Select a file to write the dynamic network data to by clicking the __Choose export file__ button on the _Export_ tab. Note that the file ending _.gexf_ needs to be added either here or after export.
 5. Push the button __Start recording__ on the _Export_ tab.
 6. Run a simulation (e.g., as described above).
 7. Push the __Pause__ button on the _Simulation_ tab.
 8. Push the __Stop recording__ button on the _Export_ tab.

Once the __Stop recording__ button as been pushed, the file export is complete and the corresponding file can be opened in [Gephi](https://gephi.org/).

### _GUI_ components

#### Main window ####
The main window consists of two areas. The left area contains tabs to define the parameter settings, control the simulation, and export network data. The right area displays the network simulations.

##### Model tab #####
The model tab contains a drop-down list to select model types (currently only the _CIDM_ model is available). Parameter values of the selected model can be defined using the interactive fields below. These differ according to the selected model type.

##### Simulation tab #####
The simulation tab contains a number of different components to control the network and the simulation:
 * Network controls:
    * Button __Add agent__: adds agents to the network. The amount can be defined in the text field to its right.
    * Button __Remove agent__: removes agents to the network. The amount can be defined in the text field to its right.
    * Button __Infect random agent__: infects a randomly selected susceptible agent.
    * Button __Create full network__: creates ties between all agents.
    * Button __Clear ties__: removes all ties.
    * Button __Clear all__: removes all ties and agents.
 * On node click - defining the action when clicking on a network node:
    * Check box _Show agent stats_: Shows the agent's statistic in the _Statistics_ window
    * Check box _Toggle infection_: Toggles between the different disease states (e.g., susceptible, infected, recovered)
 * Text field _Simulation delay_: controls how fast the simulation is running. The higher the value the slower the simulation.
 * Button __Start__: starts the simulation (network formation and disease transmission).
 * Button __Pause__: pauses the simulation.
 * Button __Reset__: resets the simulation by removing all ties and making all agents susceptible again.

##### Export tab #####
The export tab provides controls for network exports.
 * Drop-down list _Type_: sets the type of data for network exports.
    * _GEXF_: A detailed graph representation optimized for [Gephi](https://gephi.org/).
    * _Edge List_: A list of all connections between nodes.
    * _Adjacency Matrix_: A matrix of all nodes with a _1_ at the intersection of two nodes if a tie exists and _0_ otherwise.
 * If _GEXF_ and _Dynamic_ is selected as _Network type_:
    * Button __Start recording__: sets the starting point of dynamic network recordings.
    * Button __Stop recording__: sets the end point of dynamic network recordings and writes the final export file.
    * Button __Choose export file__: sets the file to export the data to. Please make sure to use _.gexf_ as file ending.
 * If _Edge list_, _Adjacency matrix_, or _GEXF_ and _Static_ is selected as _Network type_:
    * Button __Export__: sets and writes the file to export the data to. Please make sure to use _.txt_ as file ending for _Edge list_ and _Adjacency matrix_ exports, and _.gexf_ as file ending for _GEXF_ exports.

##### Network display #####
The network display displays the simulated network. Nodes are interactive. That is, they can be clicked (i.e., to show agent statistics, to toggle disease states, and/or to move them around).

#### Statistics window ####
The statistics window displays detailed information on the network and the simulation (_Global stats_) and on a single selected agent (_Agent stats_). Agent statistics can be selected by:
 1. Activating _Show agent stats_ for _One node click_ on the _Simulation_ tab of the main window.
 2. Clicking on a node in the network display.

## Using the _Data Generator_ version of the NIDM Simulator
The _Data Generator_ is used to generate and subsequently analyze large amounts of data for various parameter settings.

### Starting the _Data Generator_ ###
To start the _Data Generator_ you need to run the _CIDMDataGenerator.java_ file located in _path.to.nidm.simulator.project.folder/src/main/java/nl/uu/socnetid/nidm/mains/_. This can be done either in an IDE, such as Eclipse (for details see _Cloning the project and running it as a Maven project_), or through a terminal.

### Procedure ###
The _Data Generator_ procedure consists of two stages:
 1. _Data generation_ in which detailed network and disease data is being generated.
 2. _Data analysis_ (optional) in which the generated data is analyzed with standard methods stored in an R-script at __path.to.nidm.simulator.project.folder/analysis/analysis.R_.

### _Data Generator_ configuration ###
The _Data Generator_ uses a configuration file (_path.to.nidm.simulator.project.folder/src/main/resources/config.properties_) for all configurations. Comments indicate whether multiple values are allowed and how list elements need to be divided. The configuration file consists of three sections:
 1. _CIDM configuration_ to define the parameters of the CIDM model.
 2. _Data export configuration_ to define what types of data ought to be exported. Exported data files are stored in _path.to.nidm.simulator.project.folder/data/`date-time-of-data-generator-invocation`_. Possible exports are:
    * _export.summary_: creates a summary of a single simulation run (e.g., parameter settings, disease and network measures just before introducing a disease and at the end of the simulation, network and utility measures of the initially infected agent).
    * _export.summary.each.round_: creates a summary of each simulated round (e.g., parameter settings, network measures, disease states). __Note:__ This option may create very large amounts of data!
    * _export.agent.details_: creates a detailed overview of each single agent (e.g., parameter settings, disease states, utilities, network measures) at the end of each simulated round. If activated _export.agent.details.reduced_ is ignored. __Note:__ This option may create very large amounts of data!
    * _export.agent.details.reduced_: creates a detailed overview of each single agent (e.g., parameter settings, disease states, utilities, network measures) at the end of each simulation run. If _export.agent.details_ is activated _export.agent.details.reduced_ is ignored.
    * _export.gexf_: creates individual dynamic _.gexf_ files for each simulation run.
 3. _Data analysis configuration_ to configure and trigger (_analyze.data=true_) data analysis subsequent to data generation. Please make sure that the correct location of the _Rscript_ executable is set, if _analyze.data_ is set _true_. Analysis results are stored in _path.to.nidm.simulator.project.folder/data/`date-time-of-data-generator-invocation`_.
 
<!-- ## Published versions ##
 * Version 4.1.0 containing code and data for the manuscript ""A model for the co-evolution of dynamic social networks and infectious disease dynamics"": [![DOI](https://zenodo.org/badge/207793330.svg)](https://zenodo.org/badge/latestdoi/207793330) -->

## Third party software ##
 * GraphStream v1.3 library (Pigné et al., 2008) for internal and visual handling of graph dynamics.
 * R version 3.6.0 (R Core Team, 2019) for statistical analyses.
 * lme4 (Bates et al., 2015) for logit regressions.
 * texreg (Leifeld, 2013) for result exports.
 * ggplot2 (Wickham, 2016) for data visualization.

## References ##
 * Bates, D., Mächler, M., Bolker, B., & Walker, S. (2015). Fitting linear mixed- effects models using lme4. Journal of Statistical Software, 67, 1–48.
 * Leifeld, P. (2013). texreg: Conversion of statistical model output in R to LATEX and HTML tables. Journal of Statistical Software, 55, 1–24.
 * Nunner, H., Buskens, V. & Kretzschmar, M. A model for the co-evolution of dynamic social networks and infectious disease dynamics. Comput Soc Netw 8, 19 (2021). https://doi.org/10.1186/s40649-021-00098-9
 * Pigné, Y., Dutot, A., Guinand, F., & Olivier, D. (2008). Graphstream: A tool for bridging the gap between complex systems and dynamic graphs. CoRR, abs/0803.2093 .
 * R Core Team (2019). R: A Language and Environment for Statistical Comput- ing. R Foundation for Statistical Computing Vienna, Austria.
 * Wickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer- Verlag New York.

## Copyright ##
Copyright (C), 2017 - 2020,  Hendrik Nunner (<h.nunner@gmail.com>)

This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>.
",2022-08-12
https://github.com/hnunner/osgi-servlet,"osgi-servlet
============
Purpose
-------

On the one hand this project shows different ways to implement **servlets** in an OSGi environment. On the other hand it shows different ways of **retrieving OSGi services** from the service registry.  

The project is implemented using Maven and consists of several subprojects, with every subproject being a selfcontained example implementation.

Branches might show different ways of implementing a concrete feature.

**CAUTION**: Not all of the examples are meant to be used in the way they are implemented here. A critical look is needed for every example, deciding which approach could be useful for a concrete implementation and which example is not useful at all.  


Todos before deployment (using Apache Karaf)
--------------------------------------------
create config file ""[karaf_home]/etc/org.ops4j.pax.web.cfg"", with the following content

    org.osgi.service.http.port=8080  
    org.apache.felix.http.jettyEnabled=true
",2022-08-12
https://github.com/hnunner/reinforcement-torus,# torus-reinforcement,2022-08-12
https://github.com/hnunner/relavod,"# ReLAVOD - Reinforcement Learning Agents in the Volunteer's Dilemma

Welcome to the ReLAVOD project. This project is a stand-alone R program of reinforcement learning agents interacting in the repeated Volunteer's Dilemma (VOD). The purpose of ReLAVOD is to use reinforcement learning to investigate the role of cognitive mechanisms in the emergence of conventions. The VOD is a multi-person, binary choice collective goods game in which the contribution of only one person is necessary and sufficient to produce a benefit for the entire group. Behavioral experiments show that in the symmetric VOD, where all group members have the same costs of volunteering, a turn-taking convention emerges, whereas in the asymmetric VOD, where one “strong” group member has lower costs of volunteering, a solitary-volunteering convention emerges with the strong member volunteering most of the time. ReLAVOD offers a general framework to test reinforcement learning in the repeated VOD and offers three different classes of reinforcement learning models to test the ability of reinforcement learning to replicate empirical findings.

## Installing and running ReLAVOD
The project is a stand-alone software application that can be run within an active R session.

1. [Download R](https://cran.r-project.org/) and install if necessary.
2. **Either** download the ReLAVOD project as zip-file by clicking the __Code >> Download ZIP__ button at the top this page and unzip the downloaded file to extract the project into a folder, **or** clone the repository to your local machine.
3. Open the file *code/composition.R* in a suitable editor.
4. Adjust the parameter settings in the *fitParameters* function.
5. Run the simulation by invoking the *fitParameters* function.

## Data and analysis
When invoking the *fitParameters* function ReLAVOD will create a folder named *simulation* that contains all generated data. The folder structure is divided into the outputs for each model class (Random, ClassicQ, SequenceX, CoordinateX). These folders are further divided into outputs per single parameter combinations. Each folder contains all raw data for the three types of VOD (symmetric, asymmetric 1, asymmetric 2), interaction patterns, goodness of fit plots, and CSV files for the model parameters and comparisons between model and experimental data.

## Copyright ##
Copyright (C), 2017 - 2021,  Hendrik Nunner (<h.nunner@gmail.com>)

This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>.

## Versions
 - [![DOI](https://zenodo.org/badge/364288507.svg)](https://zenodo.org/badge/latestdoi/364288507)
",2022-08-12
https://github.com/hnunner/VideoPlayer,"VideoPlayer
===========

Tryouts using MPMoviePlayerController
",2022-08-12
https://github.com/hugoquene/emlar2020,"# emlar2020
Booklet on Statistics with R (Basics), accompanying workshop at EMLAR2020
",2022-08-12
https://github.com/hugoquene/KMS-NL,"# KMS-NL
tekstboek over Kwantitatieve Methoden en Statistiek, o.a. gebruikt in cursus Methoden en Statistiek 1 (TL2V17002), BA Taalwetenschap, Universiteit Utrecht. ",2022-08-12
https://github.com/hugoquene/QMS-EN,"# QMS-EN
textbook on Quantitative Methods and Statistics, used a.o. in undergraduate course Methods and Statistics 1 (TL2V17002), BA Linguistics, Utrecht University, the Netherlands. ",2022-08-12
https://github.com/hugoquene/tempo,"# tempo

Brief description: script for measuring speech tempo in Praat. 

This is a modified version of the script named ""Praat Script [for] Syllable Nuclei"",  
Copyright (C) 2008  Nivja de Jong and Ton Wempe.

The original script was reported in:

De Jong, N.H. & Wempe, T. (2009). 
Praat script to detect syllable nuclei and measure speech rate automatically. 
Behavior Research Methods, volume 41, pages 385–390.

The modified script creates an intensity contour of a speech fragment (a Sound must have been selected before calling the script), and then looks for peaks in the intensity contour, which presumably coincide with nuclei of syllables in the speech sound. The syllable nuclei are collected as points in a new ""point tier"" added at the bottom of the TextGrid (the TextGrid associated with the Sound must also have been selected before calling the script). 
Tempo is then expressed as the number of syllables per second (syll/s) or as the ""average syllable duration"" (ASD, s/syll), for each interval of a particular interval tier of the TextGrid. 

The version in this repository has been modified by Hugo Quené (h.quene@uu.nl) in several ways:
- 2010: arguments are pre-set within the script
- 2010: script does not loop over all files in directory, only applies on interactively selected Sound
- 2010: several changes in audio measurements
- 2022: does not only work on entire file but also reports tempo for each interval of interval tier of associated TextGrid

The script requires a running instance of Praat (www.praat.org). 

## Sample of output:

This output was produced using intervals on tier 1 (the interval tier was created by choosing _Sound: To Textgrid (silences)..._ with default settings, which however are not optimal for this example speech recording): 
```
# for entire Sound:
# sound, textgrid, nsyll (syll), dur (s), tempo (syll/s), ASD (s/syll)
kh-ckh-20210921-idwt5havx-web-hd, kh-ckh-20210921-idwt5havx-web-hd, 3590, 1273.684, 2.82, 0.355
# 
# for intervals on tier 1 (silences):
# sound, textgrid, intervaltier, interval, label, nsyll, dur, tempo (syll/s), ASD (s/syll)
...
kh-ckh-20210921-idwt5havx-web-hd, kh-ckh-20210921-idwt5havx-web-hd, 1, 9, silent, 0, 0.950, 0, 0
kh-ckh-20210921-idwt5havx-web-hd, kh-ckh-20210921-idwt5havx-web-hd, 1, 10, sounding, 8, 2.040, 3.92, 0.255
kh-ckh-20210921-idwt5havx-web-hd, kh-ckh-20210921-idwt5havx-web-hd, 1, 11, silent, 0, 1.490, 0, 0
kh-ckh-20210921-idwt5havx-web-hd, kh-ckh-20210921-idwt5havx-web-hd, 1, 12, sounding, 12, 2.100, 5.71, 0.175
kh-ckh-20210921-idwt5havx-web-hd, kh-ckh-20210921-idwt5havx-web-hd, 1, 13, silent, 2, 1.140, 1.75, 0.570
kh-ckh-20210921-idwt5havx-web-hd, kh-ckh-20210921-idwt5havx-web-hd, 1, 14, sounding, 6, 1.540, 3.90, 0.257
```
",2022-08-12
https://github.com/ikryven/ColorPercolation,"# Percolation in coloured networks 

This code reproduces findings on percolation of edge-coloured networks by Ivan Kryven. The code consists of a collection of Matlab scripts.
To start with, run demo_simple_percolation.m or demo_color_dependent_percolation.m



",2022-08-12
https://github.com/ikryven/PolyRandGrpah,"# PolyRandGrpah
supporting code for Kryven, I ""Analytic results on the polymerisation random graph model"" J Math Chem 2017, DOI: 10.1007/s10910-017-0785-1

The original paper is available in Open Access form at http://rdcu.be/uNV1
",2022-08-12
https://github.com/ikryven/RateInference,"# Rate Inference Method

This is a supplementary code for the publication ""How to Upscale The Kinetics of Complex Microsystems"" by Ivan Kryven and Ariana Torres-Knoop. The code consists of a collection of matlab scripts.
To start with, Run demo.m for a demonstration.



",2022-08-12
https://github.com/inibb/DeNovo_Plant_Transcriptome,"# DeNovo Plant Transcriptome Assembly
Building transcriptome assemblies in the absence of a reference genome is as difficult as it sounds. Luckily, technology is always improving, making it possible!

This repository contains two different pipelines to construct transcriptome assemblies _de novo_ that were written as part of my work as a PhD candidate within LettuceKnow consortium. Originally meant to be used for plant transcriptome assembly of mostly homozygous individuals, these can in theory be employed for any other type of organism, although some changes may be required in some steps (willing to adjust it to whatever case!) 


[Schemes Pipelines](/../figures/Pipelines.png?raw=true ""Schemes Pipelines"")

",2022-08-12
https://github.com/inibb/Feature-extraction-from-time-series-ROS-data,"# Feature extraction from time series ROS data
",2022-08-12
https://github.com/isaacalpizar/IntelligentTextbooks,"# IntelligentTextbooks
This project has moved to: https://github.com/intextbooks/ITCore
",2022-08-12
https://github.com/iv4xr-project/aplib,"# iv4xr-core <a name=""top""></a>

* [The build instructions are below](README.md#buildinstr)
* [APIs Javadoc documentation](http://www.staff.science.uu.nl/~prase101/research/projects/iv4xr/aplib/apidocs/)
* [Agent programming manuals and tutorials](./docs/agentprogramming.md)
* [Agent-based testing manuals and tutorials](./docs/agentbasedtesting.md)
* [Code snippets](README.md#snippets)
* [Papers](README.md#papers)
* **Video:** [10m demo of iv4xr](https://youtu.be/Hc8NP4NuHAk).

Despite the packaging name 'aplib' this project should be called *iv4XR-core*, of which aplib is part of.

**Iv4xr-core** (which we will also refer to as the 'Core') forms the core of a bigger framework with the same name: **iv4xr Framework**, and hence the name 'core'. Outside the Framework, both the Core and its aplib part can also be used independently.

Iv4xr Framework aims to provide tools for agent-based automated testing for testing highly interactive systems. The Core provides the underlying agent-based programming and agent-based testing infrastructure.
Our own use case is to use the framework for testing Extended Reality (XR) systems. Within this use case, the framework has been piloted for testing 3D games. The framework itself is generic: it can be used to target *any system* (even systems that are not interactive, such as a collection of APIs) as long as the agents can interface with the system. To do this, an interface needs to be provided, which boils down to implementing a certain interface-like class.



The position of the iv4xr-core in the **Framework** is shown in the picture below:

   <img src=""./docs/iv4xr-architecture.png"" width=""60%"">

The **Core** provides the agent-based testing infrastructure. The pure agent-programming part is provided by a package (in the Core) called **aplib** (shorthand of Agent Programming Library) within the Core. This library is actually general purpose, and can be used to program agents for purposes other than testing.
On top of aplib, the Core adds testing-related functionalities and few other extra functionalities like a common world representation.
Using iv4xr-core can be seen as a special case of using aplib, where you get some testing-specific extra capabilities, such as expressing test oracles.

In the picture above, *setup-2* is a setup where we only use the Core to target a System Under Test (SUT). This is possible, and we will have access to aplib's agent programming to program the test automation.
When the entire framework is ready, *setup-1* uses the entire framework. The 'framework' adds other testing-related tools, such as a model-based testing library, explorative testing using Testar, etc, which are then also at your disposal.

The entire iv4xr-core is a Java library. The simplest setup to use iv4xr is:

  1. Create a Java method m() where you create one agent a.
  1. An agent is typically used to control some environment (e.g. a game). Attach an interface that would connect the agent a to the environment.
  1. formulate a goal for the agent a (e.g. to win the game), and program a tactic to accomplish/solve the goal.
  1. run the method m().

When the agent is used for testing, then we need an agent that is also a test agent; this would have some extra functionalities such as collecting test verdicts. For concrete examples see the tutorials povided [here (general about agent)](./docs/agentprogramming.md) and [here (for test-agent)](./docs/agentbasedtesting.md).

**Features:**

* **Fluent interface** style of APIs.
* BDI-inspired agents: they have their own states (their belief) and goals. The latter allows them to be programmed in a goal-oriented way.
* Combinators for **high level goal and tactical programming**.
* **Multi agent**: programming multiple agents controlling the a shared environment and communicating through channels.
* **Prolog binding**: allowing agents to do prolog-based reasoning to help them solving goals.
* **Bounded LTL**: for expressing temporal properties that can be checked during testing.
* **A model checker**: also to help solving goals, when models of the problem is known.
* Data collection: agents can collect traces for post-mortem data analyses.

Planned features:

* Reinforcement learning
* Search algorithms for solving goals

**Papers** <a name=""papers""></a>

  * Concepts behind agent-based automated testing:
  [_Tactical Agents for Testing Computer Games_](https://emas2020.in.tu-clausthal.de/files/emas/papers-h/EMAS2020_paper_6.pdf)
I. S. W. B. Prasetya, Mehdi Dastani, Rui Prada, Tanja E. J. Vos, Frank Dignum, Fitsum Kifetew,
in Engineering Multi-Agent Systems workshop (EMAS), 2020.

  * The agents' execution loop is explained  in this draft: [I.S.W.B. Prasetya, _Aplib: Tactical Programming of Intelligent Agents_, draft. 2019.](https://arxiv.org/pdf/1911.04710)

  * Extended abstract: [_Aplib: An Agent Programming Library for Testing Games_](http://ifaamas.org/Proceedings/aamas2020/pdfs/p1972.pdf), I. S. W. B. Prasetya,  Mehdi Dastani, in the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2020.

**Manuals, Reference, and Tutorials**: see [above](README.md#top).

#### Some code snippets: <a name=""snippets""></a>

* Specifying a goal. The goal below is solved when offered a value x == 10.

```java
goal(""Magic number is guessed!"").toSolve((Integer x) -> x == 10)
```

* Specifying a tactic. A tactic specifies a program/heuristic for solving a goal. It could look like this:

```java
FIRSTof(guessLowerbound.on_((Belief belief) -> ! belief.feelingVeryLucky() ,
        ANYof(speculate1,
              speculate2,
              ...)
```        

* Creating and configuring an agent in the _fluent interface_ style:

```java
new AutonomousBasicAgent()
    . attachState(new StateWithMessanger() .setEnvironment(new ConsoleEnvironment()))
    . setGoal(topgoal)
    . setSamplingInterval(1000)
```

* Launching an autonomous agent on a new thread:

```java
new Thread(() -> agent.loop()) . start()
```

## Building with Maven  <a name=""buildinstr""></a>

You need Java-11 or higher.

You can run `mvn` (Maven) at the project root to do the things listed below. Maven will put artifacts it produces under the directory `./target` in the project root.

* To compile the project: `mvn compile`
* To run the project unit tests: `mvn test`
* To produce a jar of the project: `mvn package`. This will invoke `compile` and `test`, and then produce a jar containing the whole project. This is the jar you want to use if you want to include in your own project if you want to use `aplib`.
* To generate the javadoc APIs documentation: `mvn javadoc:javadoc`. The resulting documentations can be found in `./target/site/apicdocs`.
* To clean `./target`: `mvn clean`

#### Projects dir. structure

* `./src/main/java` : the root of `aplib` Java source code.
* `./src/test/java` : the root of Java source code of `aplib` unit tests.
* `./docs/manual` : contain some tutorials and documentations.
* `./libs` : external jars provided for convenience. You should not need these jars if you build using Maven. They are needed if you want to work on aplib itself and want to just link the jars immediately.

## License

Copyright (c) 2019, Utrecht University.

`Aplib` is an open source software. It can be used and distributed under the
[LGPL version 3 license](./lgpl-3.0.md).

#### Credits

Contributors:
* Wishnu Prasetya, Naraenda Prasetya, Saba Ansari, Samira Shirzadeh, Fitsum Kifetew.
",2022-08-12
https://github.com/iv4xr-project/difficultysch,"All these settings can be changed in settings.py

- To play the game manually change the variable AGENT to False

- To use an LRTA* agent to play the game set the variable AGENT to True
    
  - The agent is stopped after 1000 runs where the agent gets to the goal

  - While simulating u can use:
      - Key P to pause/unpause
      - Key R to remove/put the cap of 60 actions/sec (Default is on)
      - Key D to activate/remove drawing of the enviorment (Default is on)

  - The variable NOISE can be changed from 0 to 1 to simulate an human error where its value represents the probability of a human error and a random action is chosen

  - The variable ACTIONS represent the actions that the agent is capable of doing:
      - 0 - NOP
      - 1 - RIGHT
      - 2 - LEFT
      - 3 - JUMP
      - 4 - JUMP + RIGHT
      - 5 - JUMP + LEFT

        By default its [1,3,0]

-The map can be changed in the variable MAP and new maps can also be created:
  - '1' - Floor
  - 'P' - Player (2 squares height)
  - 'F' - Flag (8 squares height)
  - '2' - Pipe (2 squares height)
  - '.' - used to simulate empty

    Maps can have any height and width but the reccomended height is 13 squares
",2022-08-12
https://github.com/iv4xr-project/in-story-validator,"# StoryValidator

The Story Validator takes a JSON file outputted from Twine and, by treating the branching narrative as a directed graph, uses a DFS based algorithm to traverse all possible narrative paths and provide insightful data on different story metrics, and design issues encountered, via visual representations.

## Usage
Inside ""Stories"" there's two JSON files with story examples to test the tool

## Other Works Used
Tooltips - by [vegaseat](https://www.daniweb.com/programming/software-development/code/484591/a-tooltip-class-for-tkinter)
twison -  by [lazerwalker](https://github.com/lazerwalker/twison)

## To Do List
- [x] Initial submission
- [ ] Update Readme file
- [ ] Add player personas",2022-08-12
https://github.com/iv4xr-project/iv4ARDemo,"# iv4ARDemo

Demonstrating basic integration of iv4xr to target an ARCore application

# NOTES

* You need connect your real Adroid device to try AR features.

* Gradle complains about ""Unable to make field private final java.lang.String java.io.File.path accessible etc"".
This is likely because you use an incompatible JDK. Goto File > Project Structure. Set the JDK from
there. JDK 16 (or higher?) is not compatible. I use JDK 11.",2022-08-12
https://github.com/iv4xr-project/iv4XR-IntrusionSimulation,"<h1>Nuclear plant intrusion simulation</h1>

Verification of the defense strategy of a critical infrastructure (as a nuclear power plant) against the infiltration of activists.
The defense strategy will be modeled by a specific scenario in the CGE MAEV environment.

**Iv4xr interface :**

This project must be linked with aplib and lab-recruits-api (iv4xrDemo) because it uses their agents, tactics, goals, helperstructures, ...

The intrusionSimulation package resume the principle of sockets and type of requests to communication with the server (which is made in C++).
This enables to avoid wrapping classes between java and C++ or to make a specific interface dedicated to this wrapping.

Request types : DISCONNECT, START, PAUSE, RESTART, AGENTCOMMAND

The data exchange will be done by json on the same principle as lab-recruits demo.
In a second time, if this kind of exchange format slows down the process, we can consider the exchange of data by binary structure.

Config Env will contains general informations about socket configuration or paths.",2022-08-12
https://github.com/iv4xr-project/iv4xr-mbt,"# iv4xr-mbt

The tool mbt requires at least Java 11. To see the online help.

> java -jar mbt-0.0.1-SNAPSHOT-jar-with-dependencies.jar

Tool mbt has three running modes: test generation, execute on sut, and mutation analysis.

# Building

MBT is a Java library that can be built and installed with the Maven tool, in the usual maven-way. The dependency on Evosuite 1.0.6 might be a problem, if the latter fails to build. In that case, a known workaround is to use Evosuite jar-file directly. This can be found in Evosuite Github site. Use the right version as specified in MBT's Maven dependency, and also comment-out that dependency.

# Test generation

Test generation performs the following two steps:
- creates a random EFSM that can be translated into a csv for Lab Recruits  
- performs search based test suite generation for the random EFSM just created
Test generation is triggered with the following command:

> java -jar mbt-0.0.1-SNAPSHOT-jar-with-dependencies.jar -sbt -Dalgorithm=$algorithm -Dsearch_budget=$budget -Dsut_efsm=$sut -Drandom_seed=$seed -Dpopulation=$population -Dmodelcriterion=$criterion -Dshow_progress=$progress

where:
- $algorithm specifies the search algorithm. Valid values are mosa, monotonic_ga,
  steady_state_ga, spea2, and nsgaii, default=mosa;
- $budget is the budget in seconds the algorithm has to generate a test suite, default=60;
- $sut represents some predefined EFSM configuration. Valid values are
  labrecruits.random_simple, labrecruits.random_medium, labrecruits.random_large,
  and labrecruits.buttons_doors_1, default=labrecruits.random_simple. We detail below the configurations;
- $seed is the random seed used by the search algorithm, default=current_time_milliseconds;
- $population is the initial population of the search algorithm, default=50;
- $criterion is the coverage criterion. Valid values are state and transition, default=transition;
- $progress switches on or off the progress bar during the search phase, default=true

# Sut configuration
The random EFSM generator has five parameters that can be changed to define the structure of the model:
- LR_mean_buttons: mean number of buttons in a room
- LR_n_buttons: total number of buttons in the level
- LR_n_doors: number of doors in the level
- LR_seed: random seed for level generation
- LR_n_goalFlags: number of goal flags in the level

Predefined $sut corresponds to the following parameters:
- labrecruits.random_simple: LR_mean_buttons = 0.5, LR_n_buttons = 5,
  LR_n_doors = 4, LR_seed = 325439;
- labrecruits.random_medium: LR_mean_buttons = 0.5, LR_n_buttons = 10,
  LR_n_doors = 8, LR_seed = 325439;
- labrecruits.random_large: LR_mean_buttons = 0.5, LR_n_buttons = 20,
  LR_n_doors = 15, LR_seed = 325439;

Value labrecruits.buttons_doors_1 for $sut corresponds to Lab Recruits predefined
 level buttonDoors1.csv.

To generate a random EFSM with different parameters it suffices to use
labrecruits.random_default as $sut and to pass the parameters to the program.
For instance, a level with 2 buttons per room, 6 buttons, 3 doors, 1 goal flag and seed 555 is
generate with (then runs the test generation on it using default values, see above)

> java -jar mbt-0.0.1-SNAPSHOT-jar-with-dependencies.jar -sbt -Dsut_efsm=labrecruits.random_default -DLR_mean_buttons=2 -DLR_n_buttons=6 -DLR_n_doors=3 -DLR_n_goalFlags=1 -DLR_seed=555

### Test generation output
The output of the tool is saved in folder mbt-files. Subfolder 'statistics' reports
statistics about the test generation. Subfolder 'tests' contains the EFSM and the
tests generated. In particular, the tool creates folder $sut/$algorithm accordingly
with the parameters passed to the tool. A subfolder with a random name is
generated  and contains the tests and the model.


# Demo on the Lab Recruits

To run the tools it is needed to download Lab Recruits application (it is a game) from
https://github.com/iv4xr-project/labrecruits


### Run on sut
To run generated test suites on the Lab Recruits application, the game should be
in a reachable folder, say LR_FOLDER.
The mbt mode exec_on_sut allows running the test suites generated on an EFMS model
on the Lab Recruits game. To run the execution mode type

> java -jar mbt-0.0.1-SNAPSHOT-jar-with-dependencies.jar -exec_on_sut -sut_exec_dir=$SUT_EXEC_DIR -sut_executable=$SUT_EXECUTABLE -tests_dir=$test -agent_name=$agent -max_cycles=$max_cycle

where:
- $SUT_EXEC_DIR is the folder that stores Lab Recruits executable. The tool searches
  for a folder 'gym'. Within 'gym' there should be  a folder named with the OS (Linux,
  Mac, or Windows). Finally, the tool expects the executable in a folder named 'bin'.
  For instance, on an OSX machine, given $SUT_EXEC_DIR=FOLDER, the tool searches in
  FOLDER/gym/Mac/bin/ for Lab Recruits executable;
- $SUT_EXECUTABLE is the path to the csv file representing the generated Lab
  Recruit level. The extension .csv must be omitted
- $test the folder that stores the tests given as output in the generation phase
- $agent: the name for the agent (Agent1 for random generated and agent1 for
  buttonDoors1 level)
- $max_cycle: the maximum number of cycles the agent could take to perform and
  action

### Output of run on sut
In folder mbt-files/statistics/ the tool saves
- execution_statistics.csv: reports the number of tests performed, the number of tests
  failed and the time required
- execution_debug.csv: debug information about the evaluation of each transition
  of each test case

### Run mutation analysis
Mutation analysis takes the csv of a level and the tests generated in the test
generation phase, mutates it and runs exec_on_sut on each mutated level. The
parameters are the same of run on sut mode:

> java -jar mbt-0.0.1-SNAPSHOT-jar-with-dependencies.jar -mutation_analysis -sut_exec_dir=$SUT_EXEC_DIR -sut_executable=$SUT_EXECUTABLE -tests_dir=$test -agent_name=$agent -max_cycles=$max_cycle -Dmax_mutations=MAX_MUTATION

with -mutation_analysis in place of -exec_on_sut and MAX_MUTATION being the maximum
number of mutated levels to run with Lab Recruits.

### Output of run un mutation analysis
File mbt-file/statistics/mutation_statistics.csv reports the number of killed
mutations and the number of tests performed on each mutation.
Folder mbt-files/mutations/ contains a folder for each row of mutation_statistics.csv.
Field run_id corresponds to the name of the folder. For each run, the tool saves the
mutated levels, the execution data of each run. Subfolder 'tests' contains the
executed tests.
",2022-08-12
https://github.com/iv4xr-project/iv4xr-rl-env,"# iv4XR RL Environment library

This library defines a general interface to define Reinforcement Learning (RL) environments through the
iv4XR framework and a connector to work with remote Deep Reinforcement Learning agents in Python.

An example Reinforcement Learning environment is implemented, based on the iv4XR-IntrusionSimulation
system under test.

An illustration of usage is available in the project's Wiki:
https://github.com/iv4xr-project/iv4xr-rl-env/wiki/Results

## Installation and prerequisites

This project requires Java 15.

This project uses as dependencies the Jeromq and GSON libraries with Maven.
This project depends on the iv4xr-core library. See the aplib repository to generate an
executable JAR with maven. This JAR must be added as one of the project's local dependencies.

This project is currently tested with aplib 1.4.0 version.

To use the `intrusion` package and examples, the iv4XR-IntrusionSimulation must be imported
with this project.

## Usage

Examples of usage are provided in the `test` sources.

To use the RLAgentConnectTest, you will also need to run Python RL Agent code from the iv4xr-rl-trainer
package.

## Main Features

This plugin to the iv4xr-core project is aimed at defining Deep Reinforcement Learning environments with
the System Under Test (SUT). It is compliant with the widely used Gym interface in RL Research. A Gym RL Environment
provides the following methods:
```
state = reset()
state, reward, done, info = step(action)
```
To ease writing general algorithms, each environment can also provide a specification of its input/output formats with
the generic space representations. An environment specifies its Observation space and Action space. We support the common
Discrete (a set of N values), Box (a cartesian product of intervals in R^d) and Dict (a key-value map of spaces) 
spaces of Gym. With this plugin, RL States are adapted from the iv4xr Word Object Model (WOM) and RL Actions are translated
as iv4xr SUT commands. The reward and end condition of the environment are defined by a predicate over the RL State,
or directly over the WOM. Thus, the programmer of the RL Environment defines the end goal of the agent, but the agent is free 
to interact with the environment throughout its training procedure. 

## Code architecture

The iv4xr-rl-env plugin is organized with 3 main subpackages:
- The `generic` package describes the main representations of RL Environment and Spaces.
- The `intrusion` package contains the RL training environment associated with the Thales AVS
  powerplant intrusion SUT, providing an example of usage of the generic package.
- The `connector` package allows connecting to a Python DRL agent. The control loop can be managed
  either by the Python DRL agent (i.e. classic Gym control loop, useful for training), or by the
  iv4xr environment (i.e. Policy Server interface, useful for exploitation/deployment).
",2022-08-12
https://github.com/iv4xr-project/iv4xr-rl-trainer,"# iv4XR RL trainer

This repository contains code to connect to Reinforcement Learning environments 
run by the iv4XR framework and train a Reinforcement Learning agent.

## Installation

The `iv4xrl` folder is a small library that manages the interoperability with the
iv4XR framework. It is the Python counterpart connector and Gym environment to
the iv4xr-rl-env JAVA project.
The `iv4xrl` library can be installed locally by running the following command in
its folder.
```
pip install -e .
```

## Usage

The `test_agent.py` script can be run for simple tests of random or deterministic
agent on the iv4XR-controlled environment.
You will need to run the associated JAVA test `RLAgentConnectTest` found in the
iv4xr-rl-env project.

## Code architecture

The `iv4xrl` library manages the usage of iv4XR-based RL Environments as common
Gym environments.

The `trainings` folder contains the implementation of the Deep Reinforcement Learning
algorithms and their adaptation to the Maze environment, a simplified setup of the
powerplant intrusion simulation that shares the same interfaces and logic.

- The TD3 [0] algorithm is used and adapted as the goal-solving Functional Test Agent (FTA)
  for the intrusion use-case.
- The QD-RL [1] algorithm is used and adapted as the behavioural coverage FTA for the
  intrusion use-case.

## Approach and results

The approach and results are detailed in this project's Wiki:
- https://github.com/iv4xr-project/iv4xr-rl-trainer/wiki/Approach-and-Results---Goal-Solving-FTA
- https://github.com/iv4xr-project/iv4xr-rl-trainer/wiki/Approach-and-Results---Behavioural-Coverage-FTA

## References

[0] Fujimoto, Scott, Herke Hoof, and David Meger. ""Addressing function approximation error in actor-critic methods."" International Conference on Machine Learning. PMLR, 2018.

[1] Cideron, Geoffrey, Thomas Pierrot, Nicolas Perrin, Karim Beguir, and Olivier Sigaud. ""QD-RL: Efficient Mixing of Quality and Diversity in Reinforcement Learning."" arXiv preprint arXiv:2006.08505 (2020)
",2022-08-12
https://github.com/iv4xr-project/iv4xr-se-plugin,"<img src=""./JvmClient/docs/iv4xr_logo_1200dpi.png"" align=""right"" width=""15%"">

# iv4XR Space Engineers Plugin

A plugin providing integration of **[Space Engineers](https://www.spaceengineersgame.com/)** to the **iv4XR framework**. You can find the project page at [iv4xr-project.eu](https://iv4xr-project.eu/).

Status: Active development (alpha version); high-level architecture is now mostly stable

## Introduction

Space Engineers is a sandbox game by Keen Software House. This project is a plugin for the game which enables its integration with the iv4XR testing framework. The plugin runs a TCP/IP server with JSON-RPC API. It allows to access surrounding of the player's character in a structured form (a World Object Model, WOM) and to control the character. More features such as controlling some of the game screens have been added during the development. 

<img src=""./JvmClient/docs/SE-sotf1.png"" width=""100%"">

The project includes a full-featured [client in Kotlin](https://github.com/iv4xr-project/iv4xr-se-plugin/tree/main/JvmClient).

## How to run the game with this plugin

It's not necessary to build anything to try out this plugin. This section describes how to do it.

1. Obtain the binary release of Space Engineers (buy it on Steam or get a key). Install the game.
2. Obtain a binary release of **the plugin**. Look for [releases](https://github.com/iv4xr-project/iv4xr-se-plugin/releases) in this repository and for Assets of the chosen release. Download all the DLL libraries.
   1. If you want the latest changes or you'd like to edit the code, you can also build it from the sources (even if you don't have Space Engineers source code), see the section **How to build** below.
3. IMPORTANT: Make sure Windows is OK to run the libraries. **Windows (10+) blocks ""randomly"" downloaded libraries.** To unblock them, right-click each of them and open file properties. Look for Security section on the bottom part of the General tab. You might see a message: ""*This file came from another computer and might be blocked...*"". If so, check the `Unblock` checkbox.
   (If you skip this step, the game will probably crash with a message: `System.NotSupportedException`: *An attempt was made to load an assembly from a network location...*)
4. Obtain other libraries as described in the section **3rd Party Dependencies** below.
5. Put the plugin libraries (and its dependencies) into the folder with the game binaries. A common location is `C:\Program Files (x86)\Steam\steamapps\common\SpaceEngineers\Bin64`.
   Tip: It's you can put the libraries into a subfolder (such as `ivxr-debug`). Or, it can be a symbolic link to the build folder of the plugin project. In that case, you must prefix the name of each library with `ivxr-debug\` in the following step. 
6. Right-click on the game title in the Steam library list and open its properties window. Click on the **Set launch options...** button. Add the option `-plugin` followed by the location of the main plugin library. Library dependencies will be loaded automatically – just make sure they are in the same folder or some other searched location. The resulting options line should look something like this: `-plugin Ivxr.SePlugin.dll`.
7. Run the game. (If the game crashes, make sure you've seen step 3.)
8. If the plugin works correctly, a TCP/IP server is listening for JSON-RPC calls on a fixed port number. (See the config file in user's AppData folder `~\AppData\Roaming\SpaceEngineers\ivxr-plugin.config` for the current port number; the default is 3333).
   Another sign of life is a log file present in the same folder.

#### *3rd Party Dependencies*

Apart from the game libraries, the plugin requires three additional libraries to run:

* `AustinHarris.JsonRpc.dll`, which in turn requires:
* `Newtonsoft.Json.dll`
* `ImpromptuInterface.dll`

There are many ways how to obtain the libraries. For convenience we provide them as special releases in this repository. _They are **not** part of the official releases_ because – among other things – they may have different licenses. Look for ""3rd Party Library Dependencies"" among releases.

Another way how to get the libraries is the following:

* Check-out the [JSON-RPC.NET master branch on GitHub](https://github.com/Astn/JSON-RPC.NET).
  * *Side note: The binary releases are not updated (compared to NuGet packages), but the the last [release v1.1.74](https://github.com/Astn/JSON-RPC.NET/releases/tag/v1.1.74) works as well. You can try it if the master branch does not.* 
* Build the solution including the test project (tested with Visual Studio 2019).
* You will find the `AustinHarris.JsonRpc.dll` library in this path:
  `Json-Rpc\bin\Debug\netstandard2.0`
* And the `Newtonsoft.Json.dll` library in this path:
  `AustinHarris.JsonRpcTestN\bin\Debug\netcoreapp3.0`

Note: If you build the project from the sources as described in the section **How to Build**, the libraries are downloaded via NuGet packages.

## Protocol and API

The network protocol is based on [JSON-RPC 2.0](https://www.jsonrpc.org/specification). JSON-RPC messages are separated by newlines, TCP is used as the transport layer. The protocol (individual APIs) is now more stable than in the beginning of the development, but it's still possible it will change as we learn new things.

For an up to date list of provided API calls see the interface [ISpaceEngineers](https://github.com/iv4xr-project/iv4xr-se-plugin/blob/main/Source/Ivxr.SpaceEngineers/ISpaceEngineers.cs) in the project **`Ivxr.SpaceEngineers`**. 

You can also check out the [JvmClient](JvmClient/README.md) in this repository for client side implementation of the interface in Kotlin and examples how to use it.


## Game mechanics and functionality

This section links to specific, more detailed parts of the documentation which describe functionality relevant for the plugin:

- [Basic information](JvmClient/docs/Basics.MD) about Space Engineers engine for purposes of using the plugin.
- Information about [blocks](JvmClient/docs/Blocks.MD).
- Information about character/vehicle [movement](JvmClient/docs/Movement.MD).
- Controlling the game using [screens](JvmClient/docs/Screens.MD).
- Using the plugin in [multiplayer](JvmClient/docs/Multiplayer.MD).
- [Block ownership](JvmClient/docs/Ownership.MD).
- [Multiple characters](JvmClient/docs/Multiple-Characters.MD) in a single game.
- Using the plugin for [automated testing](JvmClient/docs/Automated-Testing.MD).
- [JvmClient](JvmClient/README.md) in this repository is a client side implementation of the interface in Kotlin.

## Architecture Overview

Overview of the solution projects:

* **`Ivxr.SePlugin`** – The **main plugin project**. Contains most of the important logic. It is one of the plugin libraries, the main one.
  * See the project details below.
* **`Ivxr.SePlugin.Tests`** – Unit tests for the main project.
* **`Ivxr.PlugIndependentLib`** – Contains service code that is *entirely independent of the Space Engineers codebase* for better dependency management and easier testing.
  * Contains some basic interfaces such as the logging interface.
  * It is a secondary plugin library.
* **`Ivxr.PlugIndependentLib.Test`**
* **`Ivxr.SpaceEngineers`** – Contains **the main interfaces and models of the C# API** which is exposed via JSON-RPC.
  * [ISpaceEngineers](https://github.com/iv4xr-project/iv4xr-se-plugin/blob/main/Source/Ivxr.SpaceEngineers/ISpaceEngineers.cs) – the main interface grouping other interfaces such as `ICharacterController`, `ISpaceEngineersAdmin`, and others.
  * Does not depend on the Space Engineers codebase.
  * The TCP/IP server has now been replaced by the JSON-RPC library.

#### Project details: `Ivxr.SePlugin`

List of notable classes – top level:

* `IvxrPlugin` – Entry point of the plugin, implements the game's `IConfigurablePlugin` and `IHandleInputPlugin` interfaces.
* `IvxrSessionComponent` –  Inherits from game's `MySessionComponentBase` which allows the component to hook the plugin into game events such as `UpdateBeforeSimulation` called each timestep of the game.
* `IvxrPluginContext` – Root of the dependency tree of the plugin, constructs all the important objects.

Notable sub-namespaces (and the solution sub-folders):

* `Control` – Interfacing with the game: Obtaining observation and control of the character. Notable classes:
  * `CharacterController` – Self-explanatory.
  * `Observer` – Extracts observations from the game.
  * `Screens` – Implementation of `IScreens` interface for controlling game screens.
* `Session` – Session control such as loading a saved game. Has a separate command dispatcher because it needs to run (in the ""lobby"") even when no actual game is running.
* `WorldModel` – Classes supporting the communication (JSON over TCP/IP).
* `Communication` – Classes for JSON-RPC mappings, thread synchronizations and other tools required for the communication.

## Naming convention

The project contains the plugin in C# and a JVM client in Kotlin. Those 2 languages have different naming conventions.

- Properties and methods in C# usually begin with uppercase (`PascalCase`), but they begin with lowercase in
  Kotlin (`camelCase`).
- Interfaces in C# sometimes have ""I"" prefix to signal interface. We usually refer to interface without the ""I"" in the
  documentation unless directly referencing a class.
- If you see a reference to a variable or property, always interpret it in the context of the particular language.
- Since the server is implemented in C#, JSON-RPC protocol uses C# conventions and all method names and parameters are
  in `PascalCase`.

## How to build

First of all, you don't *have to* build it from sources. There are also binary releases (but, of course, the code is usually more up to date).

The plug-in requires Space Engineers libraries to compile. There are two ways how to provide the libraries: as binaries (DLLs) or as sources. Both options are described below.

The resulting plug-in (a couple of .NET libraries) works with the official Steam version of Space Engineers without any modification of the game.

### How to build with game binaries

We are developing the plugin using source dependencies; therefore, it is necessary to perform a few steps to switch to binary dependencies: provide the library binaries and switch the references to those binaries. We assume you have installed the official release of the game from Steam.

1. **Obtain the Space Engineers libraries.** Locate the script `copydeps.bat` in the `BinaryDependencies` directory.
    1. If you have your Steam installation of Space Engineers in the default path, then just run the script and the binaries will be copied to the directory. The default path is `C:\Program Files (x86)\Steam\steamapps\common\SpaceEngineers\Bin64`.
    2. If you have SE in some other path, provide it as the first argument to the script. (Or copy the libraries listed in the script manually.)
2. **Switch references from project dependencies to binary ones.** You can do it manually, or you can apply (cherry-pick) a commit pointed to by the branch `binary-deps-switch`. (The particular commit changes as we rebase it on newer history.)
3. **Open the solution and build it.** Find the VS solution file `Iv4xrPluginBinaryDeps.sln` in the `Solutions` folder. It's just a solution containing only the plugin projects, not the SE projects – the switch to the binaries has to be done in each of the projects, as described in the previous step. Open the solution and build it. You can than run the game with the plugin as described above.

### How to build if you have SE sources

There's a VS solution file `SpaceEngineers_ivxr.sln` in this repository (in the `Solutions` folder) that contains the plugin projects as well as Space Engineers projects, some of which are dependencies of the plug-in. For this solution file to work, you need to checkout Space Engineers sources to a specific location relative to this Git repository – the relevant branch (such as ""Major"") has to be checked-out into a directory called ""`se`"" located next to the checkout of this Git repository. See the nested list below, which corresponds to the required directory structure:

* `se-plugin` – just a top level directory, can have any name
    * `iv4xr-se-plugin` – a checkout of this Git repository
    * `se` – a checkout of a Space Engineers branch (presumably from it's Subversion repository)

Before starting the build of the solution, make sure a correct build configuration is selected. Either **Debug** or **Release** configuration and the **x64** platform.
",2022-08-12
https://github.com/iv4xr-project/iv4xrDemo,"# <img src=""./docs/iv4xr_logo_1200dpi.png"" width=""20%""> <img src=""./docs/logo.png"" width=""8%""> iv4XR Demo


This is a demo version 2.x for the [iv4XR Agent-based Testing Framework](https://github.com/iv4xr-project/aplib),
demonstrating that iv4XR test agents can control a game called _Lab Recruits_ to perform testing tasks.
The game executable is no longer included in the repository. It might be included in certain releases, or else you need to build it yourself
from its [repository](https://github.com/iv4xr-project/labrecruits).

You will need correct versions of the iv4XR Framework and the game Lab Recruits, which are compatible with this version of Demo. Check the `pom.xml` to know which versions are needed.

   <img src=""./docs/LRSS1.png"" width=""48%""><img src=""./docs/LRSS3.png"" width=""50%"">

**Work in progress notice.** Keep in mind that the work here is still in progress. Things may look ugly while we are working on them, and things may change.

**What is the demo?** A set of JUnit test classes demonstrating how iv4xr test agents are used to implement a number of testing tasks for the game _Lab Recruits_. These classes are located in `src/test/java/agents/demo`. You can simply run them, or modify them yourself. When you run them you don't usually see anything because the tests run pretty fast. You can insert pause-points yourself (e.g. using console-read).

**The Lab Recruits Game.** It is a 3D game (a screenshot is shown above) written for the purpose of testing AI (like the AI of our iv4xr test agents). It features custom level that you can define yourself through a CSV file. Keep in mind that the game is also work in progress. More about this game can be found in its [repository](https://github.com/iv4xr-project/labrecruits).

### Deploying the demo

Build an executable of the [Lab Recruits](https://github.com/iv4xr-project/labrecruits). To do this you can clone or download the source code of this game. Open the project in Unity (note the specific version it needs) and build the executable from there.

For Mac: put the produced `LabRecruits.app` in `gym/Mac/bin`. For Windows: put the produced files including `LabRecruits.exe` in `gym/Windows/bin`.

The demo classes are in `src/test/java/agents/demo`. The demos are by default non-visual (you don't literally see the game runs). Set the variable `TestSettings.USE_GRAPHICS` in the corresponding demo-class to `true` if you want it to be visual.

* Eclipse

   This will allow you to run/modify/rerun the demo classes. Import the project into Eclipse as a **maven project**. The demo classes are in `src/test/java/agents/demo`. You can run them as junit tests.

* Maven

   This is if you just want to check that the project builds and that all its tests pass.
   Just do `mvn compile` and `mvn test` at the project root.

### Other documentations

* For iv4xr team: [World Object Model](./docs/Observation.md)
* For iv4xr team: [where to find goals and tactic](./docs/LRtestingLib.md)
* For others: [basic interface to control _Lab Recruits_](./docs/BasicInterface.md)

### What's in the package

* `./src` the source files. It follows Maven's convention, so the root of the source files is in `src/main/java` and the root of tests' source files is in `src/test/java`.
* `./src/test/resources/levels` contain sample level definitions for _Lab Recruits_.


### Contributors

**Computer Science students from Utrecht University:**
Adam Smits,
August van Casteren,
Bram Smit,
Frank Hoogmoed,
Jacco van Mourik,
Jesse van de Berg,
Maurin Voshol,
Menno Klunder,
Stijn Hinlopen,
Tom Tanis.
**Others:** Wishnu Prasetya, Naraenda Prasetya.
",2022-08-12
https://github.com/iv4xr-project/iv4xrdemo-efsm,"Extended finite state machine interface for the LabRecurites demo of iv4xr project.
",2022-08-12
https://github.com/iv4xr-project/iv4xrDemo-space-engineers,"# ⚠ This project has been moved elsewhere!

**⚠ This project has been moved into the plugin repository**: [Space Engineers iv4XR plugin](https://github.com/iv4xr-project/iv4xr-se-plugin). You can find it in the [JvmClient](https://github.com/iv4xr-project/iv4xr-se-plugin/tree/main/JvmClient) subdirectory of the plugin repository. **⚠**

The merge of the repositories allows us to do interface changes across both repositories in a single pull request and keep the projects compatible.

(This repository will be removed in the future.)

.

.

.

# Original readme follows

## Space Engineers Demo

This is a demo for the [iv4XR testing framework](https://github.com/iv4xr-project/aplib), demonstrating that iv4XR test agents can control [_Space Engineers_](https://www.spaceengineersgame.com/) (a game by [Keen Software House](https://www.keenswh.com/)) to perform some testing tasks. This repository started as a fork of the [*Lab Recruits* demo](https://github.com/iv4xr-project/iv4xrDemo), but has been significantly modified since.

It is not intended for general use, other than as a testing project for the development of the [Space Engineers iv4XR plugin](https://github.com/iv4xr-project/iv4xr-se-plugin). For more details, please refer to the plugin repository README. 

# Setup

## How to build

For easy copy-paste, here's the git clone command for this repository:

```
git clone git@github.com:iv4xr-project/iv4xrDemo-space-engineers.git
```

We are using Gradle as the build system. To build the project, run the Gradle task `build`:

```
./gradlew build
```

## How to run unit tests

To build and run unit tests, run:

```
./gradlew :cleanJvmTest :jvmTest --tests ""spaceEngineers.mock.*""
```

## Running iv4xr tests

The tests require Space Engineers running with the iv4XR plugin enabled.


```
./gradlew :cleanJvmTest :jvmTest --tests ""spaceEngineers.iv4xr.*""
```


## Running BDD feature tests

Test scenarios also require Space Engineers running with the iv4XR plugin enabled.

For now, we run BDD tests from IDEA.

* Make sure you have installed [plugins](https://www.jetbrains.com/help/idea/enabling-cucumber-support-in-project.html#cucumber-plugin) `Gherkin` and `Cucumber for Java`
* Right-click [.feature file](https://github.com/iv4xr-project/iv4xrDemo-space-engineers/tree/se-dev/src/jvmTest/resources/features) in IDEA and select ""Run"".

## Using Eclipse

Eclipse does not support Kotlin multiplatform projects and so far we haven't been able to configure it to run with Kotlin JVM.
We recommend using the project with JetBrains [IDEA](https://www.jetbrains.com/idea/download/).
",2022-08-12
https://github.com/iv4xr-project/iv4xrProjectTemplate,"# A template for iv4XR agent-based tests

This small project contains four classes that give you a minimalistic template of what need to be implemented to build an interface between iv4XR agents and your System Under Test (SUT), and then showing a simple test.

It contains the following:

   1. The class  `MyEnv`. This is the interface between the agents and the SUT. You need to implement this. It shows a couple of methods, but in a minimum setup, only one of them needs to be redefined by you, namely: `sendCommand_()`.

   2. The class `TacticLib` you will find a handful of typical tactics to
   implement. Tactics are needed so that your agents can operate at much higher level of control rather than literally at the level of primitive actions provided by `MyEnv`. You will need to provide the implementation
   of these tactics.

   3. The class `GoalLib`, provides a number of typical goal-structures that you can use for composing your testing tasks. You will need to implement at least some of them.

   4. The class `A_SimpleExample_of_Test_Using_TestAgent` shows a single test (with JUnit), using an iv4XR test-agent and components named above. Note that this class does not include the deployment of the SUT itself, since I don't have your SUT :) You need to add some code for launching the SUT so that your `myEnv` can talk to it.


Good luck! :)   
",2022-08-12
https://github.com/iv4xr-project/JLabGym,"# <img src=""./docs/iv4xr_logo_1200dpi.png"" width=""20%""> <img src=""./docs/logo.png"" width=""8%""> JLabGym

This package/project `JLabGym` provides a Java-based environment that will allow you to use the game [Lab Recruits](https://github.com/iv4xr-project/labrecruits) as an 'AI Gym'. An _AI Gym_ is an environment where you can try out your AI, or whatever algorithm X, to perform some tasks in the environment. JLabGym provides a set of methods to control the Lab Recruits game and to obtain its internal state information. The Lab Recruits game itself  allows you to design your own game-level through a simple CSV file, and hence you can design your own specific level layouts, puzzles, hazards, and goals/tasks that your AI can try.

<img src=""./docs/LRSS1.png"" width=""48%""><img src=""./docs/LRSS3.png"" width=""50%"">

With JLabGym you can programmatically control the player characters in Lab Recruits.

<img src=""./docs/pc.jpg"" width=""20%""><img src=""./docs/pc.jpg"" width=""15%""><img src=""./docs/pc.jpg"" width=""10%""><img src=""./docs/pc.jpg"" width=""5%"">

You can instruct a controlled character to report back to you what it observes, to move, or to interact with a certain in-game entity.

### The Lab Recruits Game

The game allows you to load a game-level and play it. It is a single player game, but you can control multiple player-characters. A game level can be setup to contain one or more goal-flags that you have to reach to earn points. However, these are typically placed in rooms with closed doors. To open a door you have to find a matching button and toggle it. Oh, some parts of building might be burning with fire. Avoid fire if you can, as it hurts you.  For more on how to play the game and how to make your own level, see [the game's site](https://github.com/iv4xr-project/labrecruits).

JLabGym allows to to programmatically control player-characters.

### Before Using JLabGym

1. You first need to get [Lab Recruits](https://github.com/iv4xr-project/labrecruits) from its Github and build it. You'll need Unity to build it.

  The game requires a game-level to be loaded. `JLabGym` contains some test-levels you can try, here: [`src/test/resources/levels`](./src/test/resources/levels). You can for example try to play (as a human) the `buttons_doors_1.csv` level. The game is not set to have a particular goal (there is no end boss) other than to collect points (and staying alive). However, you can simulate an end goal (or goals) by placing goal flags in your level definition. See the  [site of Lab Recruits](https://github.com/iv4xr-project/labrecruits), it has a wiki explaining the control to play the game as a human, and also a part that explains how to construct your own level with a csv file.

1. You also need [iv4XR Framework](https://github.com/iv4xr-project/aplib). Get it from Github. It is a Maven project, so building it should be straight forward.

### Using JLabGym

After you build Lab Recruits, now you are ready to use `JLabGym`. Below is a simple example showing how to control Lab Recruits from `JLabGym`. The full example can be found in the class [`Example1`](./src/main/java/examples/Example1.java).

#### 1. Let's start with what you need to import:

```java
// From JLabGym:
import environments.LabRecruitsConfig;
import environments.LabRecruitsEnvironment;
import environments.SocketReaderWriter;
import game.LabRecruitsTestServer;
import game.Platform;
import world.LabWorldModel;
import world.LabEntity;
// From the iv4XR package that JLabGym uses:
import eu.iv4xr.framework.spatial.Vec3;
```
#### 2. Launch Lab Recruits from Java

Locate first the parent directory where your Lab Recruits executable is placed. When you built it, Unity will typically place the produced executable in a structure like this:

```
(some executable root)
   |-- Windows
   |   |-- bin
   |-- Mac
   |-- Linux     
```

You need the path to the ""some executable root""; specify this path in the code below. The code will then launch an instance of the Lab Recruits game:

```java
String labRecruitesExeRootDir = ... ;
LabRecruitsTestServer labRecruitsBinding = new LabRecruitsTestServer(
      true, // false if you want to run the game without graphics
      Platform.PathToLabRecruitsExecutable(labRecruitesExeRootDir));
labRecruitsBinding.waitForGameToLoad();
```


#### 3. Load a level and create an instance of [`LabRecruitsEnvironment`](./src/main/java/environments/LabRecruitsEnvironment.java).

This environment will bind itself to the running instance of Lab Recruits (that you just launched above). Once you have this environment, you can use it to control Lab Recruits.

```java
var config = new LabRecruitsConfig(
  ""moveToButton"", \\ the name of the level to load
  ... , \\ the directory where levels are placed
  );
LabRecruitsEnvironment environment = new LabRecruitsEnvironment(config);
```

#### 4. Now use the env to control the game

**Asking observation.**
For example, we can ask for what the agent (the player character that you control) sees, e.g:

```java
LabWorldModel wom = environment.observe(""agent0"");
Vec3 p = wom.getFloorPosition() ; // get the agent position
System.out.println(""Agent is now at "" + p) ;
```

Observation is given as a structured information (so, not as an image!) of type [`LabWorldModel`](./src/main/java/world/LabWorldModel.java), which in turn is a subclass of iv4XR's generic representation of a worl called [`WorldModel`](https://github.com/iv4xr-project/aplib/blob/master/src/main/java/eu/iv4xr/framework/mainConcepts/WorldModel.java).

To ask for an observation, you need to know the Lab Recruits' id of the agent; you can find information in the file that defines the level you load (`moveToButton.csv` in the above example).

We can also ask the state of a game-entity, if it is visible to the agent. You have to know Lab Recruits' id of the entity. You usually can find this in the level definition file.

```java
LabEntity e = wom.getElement(""button1"") // null if the button is not visible
Vec3 q = e.getFloorPosition() ;
boolean isOn = e.getBooleanProperty(""isOn"") ;
System.out.println(""Position of button1:"" + q) ;
System.out.println(""The button is turned-on:"" + isOn) ;
```

**Moving the agent.** Below we show how you can move the agent towards a certain destination. You can do this by invoking the method `moveToward(p1,p2)` from the environment; `p1` is the agent current position, and `p2` is the destination. This will move the agent in a straight line towards `p2`. Note that the agent is bounded by its movement speed, so it cannot reach `p2` in a single `moveToward` if `p2` is far. Also, `moveToward` assumes that the line segment it tries to move the agent is free from obstacle. For example, the agent cannot move through a wall.

```java
Vec3 destination = new Vec3(1.5f,0,4f) // (x,y,z) position with y on the vertical axis.
LabWorldModel wom = environment.observe(""agent0"");
environment.moveToward(agent, wom.position, destination);
Thread.sleep(50); // give some delay, then move again:
wom = environment.observe(""agent0"");
environment.moveToward(agent, wom.position, destination);
```

**Interacting with an entity** In Lab Recruits you can interact with in-game buttons. A button can be in the on-state, or off. Toggling a button might open a door (or more), or close it again. Interacting with a button requires your agent to stand close enough to the button (<= 1.0).

<img src=""./docs/button.png"" width=""10%""> <img src=""./docs/door.png"" width=""10%"">

Assuming your agent stands close enough to button1, we can do this:

```java
LabWorldModel wom = environment.observe(agent) ;
boolean stateBefore = wom.getElement(""button1"").getBooleanProperty(""isOn"") ;
System.out.println("""" + stateBefore) // false
wom = environment.interact(""agent0"", ""button1"", """");
boolean stateAfter = wom.getElement(""button1"").getBooleanProperty(""isOn"") ;
System.out.println("""" + stateAfter) // true
```


### Other documentations

* [Controlling and observing Lab Recruits](./docs/ControlAndObservation.md)
* [Navigation mesh](./docs/navigation.md)


### Copyright and License

Copyright (c) 2021, Utrecht University (Department of Information and
Computing Sciences).

License: GNU LGPL.

### Contributors

**Computer Science students from Utrecht University:** Adam Smits, August van Casteren, Bram Smit, Frank Hoogmoed, Jacco van Mourik, Jesse van de Berg, Maurin Voshol, Menno Klunder, Stijn Hinlopen, Tom Tanis. **Others:** Wishnu Prasetya, Naraenda Prasetya.
",2022-08-12
https://github.com/iv4xr-project/jocc,"## JOCC

This is an implementation of an appraisal transition system for event-driven emotions, based on the [OCC theory of emotions](https://doi-org.proxy.library.uu.nl/10.1017/CBO9780511571299). The main use case of this library is for automated player experience (PX) testing. The implementation includes 6 out of 22 of the emotion types in the OCC theory, which are most directly relevant for PX testing.

The formal theory behind this implementation is presented in this paper:

[__An Appraisal Transition System for Event drivenEmotions in Agent-based Player Experience Testing__](https://arxiv.org/abs/2105.05589),
   Saba Gholizadeh Ansari,
   I. S. W. B.Prasetya,
   Mehdi Dastani,
   Frank Dignum,
   Gabriele Keller. Utrecht University.
   Presented in the 9th International Workshop on Engineering Multi-Agent Systems (EMAS), 2021.



## License

Copyright (c) 2021, Utrecht University.

`Jocc` is an open source software. It can be used and distributed under the [LGPL version 3 license](./lgpl-3.0.md).
",2022-08-12
https://github.com/iv4xr-project/labrecruits,"# __Lab Recruits__

![screenshot](https://github.com/iv4xr-project/labrecruits/wiki/uploads/images/LRSS3.png)

**Lab Recruits** is a 3D computer game intended for testing AI. It is work in progress as we work on adding more game features. You can create your own multi-floor level using a CSV file to challenge your AI. A level is in principle a maze with decorative and dynamic game objects. Dynamic game objects include doors, switches to operate doors, fire, and goal flags. Switches and doors can be linked in many-to-many relations to create a challenging puzzle. The game does not have an explicit end-game plot, other than to collect as many points as possible (and surviving the level). Since there is a maximum in how much point can be obtained in a level, reaching the maximum point can be interpreted as finishing the level.

For more information check the wiki.

#### To Build

The project was developed for Unity 2019.2.6f1, so you need this version to build the project.

Open the project in Unity. From there, there should be a menu item to build the executable. The game can be built for Windows, Mac, and Linux.

### Manual

How to play the game, how to create a level: see the wiki.

#### Contributors

**Computer Science students from Utrecht University:** Adam Smits, August van Casteren, Bram Smit, Frank Hoogmoed, Jacco van Mourik, Jesse van de Berg, Maurin Voshol, Menno Klunder, Stijn Hinlopen, Tom Tanis. **Game Artists from Mediacollege Amsterdam:** Quinto Veldhuyzen, Sophie Meester.
**Later also:** Wishnu Prasetya, Naraenda Prasetya.
",2022-08-12
https://github.com/iv4xr-project/labrecruits-levelgenerator,# labrecruits-levelgenerator,2022-08-12
https://github.com/iv4xr-project/NetCall,"# NetCall
",2022-08-12
https://github.com/iv4xr-project/PAD_emotion_game,"# PAD_emotion_game
This repository contains a simple 2-D game used to train emotional agents along with the code necessary for generating such agents. In this repository can be found the code for the game (infinite_game.py), the code for training a predictive machine learning model (predictor.py), the code for deplying AI agents on said game (rule_based_agents.py) and the data collected from the fist user study conducted using this game and continuous annotation of the 3 dimensions of PAD model of emotion (First_Study folder).

## Running the Game
 
To run the game, run the ""infinite_game.py"" file. One way to do so is to write ""python3 infinite_game.py"" on the command line while in the project's directory.

The game can be run in several different ways. Currently, this is done by commenting and uncommenting blocks of code at the end of the ""infinite_game.py"" file. These blocks of code are easily identifiable by their headers. An example block of code can be seen here:

![Alt text](Images/block_code.png?raw=true ""Block of Code"")

The possible modes of running the game as are follows:

1. Reproduce the original data collecting experiment. This will have the user answer a number of questions and then play three levels of the game, annotating one of the PAD dimensions after each level. The experiment was in Portugal, as such, all text is written in Portuguese.
2. Play and annotate a single level.
3. Playing all the traces which are present on the ""Generated_Traces"" folder. Such traces can be hand made, collected or generated using the functions present in the ""trace_generator.py"" file.
4. Play a single level using a previously trained behavioural model. The behavioural_trainer() function in the ""predictor.py"" file can be used to train a new behavioural model using the traces of the user study. This takes a while and requires that the aforementioned traces are re-played following 2. since new data needs to be collected from the traces which wasn't originally collected during the study.
5. Play a single level using one of the agents defined in the ""rule_based_agents.py"" file. This is the method that will run by default, using a parametrized agent. To experiment with the different possible behaviours for the agent, one needs only to alter the parameter list defined on the ParameterAgent class in the ""rule_based_agent.py"" file.


## The Collected Traces

Under the folder ""First_Study"", you can find several gameplay traces. These traces were collected as part of a study where users were asked to play 3 different levels of the game and then report their levels of the PAD emotional dimensions. The levels played can be found on the ""Maps"" directory and were the ""Level1.csv"", ""Level2.csv"" and ""Level3.csv"".

The traces are divided in 3 folders: one for each of the PAD emotional dimensions.

There are different types of traces in the folders, which can be identified by the name of the file. The final part of the name is always a numeric unique identifier of the player preceded, when appliable, by the game map that the trace corresponds to. 

* **Answers_Answers**
  * These files contain answers to a number of questions. The questions are in portuguese and can be found on the file ""Questions.txt""
* **Answers_Order**
  * These files describe the order on which the identified player played the 3 levels.
* **Traces_Actions**
  * These files describe the actions taken by the player at each tick of the game. The actions are key presses or releases. For example, ""dd"" means that the key 'D' was pressed down, whereas ""du"" means que key 'D' was pressed up, that is, was released. ' ' means that the spacebar was pressed.
* **Traces_DIMENSION_NAME**
  * These files have the reported value of the corresponding emotional dimension for the given level and player.
* **Traces_Perceptor**
  * These files have the values for each tick of the game of all the collected input variables that were considered relevant for training the predictor.
* **Traces_Position**
  * These files have the x and y coordinates of the player throughout the trace 

The names of the trace files end up with a unique number identifier, including the date of the trace, which can be used to identify the player. The name also has, when applicable, the corresponding level.


## Training a PAD Prediction Model Using the Traces Provided

To train a predictive model based on the emotional traces, run the ""predictor.py"" file. One way to do so is to write ""python3 predictor.py"" on the command line while in the project's directory.
This will train several models using different parameters and provide the accuracy and confusion matrices for all of them.


",2022-08-12
https://github.com/iv4xr-project/rl-behaviors-verification,# rl-behaviors-verification,2022-08-12
https://github.com/iv4xr-project/TESTAR_iv4xr,"## iv4xr project

This project contains the first version of the integration of the TESTAR tool with the iv4xr framework.

TESTARtool development repository: https://github.com/TESTARtool/TESTAR_dev

### TESTAR-iv4xr distributed binaries

Release version 3.0: https://github.com/iv4xr-project/TESTAR_iv4xr/releases/tag/v3.0

Requirements for distributed version:
- Windows 10 OS
- Java 11 to Java 14 (With Java 15 it is not possible to modify the protocol at the moment)
- OrientDB 3.0.34 to infer the State Model (more info about State Model below)

TESTAR current execution and functional modes:
- Generate to launch and test iv4xr SUTs
- View to open the HTML report for visualization
- Spy, Record and Replay works with desktop and web applications, but not yet with iv4xr systems (so TESTAR will throw an exception :D)

### LabRecruits SUT

LabRecruits SUT:
- ``testar\bin\suts\gym\Windows\bin`` contains LabRecruits demo game
- ``testar\bin\suts\levels`` contains LabRecruits demo levels

TESTAR SUT specific protocols allow users to define how the tool connects and interacts with the SUT. 
These protocols are a set of directories inside ``testar\bin\settings`` that contain a java protocol and test.setting file, 
on which it is possible to add new directories (with java + test.setting) to create additional protocols.

By default there are 7 protocols with 7 different implementations, 
which can be modified or used to create additional protocols:
- ``labrecruits_commands_agent_listener``
TESTAR is listening the Goal agents primitive commands to infer a StateModel
- ``labrecruits_commands_testar_agent_dummy_explorer``
TESTAR is an agent that randomly explores (without a navigational map) the environment using primitive commands.
- ``labrecruits_commands_testar_agent_navmesh_explorer``
TESTAR is an agent that randomly explores the environment using primitive commands and the intenral navigational map.
- ``labrecruits_emotional_agent``
Demo protocol to integrate the emotional agent properties inside the TESTAR State Model
- ``labrecruits_goal_agent_listener_complete``
TESTAR is listening the complete Goal agents tactics to infer a StateModel
- ``labrecruits_goal_agent_listener_tick``
TESTAR is listening the deliberation ticks Goal agents tactics to infer a StateModel
- ``labrecruits_goal_testar_agent``
TESTAR is an agent that randomly explores the environment using the LabRecruits Goals.

With TESTAR GUI (SUTConnectorValue test.setting) we need to indicate to the tool:
- Where the LabRecruits executable is located
- Where the LabRecruits levels are located
- Which level we want to test

Inside TESTAR Java Protocol (Edit protocol) we can modify the behaviour of the tool,
and the LabRecruits agent goals (Example: labrecruits_goal_agent_listener_complete):
- Imagine we changed the LabRecruits level we want to test
- Click Edit protocol
- Go to beginSequence method
- Change the goal testing-task
- Compile and close

### SpaceEngineers SUT

Before execute TESTAR the user has to:
- Install SpaceEngineers game from Steam
- Follow the dll plugins instructions to enable the iv4xr game-framework communication
https://github.com/iv4xr-project/iv4xr-se-plugin
- Start the level on which we want to execute TESTAR tool

At the moment we only have 1 protocol by default:
- ``se_commands_testar_dummy``
TESTAR is an agent that randomly explores (without a navigational map) the environment using primitive commands.

With TESTAR GUI (SUTConnectorValue test.setting) we need to indicate to the tool:
- The SpaceEngineers process name (SpaceEngineers.exe)

The internal framework-game plugin communication is integrated by default.

## State Model / Graph database support
TESTAR uses orientdb graph database https://www.orientdb.org/ , to create TESTAR GUI State Models.
Detected Widget's, Actions, States and their respective relations are recorded to this graph database.

### Download OrientDB 3.0.34 GA Community Edition (August 31st, 2020)
https://www.orientdb.org/download

https://s3.us-east-2.amazonaws.com/orientdb3/releases/3.0.34/orientdb-3.0.34.zip

``Warning: Since August 2020 there is version 3.1.X of OrientDB, however TESTAR currently requires the use of versions 3.0.X``

### TESTAR remote connection issue

Due to an issue with the remote connection to OrientDB (https://github.com/iv4xr-project/TESTAR_iv4xr/issues/1), it is necessary to use the **plocal** connection to infer the State Model.

### Install and configure OrientDB Server
In order to use the State Model feature it's advised to install a graph database on your machine or in a remote server.

Follow the installation instructions about how to configure TESTAR State Model on slide 28:

https://testar.org/images/development/TESTAR_webdriver_state_model.pdf 

Also TESTAR HandsOn (Section 6) contains more information about State Model settings:

https://testar.org/images/development/Hands_on_TESTAR_Training_Manual_2020_October_14.pdf

When orientdb is started the first time. The root password needs to be configured. Make sure you remember this password.

In order to use the graphdb feature. A database must be created in OrientDB. To do this follow the following procedure:
- Start the database server (ORIENTDB_HOME/bin/server.bat)
- Start orientdb studio in a webbrowser [http://localhost:2480](http://localhost:2480)
- Choose ""New DB"" and provide the name, root user and password. (The database will also get a default admin/admin  user/password).
- Go to Security tab and create a new user (testar/testar) with an active status and the admin role

### Using OrientDB graphdb on the local filesystem
OrientDB graph database can be used remotely or locally.
Default TESTAR settings are predefined to connect with remote mode to a local OrientDB server:

		StateModelEnabled = true
		DataStore = OrientDB
		DataStoreType = remote
		DataStoreServer = localhost
		DataStoreDB = testar
		DataStoreUser = testar
		DataStorePassword = testar

Also is possible to connect at file level without deploy the OrientDB locally:

		StateModelEnabled = true
		DataStore = OrientDB
		DataStoreType = plocal
		DataStoreDirectory = C:\\Users\\testar\\Desktop\\orientdb-3.0.34\\databases
		DataStoreDB = testar
		DataStoreUser = testar
		DataStorePassword = testar


## TESTAR-iv4xr development

This is the github development root folder for TESTAR development. 
The software can be build with gradle.

### Import Gradle project into Eclipse (similar for other IDEs with Gradle)

1. Create a new empty workspace for Eclipse in a folder which is not the folder that contains the source
code.
2. Select File -> Import to open the import dialog
3. Select Gradle -> Existing Gradle project to open te import dialog 
4. Select the folder that contains the root of the source code and start the import

It should be possible to build the project using the instructions provided in the next section

### windows.dll (Allows TESTAR execution on Windows)

TESTAR uses the ``windows.dll`` library to make calls to the Windows 10 systems.

By default there is a ``windows.dll`` inside ``\testar\resources\windows10\`` directory, 
which is copied when we create a default distributed versions avoiding creating a new windows.dll in each compilation.

If we add or modify functionality on the interaction with Windows 10 environments, 
we will need to compile a new windows.dll instead of use the copy of the current version.

### Gradle tasks

`gradlew` is the instruction to use the gradle wrapper. 

This basically means that TESTAR will download in the system, and will use to compile, 
the gradle version indicated inside `TESTAR_dev\gradle\wrapper\gradle-wrapper.properties`

`gradle` uses the environment variables to use the gradle version of the system.

#### gradle build (Files Compilation)
``gradle build`` task : is configured to compile TESTAR project at Java level for error and warning checking.

NOTE: This task also automatically downloads the labrecruits game


#### gradle iv4xrDefaultDistribution (copy default windows.dll)
``gradle iv4xrDefaultDistribution`` task : uses the default ``windows.dll`` to prepare the distributed version.

NOTE: Use this task to create a distributed TESTAR version without the need of Visual Studio.

This task will also execute ``downloadAndUnzipLabRecruits`` task : to download LabRecruits game from [github LabRecruits](https://github.com/iv4xr-project/TESTAR_iv4xr/releases/download/v3.0/labrecruits_2.1.4_windows_30fps.zip).

Distributed files created inside:
- ``testar\target\install\testar\bin``
- ``testar\target\distributions\testar.zip``


#### gradle iv4xrWindowsDistribution (creates a new windows.dll)
``gradle iv4xrWindowsDistribution`` task : uses the ``Required tools to create a new TESTAR windows.dll`` (see below) to create a new file `windows.dll`, which has preference over the default one.


### Required tools to create a new TESTAR windows.dll

In order to build the native code, a view manual steps need to be executed;

1. In order to build the windows native code, Nmake and the compile for Microsoft visual studio are required.
These tools can be downloaded using the following [link](https://www.visualstudio.com/thank-you-downloading-visual-studio/?sku=BuildTools&rel=15#).
2. Install the Visual Studio tools on your machine (remember the path where the tools are installed)
3. Download [compile_w10.bat](https://github.com/florendg/testar_floren/releases/download/PERFORMANCE/compile_w10.bat) 
and [clean_w10.bat](https://github.com/florendg/testar_floren/releases/download/PERFORMANCE/clean_w10.bat)
4. Copy clean.bat and compile.bat to the folder windows/native_src within the TESTAR project
5. Adapt compile.bat and clean.bat. Set *PATH* to the installation folder used in step 2.
CALL ""C:<*PATH*>\2017\BuildTools\Common7\Tools\VsDevCmd.bat"" -arch=x64


#### gradle windowsDistribution (Allows TESTAR execution on Windows)
`gradle windowsDistribution` task : uses the `Required tools to build the software` (see above) to create a new file `windows.dll`, which has preference over the default one.

NOTE: TESTAR requires Visual Redistributable which can be downloaded from the following
 [link]( https://go.microsoft.com/fwlink/?LinkId=746572 ). Also a JAVA 1.8 JDK is required.

#### gradle installDist (Create TESTAR Distribution)
`gradle installDist` task : creates a runnable TESTAR distribution in the `\testar\target\install\testar\bin\` directory.
By default, `windows.dll` should be copied from `\testar\resources\windows10\` directory and overwritten by the new dll file if the `gradle windowsDistribution` task was executed.

1. Run `.\gradle installDist` in the root of the project, or `TESTAR_dev -> distribution -> installDist` with the IDE
2. Change directory to `\testar\target\install\testar\bin\`
3. Run testar.bat

#### gradle distZip (Creates a TESTAR Distribution)
It is also possible to generate a zip file containing TESTAR. This zip can be extracted on any other machine
that has a 64-bit Windows operating system and Visual Studio redistributable installed. A proper way of using
TESTAR is to run the tool in a virtual-machine.
To build the zip execute the following command.

1. Run `.\gradle distZip` in the root of the project. 
2. Extract the zip on the machine where TESTAR is used.

NOTE: TESTAR requires Visual Redistributable which can be downloaded from the following
 [link](https://go.microsoft.com/fwlink/?LinkId=746572) .Also a JAVA 1.8 JDK is required.

#### Running Gradle in Eclipse
The following procedure has been performed

1. Create a new empty workspace for Eclipse in a folder which is not the folder that contains the source
code.
2. Select File -> Import to open the import dialog
3. Select Gradle -> Existing Gradle project to open te import dialog 
4. Select the folder that contains the root of the source code and start the import

#### Running TESTAR from Gradle
`gradle runTestar` task : creates a TESTAR distribution with `gradle installDist` task, and executes TESTAR from the runnable file `\testar\target\install\testar\bin\testar.bat`

TESTAR can be started using a gradle command from the root of the project.
1. .\gradle runTestar

##### In Eclipse
Within Eclipse, TESTAR can be executed by running the task runTestar which is available in the map custom_testar.
To debug the application with the runTestar task, provide your onw run configuration in which the option -DDEBUG is set.

#### Debug TESTAR from Gradle
In order to debug the TESTAR code, you must run;
1. .\gradle -DDEBUG=true runTestar.  

Optionally you can build TESTAR (.\gradle -DDBEBUG=true distZip ), copy the result to 
the machine where you want to run TESTAR and run TESTAR on the target machine. This allows
the user to debug TESTAR from a different machine. 

### How to execute TESTAR from command line

TESTAR allow its execution and settings configuration from the command line. By default is executed with the selected protocol (.sse file) and the test.settings values of that protocol.

From the command line it is also possible to select the desired protocol to execute TESTAR and change the values of the test.settings.

The protocol to be executed can be selected using the ""sse"" parameter next to the name of the desired protocol. Ex: testar sse=desktop_generic

Other settings are input using the pairs ""parameterX=valueX"" separated by space. Ex: testar ShowVisualSettingsDialogOnStartup=false Mode=Generate

Certain characters such the slashes or the quotation marks must be entered in a double way to respect the treaty of special characters.

Some of the most interesting parameters that can help to integrate TESTAR as an CI tool are:

		sse -> to change the protocol

		ShowVisualSettingsDialogOnStartup -> To run TESTAR without the GUI

		Mode -> TESTAR execution Mode (Spy, Generate, Record, Replay, View)

		SUTConnector & SUTConnectorValue -> The way to link with the desired application to be tested

		Sequences & SequenceLength -> The number of iterations and actions that TESTAR will execute

		SuspiciousTitles -> The errors that TESTAR will search in the execution

Example: 

``testar sse=desktop_generic ShowVisualSettingsDialogOnStartup=false Sequences=5 SequenceLength=100 Mode=Generate SUTConnectorValue="" """"C:\\Program Files\\VideoLAN\\VLC\\vlc.exe"""" "" SuspiciousTitles="".*[eE]rror.*|.*[eE]xcep[ct]ion.*""``

https://github.com/iv4xr-project/TESTAR_iv4xr/issues

",2022-08-12
https://github.com/iv4xr-project/userexperienceeval,"# userexperienceeval


## How to use with the iv4xrDemo repository

* Add the EmotionalCritic.java file to the folder lab-recruits-api/src/main/java/emotions
* Add the three .csv maps to the folder lab-recruits-api/src/test/resources/levels
* Add the three .java tests to the folder lab-recruits-api/src/test/java/agents
* Create the folder lab-recruits-api/src/test/resources/emotions
* Run any of the tests using JUnit. A test file with the emotional results will be created.
* Use the emotionalPlots.py program to analise the test file. The tests will be named according to the time they are saved and will be on the ""lab-recruits-api/src/test/resources/emotions"" folder. The name of the test file will need to be given to the python program for it to read the data. The path of the repository will also need to be given.
",2022-08-12
https://github.com/ivasconcelosUU/IPils2019,"# Inverse Problems in Imaging 2019

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/TristanvanLeeuwen/IPils2019/master?filepath=index.ipynb)

## Lecture notes

The most recent version of the lecture notes can be found here.

## Jupyter Notebooks

You can download the Jupyter notebooks or run them directly from your browser by accessing this link:

https://mybinder.org/v2/gh/TristanvanLeeuwen/IPils2019/master?filepath=index.ipynb

To run the notebooks on your own laptop you need to install ![Python](https://www.python.org/) and ![Jupyter](https://jupyter.org).
",2022-08-12
https://github.com/J535D165/cbsodata,"Statistics Netherlands opendata API client for Python
=====================================================

|pypi| |tests|

.. |pypi| image:: https://badge.fury.io/py/cbsodata.svg
    :target: https://badge.fury.io/py/cbsodata

.. |tests| image:: https://github.com/J535D165/cbsodata/workflows/tests/badge.svg
    :target: https://github.com/J535D165/cbsodata/actions

Retrieve data from the `open data interface of Statistics Netherlands
<http://www.cbs.nl/nl-NL/menu/cijfers/statline/open-data/default.htm>`__
(Centraal Bureau voor de Statistiek) with *Python*. The data is identical in
content to the tables which can be retrieved and downloaded from `StatLine
<http://statline.cbs.nl/>`__. CBS datasets are accessed via the `CBS open data
portal <https://opendata.cbs.nl/statline/portal.html>`__.

The documentation of this
package is found at this page and on `readthedocs.io
<http://cbsodata.readthedocs.io/>`__.

R user? Use `cbsodataR <https://cran.r-project.org/web/packages/cbsodataR/index.html>`__. 

Installation
------------

From PyPi

.. code:: sh

    pip install cbsodata

Usage
-----

Load the package with

.. code:: python

    >>> import cbsodata

Tables
~~~~~~

Statistics Netherlands (CBS) has a large amount of public available
data tables (more than 4000 at the moment of writing). Each table is
identified  by a unique identifier (``Identifier``).

.. code:: python

    >>> tables = cbsodata.get_table_list()
    >>> print(tables[0])
    {'Catalog': 'CBS',
     'ColumnCount': 18,
     'DefaultPresentation': '_la=nl&_si=&_gu=&_ed=LandVanUiteindelijkeZeggenschapUCI&_td=Perioden&graphType=line',
     'DefaultSelection': ""$filter=((LandVanUiteindelijkeZeggenschapUCI eq '11111') or (LandVanUiteindelijkeZeggenschapUCI eq '22222')) and (Bedrijfsgrootte eq '10000') and (substringof('JJ',Perioden))&$select=LandVanUiteindelijkeZeggenschapUCI, Bedrijfsgrootte, Perioden, FiscaalJaarloonPerBaan_15"",
     'ExplanatoryText': '',
     'Frequency': 'Perjaar',
     'GraphTypes': 'Table,Bar,Line',
     'ID': 0,
     'Identifier': '82010NED',
     'Language': 'nl',
     'MetaDataModified': '2014-02-04T02:00:00',
     'Modified': '2014-02-04T02:00:00',
     'OutputStatus': 'Regulier',
     'Period': '2008 t/m 2011',
     'ReasonDelivery': 'Actualisering',
     'RecordCount': 32,
     'SearchPriority': '2',
     'ShortDescription': '\nDeze tabel bevat informatie over banen en lonen bij bedrijven in Nederland, uitgesplitst naar het land van uiteindelijke zeggenschap van die bedrijven. Hierbij wordt onderscheid gemaakt tussen bedrijven onder Nederlandse zeggenschap en bedrijven onder buitenlandse zeggenschap. In de tabel zijn alleen de bedrijven met werknemers in loondienst meegenomen. De cijfers hebben betrekking op het totale aantal banen bij deze bedrijven en de samenstelling van die banen naar kenmerken van de werknemers (baanstatus, geslacht, leeftijd, herkomst en hoogte van het loon). Ook het gemiddelde fiscale jaarloon per baan is in de tabel te vinden. \n\nGegevens beschikbaar vanaf: 2008 \n\nStatus van de cijfers: \nDe cijfers in deze tabel zijn definitief.\n\nWijzigingen per 4 februari 2014\nDe cijfers van 2011 zijn toegevoegd.\n\nWanneer komen er nieuwe cijfers?\nDe cijfers over 2012 verschijnen in de eerste helft van 2015.\n',
     'ShortTitle': 'Zeggenschap bedrijven; banen, grootte',
     'Source': 'CBS.',
     'Summary': 'Banen en lonen van werknemers bij bedrijven in Nederland\nnaar land van uiteindelijke zeggenschap en bedrijfsgrootte',
     'SummaryAndLinks': 'Banen en lonen van werknemers bij bedrijven in Nederland<br />naar land van uiteindelijke zeggenschap en bedrijfsgrootte<br /><a href=""http://opendata.cbs.nl/ODataApi/OData/82010NED"">http://opendata.cbs.nl/ODataApi/OData/82010NED</a><br /><a href=""http://opendata.cbs.nl/ODataFeed/OData/82010NED"">http://opendata.cbs.nl/ODataFeed/OData/82010NED</a>',
     'Title': 'Zeggenschap bedrijven in Nederland; banen en lonen, bedrijfsgrootte',
     'Updated': '2014-02-04T02:00:00'}

Info
~~~~

Get information about a table with the ``get_info`` function.

.. code:: python

    >>> info = cbsodata.get_info('82070ENG') # Returns a dict with info
    >>> info['Title']
    'Caribbean Netherlands; employed labour force characteristics 2012'
    >>> info['Modified']
    '2013-11-28T15:00:00'

Data
~~~~

The function you are looking for!! The function ``get_data`` returns a list of
dicts with the table data.

.. code:: python

    >>> data = cbsodata.get_data('82070ENG')
    [{'CaribbeanNetherlands': 'Bonaire',
      'EmployedLabourForceInternatDef_1': 8837,
      'EmployedLabourForceNationalDef_2': 8559,
      'Gender': 'Total male and female',
      'ID': 0,
      'Periods': '2012',
      'PersonalCharacteristics': 'Total personal characteristics'},
     {'CaribbeanNetherlands': 'St. Eustatius',
      'EmployedLabourForceInternatDef_1': 2099,
      'EmployedLabourForceNationalDef_2': 1940,
      'Gender': 'Total male and female',
      'ID': 1,
      'Periods': '2012',
      'PersonalCharacteristics': 'Total personal characteristics'},
     {'CaribbeanNetherlands': 'Saba',
      'EmployedLabourForceInternatDef_1': 1045,
      'EmployedLabourForceNationalDef_2': 971,
      'Gender': 'Total male and female',
      'ID': 2,
      'Periods': '2012',
      'PersonalCharacteristics': 'Total personal characteristics'},
     # ...
    ]

The keyword argument ``dir`` can be used to download the data directly to your
file system.

.. code:: python

    >>> data = cbsodata.get_data('82070ENG', dir=""dir_to_save_data"")

Catalogs (dataderden)
~~~~~~~~~~~~~~~~~~~~~ 

There are multiple ways to retrieve data from catalogs other than
'opendata.cbs.nl'. The code below shows 3 different ways to retrieve data from
the catalog 'dataderden.cbs.nl' (known from Iv3).

On module level.

.. code:: python

   cbsodata.options.catalog_url = 'dataderden.cbs.nl'
   # list tables
   cbsodata.get_table_list()
   # get dataset 47003NED
   cbsodata.get_data('47003NED')

With context managers.

.. code:: python

   with cbsodata.catalog('dataderden.cbs.nl'):
       # list tables
       cbsodata.get_table_list()
       # get dataset 47003NED
       cbsodata.get_data('47003NED')

As a function argument.

.. code:: python

   # list tables
   cbsodata.get_table_list(catalog_url='dataderden.cbs.nl')
   # get dataset 47003NED
   cbsodata.get_data('47003NED', catalog_url='dataderden.cbs.nl')

Pandas users
~~~~~~~~~~~~

The package works well with Pandas. Convert the result easily into a pandas
DataFrame with the code below.

.. code:: python

    >>> data = pandas.DataFrame(cbsodata.get_data('82070ENG'))
    >>> data.head()

The list of tables can be turned into a pandas DataFrame as well.

.. code:: python

    >>> tables = pandas.DataFrame(cbsodata.get_table_list())
    >>> tables.head()


Command Line Interface
----------------------

This library ships with a Command Line Interface (CLI). 

.. code:: bash 
    
    > cbsodata -h 
    usage: cbsodata [-h] [--version] [subcommand]

    CBS Open Data: Command Line Interface

    positional arguments:
      subcommand  the subcommand (one of 'data', 'info', 'list')

    optional arguments:
      -h, --help  show this help message and exit
      --version   show the package version

Download data:

.. code:: bash 
    
    > cbsodata data 82010NED 
    
Retrieve table information:

.. code:: bash 
    
    > cbsodata info 82010NED 

Retrieve a list with all tables:

.. code:: bash

    > cbsodata list


Export data
~~~~~~~~~~~

Use the flag ``-o`` to load data to a file (JSON lines). 

.. code:: bash
    
    > cbsodata data 82010NED -o table_82010NED.jl
",2022-08-12
https://github.com/J535D165/cbsshape,"# CBS Wijk en Buurtkaart interface

This package provides a simple interface for the [CBS Wijk en Buurtkaart shapefiles](https://www.cbs.nl/nl-nl/reeksen/geografische-data). These shapefiles are available on the website of CBS.

Looking for open data from CBS? Check out [cbsodataR](https://github.com/edwindj/cbsodataR).

## Installation

Install the package directly from Github with `devtools`. The package requires `sf` for reading shapefiles.
``` R
require(devtools)
install_github(""J535D165/cbsshape"")
```

## Usage

Read municipality shapefiles (gemeenten) from 2017.

``` R
library(cbsshape)

cbs_shape_read(2017)
```

### Gemeente, Wijk en Buurt

Read data on 3 different levels.

``` R
# read CBS Wijk en Buurtkaart kaart on ""gem""
cbs_shape_read(2018, level=""gem"")
cbs_shape_read(2018, level=""buurt"")
cbs_shape_read(2018, level=""wijk"")
```

By default, the Simple Features are returned in [Rijksdriehoek coordinates](https://nl.wikipedia.org/wiki/Rijksdriehoeksco%C3%B6rdinaten). One can get the result in WGS84 coordinates with the following code:

``` R
# read CBS Wijk en Buurtkaart kaart on ""gem""
cbs_shape_read(2018, wgs84=TRUE)
```

### Available shapefiles

Get a list of all available shapefiles:

``` R 
> cbs_shape_list()
                                                                                                              2009 
                                               ""http://download.cbs.nl/regionale-kaarten/2009-buurtkaart-gn-3.zip"" 
                                                                                                              2010 
                                     ""http://download.cbs.nl/regionale-kaarten/2010-buurtkaart-shape-versie-3.zip"" 
                                                                                                              2011 
                                     ""http://download.cbs.nl/regionale-kaarten/2011-buurtkaart-shape-versie-3.zip"" 
                                                                                                              2012 
                                              ""http://download.cbs.nl/regionale-kaarten/shape-2012-versie-3.0.zip"" 
                                                                                                              2013 
                                              ""http://download.cbs.nl/regionale-kaarten/shape-2013-versie-3-0.zip"" 
                                                                                                              2014 
                                                ""https://www.cbs.nl/-/media/_pdf/2016/35/shape-2014-versie-30.zip"" 
                                                                                                              2015 
                                                          ""https://www.cbs.nl/-/media/_pdf/2017/36/buurt_2015.zip"" 
                                                                                                              2016 
  ""https://www.cbs.nl/-/media/cbs/dossiers/nederland-regionaal/wijk-en-buurtstatistieken/shape-2016-versie-30.zip"" 
                                                                                                              2017 
""https://www.cbs.nl/-/media/cbs/dossiers/nederland-regionaal/wijk-en-buurtstatistieken/wijkbuurtkaart_2017_v3.zip"" 
                                                                                                              2018 
""https://www.cbs.nl/-/media/cbs/dossiers/nederland-regionaal/wijk-en-buurtstatistieken/wijkbuurtkaart_2018_v3.zip"" 
                                                                                                              2019 
""https://www.cbs.nl/-/media/cbs/dossiers/nederland-regionaal/wijk-en-buurtstatistieken/wijkbuurtkaart_2019_v2.zip"" 
                                                                                                              2020 
""https://www.cbs.nl/-/media/cbs/dossiers/nederland-regionaal/wijk-en-buurtstatistieken/wijkbuurtkaart_2020_v1.zip"" 

```

### Download

One can use `cbsshape` to download shapefiles.
``` R
cbs_shape_download(2017, ""wijkenbuurt2017/"")
```

and read the local shapefile

``` R
cbs_shape_read(2017, ""wijkenbuurt2017/"")

```

Combine the download and read function to download the data only if not available. 

``` R
if (!cbs_shape_exists(2017, path=""wijkenbuurt2017"")){
  cbs_shape_download(2017, path=""wijkenbuurt2017"")
}

cbs_shape_read(2017, level=""gem"", path=""wijkenbuurt2017"")
```

## Examples

### Population density

``` R
library(cbsshape)
library(ggplot2)
library(dplyr)

# download 2017 data
wijk_en_buurt_2017 <- cbs_shape_read(2017)

# plot map
wijk_en_buurt_2017 %>% 
  # remove water polygons
  filter(WATER == ""NEE"") %>%
  # compute the population density
  mutate(density = AANT_INW/OPP_LAND*100) %>% 
  # plot the map
  ggplot() + 
    geom_sf(aes(fill=density))
```

![Population density](figs/demo_population.png)

### Distance to Amsterdam

Compute the distance between every municipality in The Netherlands and the
city of Amsterdam. 

``` R
library(cbsshape)
library(sf)
library(tidyverse)

# download 2017 data
wijk_en_buurt_2017 <- cbs_shape_read(2017) %>%     
  filter(GM_CODE != ""GM9999"") %>% 
  filter(WATER == ""NEE"")
  
# compute the centroids
df_centroids <- wijk_en_buurt_2017 %>% 
  mutate(centroid = st_centroid(geometry)) %>%
  st_drop_geometry()

# grab the centroid of Amsterdam
centroid_amsterdam <- df_centroids %>% 
  filter(GM_NAAM == ""Amsterdam"") %>% 
  pull(centroid)

# Compute the distance to Amsterdam
st_distance(df_centroids$centroid, centroid_amsterdam)

```



",2022-08-12
https://github.com/J535D165/CoronaWatchNL,"> Dear CoronaWatchers, 
>
> One year after the start of the CoronaWatchNL project, the coronavirus is still with us. As a community, we made an extensive collection of data on COVID-19 case counts Findable, Accessible, Interoperable, and Reusable (FAIR) ([Wilkinson, M. D. et al., 2016](https://doi.org/10.1038/sdata.2016.18)). During the first wave of the COVID-19 outbreak in The Netherlands, our project was the primary source of structured open data for researchers, hospitals, (local) governments, and the public. In June 2020, RIVM started to publish their first open and structured data on COVID-19 case counts. More and more users of our project migrated to the RIVM open data. 
> 
> It's essential to have data openly available and FAIR. Data is an important building block for nowadays research and policymaking. We see that many researchers and organizations struggle with making data and software FAIR. It is important to realize that FAIRness of data is a step-by-step process, and there is no such a thing as a perfect FAIR dataset. We see that many organizations with an important role in the COVID-19 pandemic in the Netherlands started to publish their data openly and take their first steps in making data FAIR. It is still far from perfect, but we are moving forward. 
>
> For the award-winning CoronaWatchNL project, this implies we are no longer the connecting link between the user and the suppliers of the COVID-19 data. Users can now make use of the data of RIVM, LCPS, and NICE directly. We have become largely redundant, and therefore we decided to no longer update the project. Our main goal was to become redundant, so we are pleased with the outcome. We will keep an eye on the developments and stay in contact with the suppliers of COVID-19 data. The journey for them has only yet begun. 
>
> Users who are still using our data should migrate. Our main sources of data in CoronaWatchNL were RIVM, LCPS, and NICE. Most of the datasets we offer are nowadays available on their websites. See the following sources for more information:
> 
> - RIVM: [https://data.rivm.nl](https://data.rivm.nl/geonetwork/srv/dut/catalog.search#/search?any_OR__title=covid-19&isChild='false'&fast=index)
> - LCPS: https://lcps.nu/datafeed/
> - NICE: https://www.stichting-nice.nl/
>
> Now we are no longer updating the data anymore; I would like to thank the CoronaWatchNL community and users. This project was an open community project from the start. Without the help of more than 50 CoronaWatchers, it wouldn't have been possible to collect this amount of data for more than a year. The importance and quality of the data collection were widely recognized in academia, and we were awarded the [Dutch Data Prize 2020](https://researchdata.nl/diensten/de-nederlandse-dataprijs/). Hopefully, we see each other in the future in a new project! 
> 
> Feel free to contact me with questions: j.debruin1@uu.nl.
>
> Best regards, 
>
> Jonathan de Bruin
>



![corona_artwork.jpg](corona_artwork.jpg)

# Dataset: COVID-19 case counts in The Netherlands

**CoronaWatchNL** collects numbers on COVID-19 disease count cases in **The Netherlands**. The numbers are collected from various sources on a daily basis, like [RIVM (National Institute for Public Health and the Environment)](https://www.rivm.nl/coronavirus-covid-19/actueel), [LCPS (Landelijk Coördinatiecentrum Patiënten Spreiding)](https://www.lcps.nu/), [NICE (Nationale Intesive Care Evaluatie)](https://www.stichting-nice.nl/), and the [National Corona Dashboard](https://coronadashboard.rijksoverheid.nl/). This project standardizes, and publishes data and makes it **Findable, Accessible, Interoperable, and Reusable (FAIR)**. We aim to collect a complete time series and prepare a dataset for reproducible analysis and academic use.

Dutch:
> CoronalWatchNL verzamelt ziektecijfers over COVID-19 in Nederland. Dagelijks worden de cijfers verzameld van het [RIVM (Rijksinstituut voor de Volksgezondheid en Milieu)](https://www.rivm.nl/coronavirus-covid-19/actueel), [LCPS (Landelijk Coördinatiecentrum Patiënten Spreiding)](https://www.lcps.nu/), [NICE (Nationale Intesive Care Evaluatie)](https://www.stichting-nice.nl/) en [Nationale Corona Dashboard](https://coronadashboard.rijksoverheid.nl/). Dit project standaardiseert en publiceert de gegevens en maakt ze vindbaar, toegankelijk, interoperabel en herbruikbaar (FAIR). We streven ernaar om een dataset beschikbaar te stellen voor reproduceerbare analyses en wetenschappelijk gebruik.


## Datasets
The datasets available on CoronaWatchNL are updated on a daily basis. Availability depends on the publication by the respective sources (N.B. since July 1st, the epidemiological reports published by RIVM will be released on a *weekly* instead of a daily basis). The CoronaWatchNL project divides the datasets into four main categories:

* [Geographical data](#geographical-datasets)
* [Descriptive data](#descriptive-datasets)
* [Intensive care data](#intensive-care-datasets)
* [Dashboard data](#dashboard-datasets)
* [Miscellaneous datasets](#miscellaneous-datasets)


For (interactive) applications based on these datasets, have a look at the [applications folder](/applications). For predictive models based on these datasets, check out the parallel repository [CoronaWatchNL Extended](https://github.com/J535D165/CoronaWatchNLExtended). Please note that the intention of these (too) simplistic models - made by CoronaWatchNL volunteers - is to show how the data can be used for modelling,  *not* to answer specific hypotheses or follow scientific protocol.

Please see the [Remarks](REMARKS.md) document for notes about the datasets. Do you have remarks? Please let us know.


### Geographical datasets

#### Reference time: 10:00 AM

These datasets describe the new and cumulative number of confirmed, hospitalized, and deceased COVID-19 cases. Every day, RIVM retrieves the data from the central database [OSIRIS](https://www.rivm.nl/sniv/handleiding-osiris) at 10:00 AM. Here, the datasets are categorized by their geographical level (i.e., national, provincial, municipal).

For more detail about the specific structure of these geographical datasets, have a look at the `data-geo`[codebook](/data-geo/README.md). <br/>

| Dataset | Source | Variables |
|---|---| --- |
| [Reported case counts by date in NL](data-geo#national) | [RIVM](#data-collection-sources) | Date, Type (Total, hopitalized and deceased COVID-19 cases), (Cumulative) Count|
| [Reported case counts by date in NL per province](data-geo#provincial) | [RIVM](#data-collection-sources) | Date, Province, Type (Total, hopitalized and deceased COVID-19 cases), (Cumulative) Count |
| [Reported case counts by date in NL per municipality](data-geo#municipal) | [RIVM](#data-collection-sources) | Date, Municipality, Province, Type (Total, hopitalized and deceased COVID-19 cases), (Cumulative) Count |

#### Reference time: by day (0:00 AM)

These datasets describe the new and cumulative number of confirmed, hospitalized, and deceased COVID-19 cases per day. The data is retrieved from the central database [OSIRIS](https://www.rivm.nl/sniv/handleiding-osiris) and counts the number of cases per day (0:00 AM) by RIVM. The dataset concerns numbers on a national level.

For more detail about the specific structure of this geographical dataset, have a look at the `data-geo`[codebook](/data-geo/README.md). <br/>

| Dataset | Source | Variables |
|---|---| --- |
| [ Case counts by date in NL ](data/rivm_NL_covid19_national_by_date/) | [RIVM](#data-collection-sources) | Date, Type (Total, hopitalized and deceased patients), (Cumulative) Count |

#### Visualizations geographical data

To get a better picture of the content of the geographical datasets, have a look at the following visuals. These visuals show the development of the COVID-19 disease outbreak on a national level.

[<img src=""plots/overview_plot.png"" width=""400"">](/data-geo/README.md)[<img src=""plots/overview_plot_diff.png"" width=""400"">](/data-geo/README.md)

[<img src=""plots/overview_plot_true_vs_reported.png"" width=""400"">](/data-geo/README.md)[<img src=""plots/overview_plot_true_vs_reported_diff.png"" width=""400"">](/data-geo/README.md)

[<img src=""plots/overview_reports.png"" width=""400"">](/data-geo/README.md)

[![plots/map_province.png](plots/map_province.png)](/data-geo/README.md)


### Descriptive datasets

The datasets in this section describe the new and cumulative number of confirmed, hospitalized, and deceased COVID-19 cases per day and contain variables like age and sex.

For more detail about the specific structure of these descriptive datasets, have a look at the `data-desc`[codebook](/data-desc/README.md). <br/>

| Dataset | Source | Variables |
|---|---| --- |
| [Case counts in NL per age](data-desc#age) | [RIVM](#data-collection-sources) | Date, Age group, Type (Total, hopitalized and deceased COVID-19 cases), (Cumulative) Count |
| [Case counts in NL per sex](data-desc#sex) | [RIVM](#data-collection-sources) | Date, Sex, Type (Total, hopitalized and deceased COVID-19 cases), (Cumulative) Count|
| [Deceased case counts in NL per sex and age group](data-desc#deceased) | [RIVM](#data-collection-sources) | Date, Age group, Sex, (Cumulative) Count of deceased cases |


#### Visualizations descriptive data

The graphs below visualize the development of the COVID-19 disease outbreak per sex and age group.

[<img src=""plots/overview_plot_geslacht.png"" width=""400"">](/data-desc/README.md)[<img src=""plots/toename_plot_geslacht.png"" width=""400"">](/data-desc/README.md)

[<img src=""plots/ratio_plot_geslacht.png"" width=""400"">](/data-desc/README.md)[<img src=""plots/ratio_toename_geslacht.png"" width=""400"">](/data-desc/README.md)

[<img src=""plots/overview_plot_leeftijd_cum.png"" width=""400"">](/data-desc/README.md)[<img src=""plots/overview_plot_leeftijd.png"" width=""400"">](/data-desc/README.md)

[<img src=""plots/deceased_age_sex.png"" width=""400"">](/data-desc/README.md)[<img src=""plots/deceased_age_sex_toename.png"" width=""400"">](/data-desc/README.md)


### Intensive care datasets

The intensive care datasets describe the new and cumulative number of COVID-19 intensive care unit (ICU) admissions per day. The datasets are categorized by their source. Compared to RIVM (reporting COVID-19 hospital admissions), CoronaWatchNL collects COVID-19 related intensive care data from LCPS and NICE.

* **RIVM** reports hospitalized COVID-19 cases, including - but not limited to - the intensive care unit (ICU) admissions. These are the largest numbers and most inclusive counts.
* **NICE** only reports COVID-19 cases that are admitted to the ICU.
* **LCPS**, similarly to NICE, reports COVID-19 ICU admissions. However, LCPS tries to compensate for the reporting lag, by estimating its size and adding it to the numbers reported by NICE.


For more detail about the specific structure of the intensive care datasets, have a look at the `data-ic`[codebook](/data-ic/README.md). <br/>

| Dataset | Source | Variables |
| --- | --- | --- |
| [COVID-19 intensive care patient counts in NL ](data-ic#nice) | [Stichting NICE](#data-collection-sources) | Date, New, Total and Cumulative ICU admissions per day, Number of ICUs with at least one COVID-19 case, New and Cumulative fatal, survived and discharged ICU admissions |
| [COVID-19 intensive care patient counts with country of hospitalisation ](data-ic#lcps) | [LCPS](#data-collection-sources) | Date, Country of Hospitalization, Total COVID-19 ICU admissions |


#### Visualizations intensive care
The first two graphs show the number of new (*Nieuw*), total (*Actueel*), cumulative (*Cumulatief*), deceased (*Overleden*), and survived (*Overleefd*) COVID-19 ICU admissions per day, as declared by NICE. The total number of ICU admissions per day as reported by LCPS is also shown.

[<img src=""plots/ic_nice_intakes.png"" width=""400"">](/data-ic/README.md)[<img src=""plots/ic_nice_vrijkomst.png"" width=""400"">](/data-ic/README.md)

[<img src=""plots/ic_lcps_intakes.png"" width=""400"">](/data-ic/README.md)[<img src=""plots/ic_lcps_intakes_country.png"" width=""400"">](/data-ic/README.md)

[<img src=""plots/overview_IC_actueel.png"" width=""400"">](/data-ic/README.md)[<img src=""plots/overview_IC_nieuw.png"" width=""400"">](/data-ic/README.md)

[<img src=""plots/overview_IC_totaal.png"" width=""400"">](/data-ic/README.md)

### Dashboard datasets
The datasets underlying the [National Dashboard](#data-collection-sources) are listed in this folder. These datasets concern various topics, such as an overview of the number and age distribution of hospitalized, positively tested, and suspected cases, an estimate of the number of contagious people, the reproduction index, the number of (deceased) infected nursery home residents, and the amount of virus particles measured in the sewage water.

For more detail about the specific structure of the dashboard datasets, have a look at the `data-dashboard`[codebook](/data-dashboard/README.md).

| Dataset | Source | Variables |
|---|---| --- |
| [Reported case counts in NL](data-dashboard#cases) | [National Dashboard](#data-collection-sources) | Date, Type of measure, (Cumulative) Count |
| [Age distribution of reported cases in NL](data-dashboard#descriptive) | [National Dashboard](#data-collection-sources) | Date, Age group, Count |
| [Suspected patients in NL](data-dashboard#suspects) | [National Dashboard](#data-collection-sources) | Date, Type of measure, Count |
| [COVID-19 particles in sewage](data-dashboard#sewage) | [National Dashboard](#data-collection-sources) | Date, Type of measure, Count, Measurement units |
| [Reproduction index COVID-19 virus](data-dashboard#reproduction) | [National Dashboard](#data-collection-sources) | Date, Type of measure, Value |
| [Contagion estimate COVID-19 virus](data-dashboard#contagious) | [National Dashboard](#data-collection-sources) | Date, Type of measure, Value |
| [Number of infected and deceased nursery home cases](data-dashboard#nursery) | [National Dashboard](#data-collection-sources) | Date, Type of measure, (Cumulative) Count |

#### Visualizations dashboard data
These visuals show the development of the COVID-19 disease outbreak on a national level as reported by the National Dashboard and by the RIVM reports.

[<img src=""plots/overview_national_dashboard.png"" width=""400"">](/data-dashboard/README.md)[<img src=""plots/overview_national_dashboard_new.png"" width=""400"">](/data-dashboard/README.md)

[<img src=""plots/overview_national_vs_dashboard.png"" width=""400"">](/data-dashboard/README.md)[<img src=""plots/overview_national_vs_dashboard_new.png"" width=""400"">](/data-dashboard/README.md)

[<img src=""plots/overview_desc_dashboard.png"" width=""400"">](/data-dashboard/README.md)

Below, the number of suspected COVID-19 patients as registered by the GPs, and the amount of COVID-19 particles per milliliter sewage water are depicted.

[<img src=""plots/overview_suspects.png"" width=""400"">](/data-dashboard/README.md)[<img src=""plots/overview_sewage.png"" width=""400"">](/data-dashboard/README.md)

The reproduction index and estimated contagious people are plotted with their corresponding minimum and maximum values. The reproduction index indicates how quickly the COVID-19 virus is spreading in the Netherlands. The estimated contagious people represent the number of COVID-19 people per 100.000 inhabitants that are contagious for others. <br/>

[<img src=""plots/reproductie_index.png"" width=""400"">](/data-dashboard/README.md)[<img src=""plots/contagious.png"" width=""400"">](/data-dashboard/README.md)

The number of (deceased) nursery home residents infected with COVID-19 are shown here. <br/>

[<img src=""plots/overview_nursery_cumulative.png"" width=""400"">](/data-dashboard/README.md)[<img src=""plots/overview_nursery_count.png"" width=""400"">](/data-dashboard/README.md)

[<img src=""plots/overview_nursery_homes.png"" width=""400"">](/data-dashboard/README.md)[<img src=""plots/overview_nursery_homes_new.png"" width=""400"">](/data-dashboard/README.md)

### Miscellaneous datasets

This folder contains datasets describing various miscellaneous topics, such as the number of (positively) tested people, the underlying conditions and/or pregnancy of deceased cases younger than 70, an overview of the reinforced measures and press releases in the Netherlands, and a list of companies that requested and received an advance on their reimbursement.

For more detail about the specific structure of the miscellaneous datasets, have a look at the `data-misc`[codebook](/data-misc/README.md).

| Dataset | Source | Variables |
|---|---| --- |
| [COVID-19 tests in NL per week](data-misc#test-data-virologische-dagstaten) | [RIVM](#data-collection-sources) | Year, Calendar week, Start date (Monday), End date (Sunday), Included labs, Type (Total and positive tests), Count |
| [COVID-19 tests in NL per week by GGD-GHOR](data-misc#test-data-ggd-ghor) | [GGD-GHOR](#data-collection-sources) | Year, Calendar week, Start date (Monday), End date (Sunday), Type (Total), Count |
| [Underlying conditions and/or pregnancy in deceased COVID-19 cased under the age of 70](data-misc#underlying) | [RIVM](#data-collection-sources) | Date, Type of condition, Cumulative count |
| [COVID-19 measures by the government](data-misc#measures) | [European Commission Joint Research Centre](#data-collection-sources) | Various variables on governmental measures (in English) |
| [RIVM press releases](data/rivm_press_releases.csv) | [RIVM](#data-collection-sources) | Date and Time, Content of press release |
| [NOW registry](data-misc#economy) | [UWV](#data-collection-sources) | Company, Location, Advance |

#### Visualizations miscellaneous data
These graphs display the number of (positively) tested people per week. The end date of each week - Sunday - is used as indicator for the respective week.<br/>

[<img src=""plots/overview_plot_tests_weeks_cum.png"" width=""400"">](/data-misc/README.md)[<img src=""plots/overview_plot_tests_weeks.png"" width=""400"">](/data-dashboard/README.md)

Below, the cumulative number of deceased COVID-19 cases younger than 70 with and without underlying conditions and/or pregnancy are displayed per notification date. <br/>

[<img src=""plots/conditions_statistics.png"" width=""700"">](/data-misc/README.md)

The cumulative number of specific conditions found in these deceased COVID-19 cases are shown here. <br/>
[<img src=""plots/underlying_conditions.png"" width=""700"">](/data-misc/README.md)

## Inactive/deprecated datasets

### Deprecated (pending)

The following datasets are awaiting deprecation. They are (being) replaced by new datasets.

| Dataset | Source | Variables | Alternative  |
|---|---|---|---|
| [COVID-19 disease case counts in NL](https://github.com/J535D165/CoronaWatchNL/blob/b71cd70e51bb2e30e8fb9244f360a1e70446c939/data/rivm_corona_in_nl_daily.csv) | [RIVM](#data-collection-sources) | Date, Number of positive COVID-19 disease cases in NL| [COVID-19 case counts in NL](data/rivm_NL_covid19_national.csv) |
| [COVID-19 fatalities in NL](https://github.com/J535D165/CoronaWatchNL/blob/b71cd70e51bb2e30e8fb9244f360a1e70446c939/data/rivm_corona_in_nl_fatalities.csv) | [RIVM](#data-collection-sources) | Date, Number of COVID-19 fatalities in NL | [COVID-19 case counts in NL](data/rivm_NL_covid19_national.csv) |
| [COVID-19 hospitalizations in NL](https://github.com/J535D165/CoronaWatchNL/blob/b71cd70e51bb2e30e8fb9244f360a1e70446c939/data/rivm_corona_in_nl_hosp.csv) | [RIVM](#data-collection-sources) | Date, Number of COVID-19 hospitalized patients in NL | [COVID-19 case counts in NL](data/rivm_NL_covid19_national.csv) |
| [Newly reported relative case counts by date in NL per municipality (PDF maps)\*](data/rivm_NL_covid19_municipality_range.csv) | [RIVM](#data-collection-sources) | Date, Type, Number of positive COVID-19 disease cases, hospitalizations and fatalities per 100.000 people, Municipality, Province| [Reported case counts by date in NL per municipality](data-geo#municipal) |
| [COVID-19 age distribution](https://github.com/J535D165/CoronaWatchNL/blob/b64201202e9e50eeea5d1389eea6930c041fbaaf/data/rivm_NL_covid19_age.csv) | [RIVM](#data-collection-sources) | Date, Type, Age, number of cases| [data-desc#age](https://github.com/J535D165/CoronaWatchNL/tree/master/data-desc#age) |
| [COVID-19 sex distribution](https://github.com/J535D165/CoronaWatchNL/blob/b64201202e9e50eeea5d1389eea6930c041fbaaf/data/rivm_NL_covid19_sex.csv) | [RIVM](#data-collection-sources) | Date, Type, Sex, number of cases| [data-desc#sex](https://github.com/J535D165/CoronaWatchNL/tree/master/data-desc#sex) |

**\*** This dataset is extracted from the maps in the PDF's. The values are relative counts per 100.000 residents in the municipality.

### Inactive

The following datasets are no longer appended with new data (because RIVM is no longer providing the data).

| Dataset | URL | Source | Variables | Expire date
|---|---| --- | --- | --- |
| COVID-19 disease case counts in NL\* |[[long format]](https://github.com/J535D165/CoronaWatchNL/blob/b71cd70e51bb2e30e8fb9244f360a1e70446c939/data/rivm_corona_in_nl.csv) [[wide format]](https://github.com/J535D165/CoronaWatchNL/blob/b71cd70e51bb2e30e8fb9244f360a1e70446c939/data/rivm_corona_in_nl_table.csv) | [RIVM](#data-collection-sources) | Date, Number of positive COVID-19 disease cases in NL, Municipality of residence, Municipality code (2019), Province | 2020-03-30
| Test count (before 2020-04-20) |[Test count](https://github.com/J535D165/CoronaWatchNL/blob/9eaf2bec1789635dc3d3c8321fb033d988c11422/data/rivm_NL_covid19_tests.csv) | [RIVM](#data-collection-sources) | PublicatieDatum, Datum, Labs, Type, Aantal | 2020-04-20

**\*** Nowadays, the data is published again. Please use dataset [data-geo#municipal](https://github.com/J535D165/CoronaWatchNL/tree/master/data-geo#municipal).

## Raw data

CoronaWatchNL collects copies of the raw data such that data collection is verifiable. Copies of the collected data can be found in the folder [raw_data](raw_data/). The data isn't standardised.


## Data collection sources

The following sources are used for data collection.

| Source | Institute | Variables |
|---|---| --- |
| https://www.rivm.nl/coronavirus-covid-19/actueel | RIVM | National cumulative numbers and press releases |
| https://www.rivm.nl/coronavirus-covid-19/grafieken | RIVM | Case counts per day |
| https://www.rivm.nl/coronavirus-covid-19/actueel/wekelijkse-update-epidemiologische-situatie-covid-19-in-nederland | RIVM | Epidemiological report |
| https://ggdghor.nl/actueel-bericht/ | GGD-GHOR | Test data |
| https://www.stichting-nice.nl/ | Stichting NICE | Intensive care numbers on COVID-19 patients |
| https://www.lcsp.nu/ | LCPS | Intensive care numbers on COVID-19 patients |
| https://coronadashboard.rijksoverheid.nl/ | National Dashboard | Various variables and estimations like Reproduction Index |
| https://covid-statistics.jrc.ec.europa.eu/ | European Commision Joint Research Centre | Governmental measures database |
| https://www.uwv.nl/overuwv/pers/documenten/2020/gegevens-ontvangers-now-1-0-regeling.aspx/ | Employee Insurance Agency | NOW registry |


## License and academic use

The graphs and data are licensed [CC0](https://creativecommons.org/share-your-work/public-domain/cc0/). The original data is copyright RIVM.

For academic use, use presistent data from [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3711574.svg)](https://doi.org/10.5281/zenodo.3711574). This is a persistent copy of the data. Version number refer to the date. Please cite:

```De Bruin, J. (2020). Number of diagnoses with coronavirus disease (COVID-19) in The Netherlands (Version v2020.3.15) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.3711575```

Image from [iXimus](https://pixabay.com/nl/users/iXimus-2352783/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4901881) via [Pixabay](https://pixabay.com/nl/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4901881)

## CoronaWatchNL

CoronaWatchNL is collective of researchers and volunteers in The Netherlands. We aim to make the reported number on COVID-19 disease in The Netherlands FAIR. The project is initiated and maintained by [Utrecht University Research Data Management Support](https://www.uu.nl/en/research/research-data-management) and receives support from [Utrecht University Applied Data Science](https://www.uu.nl/en/research/applied-data-science).

Help on this project is appreciated. We are looking for new datasets, data updates, graphs and maps. Please report issues in the Issue Tracker. Want to contribute? Please check out the `help wanted` tag in the [Issue Tracker](https://github.com/J535D165/CoronaWatchNL/issues). Do you wish to share an application based on these [datasets](/data)? Have a look at the [applications folder](/applications). For predictive models, check out the parallel repository [CoronaWatchNL Extended](https://github.com/J535D165/CoronaWatchNLExtended).

Please send an email to jonathandebruinos@gmail.com and/or r.voorvaart@uu.nl
",2022-08-12
https://github.com/J535D165/CoronaWatchNLExtended,"![corona_artwork.jpg](corona_artwork.jpg)

# CoronaWatchNL: COVID-19 case counts in The Netherlands

[**CoronaWatchNL**](https://github.com/J535D165/CoronaWatchNL) collects COVID-19 disease count cases in **The Netherlands**. Numbers are collected from the RIVM (National Institute for Public Health and the Environment) website on a daily basis. This project standardizes, and publishes data and makes it **Findable, Accessible, Interoperable, and Reusable (FAIR)**. [We](#about-coronawatchnl) aim to collect a complete time series and prepare a dataset for reproducible analysis and academic use.

Dutch:
> CoronalWatchNL verzamelt ziektecijfers over COVID-19 in Nederland. Dagelijks worden de cijfers verzameld van de website van het RIVM. Dit project standaardiseert en publiceert de gegevens en maakt ze vindbaar, toegankelijk, interoperabel en herbruikbaar (FAIR). We streven ernaar om een dataset beschikbaar te stellen voor reproduceerbare analyses en wetenschappelijk gebruik.


# CoronaWatchNL_Models

**CoronaWatchNL_Models** is a collection of models made by CoronaWatchNL volunteers\* based on COVID-19 case counts in the Netherlands. All graphics can be found in the [plots folder](/plots). The underlying data can be found on the [CoronaWatchNL repository](https://github.com/J535D165/CoronaWatchNL). The graphs are updated on a daily basis and were generated automatically.<br/> 
\* N.B. The intention of these (too) simplistic models is to show how the data can be used for modelling,  *not* to answer specific hypotheses or follow scientific protocol.

## :chart_with_upwards_trend: COVID-19 case counts
The following graphs show various predictions about the development of the coronavirus outbreak in the Netherlands.

### Linear model: Growth rate

We try to fit a sigmoidal curve. One way to fit this, is to first estimate
the growth rate, which we define here as the ratio of new cases over previous
new cases. Once this growth rate reaches 1, it is likely that the data will
stop following an exponential pattern and will taper down into a sigmoid
curvature.

Here is the development of the growth factor over time, with a linear model fit
to try to estimate when the inflection point will occur (or has occurred).

![plots/growthfactor.png](/plots/growthfactor.png)


### Sigmoidal model

This then results in the following sigmoidal fit:
![plots/sigmoid.png](/plots/sigmoid.png)


### Linear model: Growth rate per province

As some provinces had the outbreak earlier than others, it's relevant to see the individual provinces. The same linear model is used to estimate the inflection point.
![plots/growthfactor_Drenthe.png](/plots/growthfactor_Drenthe.png)
![plots/growthfactor_Flevoland.png](/plots/growthfactor_Flevoland.png)
![plots/growthfactor_Friesland.png](/plots/growthfactor_Friesland.png)
![plots/growthfactor_Gelderland.png](/plots/growthfactor_Gelderland.png)
![plots/growthfactor_Groningen.png](/plots/growthfactor_Groningen.png)
![plots/growthfactor_Limburg.png](/plots/growthfactor_Limburg.png)
![plots/growthfactor_Noord-Brabant.png](/plots/growthfactor_Noord-Brabant.png)
![plots/growthfactor_Noord-Holland.png](/plots/growthfactor_Noord-Holland.png)
![plots/growthfactor_Overijssel.png](/plots/growthfactor_Overijssel.png)
![plots/growthfactor_Utrecht.png](/plots/growthfactor_Utrecht.png)
![plots/growthfactor_Zeeland.png](/plots/growthfactor_Zeeland.png)
![plots/growthfactor_Zuid-Holland.png](/plots/growthfactor_Zuid-Holland.png)


### Sigmoidal model per province

Also a sigmoid function per province:
![plots/sigmoid_Drenthe.png](/plots/sigmoid_Drenthe.png)
![plots/sigmoid_Flevoland.png](/plots/sigmoid_Flevoland.png)
![plots/sigmoid_Friesland.png](/plots/sigmoid_Friesland.png)
![plots/sigmoid_Gelderland.png](/plots/sigmoid_Gelderland.png)
![plots/sigmoid_Groningen.png](/plots/sigmoid_Groningen.png)
![plots/sigmoid_Limburg.png](/plots/sigmoid_Limburg.png)
![plots/sigmoid_Noord-Brabant.png](/plots/sigmoid_Noord-Brabant.png)
![plots/sigmoid_Noord-Holland.png](/plots/sigmoid_Noord-Holland.png)
![plots/sigmoid_Overijssel.png](/plots/sigmoid_Overijssel.png)
![plots/sigmoid_Utrecht.png](/plots/sigmoid_Utrecht.png)
![plots/sigmoid_Zeeland.png](/plots/sigmoid_Zeeland.png)
![plots/sigmoid_Zuid-Holland.png](/plots/sigmoid_Zuid-Holland.png)

As testing capacity is limited the numbers of positively tested people doesn't give a realistic picture of the outbreak. Using the data of people being hospitalised should give a more realistic picture.

## :chart_with_upwards_trend: Hospitalisation
### Linear model: Growth rate
Here is the development of the growth factor of hospitalisations over time, with a linear model fit
to try to estimate when the inflection point will occur (or has occurred).

![plots/growthfactor_hospitalisation.png](/plots/growthfactor_hospitalisation.png)

### Sigmoidal model
This then results in the following sigmoidal fit:
![plots/sigmoid_hospitalisation.png](/plots/sigmoid_hospitalisation.png)

## :chart_with_upwards_trend: Fatalities
### Linear model: Growth rate
Here is the development of the growth factor of fatalities over time, with a linear model fit
to try to estimate when the inflection point will occur (or has occurred).

![plots/growthfactor_fatalities.png](/plots/growthfactor_fatalities.png)

### Sigmoidal model
This then results in the following sigmoidal fit:
![plots/sigmoid_fatalities.png](/plots/sigmoid_fatalities.png)

For more information about this approach, please watch
[the YouTube video](https://www.youtube.com/watch?v=Kas0tIxDvrg) that inspired
this approach, by Grant Sanderson
([3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)).


## Sources

The used [datasets](https://github.com/J535D165/CoronaWatchNL/tree/master/data-geo) are obtained from the following sources.

| Source | Institute |Collected variables |
|---|---| --- |
| https://www.rivm.nl/nieuws/actuele-informatie-over-coronavirus | RIVM | Positively tested patients, Fatalities (total), Hospitalized (total) |
| https://www.rivm.nl/coronavirus-kaart-van-nederland-per-gemeente | RIVM | Positive tests per municipality |
| https://www.rivm.nl/nieuws/actuele-informatie-over-coronavirus/data | RIVM | Epidemiological reports |
| https://www.stichting-nice.nl/ | Stichting NICE | Postively tested patients admitted to IC, Number of ICUs with positively tested patient(s), Number of fatal IC cases, Number of survived IC cases  |


## Remarks

Since 3 March 2020, RIVM reports the number of diagnoses with the coronavirus and their municipality of residence on a daily base. The data contains the total number of positively tested patients. It is not a dataset with the current number of sick people in the Netherlands. The RIVM does not currently provide data on people who have been cured.


## License and academic use

The graphs and data are licensed [CC0](https://creativecommons.org/share-your-work/public-domain/cc0/). The original data is copyright RIVM.

For academic use, use presistent data from [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3711575.svg)](https://doi.org/10.5281/zenodo.3711575). This is a persistent copy of the data. Version number refer to the date. Please cite:

```De Bruin, J. (2020). Number of diagnoses with coronavirus disease (COVID-19) in The Netherlands (Version v2020.3.15) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.3711575```

Image from [iXimus](https://pixabay.com/nl/users/iXimus-2352783/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4901881) via [Pixabay](https://pixabay.com/nl/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4901881)


## About CoronaWatchNL

**CoronaWatchNL** is collective of researchers and volunteers in The Netherlands. We aim to make the reported number on COVID-19 disease in The Netherlands FAIR. The project is initiated and maintained by [Utrecht University Research Data Management Support](https://www.uu.nl/en/research/research-data-management) and receives support from [Utrecht University Applied Data Science](https://www.uu.nl/en/research/applied-data-science). 

Help on this project is appreciated. We are looking for new graphs, forecasts, and maps. Please report issues in the Issue Tracker. Want to contribute? Please check out the `help wanted` tag in the [Issue Tracker](https://github.com/J535D165/CoronaWatchNL/issues). Do you wish to share an application related to these visuals? Have a look at the CoronaWatchNL [applications folder](https://github.com/J535D165/CoronaWatchNL/tree/master/applications).

Please send an email to jonathandebruinos@gmail.com and/or r.voorvaart@uu.nl
",2022-08-12
https://github.com/J535D165/data-matching-software,"# Data Matching software

- [Overview](#overview)
- [Software](#software)
- [Outdated](#outdated-no-longer-available)
- [Contributing](#contributing)

This is a list of (Fuzzy) Data Matching software. The software in this list is
open source and/or freely available.

The term data matching is used to indicate the procedure of bringing together
information from two or more records that are believed to belong to the same
entity. Data matching has two applications: (1) to match data across multiple
datasets (linkage) and (2) to match data within a dataset (deduplication).
See the [Wikipedia page](https://en.wikipedia.org/wiki/Record_linkage) about
data matching for more information.

*Similar terms:* record linkage, data matching, deduplication, fuzzy matching,
    entity resolution

## Overview

The table below gives a dense overview of data matching software properties.
The properties evaluated are [Application Programming Interface
(API)](https://en.wikipedia.org/wiki/Application_programming_interface),
[Graphical User Interface
(GUI)](https://en.wikipedia.org/wiki/Graphical_user_interface), Linking,
Deduplication, [Supervised
Learning](https://en.wikipedia.org/wiki/Supervised_learning), [Unsupervised
Learning](https://en.wikipedia.org/wiki/Unsupervised_learning) and [Active
Learning](https://en.wikipedia.org/wiki/Active_learning_(machine_learning)).

| Software                                                        | API    |        GUI         |        Link        |       Dedup        | Supervised <br/> Learning | Unsupervised <br/> Learning | Active <br/> Learning |
|:----------------------------------------------------------------|:-------|:------------------:|:------------------:|:------------------:|:-------------------------:|:---------------------------:|:---------------------:|
| [AtyImo](#atyimo)		                                  | PySpark|         :x:	| :white_check_mark: | :white_check_mark: |            :x:            |             :x:               |          :x:          |
| [Dedupe](#dedupe)                                               | Python |        :x:         | :white_check_mark: | :white_check_mark: |    :white_check_mark:     |             :x:             |  :white_check_mark:   |
| [fastLink](#fastlink)                                           | R      |        :x:         | :white_check_mark: |  :grey_question:   |            :x:            |     :white_check_mark:      |          :x:          |
| [FEBRL](#febrl)                                                 | Python | :white_check_mark: | :white_check_mark: | :white_check_mark: |            :x:            |             :x:             |          :x:          |
| [FRIL](#fril)                                                   | Java   | :white_check_mark: | :white_check_mark: |        :x:         |      :grey_question:      |     :white_check_mark:      |          :x:          |
| [FuzzyMatcher](#fuzzymatcher)                                   | Python |        :x:         | :white_check_mark: |        :x:         |            :x:            |     :white_check_mark:      |          :x:          |
| [JedAI](#jedai)                                                 | Java   | :white_check_mark: | :white_check_mark: |  :grey_question:   |    :white_check_mark:     |       :grey_question:       |    :grey_question:    |
| [PRIL](#pril)                                                   | SQL    |        :x:         | :white_check_mark: |  :grey_question:   |      :grey_question:      |       :grey_question:       |    :grey_question:    |
| [Python Record Linkage Toolkit](#python-record-linkage-toolkit) | Python |        :x:         | :white_check_mark: | :white_check_mark: |    :white_check_mark:     |     :white_check_mark:      |          :x:          |
| [RecordLinkage (R)](#recordlinkage-r)                           | R      |        :x:         | :white_check_mark: | :white_check_mark: |    :white_check_mark:     |     :white_check_mark:      |          :x:          |
| [Reclin2](#reclin2)                           | R      |        :x:         | :white_check_mark: | :white_check_mark: |    :white_check_mark:     |     :x:      |          :x:          |
| [RELAIS](#relais)                                               | :x:    | :white_check_mark: | :white_check_mark: |  :grey_question:   |      :grey_question:      |     :white_check_mark:      |          :x:          |
| [ReMaDDer](#remadder)                                           | :x:    | :white_check_mark: | :white_check_mark: | :white_check_mark: |            :x:            |     :white_check_mark:      |          :x:          |
| [RLTK](#rltk) | Python |        :x:         | :white_check_mark: | :white_check_mark: |    :white_check_mark:     |     :x:      |          :x:          |
| [Splink](#splink)                                               | PySpark|        :x:         | :white_check_mark: | :white_check_mark: |            :x:            |     :white_check_mark:      |          :x:          |


:white_check_mark: Yes/Implemented
:x: No/Not implemented
:grey_question: Unknown

## Software

This section describes **data matching** software. The software is
alphabetically ordered.

#### [AtyImo](https://github.com/pierrepita/atyimo)
AtyImo implements a mixture of deterministic and probabilistic routines for data 
linkage. Initially developed in 2013 to serve as a linkage tool supporting a joint 
Brazil–U.K. project aiming at building a large population-based cohort with data 
from more than 100 million participants and producing disease-specific data to facilitate 
diverse epidemiological research studies. 

|  |  |
|---|---| 
| License | ![GitHub](https://img.shields.io/github/license/pierrepita/atyimo) |
| Language | `Python` `Spark` | 
| Latest release | NA |
| Downloads per month |  |
| GitHub stars | [![GitHub stars](https://img.shields.io/github/stars/pierrepita/atyimo.svg?style=social&label=Star)](https://github.com/pierrepita/atyimo) |

#### [Dedupe](https://github.com/dedupeio/dedupe)

Dedupe is a python library for fuzzy matching, deduplication and entity
resolution on structured data. The library makes use of active learning to
match record pairs. Active learning is useful in cases without training data.
Dedupe has a side-product for deduplicating CSV files,
[csvdedupe](https://github.com/dedupeio/csvdedupe), through the command line.
Dedupeio also offers commercial products for data matching.  

|  |  |
|---|---| 
| License | ![PyPI - License](https://img.shields.io/pypi/l/dedupe) |
| Language | ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/dedupe) | 
| Latest release | [![PyPI](https://img.shields.io/pypi/v/dedupe.svg)](https://pypi.python.org/pypi/dedupe/) |
| Downloads per month | ![PyPI - Downloads](https://img.shields.io/pypi/dm/dedupe) |
| GitHub stars | [![GitHub stars](https://img.shields.io/github/stars/dedupeio/dedupe.svg?style=social&label=Star)](https://github.com/dedupeio/dedupe) |

#### [fastLink](https://cran.r-project.org/web/packages/fastLink/index.html)

Implements a Fellegi-Sunter probabilistic record linkage model that allows for
missing data and the inclusion of auxiliary information. This includes
functionalities to conduct a merge of two datasets under the Fellegi-Sunter
model using the Expectation-Maximization algorithm. fastLink is a programming
API written in R. ([Enamorado, Fifield & Imai,
2017](http://imai.princeton.edu/research/files/linkage.pdf))  [[source
code]](https://github.com/kosukeimai/fastLink) 

|  |  |
|---|---| 
| License | ![CRAN/METACRAN](https://img.shields.io/cran/l/fastLink) |
| Language | `R`  | 
| Latest release | [![CRAN](https://img.shields.io/cran/v/fastLink.svg)](https://cran.r-project.org/web/packages/fastLink/index.html) |
| Downloads per month | [![metacran downloads](https://cranlogs.r-pkg.org/badges/last-month/fastLink)](https://cran.r-project.org/package=fastLink) |
| GitHub stars | [![GitHub stars](https://img.shields.io/github/stars/kosukeimai/fastLink.svg?style=social&label=Star)](https://github.com/kosukeimai/fastLink) |

#### [FEBRL](https://sourceforge.net/projects/febrl/)

Febrl (Freely Extensible Biomedical Record Linkage) is a training tool
suitable for users to learn and experiment with record linkage techniques, as
well as for practitioners to conduct linkages with data sets containing up to
several hundred thousand records. Febrl is a data matching tool with a large
number of algorithms implemented and offers a Python programming interface as
well as simple GUI. Febrl doesn't offer unsupervised and active learning
algorithms. The software is no longer actively maintained. ([Christen,
2008](http://crpit.com/confpapers/CRPITV80Christen.pdf)) [[source
code]](https://sourceforge.net/projects/febrl/)

|  |  |
|---|---| 
| License | Custom |
| Language | `Python` | 
| Latest release |  |
| Downloads per month |  |
| GitHub stars |  |

#### [FRIL](http://fril.sourceforge.net/)

FRIL (Fine-grained Records Integration and Linkage tool) is free tool that
enables record linkage through a GUI. The tool implements automatic weights
estimation through the EM-algorithm and offers serveral techniques to make
record pairs. FRIL was developed by the Emory University and is not longer
maintained. [[source code]](http://fril.sourceforge.net/download.html)

|  |  |
|---|---| 
| License | Custom |
| Language | `Java` | 
| Latest release |  |
| Downloads per month |  |
| GitHub stars |  |

#### [FuzzyMatcher](https://pypi.python.org/pypi/fuzzymatcher) 

A Python package that allows the user to fuzzy match two pandas dataframes
based on one or more fields in common. The functionality is limited at the
moment. [[source code]](https://github.com/RobinL/fuzzymatcher) 

|  |  |
|---|---| 
| License | ![PyPI - License](https://img.shields.io/pypi/l/fuzzymatcher) |
| Language | ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/fuzzymatcher) | 
| Latest release | [![PyPI](https://img.shields.io/pypi/v/fuzzymatcher.svg)](https://pypi.python.org/pypi/fuzzymatcher/) |
| Downloads per month | ![PyPI - Downloads](https://img.shields.io/pypi/dm/fuzzymatcher) |
| GitHub stars | [![GitHub stars](https://img.shields.io/github/stars/RobinL/fuzzymatcher.svg?style=social&label=Star)](https://github.com/RobinL/fuzzymatcher) |

#### [JedAI](http://jedai.scify.org/) 

Java gEneric DAta Integration (JedAI) Toolkit is a Entity Resolution Tool
developed by a group of univeristies. JedAI offers a Graphical User Interface.
[[source code]](https://github.com/scify/JedAIToolkit) 

|  |  |
|---|---| 
| License | ![GitHub](https://img.shields.io/github/license/scify/JedAIToolkit) |
| Language | `Java` | 
| Latest release |  |
| Downloads per month |  |
| GitHub stars | [![GitHub stars](https://img.shields.io/github/stars/scify/JedAIToolkit.svg?style=social&label=Star)](https://github.com/scify/JedAIToolkit) |

#### [PRIL](https://github.com/LSHTM-ALPHAnetwork/PIRL_RecordLinkageSoftware) 

PRIL (Point-of-contact Interactive Record Linkage) is a record linkage program
with a GUI. PRIL can be used to link datasets about individuals. ([Rentsch CT,
Kabudula CW, Catlett J et al.,
2017](https://gatesopenresearch.org/articles/1-8/v1)) [[source
code]](https://github.com/LSHTM-ALPHAnetwork/PIRL_RecordLinkageSoftware)

|  |  |
|---|---| 
| License | ![GitHub](https://img.shields.io/github/license/LSHTM-ALPHAnetwork/PIRL_RecordLinkageSoftware) |
| Language | `SQLPL` | 
| Latest release |  |
| Downloads per month |  |
| GitHub stars | [![GitHub stars](https://img.shields.io/github/stars/LSHTM-ALPHAnetwork/PIRL_RecordLinkageSoftware.svg?style=social&label=Star)](https://github.com/LSHTM-ALPHAnetwork/PIRL_RecordLinkageSoftware) |

#### [Python Record Linkage Toolkit](https://github.com/J535D165/recordlinkage) 

The Python Record Linkage Toolkit is a library to link records in or between
data sources. The toolkit provides most of the tools needed for record linkage
and deduplication. The package is developed for research and the linking of
small or medium sized files. 

|  |  |
|---|---| 
| License | ![PyPI - License](https://img.shields.io/pypi/l/recordlinkage) |
| Language | ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/recordlinkage) | 
| Latest release | [![PyPI](https://img.shields.io/pypi/v/recordlinkage.svg)](https://pypi.python.org/pypi/recordlinkage/) |
| Downloads per month | ![PyPI - Downloads](https://img.shields.io/pypi/dm/recordlinkage) |
| GitHub stars | [![GitHub stars](https://img.shields.io/github/stars/J535D165/recordlinkage.svg?style=social&label=Star)](https://github.com/J535D165/recordlinkage) |

#### [RecordLinkage (R)](https://cran.r-project.org/web/packages/RecordLinkage/index.html) 

Package written in R that provides functions for linking and de-duplicating
data sets. Both supervised and unsupervised classification algorithms are
available. Record pairs can be compared with a limited set of algorithms. The
package is published on CRAN. 

|  |  |
|---|---| 
| License | ![CRAN/METACRAN](https://img.shields.io/cran/l/RecordLinkage) |
| Language | `R` | 
| Latest release | [![CRAN](https://img.shields.io/cran/v/RecordLinkage.svg)](https://cran.r-project.org/web/packages/RecordLinkage/index.html) |
| Downloads per month | [![metacran downloads](https://cranlogs.r-pkg.org/badges/last-month/RecordLinkage)](https://cran.r-project.org/package=RecordLinkage) |
| GitHub stars |  |


#### [Reclin2](https://github.com/djvanderlaan/reclin2)

Package written in R that provides functions for linking data sets. The framework offers
the option to compute the weigths of the Fellegi-Sunter model. It doesn't implement an
undersupervised algorithms to predict the cutoff. The
package is published on CRAN. Formerly https://github.com/djvanderlaan/reclin. 

|  |  |
|---|---| 
| License | ![CRAN/METACRAN](https://img.shields.io/cran/l/reclin2) |
| Language | `R` | 
| Latest release | [![CRAN](https://img.shields.io/cran/v/reclin2.svg)](https://cran.r-project.org/web/packages/reclin2/index.html) |
| Downloads per month | [![metacran downloads](https://cranlogs.r-pkg.org/badges/last-month/reclin2)](https://cran.r-project.org/package=reclin2) |
| GitHub stars | [![GitHub stars](https://img.shields.io/github/stars/djvanderlaan/reclin2.svg?style=social&label=Star)](https://github.com/djvanderlaan/reclin2) |

#### [RELAIS](https://www.istat.it/en/methods-and-tools/methods-and-it-tools/process/processing-tools/relais)

RELAIS (REcord Linkage At IStat) is a toolkit providing a set of techniques
for dealing with record linkage projects. IStat is the main producer of
official statistics in Italy.

|  |  |
|---|---| 
| License | `EUPL-1.1` |
| Language | `R/Java` | 
| Latest release |  |
| Downloads per month |  |
| GitHub stars |  |

#### [ReMaDDer](http://remadder.findmysoft.com/)

ReMaDDer is unsupervised free fuzzy data matching software with a GUI.
ReMaDDer is capable to perform fully automatic fuzzy record matching without
human expert intervention, while attaining accuracy of human clerical review.
NOTE: The software is free, but not open source and requires an internet
connection to work.

|  |  |
|---|---| 
| License |  |
| Language |  | 
| Latest release |  |
| Downloads per month |  |
| GitHub stars |  |

#### [RLTK](https://github.com/usc-isi-i2/rltk)

The Record Linkage ToolKit (RLTK) is a general-purpose open-source record
linkage package. The toolkit provides a full pipeline needed for record linkage
and deduplication. 

|  |  |
|---|---| 
| License | ![PyPI - License](https://img.shields.io/pypi/l/rltk) |
| Language | `Python` | 
| Latest release | [![PyPI](https://img.shields.io/pypi/v/rltk.svg)](https://pypi.python.org/pypi/rltk/) |
| Downloads per month | ![PyPI - Downloads](https://img.shields.io/pypi/dm/rltk) |
| GitHub stars | [![GitHub stars](https://img.shields.io/github/stars/usc-isi-i2/rltk.svg?style=social&label=Star)](https://github.com/usc-isi-i2/rltk) |

#### [Splink](https://github.com/moj-analytical-services/splink)

Splink is a Python/PySpark package that implements Fellegi-Sunter's canonical model of
record linkage in Apache Spark. It uses the Expectation Maximisation algorithm to estimate
parameters of the model. It is able to perform linking and deduplication of very large datasets
of tens of millions of records with runtimes of less than an hour. [[source
code]](https://github.com/moj-analytical-services/splink)

|  |  |
|---|---| 
| License | ![PyPI - License](https://img.shields.io/pypi/l/splink) |
| Language | ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/splink) `Spark` | 
| Latest release | [![PyPI](https://img.shields.io/pypi/v/splink.svg)](https://pypi.python.org/pypi/splink/) |
| Downloads per month | ![PyPI - Downloads](https://img.shields.io/pypi/dm/splink) |
| GitHub stars | [![GitHub stars](https://img.shields.io/github/stars/moj-analytical-services/splink.svg?style=social&label=Star)](https://github.com/moj-analytical-services/splink) |



## Outdated/ no longer available

#### BigMatch (by USA census)

A record linkage tool for use in matching a very large file against a moderate
size file developed by the USA Census Bureau. There are several papers
available about this program [(BigMatch,
2007)](https://www.census.gov/srd/papers/pdf/rrc2007-01.pdf)

#### [The Link King](http://the-link-king.party/) 

The Link King’s graphical user interface (GUI) makes record linkage and
unduplication easy for beginning and advanced users. The software requires a
SAS license. `SAS`

## Contributing

Do you know an open source and/or free data matching tool? Please open an
issue or do a Pull Request. The same holds for missing or incomplete
information.

This project is initiated by the author of the [Python Record Linkage
Toolkit](https://github.com/J535D165/recordlinkage) @J535D165. The aim is to
get a list and comparison of data matching software. 

This list is licensed under [CC-BY-SA 3.0](http://creativecommons.org/licenses/by-sa/3.0/). 
",2022-08-12
https://github.com/J535D165/Data-Science-Day,"# Data Science Day Workshop - Data Engineering: Clean and Integrate Your Data!

*Authors: [Jonathan de Bruin](https://www.uu.nl/medewerkers/JdeBruin1) and [Frans de Liagre Böhl](https://www.uu.nl/medewerkers/FMTdeLiagreBohl)*

This repository contains the material for the workshop *""Data Engineering: Clean and Integrate Your Data!""*. The workshop is part of the [Data Science Day](https://www.uu.nl/en/organisation/information-and-technology-services-its/13-april-2017-data-science-day/programme-overview/workshops/data-engineering-clean-and-integrate-your-data-workshop) of the Utrecht University. The workshop covers the basics of data cleaning and data integration. Examples of data cleaning, imputing and integrating are demonstrated with Open Refine, Python and R. In the workshop, we study the relation between mortality and diseases. This is done by linking a mortality register dataset with a dataset containing medical (diagnosis) records. The datasets are fictitious. A research questions can be: Is there a relation between (surviving) Salmonella infection and the age of death?

## Material 

- [Github](https://github.com/J535D165/Data-Science-Day/)
- [Data](https://github.com/J535D165/Data-Science-Day/tree/master/data/)
- [Presentation](https://github.com/J535D165/Data-Science-Day/tree/master/presentation/)
- [License: CC BY 4.0](http://creativecommons.org/licenses/by/4.0/)
",2022-08-12
https://github.com/J535D165/FEBRL-fork-v0.4.2,"Febrl (Freely extensible biomedical record linkage) Version 0.4.2

Copyright 2002 - 2011 Australian National University and others.
All Rights reserved.

See the file ANUOS-1.3.txt or the manual for the terms under which
the computer program code and associated documentation and data files
in the Febrl package are licensed.


Peter Christen, 14 December 2011.
",2022-08-12
https://github.com/J535D165/J535D165,"#  Hi, I'm `Jonathan` `//J535D165`

I'm a research software engineer and project leader working on advancing scientific research with modern and open technology. The scientific projects I'm involved in cover a wide range of areas like computer science, social science, and humanities. My favorite topics are [data linkage](https://www.github.com/J535D165/recordlinkage), open and reproducible workflows, and API design. I'm project lead of the open and [FAIR](https://www.go-fair.org/fair-principles/) Data and Software movement at [Utrecht University](https://www.uu.nl/en/research/open-science/tracks/fair-data-and-software). 


## 🌎 💬 🌱
- How to reach me? Send me an email (see bio) 📫 
- Donate to https://github.com/sponsors/J535D165 or https://asreview.ai/donate :moneybag:
- Wondering what my username _J535D165_ is about? Learn about the [Soundex algorithm](https://www.ics.uci.edu/~dan/genealogy/Miller/javascrp/soundex.htm) 💬
",2022-08-12
https://github.com/J535D165/libact-lite,"# libact-lite: Pool-based Active Learning in Python

Lite version of the [libact](https://github.com/ntucllab/libact) package by Yao-Yuan Yang, Shao-Chuan Lee, Yu-An Chung, Tung-En Wu, Si-An Chen and Hsuan-Tien Lin. 

The libact package is great, altough it is a bit hard to install. This 'lite' version strips all components that need compilation (SVM and variance reduction). The framework still working. 

Modification by Jonathan de Bruin. All modifications are BSD 2-Clause licensed.
",2022-08-12
https://github.com/J535D165/PublicSectorNL,"# Open source & Dutch public sector

Open source gains popularity in the public sector of the Netherlands. This is
a list of profiles on open source platforms in use by organizations in the
Dutch public sector.

> Open source wint aan populariteit in de publieke sector van Nederland. Dit is een lijst met profielen op open source platforms die in gebruik zijn door organisaties in de Nederlandse publieke sector.

- [Open source & Dutch public sector](#open-source---dutch-public-sector)
  * [Organisations](#organisations)
    + [Ministries](#ministries)
    + [Provinces](#provinces)
    + [Municipalities](#municipalities)
    + [Judiciary](#judiciary)
    + [Emergency services and law enforcement](#emergency-services-and-law-enforcement)
    + [Transport, Land, and Water Management](#transport-land-and-water-management)
    + [Data and statistics](#data-and-statistics)
    + [Financial](#financial)
    + [Research Institutes](#research-institutes)
    + [Libraries](#libraries)
    + [Universities](#universities)
    + [University Medical Centers](#university-medical-centers)
    + [University of Applied Sciences](#university-of-applied-sciences)
  * [Contributing](#contributing)
  * [About](#about)


---

![logo_PublicSectorNL.png](docs/img/logo_PublicSectorNL.png)


## Organisations

### Ministries

*High governmental organisations in the Netherlands headed by a minister*

* Ministerie van Algemene Zaken
    * [Dienst Publiek en Communicatie](https://github.com/azdpc)
* [Ministerie van Binnenlandse Zaken en Koninkrijksrelaties](https://github.com/MinBZK)
    * [Logius](https://github.com/LogiusNL)
    * [Logius Standaarden](https://github.com/Logius-standaarden)
* Ministerie van Economische Zaken
    * [Dienst ICT Uitvoering (DICTU)](https://github.com/Dictu)
    * [GIS Competence Centre](https://github.com/MinELenI)
* [Ministerie van Infrastructuur en Milieu](https://github.com/MinIenM)
* Ministerie van Justitie en Veiligheid
    * [Nationaal Cyber Security Centrum (NCSC)](https://github.com/NCSC-NL)
* Ministerie van Sociale Zaken en Werkgelegenheid
    * Sociale Verzekeringsbank (SVB)
        * [Novum](https://gitlab.com/NovumGit)
* Ministerie van Onderwijs, Cultuur en Wetenschap
    * [Rijksdienst voor het Cultureel Erfgoed](https://github.com/cultureelerfgoed)
* [Ministerie van Volksgezondheid, Welzijn en Sport](https://github.com/minvws)


### Provinces

*The ‘middle tier’ of public administration in the Netherlands*

* [Provincie Groningen](https://github.com/ProvincieGroningen)
* [Provincie Noord-Holland](https://github.com/provincieNH)
* [Provincie Zuid-Holland](https://github.com/ProvZH)

### Municipalities

*The public administration of municipalities in the Netherlands*

* [Gemeente Amsterdam](https://github.com/Amsterdam)
* [Gemeente Delft](https://github.com/Gemeente-Delft)
* [Gemeente Haarlem](https://github.com/Haarlem)
* [Gemeente 's-Hertogenbosch](https://github.com/gemeenteshertogenbosch)
* [Gemeente Nijmegen](https://github.com/GemeenteNijmegen)
* [Gemeente Súdwest-Fryslân](https://github.com/Sudwest-Fryslan)
* [Gemeente Utrecht](https://github.com/GemeenteUtrecht)
* [Gemeente Zaanstad](https://github.com/zaanstad)

Partnerships between municipalities:

* Vereniging van Nederlandse Gemeenten (VNG) Realisatie
    * [VNG Realisatie (GitHub)](https://github.com/VNG-Realisatie)
    * [VNG Realisatie (GitLab)](https://gitlab.com/vng-realisatie)
    * [Common Ground](https://gitlab.com/commonground)

### Judiciary

*The judiciary power in the Netherlands*

* [Staten-Generaal](https://github.com/statengeneraal)

### Emergency services and law enforcement

*Emergency services and law enforcement in the Netherlands*

* [Nationale Politie](https://github.com/politie)

### Transport, Land, and Water Management

*Organisations in field of managing the public space*

* Rijkswaterstaat
    * [Rijkswaterstaat 1](https://github.com/Rijkswaterstaat)
    * [Rijkswaterstaat 2](https://github.com/RWS-NL)
* [Landelijke voorziening BAG](https://github.com/lvbag)
* [Kadaster](https://github.com/kadaster)
    * [Labs](https://github.com/kadaster-labs)
    * [PDOK](https://github.com/pdok) (Publieke Diensten Op de Kaart)

### Data and statistics

*Organisations in field of national data and statistics*

* [Centraal Bureau voor de Statistiek (CBS) (English: National Statistics)](https://github.com/statistiekcbs)
* [Dutch National Data portal (Data.overheid.nl)](https://github.com/dataoverheid)
* [Geonovum](https://github.com/Geonovum)

### Financial

*Organizations related to finance*

* [De Nederlandsche Bank (DNB)](https://github.com/DeNederlandscheBank)

### Research Institutes

*Research institutes in the Netherlands*

* [DANS-KNAW](https://github.com/DANS-KNAW)
* [Huygens ING](https://github.com/HuygensING)
* [KNAW Digital Humanities Lab](https://github.com/dhlab-nl)
* [KNAW Humanities Cluster](https://github.com/knaw-huc)
* [Meertens Institute](https://github.com/meertensinstituut)
* Netherlands Cancer Institute (NKI)
    * [Computational Cancer Biology group (CCB)](https://github.com/nki-ccb)
    * [Genomics Core Facility](https://github.com/nki-gcf)
* National Institute for Subatomic Physics (Nikhef)
    * [Nikhef Data Processing Facility](https://github.com/NDPF)
* [Netherlands eScience Center](https://github.com/NLeSC)
* [Netherlands Institute for Space Research (SRON)](https://gitlab.sron.nl/explore)
* [Netherlands Institute of Ecology (NIOO-KNAW)](https://github.com/nioo-knaw)
* [Rijksinstituut voor Volksgezondheid en Milieu (RIVM)](https://github.com/rivm-syso)
* [Royal Netherlands Meteorological Institute (KNMI)](https://github.com/KNMI)
* [TNO](https://github.com/TNO)

### Libraries

*Libraries in the Netherlands*

* [National Library of the Netherlands (Koninklijke bibliotheek)](https://github.com/KBNLresearch)

### Universities

*Universities in the Netherlands*

*Universities can have many suborganizations. We aim to be inclusive.*

* Erasmus University Rotterdam
    * [Erasmus University Library](https://github.com/erasmus-university-library)
    * [Rotterdam School of Management](https://github.com/eur-rsm)
* Groningen University
    * [Center for Information Technology & Educational Support and Innovation](https://github.com/rijksuniversiteit-groningen)
    * [Geodienst](https://github.com/geodienst)
    * [High Performance Computing - Center for Information Technology](https://github.com/rug-cit-hpc)
* Leiden University
    * [Leiden University Library](https://github.com/LeidenUniversityLibrary)
    * [Leiden University Centre for Innovation](https://github.com/LU-C4i)
    * [Leiden Institute of Advanced Computer Science](https://git.liacs.nl/) (GitLab, self-hosted)
* Maastricht University
    * [Cognitive Computational Neuroscience](https://github.com/ccnmaastricht)
    * [DataHub Maastricht](https://github.com/MaastrichtUniversity)
    * [Institute of Data Science](https://github.com/MaastrichtU-IDS)
    * [Maastricht Centre for Systems Biology](https://github.com/macsbio)
    * [Maastricht Law & Tech Lab](https://github.com/maastrichtlawtech)
* [Radboud University](https://github.com/radbouduniversity)
    * [Radboud University (Open Science?)](https://github.com/Radboud-University)
    * [Faculty of Science (GitLab, self-hosted)](https://gitlab.science.ru.nl/explore)
    * [Faculty of Social Sciences (GitLab self-hosted)](https://gitlab.socsci.ru.nl/explore)
    * [NLP Research group at Centre for Language Studies](https://github.com/LanguageMachines)
    * [Donders Institute for Brain, Cognition and Behaviour](https://github.com/Donders-Institute)
* TU Delft [(GitLab, self-hosted)](https://gitlab.tudelft.nl/explore)
    * [3D geoinformation](https://github.com/tudelft3d)
    * [Micro Air Vehicle Laboratory](https://github.com/tudelft)
    * [QuTech](https://github.com/QuTech-Delft)
* TU Eindhoven [(GitLab, self-hosted)](https://gitlab.tue.nl/explore)
    * [Robotics Group - RoboCup team](https://github.com/tue-robotics)
    * [Model Driven Software Engineering](https://github.com/tue-mdse)
* University of Amsterdam
    * [Computer Vision](https://github.com/uvavision)
    * [Deep Learning](https://github.com/uvadlc)
    * [Faculteit der Natuurwetenschappen, Wiskunde en Informatica](https://gitlab-fnwi.uva.nl/explore)
    * [Minor programmeren](https://github.com/uva)
* [University of Twente](https://github.com/utwente)
    * [Formal Methods and Tools](https://github.com/utwente-fmt)
    * [RoboCup team of the University of Twente](https://github.com/RoboTeamTwente)
* Utrecht University
    * [Research IT](https://github.com/UtrechtUniversity)
    * [Webdiensten](https://github.com/UniversiteitUtrecht)
    * [Digital Humanities Lab](https://github.com/UUDigitalHumanitieslab)
    * [Institute for Marine and Atmospheric research Utrecht](https://github.com/UU-IMAU)
    * [Science Faculty (GitLab self-hosted)](https://git.science.uu.nl/explore)
* VU Amsterdam
    * [University Library 1](https://github.com/VUAmsterdam-UniversityLibrary)
    * [University Library 2](https://github.com/ubvu)
    * [Software and Sustainability Group](https://github.com/S2-group)
    * [Instituut voor Milieuvraagstukken](https://github.com/VU-IVM)
* Wageningen University & Research [(GitLab, self-hosted)](https://git.wageningenur.nl/explore)
    * [Geo-scripting](https://github.com/GeoScripting-WUR)
    * [Production Ecology & Resource Conservation](https://github.com/wageningen)


### University Medical Centers

*University medical centers are hospitals with a close relationship between the medical faculty and the hospital*

* [AMC](https://github.com/AmsterdamUMC)
* Erasmus Medical Center
    * [Bioinformatics](https://github.com/ErasmusMC-Bioinformatics)
    * [Medical Informatics](https://github.com/mi-erasmusmc)
* LUMC
    * [LUMC](https://github.com/LUMC)
    * [LUMC GitLab](https://git.lumc.nl/explore) (GitLab, self-hosted)
* [Radboud UMC](https://github.com/Radboudumc)
* UMC Groningen
    * [Medical Imaging Center](https://github.com/mic-umcg)
* UMC Utrecht
    * [Medical microbiology department](https://gitlab.com/mmb-umcu)
    * [Genetics](https://github.com/UMCUGenetics)
    * [Registration and Imaging of Brain Systems](https://github.com/UMCU-RIBS)
* VUMC
    * [IT](https://github.com/VUMC-IT)
    * [Tumor Genome Analysis Core](https://github.com/tgac-vumc)
    * [Vanderbilt Institute for Clinical and Translational Research](https://github.com/vanderbilt)
    * [Clinical Genetics/Pathology](https://github.com/VUmcCGP)

### University of Applied Sciences

*Universities of Applied Sciences (Dutch: hogescholen) in the Netherlands*

* Haagse Hogeschool
    * [Technische Natuurkunde](https://github.com/HHS-TN)
* [Hogeschool Rotterdam](https://github.com/hogeschoolrotterdam)
    * [Beheer, Infrastructuur & Telecom](https://github.com/hr-bit)
    * [Creative Media and Game Technologies](https://github.com/HR-CMGT)
    * [Hogeschool Rotterdam](https://github.com/hogeschool)
* [Hogeschool Utrecht](https://github.com/uashogeschoolutrecht)
    * [HU Institute for ICT](https://github.com/HUInstituteForICT)
* Hogeschool Zeeland
    * [HBO - ICT](https://github.com/HZ-HBO-ICT)
* [Hogeschool der Kunsten Den Haag](https://github.com/kabk)

## Contributing

Your contributions are always welcome!

Everyone with a Github account can add the details of missing institutes. No git skills required. It's super easy!

* Navigate to this [link](https://github.com/J535D165/PublicSectorNL/edit/master/README.md).
* Add the link.
    * Use the format `* [institution-name](http://example.com/)`
    * Add a section if needed (don't forget the description and Table of Contents).
* Create a Pull Request.
    * No need to make an issue first.
    * Make sure the PR title is in the format of `Add institution-name`.
* Submit Pull Request

## About

Open software and open data is important for a transparent and efficient
public sector. Many organizations in the public sector have open source
activities. An example is the [Dutch COVID-19 Notification App
CoronaMelder](https://github.com/minvws/nl-covid19-notification-app-website)
by the [Ministry of Health, Welfare and Sport](https://github.com/minvws).

The [PublicSectorNL](https://github.com/J535D165/PublicSectorNL) project aims 
to make the accounts and projects of the Dutch public
sector easily findable. Findability is the key to reuse and collaboration. 

The project is initiated and maintained by [Jonathan
de Bruin](https://github.com/J535D165), [Wouter van
Bijsterveld](https://github.com/wbijster), and [Paul van Gent](https://github.com/paulvangentcom).
Special thanks to [Sietse Snel](https://github.com/stsnel) for contributing a large 
number of organizations to this list. The list of organizations
is published under a CC-BY 4.0 license. We decided to write this repository in 
English because the content might be interesting for non-Dutch speaking 
developers, governments, organizations. Some organizations are more recognizable
in Dutch and therefore enlisted by their Dutch name. In general, this project lists
administrative organizations in Dutch and research institutes and medical centers in English.
If relevant, a translation can be added. 

Inspired and technically based on [awesome-python](https://github.com/vinta/awesome-python) ([CC-BY 4.0 license](https://github.com/vinta/awesome-python/blob/master/LICENSE)).


",2022-08-12
https://github.com/J535D165/recordlinkage,"<div align=""center"">
  <img src=""https://raw.githubusercontent.com/J535D165/recordlinkage/master/docs/images/recordlinkage-banner-transparent.svg""><br>
</div>

# RecordLinkage: powerful and modular Python record linkage toolkit

[![Pypi Version](https://badge.fury.io/py/recordlinkage.svg)](https://pypi.python.org/pypi/recordlinkage/)
[![Github Actions CI Status](https://github.com/J535D165/recordlinkage/workflows/tests/badge.svg?branch=master)](https://github.com/J535D165/recordlinkage/actions)
[![Code Coverage](https://codecov.io/gh/J535D165/recordlinkage/branch/master/graph/badge.svg)](https://codecov.io/gh/J535D165/recordlinkage)
[![Documentation Status](https://readthedocs.org/projects/recordlinkage/badge/?version=latest)](https://recordlinkage.readthedocs.io/en/latest/?badge=latest)
[![Zenodo DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3559042.svg)](https://doi.org/10.5281/zenodo.3559042)

**RecordLinkage** is a powerful and modular record linkage toolkit to
link records in or between data sources. The toolkit provides most of
the tools needed for record linkage and deduplication. The package
contains indexing methods, functions to compare records and classifiers.
The package is developed for research and the linking of small or medium
sized files.

This project is inspired by the [Freely Extensible Biomedical Record
Linkage (FEBRL)](https://sourceforge.net/projects/febrl/) project, which
is a great project. In contrast with FEBRL, the recordlinkage project
uses [pandas](http://pandas.pydata.org/) and
[numpy](http://www.numpy.org/) for data handling and computations. The
use of *pandas*, a flexible and powerful data analysis and manipulation
library for Python, makes the record linkage process much easier and
faster. The extensive *pandas* library can be used to integrate your
record linkage directly into existing data manipulation projects.

One of the aims of this project is to make an easily extensible record
linkage framework. It is easy to include your own indexing algorithms,
comparison/similarity measures and classifiers.

## Basic linking example

Import the `recordlinkage` module with all important tools for record
linkage and import the data manipulation framework **pandas**.

``` python
import recordlinkage
import pandas
```

Load your data into pandas DataFrames.

``` python
df_a = pandas.DataFrame(YOUR_FIRST_DATASET)
df_b = pandas.DataFrame(YOUR_SECOND_DATASET)
```

Comparing all record can be computationally intensive. Therefore, we
make set of candidate links with one of the built-in indexing techniques
like **blocking**. In this example, only pairs of records that agree on
the surname are returned.

``` python
indexer = recordlinkage.Index()
indexer.block('surname')
candidate_links = indexer.index(df_a, df_b)
```

For each candidate link, compare the records with one of the comparison
or similarity algorithms in the Compare class.

``` python
c = recordlinkage.Compare()

c.string('name_a', 'name_b', method='jarowinkler', threshold=0.85)
c.exact('sex', 'gender')
c.date('dob', 'date_of_birth')
c.string('str_name', 'streetname', method='damerau_levenshtein', threshold=0.7)
c.exact('place', 'placename')
c.numeric('income', 'income', method='gauss', offset=3, scale=3, missing_value=0.5)

# The comparison vectors
feature_vectors = c.compute(candidate_links, df_a, df_b)
```

Classify the candidate links into matching or distinct pairs based on
their comparison result with one of the [classification
algorithms](https://recordlinkage.readthedocs.io/en/latest/ref-classifiers.html).
The following code classifies candidate pairs with a Logistic Regression
classifier. This (supervised machine learning) algorithm requires
training data.

``` python
logrg = recordlinkage.LogisticRegressionClassifier()
logrg.fit(TRAINING_COMPARISON_VECTORS, TRAINING_PAIRS)

logrg.predict(feature_vectors)
```

The following code shows the classification of candidate pairs with the
Expectation-Conditional Maximisation (ECM) algorithm. This variant of
the Expectation-Maximisation algorithm doesn't require training data
(unsupervised machine learning).

``` python
ecm = recordlinkage.ECMClassifier()
ecm.fit_predict(feature_vectors)
```

## Main Features

The main features of this Python record linkage toolkit are:

-   Clean and standardise data with easy to use tools
-   Make pairs of records with smart indexing methods such as
    **blocking** and **sorted neighbourhood indexing**
-   Compare records with a large number of comparison and similarity
    measures for different types of variables such as strings, numbers
    and dates.
-   Several classifications algorithms, both supervised and unsupervised
    algorithms.
-   Common record linkage evaluation tools
-   Several built-in datasets.

## Documentation

The most recent documentation and API reference can be found at
[recordlinkage.readthedocs.org](http://recordlinkage.readthedocs.org/en/latest/).
The documentation provides some basic usage examples like
[deduplication](http://recordlinkage.readthedocs.io/en/latest/notebooks/data_deduplication.html)
and
[linking](http://recordlinkage.readthedocs.io/en/latest/notebooks/link_two_dataframes.html)
census data. More examples are coming soon. If you do have interesting
examples to share, let us know.

## Installation

The Python Record linkage Toolkit requires Python 3.6 or higher. Install the
package easily with pip

``` sh
pip install recordlinkage
```

Python 2.7 users can use version \<= 0.13, but it is advised to use
Python \>= 3.5.

The toolkit depends on popular packages like
[Pandas](https://github.com/pydata/pandas),
[Numpy](http://www.numpy.org), [Scipy](https://www.scipy.org/) and,
[Scikit-learn](http://scikit-learn.org/). A complete list of
dependencies can be found in the [installation
manual](https://recordlinkage.readthedocs.io/en/latest/installation.html)
as well as recommended and optional dependencies.

## License

The license for this record linkage tool is BSD-3-Clause.

## Citation

Please cite this package when being used in an academic context. Ensure
that the DOI and version match the installed version. Citatation styles
can be found on the publishers website
[10.5281/zenodo.3559042](https://doi.org/10.5281/zenodo.3559042).

``` text
@software{de_bruin_j_2019_3559043,
  author       = {De Bruin, J},
  title        = {{Python Record Linkage Toolkit: A toolkit for
                   record linkage and duplicate detection in Python}},
  month        = dec,
  year         = 2019,
  publisher    = {Zenodo},
  version      = {v0.14},
  doi          = {10.5281/zenodo.3559043},
  url          = {https://doi.org/10.5281/zenodo.3559043}
}
```

## Need help?

Stuck on your record linkage code or problem? Any other questions? Don't
hestitate to send me an email (<jonathandebruinos@gmail.com>).
",2022-08-12
https://github.com/J535D165/recordlinkage-annotator,"# RecordLinkage ANNOTATOR

*RecordLinkage ANNOTATOR* is a browser-based user interface for
manual labeling of record pairs. Manual labeled or annotated data is useful 
for training and validation models. The application provides the annotator with a clean and intuitive interface. The annotator labels the record pair as ""Match"" or ""Distinct"". The output can be exported and used for training and validation. This application is part of the 
[Python Record Linkage Toolkit](https://github.com/J535D165/recordlinkage), but can be used as a standalone tool. 

:arrow_right: :arrow_right: Check out the [hosted version of *RecordLinkage ANNOTATOR*](https://j535d165.github.io/recordlinkage-annotator/) :arrow_left: :arrow_left:

Want to give it a try? Download our [**Prison book**](https://raw.githubusercontent.com/J535D165/recordlinkage-annotator/master/examples/annotation_dishonesty.json) example file :cop: :suspect:

[![Review screen of RecordLinkage ANNOTATOR](images/annotator_review.png)](https://j535d165.github.io/recordlinkage-annotator/)

The hosted version of [*RecordLinkage ANNOTATOR*](https://j535d165.github.io/recordlinkage-annotator/) on [Github-pages](https://pages.github.com/) makes use of client-side Javascript 
only. The tool doesn't upload data. Not convinced about the safety of your data? That's 
fine. You can deploy the code yourself locally. Please read the [Development](#development) instructions.  

## Create annotation file

*RecordLinkage ANNOTATOR* requires an annotation file as input. This JSON file contains your record pairs and defines a comparison schema. The [Python Record Linkage Toolkit](https://github.com/J535D165/recordlinkage) can be used to render such a file. For more information, see the documentation on [Manual Labeling](https://recordlinkage.readthedocs.io/en/latest/annotation.html). 

Version 1 of the schema is open source and can be found [here](/schema). Examples of annotation files can be found in the [examples](/examples) folder. 

## Extract results

*RecordLinkage ANNOTATOR* exports the results of the annotation in an annotation file with the same structure as the input annotation file. This makes it simple to review the annotation or continue labeling the data. The [Python Record Linkage Toolkit](https://github.com/J535D165/recordlinkage) can be used to read the annotation file and extract the links and distinct pairs. For more information, see the documentation on [Manual Labeling](https://recordlinkage.readthedocs.io/en/latest/annotation.html).

## Development

*RecordLinkage ANNOTATOR* is a [React](https://reactjs.org/) application. You can develop or deploy the project locally with `npm start`.

## License 

BSD 3-Clause License
",2022-08-12
https://github.com/J535D165/recordlinkage-notebooks,"# Record linkage notebooks

This repository contains notebooks with record linkage experiments. The experiments use the [Python Record Linkage Toolkit](https://github.com/J535D165/recordlinkage) ([documentation](https://github.com/J535D165/recordlinkage))
",2022-08-12
https://github.com/J535D165/recordlinkage-performance,"# Record linkage performance experiments

This repository contains a several python notebooks to compare the performance of record linkage algorithms. The experiments may use the [Python Record Linkage Toolkit](https://github.com/J535D165/recordlinkage) ([documentation](https://github.com/J535D165/recordlinkage)). The experiments cover indexing, comparing and classifcation algorithms. 

",2022-08-12
https://github.com/J535D165/recordlinkage-review,"Record Linkage Review
=====================

Please don't use. Experimental.
===============================

**Record Linkage Review** is a browser-tool to compare two or more records on specified fields. Such a tool is often needed in deduplication or linking datasets. A few of the possible uses:

- Link records in small datasets.
- Verify an existing record linkage.
- Make a golden dataset for training classifiers.

Record Linkage Review has a user friendly interface. The interface is extraction free/low to prevent human errors. 

![Image of RLR](docs/images/review.png)

Getting started
---------------

The **Record Linkage Review** tool runs in the browser and needs a local file server (to prevent issues with cross-site scripting). Start with running a local static file server.  A few examples are listed below: 

Python 2

```shell
$ python -m SimpleHTTPServer 8000
```

Python 3

```shell
$ python -m http.server 8000
```

Node.js

```shell
$ npm install -g node-static   # install node-static
$ static -p 8000
```

Is your favorate programming language not listed above? See https://gist.github.com/willurd/5720255 for more static file servers. 

Then navigate in your browser to http://localhost:8000/review.html. Now you can start comparing records. 

Reviewing
---------

The reviewer reviews the records pairs with it's keyboard or by clicking the classification button.

Features:

- Use keyboard for classification; Right arrow is match, Left arrow is distinct. Use arrow up and down for next record pair. 
- Use buttons for classification
- Use navigation menu for changing categories. 

Configuration
-------------

Before the reviewer can start, a configuration file needs to be made. This file specifies the record files to compare. Here you can also specify which fields to compare with each other. The settings are stored in a JSON file and it has the following structure:

``` json
{
    ""version"":1,
    ""analysis"":""simple"",
    ""records"":{
        ""census1990"":{
            ""filepath"":""census1990.csv"",
            ""type"":""csv"",
            ""delimiter"":"","",
            ""description"":""Census data of 1990."",
            ""index_column"":""id""
        },
        ""census2000"":{
            ""filepath"":""census2000.csv"",
            ""type"":""csv"",
            ""delimiter"":"","",
            ""description"":""Census data of 2000.""
        }
    },
    ""compare"":[
        {
            ""values"":[
                {
                    ""records"":""census1990"",
                    ""label"":""name""                
                },
                {
                    ""records"":""census2000"",
                    ""label"":""firstname""
                }
            ],
            ""type"":""string""
        },
 
        ...

        {
            ""values"":[
                {
                    ""records"":""census1990"",
                    ""label"":""date_of_birth""
                },
                {
                    ""records"":""census2000"",
                    ""label"":""date_of_birth""
                },
            ],
            ""type"":""string""
        }
    ]
}
```

License
-------

The license for this record linkage tool is GPLv3.
",2022-08-12
https://github.com/J535D165/spellingsvarianten-van-nederlandse-plaatsnamen,"# Spellingsvarianten van Nederlandse plaatsnamen 

Deze repository bevat een overzicht van veelvoorkomende spelfouten van **Nederlandse woonplaatsen**. Het bestand is gebaseerd op gegevens van het CBS. Het bestand heeft een variable ``Variant`` welke een spellingsvariant van de daadwerkelijke plaatsnaam kan zijn (ook de juiste versie is terug te vinden). De werkelijke plaatsnaam is gegeven evenals de plaatsnaamcode, de provincie en landsdeel.

In het bestand ``input/miscellaneous.csv`` kunnen plaatsnamen of spellingsvarianten worden toegevoegd waarna ze met ``parse.py`` bij het hoofdbestand kunnen worden gevoegd. Het is belangrijk dat de Woonplaats in dit bestand een woonplaats is die terug te vinden is in ``raw/Woonplaatsen_in_Nederland.csv``. Een voorbeeld is Den Haag als variant van 's-Gravenhage. 

In het bestand ``input/misspelling.csv`` kunnen bekende spellingsfouten/varianten worden toegevoegd. Bijvoorbeeld 'wijk' en 'wijck'. 

De csv heeft als scheidingsteken **;** en is **UTF-8** gecodeerd. 

Voorbeeld van de data:

<table border=""1"" class=""dataframe"">
  <thead>
    <tr style=""text-align: right;"">
      <th>Variant</th>
      <th>Woonplaats</th>
      <th>Woonplaatscode</th>
      <th>Gemeente</th>
      <th>Gemeente_code</th>
      <th>Provincie</th>
      <th>Provincie_code</th>
      <th>Landsdeel</th>
      <th>Landsdeel_code</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>bos &amp; duin</td>
      <td>Bosch en Duin</td>
      <td>2823</td>
      <td>Zeist</td>
      <td>GM0355</td>
      <td>Utrecht</td>
      <td>PV26</td>
      <td>West-Nederland</td>
      <td>LD03</td>
    </tr>
    <tr>
      <td>broeksterwald</td>
      <td>Broeksterwâld</td>
      <td>3265</td>
      <td>Dantumadiel</td>
      <td>GM1891</td>
      <td>Friesland</td>
      <td>PV21</td>
      <td>Noord-Nederland</td>
      <td>LD01</td>
    </tr>
    <tr>
      <td>loenga</td>
      <td>Loënga</td>
      <td>3505</td>
      <td>Súdwest-Fryslân</td>
      <td>GM1900</td>
      <td>Friesland</td>
      <td>PV21</td>
      <td>Noord-Nederland</td>
      <td>LD01</td>
    </tr>
    <tr>
      <td>westerhaar vriezenveensewyk</td>
      <td>Westerhaar-Vriezenveensewijk</td>
      <td>3327</td>
      <td>Twenterand</td>
      <td>GM1700</td>
      <td>Overijssel</td>
      <td>PV23</td>
      <td>Oost-Nederland</td>
      <td>LD02</td>
    </tr>
    <tr>
      <td>kootwijck</td>
      <td>Kootwijk</td>
      <td>1062</td>
      <td>Barneveld</td>
      <td>GM0203</td>
      <td>Gelderland</td>
      <td>PV25</td>
      <td>Oost-Nederland</td>
      <td>LD02</td>
    </tr>
    <tr>
      <td>wanswert</td>
      <td>Wânswert</td>
      <td>3210</td>
      <td>Ferwerderadiel</td>
      <td>GM1722</td>
      <td>Friesland</td>
      <td>PV21</td>
      <td>Noord-Nederland</td>
      <td>LD01</td>
    </tr>
    <tr>
      <td>bleiswijck</td>
      <td>Bleiswijk</td>
      <td>1689</td>
      <td>Lansingerland</td>
      <td>GM1621</td>
      <td>Zuid-Holland</td>
      <td>PV28</td>
      <td>West-Nederland</td>
      <td>LD03</td>
    </tr>
    <tr>
      <td>beek &amp; donk</td>
      <td>Beek en Donk</td>
      <td>1446</td>
      <td>Laarbeek</td>
      <td>GM1659</td>
      <td>Noord-Brabant</td>
      <td>PV30</td>
      <td>Zuid-Nederland</td>
      <td>LD04</td>
    </tr>
    <tr>
      <td>berg &amp; terblijt</td>
      <td>Berg en Terblijt</td>
      <td>1716</td>
      <td>Valkenburg aan de Geul</td>
      <td>GM0994</td>
      <td>Limburg</td>
      <td>PV31</td>
      <td>Zuid-Nederland</td>
      <td>LD04</td>
    </tr>
    <tr>
      <td>sibrandahus</td>
      <td>Sibrandahûs</td>
      <td>3274</td>
      <td>Dantumadiel</td>
      <td>GM1891</td>
      <td>Friesland</td>
      <td>PV21</td>
      <td>Noord-Nederland</td>
      <td>LD01</td>
    </tr>
  </tbody>
</table>

",2022-08-12
https://github.com/JannekeKrabbendam93/Python-Climate-Physics-2021,"# Python-Climate-Physics-2021
This repository contains all information and materials for the introduction to Python for Climate Physics students 2021 of IMAU at Utrecht University. 

# Workshops
There will be four workshops, which will be face to face!
For now, I am planning to meet you each Friday 13:00-15:00, starting on September 17th until October 8th. 
During the workshops, I will first introduce the topic and the exercises, then we will work on the exercises and questions that may arise. Feel free to attend only
one or a few of the workshops if you are already familiar with Python.

1 - [Friday 17 Sept; 13:00-15:00 (Min 2.01)] basic programming in Python: data types, lists, dictionaries, loops, conditionals, functions, basic unix commands

2 - [Friday 24 Sept; 13:00-15:00 (BBG 079)] common Python packages and basic plotting: numpy, matplotlib, scipy

3 - [Friday 1 Oct; 13:00-15:00 (BBG 065)] working with data: pandas, netcdf4/xarray

4 - [Friday 8 Oct; 13:00-15:00 (BBG 065)] mapping with cartopy

# Materials
There are several editors in which you can write Python code, but during this introduction I will provide you with Jupyter Notebooks. 
Jupyter Notebook is an interactive computing environment, through which you are able to write and run Python code. The advantages of using 
such an environment are numerous, among which the possibility of interactively writing and running code, and integrating code and text elements 
in the same document. Jupyter Notebook is part of the Anaconda package, instructions on how to download and install can be found in this repository. The Notebooks for each workshop are also found here.

If you are very new to Python and want some extra practice: there is an app called DataCamp. This is similar to DuoLingo, which most of you probably know, 
but then for coding in Python :)
",2022-08-12
https://github.com/japhir/2020-02_paleontology,"#+TITLE: Making Paleontology---fauna excercise 8 in R
#+OPTIONS: ^:{}
#+PROPERTY: header-args:R  :session *R* :exports both :results output :eval no-export
#+AUTHOR: Ilja J. Kocken
#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup

#+begin_src R :exports none :results none
  options(crayon.enabled = FALSE)
#+end_src

* Preparation
If you're curious about learning R, this exercise could be a fun little first
challenge!

While excel works, it often introduces problems that shouldn't be there: in
this exercise it isn't able to plot the age in millions of years ago (~Ma~) as a
~date~, because it doesn't have enough digits for conversion. As a workaround you
can convert it to ~kyr~ in stead.

That is obviously stupid. And it's not the only frustrating excel thing you may
run into. One of the nice things about using a scripting language---such as
python or R---instead, is that your work becomes reproducible and you and
others can use your tricks again in the future!

Here we show you how you could do this exercise in R. Please play around first
and enjoy the struggle for a bit.

** installation
Install [[https://www.r-project.org/][R]] and [[https://rstudio.com/][RStudio]]. R is the programming language and RStudio is a friendly
graphical interface (or technically Integrated Development Environment) for R.

If you don't know how, you can follow many online guides, such as this [[https://courses.edx.org/courses/UTAustinX/UT.7.01x/3T2014/56c5437b88fa43cf828bff5371c6a924/][edX course]].
Note that for Mac users there are some additional installation steps required.

Install the required packages---extensions of R that make life even
easier!---by opening RStudio and going to the bottom-left panel (the terminal)
and typing, e.g.:

#+begin_src R :eval never
  install.packages(""ggplot2"")
#+end_src

Or, if you want to use ~ggplot2~, ~dplyr~, ~purrr~, etc., which I highly recommend,
install them all at once with:

#+begin_src R :eval never
  install.packages(""tidyverse"")
#+end_src

since they are part of the [[https://www.tidyverse.org/][tidyverse]], a collection of packages that makes R
amazing.

** load the libraries
These commands make the package functions available for use in your session:
#+begin_src R :results output
  library(readr)      # to read in the csv file (or use readxslx to read in excel files)
  library(ggplot2)    # for plotting
  library(patchwork)  # for composite plots
  library(dplyr)      # for tidy data manipulation
  library(tidyr)
#+end_src

#+RESULTS:
#+begin_example

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union
#+end_example

** getting started
As a start, I highly recommend reading through the freely available book [[https://r4ds.had.co.nz/][R for
data science]] by Hadley Wickham. You can also try the datacamp [[https://www.datacamp.com/courses/free-introduction-to-r][Introduction to R]]
online course, which helped me get started initially. One last resource to get
you started is to install the package ~swirl~ and interactively learn the basics
right from R.

#+begin_src R :eval never
  install.packges(""swirl"")
  library(swirl)
  swirl()
#+end_src

** getting help
When you have problems or questions, have a look at any error messages if
present and consult the documentation of the function of interest with,
~?function_name~ or search for a topic with ~??topic~.

e.g., enter:
#+begin_src R :eval never
 ?ggplot
#+end_src
in the terminal.

+If+ When you have questions, search! This is not considered cheating!
[[https://stackoverflow.com/][StackOverflow]], your search engine, and sometimes even twitter ~#Rlang~ are your
friends!
",2022-08-12
https://github.com/japhir/ArchConfigs,"This is my config dot file repository.

I use:
- [arch linux](https://archlinux.org/) as operating system
- [emacs](https://www.gnu.org/software/emacs/) for writing, email, agenda, note-taking, and text-editor
  - [evil](https://github.com/emacs-evil/evil) to use vim keys in emacs
  - [org-mode](https://orgmode.org/) mode that I use for GTD, writing, literate programming
  - [evil-org-mode](https://github.com/Somelauw/evil-org-mode) to use vim keybindings in org-mode
  - [org-roam](https://www.orgroam.com/) connected note-taking following the zettelkasten method
  - [ess](https://ess.r-project.org/) data analysis in R
  - [modus-themes](https://gitlab.com/protesilaos/modus-themes/) beautiful light and dark theme
- [sway](https://swaywm.org/) window manager
  - [waybar](https://github.com/Alexays/Waybar/) for workspaces, bar, and other nice widgets
  - [wofi](https://hg.sr.ht/~scoopta/wofi) for launching programs that I don't have a keybinding for
  - [mako](https://github.com/emersion/mako) for notifications
- [alacritty](https://github.com/alacritty/alacritty) terminal emulator
 - [zsh](https://grml.org/zsh/) bash alternative
 - [fzf](https://github.com/junegunn/fzf) fuzzy file finder, this makes terminal's <kbd>ctrl</kbd>+<kbd>r</kbd> so nice!
- [firefox](https://firefox.org/) browser
  - [vim-vixen](https://github.com/ueokande/vim-vixen/releases/tag/0.30) vim keybindings in firefox
  - [org-capture](https://github.com/sprig/org-capture-extension) capture websites into emacs org-mode

Personal scripts are in /bin and user services in /services.

If you want to use (some) of my configs, I recommend that you proceed with caution and use only the bits that you like!

Simply git clone in where you want and symlink the files to their proper destination.
You can also just execute the `symlink` file to link everything in one go, but I'm not keeping it fully up to date.

For individual entries type: `ln -s {target-filename} {symbolic filename}`, for example for i3: `ln -s i3config ~/.config/i3/config`

For all the executables, make a directory (e.g. `~/bin`), add it to your `PATH`, then symlink with `ln -s /path/to/ArchConfigs/bin/* ~/bin`.
Possibly, you'll have to `chmod +x filename` to make the files executable.

Kind regards,

Japhir
",2022-08-12
https://github.com/japhir/corepics,"[[file:1411_corepics.png]]

#+TITLE: CorePics
#+property: header-args:R  :session *R:corepics* :exports both :results output :eval no-export

This is how I use R, some command line utilities, and imagemagick to get IODP core photographs in a reduced size into my scientific figures!

* what you need
- [[R][R]]
- [[https://www.gnu.org/software/bash/][bash]] or another command line shell
- [[https://imagemagick.org/][imagemagick]]
- the develpment R package [[https://github.com/clauswilke/ggtextures][ggtextures]]

** R libraries
#+begin_src R
  library(stringr)
  library(dplyr)
  library(glue)
  library(readxl)
  library(readr)
  ## library(ggtextures)
  devtools::load_all(""~/Downloads/ggtextures"")
#+end_src

#+RESULTS:

: Loading ggtextures

* download the images and section depth
IODP info is available on [[http://iodp.tamu.edu/][the IODP website]]

- Core data from all ODP, DSDP and IODP expedition 301--312 data are available on the [[http://iodp.tamu.edu/janusweb/imaging/photo.shtml][janus database]]
  - For older core images (before leg 198) we need the url: http://www-odp.tamu.edu/publications/###_IR/VOLUME/CORES/IMAGES to get PDFs of the whole core, sections laid out next to each other.
  - There is also a newer file format called MrDIT, but this was too hard to get to work.
- Core data for all newer IODP expeditions from 317--present are available in the [[http://web.iodp.tamu.edu/LORE/][LORE database]].

** get image files/links from the janus database
In this example setup we're looking at Expedition 208, Site 1264. After some digging around on the janus database, we find that the JPG images are hosted with a regular url:
http://www-odp.tamu.edu/publications/208_IR/VOLUME/CORES/JPEG/1264B/1264b_026h/1264b_026h_06.jpg

These are the ones I'll be using.

This function converts leg, site, hole, etc. data to create the URL to the image.
#+begin_src R
  create_image_url <- function(leg, site, hole, core, type = ""H"", section, image_type = ""JPEG"", extension = "".jpg"") {
    base_url <- ""http://www-odp.tamu.edu/publications/""
    core_padded <- str_pad(core, 3, pad = 0)
    type_l <- str_to_lower(type)
    sec <- section %>% str_to_lower() %>% str_pad(2, pad=0)
    glue(""{base_url}{leg}_IR/VOLUME/CORES/{image_type}/{site}{hole}/{site}{str_to_lower(hole)}_{core_padded}{type_l}/{site}{str_to_lower(hole)}_{core_padded}{type_l}_{sec}{extension}"")
  }

  # some tests
  create_image_url(leg = 208, site = 1264, hole = ""B"", core = 30, type = ""H"", section = ""CC"")
  create_image_url(leg = 208, site = 1264, hole = ""C"", core = 1, type = ""H"", section = 2)
  create_image_url(leg = 208, site = 1264, hole = ""B"", core = 26, type = ""H"", section = 6)
#+end_src

#+RESULTS:

: http://www-odp.tamu.edu/publications/208_IR/VOLUME/CORES/JPEG/1264B/1264b_030h/1264b_030h_cc.jpg
: http://www-odp.tamu.edu/publications/208_IR/VOLUME/CORES/JPEG/1264C/1264c_001h/1264c_001h_02.jpg
: http://www-odp.tamu.edu/publications/208_IR/VOLUME/CORES/JPEG/1264B/1264b_026h/1264b_026h_06.jpg

*** get core/section/depth info
This section is probably something that you need to tweak some more for your use-case.

In the end, you need the IODP core info up to the section level. Each sections hould have a ~top_depth~ (in mbsf, mcd, or armcd) and ~bot_depth~.

#+begin_src R
  # this is the affine table, with the recovery and adjusted depths of the cores
  md <- read_excel(""~/SurfDrive/PhD/sites/site_1264_walvisridge/208 1264 Composite.xlsx"",
                   sheet = ""Affine Table"", range = ""A1:S31"") %>% # hole A
    # I combine the info for the three holes
    bind_rows(read_excel(""~/SurfDrive/PhD/sites/site_1264_walvisridge/208 1264 Composite.xlsx"",
                         sheet = ""Affine Table"", range = ""A33:S63"")) %>% # hole B
    bind_rows(read_excel(""~/SurfDrive/PhD/sites/site_1264_walvisridge/208 1264 Composite.xlsx"",
                         sheet = ""Affine Table"", range = ""A65:S66"")) # hole C

  # now I need the recovered depth per section from somewhere
  sd <- read_tsv(""~/SurfDrive/PhD/programming/iodp_read_info/data/208-1264/sections/208_1264_sections.txt"",
                 skip = 2) %>%
    rename_all(str_to_lower)

  # combine the metadata and clean up
  cm <- left_join(sd, md, by = c(""expedition"" = ""Leg"",
                                 ""site"" = ""Site"",
                                 ""hole"" = ""H"",
                                 ""core"" = ""Cor"",
                                 ""core_type"" = ""T"")) %>%
    mutate(armcd_top = `New Cum Offset` + top_depth,
           armcd_bot = `New Cum Offset` + bottom_depth) %>%
    select(expedition:core_type, section_label, top_depth, rev_length, armcd_top, armcd_bot) %>%
    # here we create the image url based on the metadata
    mutate(url = create_image_url(leg=expedition,
                                  site=site,
                                  hole=hole,
                                  core=core,
                                  type=core_type,
                                  section=section_label,
                                  image_type=""JPEG""))
#+end_src

*** download the images so I can crop them
We create a directory where we save all the pretty large jpg images. For walvis ridge, this takes quite a while: 4 minutes and 30 seconds to download 455 files.

This is the part where you have to have bash or another shell with the cat, awk, grep, and wget utilities installed.
#+begin_src R
  dir <- ""1264_corepics""
  dir.create(dir)

  # save the clean metadata to a tsv so we can use awk to work with it
  write_tsv(cm, ""cm.txt"")

  # download the corepics
  system(""cat cm.txt | awk '{ print $11 }' | grep '^http' | wget -P 1264_corepics -i-"")
#+end_src

#+RESULTS:

FINISHED --2020-11-09 12:02:14--
Total wall clock time: 5m 48s
Downloaded: 455 files, 1.2G in 4m 30s (4.57 MB/s)

** get core/section/depth info from the LORE database
If you are working with a newer core, good on you! It's much easier to get those data in shape!

1. Go to http://web.iodp.tamu.edu/LORE/ in a modern browser
2. In the left menu, click on ""images""
3. click ""Core Sections (LSIMG)""
4. click ""standard""
5. in the filtering view, filter by your site/core/section as desired
6. optional: click an alternate depth scale
7. click ""View data""
8. click ""Download tabular data"" to save the csv with the metadata
9. click ""Batch download linked files"" to save the jpg images
10. Choose ""cropped images""

We can merge the file paths with the metadata by their image link:

#+begin_src R :eval never
  cp <- read_csv(""dat/core_site_pictures.csv"")

  # list all the downloaded jpg files
  fl <- tibble(file = list.files(""dat/corepics"", pattern="".jpg"")) %>%
    # I'm assuming that these ID's are unique here
    mutate(id = str_extract(file, ""[0-9]+.jpg$"") %>% str_replace("".jpg"", """") %>% parse_double())

  cp <- cp %>%
    # merge the file info (section depth etc.) with the image name
    left_join(fl, by = c(""Cropped image (JPG) link"" = ""id"")) %>%
    mutate(file = str_replace(file, "".jpg"", "".png""))
#+end_src

The image at the top is the result of reading in and combining data for IODP site U1411 using this method! Don't forget the next cropping step though, or your computer might crash.

** resize and crop the images
The images are much too large to load into memory all at once, so we downsize and crop them all.

The target width at 300 dpi if we want to plot it at half a cm:
#+begin_src R
 300 / 2.54 * .5 # 300 dpi in cm, for half a cm
#+end_src

#+RESULTS:

: [1] 59.05512

Use magick's mogrify to batch resize and crop the images. Play around with the magick commandline options to get the cropping correct.
#+begin_src R
  system(""mogrify -resize 60 -crop -17-54 -format png 1264_corepics/*.jpg"")
#+end_src

** add file paths to small images
#+begin_src R
  cm <- cm %>%
    mutate(file = paste0(""1264_corepics/"", basename(url) %>% str_replace("".jpg"", "".png""))) %>%
    # some of the sections don't have images, or our download failed perhaps?
    mutate(file_exists = !is.na(file.info(file)$size),
           file = ifelse(file_exists, file, NA_character_))
#+end_src

* plot the smaller images on the correct locations
#+begin_src R :results output graphics file :file 1264_corepics.png :width 200 :height 1600
  cm %>%
    ggplot(aes(y = armcd_bot, xmin = 0L, xmax = 1L, ymin = armcd_bot, ymax = armcd_top, image = file)) +
    geom_rect(alpha = .2) +
    ## coord_cartesian(ylim = c(17, 3)) +
    geom_textured_rect(colour = NA, nrow = 1, ncol = 1, img_width = unit(1, ""null""), img_height = unit(1, ""null""), interpolate = FALSE) +
    facet_grid(cols = vars(hole)) +
    scale_y_reverse() +
    labs(title=""IODP Leg 208 Site 1264"", subtitle = ""core photographs"", caption = ""created by Ilja Kocken"") +
    theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(),
          panel.grid = element_blank()) -> pl
  pl
#+end_src

#+RESULTS:

[[file:1264_corepics.png]]

* combine the core photographs with your figures
To do this, I recommend the patchwork package. Make sure to set the exact same y-axis for both plots.

#+begin_src R
  cm %>%
    ggplot(aes(y = armcd_bot, xmin = 0L, xmax = 1L, ymin = armcd_bot, ymax = armcd_top, image = file)) +
    geom_rect(alpha = .2) +
    coord_cartesian(ylim = c(17, 3)) +
    geom_textured_rect(colour = NA, nrow = 1, ncol = 1, img_width = unit(1, ""null""), img_height = unit(1, ""null""), interpolate = FALSE) +
    facet_grid(cols = vars(hole)) +
    scale_y_reverse() +
    theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(),
          panel.grid = element_blank()) -> pl

  cm %>%
    mutate(D47 = rnorm(n(), mean = 0.76, sd = 0.2)) %>%
    ggplot(aes(x = D47, y = armcd_bot)) +
    scale_y_reverse() +
    coord_cartesian(ylim = c(17, 3)) +
    theme(axis.title.y = element_blank(), axis.text.y = element_blank()) +
    geom_point() -> pl2
#+end_src

#+begin_src R :results output graphics file :file 1264_corepics_with_data.png :width 500 :height 600
  library(patchwork)

  pc <- (pl + pl2) + plot_layout(widths = c(.1, .9))
  pc
#+end_src

#+RESULTS:

[[file:1264_corepics_with_data.png]]

** putting the depth on the x-axis
If you need to put the depth on the x-axis, the images need to be rotated. Make sure you rotate them in the correct way! If you want to put greater depths to the right, rotate the images 90° anti-clockwise, like so:

#+begin_src R :eval never
  system(""mogrify -rotate -90 1264_corepics/*.png"")
#+end_src

#+RESULTS:

If you want to put the deeper sediments to the left, so that time progresses from left to right, rotate them by +90°.

#+begin_src R
  system(""mogrify -rotate 90 1264_corepics/*.png"")
#+end_src

#+RESULTS:

#+begin_src R
  cm %>%
    ggplot(aes(x = armcd_bot, ymin = 0L, ymax = 1L, xmin = armcd_bot, xmax = armcd_top, image = file)) +
    geom_rect(alpha = .2) +
    coord_cartesian(xlim = c(17, 3)) +
    geom_textured_rect(colour = NA, nrow = 1, ncol = 1, img_width = unit(1, ""null""), img_height = unit(1, ""null""), interpolate = FALSE) +
    facet_grid(rows = vars(hole)) +
    ## scale_x_reverse() +
    theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(),
          panel.grid = element_blank()) -> pl

  cm %>%
    mutate(D47 = rnorm(n(), mean = 0.76, sd = 0.2)) %>%
    ggplot(aes(y = D47, x = armcd_bot)) +
    scale_y_reverse() +
    coord_cartesian(xlim = c(17, 3)) +
    theme(axis.title.x = element_blank(), axis.text.x = element_blank()) +
    geom_point() -> pl2
#+end_src

#+RESULTS:

#+begin_src R :results output graphics file :file 1264_corepics_with_data_on_x-axis.png :width 600 :height 500
  pc <- (pl2/pl) + plot_layout(heights = c(.9, .1))
  pc
#+end_src

#+RESULTS:

[[file:1264_corepics_with_data_on_x-axis.png]]
",2022-08-12
https://github.com/japhir/cp-2018-42,"# Scripts and data used in ‘The 405 kyr and 2.4 Myr eccentricity components in Cenozoic carbon isotope records’
[![DOI](https://zenodo.org/badge/157687257.svg)](https://zenodo.org/badge/latestdoi/157687257)

[climate of the past discussions](https://www.clim-past-discuss.net/cp-2018-42/)

## Data composite

This repository contains the R-scripts used to generate the data composite
(`scripts/read_data.R` and `scripts/make_datacomposite.R`), as well as the data
sources used from the cited publications:

| file                                     | reference                 |
|------------------------------------------|---------------------------|
| `data/320-U1334_d18O-d13C.tab`           | Beddow et al., 2016       |
| `data/321-U1337_isotope_benth_foram.tab` | Holbourn et al., 2015     |
| `data/Pälike_clean.csv`                  | Pälike et al., 2006       |
| `data/Tian_U1337_all.csv`                | Tian et al., 2013         |
| `data/Zachos2008.csv`                    | Zachos et al., 2001, 2008 |

The `out/` directory contains the results of the R-scripts.

R-code used to generate the figures was omitted, because it would require
substantial cleaning up and commenting before being publication-ready and 
possibly useful to others.

## LOSCAR astronomical forcing

The LOSCAR source code in C can be obtained from the authors by sending a
request to [loscar.model@gmail.com](mailto:loscar.model@gmail.com).

The LOSCAR model version used here was `2.0.4`. To see which changes were made
to the 2.0.4 LOSCAR model, see the patch file (`Loscar-2.0.4.patch`). In short,
it introduces astronomical forcing as a new input source, as well as noise. To
achieve this, we define some new input paramters (`ORBFILE`, `ORBSTRENGTH`, and
`NOISELEVEL`) and some new global variables for turning on/off the specific
components (`NOISE`, `SCALE`, `VERBOSE`, `ADDNOISE`, etc.). If you have access
to the Loscar source code, you can apply the patch by running the following comand
from your terminal:

```{bash}
patch /Loscar/directory/ loscar.patch
```
",2022-08-12
https://github.com/japhir/DnDRaces,"#+TITLE: Simulating D&D 5e Race Heights and Weights
#+OPTIONS: ^:{}

* D&D Character height and weight
In D&D you can roll dice to determine your character's height and weight based
on it's race.

This is the table of race size and weight in the D&D player handbook:

| Race            | Base Height | Height Modifier | Base Weight | Weight Modifier |
|-----------------+-------------+-----------------+-------------+-----------------|
| Human           | 4'8""        |           +2d10 | 110 lb.     | × (2d4) lb.     |
| Dwarf, hill     | 3'8""        |            +2d4 | 115 lb.     | × (2d6) lb.     |
| Dwarf, mountain | 4'          |            +2d4 | 130 lb.     | × (2d6) lb.     |
| Elf, high       | 4'6""        |           +2d10 | 90 lb.      | × (1d4) lb.     |
| Elf, wood       | 4'6""        |           +2d10 | 100 lb.     | × (1d4) lb.     |
| Elf, drow       | 4'5""        |            +2d6 | 75 lb.      | × (1d6) lb.     |
| Halfling        | 2'7""        |            +2d4 | 35 lb.      | × 1 lb.         |
| Dragonborn      | 5'6""        |            +2d8 | 175 lb.     | × (2d6) lb.     |
| Gnome           | 2'11""       |            +2d4 | 35 lb.      | × 1 lb.         |
| Half-elf        | 4'9""        |            +2d8 | 110 lb.     | × (2d4) lb.     |
| Half-orc        | 4'10""       |           +2d10 | 140 lb.     | × (2d6) lb.     |
| Tiefling        | 4'9""        |            +2d8 | 110 lb.     | × (2d4) lb.     |

To calculate the height, you have to add the ~Base Height~ to, e.g. 2d10 dice
rolls for the Human, which means you have to roll two 10-sided-die to determine
how much to add, in inches.

* Problem: I think metric
I don't think in terms of feet, inches, and pounds, so I thought I'd convert
some things to centimetres and kilogrammes in stead. Furthermore, I wanted to
better understand how these dice-rolls influence the final distribution of
weights and heights in the D&D universe.

And so I've simulated some dice-rolls to answer those questions!

* Rolling Virtual Dice
** Load libraries
# this is so that we work in an R session in emacs with ess
#+PROPERTY: header-args:R  :session *R*

First load the libraries we use here:
#+begin_src R :results none
  library(dplyr)
  library(ggplot2)
  library(purrr)
  library(tidyr)
#+end_src

** Virtual dice
Then, let's figure out how to roll a virtual die in R. This function should do it:

#+begin_src R
  roll_dice <- function(n = 1, d = 20) {
    sum(sample(seq_len(d), size = n, replace = TRUE))
  }
#+end_src

#+RESULTS:

By creating a vector of length ~d~, and sampling from it ~n~ times, with
replacement, and then taking the sum of those numbers we get simulated
dice-rolls!

Test if it works
#+begin_src R
    c(roll_dice(),       # 1d20
      roll_dice(1, 10),  # 1d10
      roll_dice(2, 6),   # 2d6
      roll_dice(2, 4))   # 2d4
#+end_src

#+RESULTS:
| 9 |
| 2 |
| 8 |
| 8 |

| 10 |
|  7 |
|  7 |
|  3 |

This seems to work!

** Repeated rolls
Now we need to call the ~roll_die~ function many times for the simulations.

We can do this with ~replicate~ as follows:

#+begin_src R :results none
  replicate(1e3, roll_dice(1, 20))
#+end_src

** Histogram of roll results
We can have a very quick glance at what this results in by creating a bargraph
with the resulting roll total on the x-axis, and the probability of getting
that roll on the y-axis.

#+begin_src R
  make_hist <- function(vec) {
    dat <- tibble(Result = vec)

    pl <- dat %>%
      ggplot(aes(Result)) +
      geom_bar(aes(y = stat(count) / sum(stat(count)))) +
      labs(y = ""Probability"")

    pl
  }
#+end_src

#+RESULTS:

#+begin_src R :results graphics file :file 1d20hist.png :height 300
  replicate(1e5, roll_dice(1, 20)) %>%
    make_hist() + labs(title = ""Histogram of 1e5 simulations of a d20 roll"")
#+end_src

#+RESULTS:

[[file:1d20hist.png]]

This seems ok.

What happens if we roll 2d4?
#+begin_src R :results graphics file :file 2d4hist.png :height 300
  replicate(1e5, roll_dice(2, 4)) %>%
    make_hist() + labs(title = ""Histogram of 1e5 simulations of 2d4 rolls"")
#+end_src

#+RESULTS:

[[file:2d4hist.png]]

Or 2d6?
#+begin_src R :results graphics file :file 2d6hist.png :height 300
  replicate(1e5, roll_dice(2, 6)) %>%
    make_hist() + labs(title = ""Histogram of 1e5 simulations of 2d6 rolls"")
#+end_src

#+RESULTS:

[[file:2d6hist.png]]

* Tidying of the table
Here I've quickly (manually) tidied the table up for use in R.

#+TBLNAME: races
| Race            | Base Height | bh_f | bh_i | Height Modifier | n_height | d_height | Base Weight | Weight Modifier | n_weight | d_weight |
|-----------------+-------------+------+------+-----------------+----------+----------+-------------+-----------------+----------+----------|
| Human           | ""4\'8\""""    |    4 |    8 |           +2d10 |        2 |       10 |         110 | ×(2d4) lb.      |        2 |        4 |
| Dwarf, hill     | ""3\'8\""""    |    3 |    8 |            +2d4 |        2 |        4 |         115 | ×(2d6) lb.      |        2 |        6 |
| Dwarf, mountain | ""4\'""       |    4 |    0 |            +2d4 |        2 |        4 |         130 | ×(2d6) lb.      |        2 |        6 |
| Elf,  high      | ""4\'6\""""    |    4 |    6 |           +2d10 |        2 |       10 |          90 | ×(1d4) lb.      |        1 |        4 |
| Elf, wood       | ""4\'6\""""    |    4 |    6 |           +2d10 |        2 |       10 |         100 | ×(1d4) lb.      |        1 |        4 |
| Elf,  drow      | ""4\'5\""""    |    4 |    5 |            +2d6 |        2 |        6 |          75 | ×(1d6) lb.      |        1 |        6 |
| Halfling        | ""2\'7\""""    |    2 |    7 |            +2d4 |        2 |        4 |          35 | ×1 lb.          |          |          |
| Dragonborn      | ""5\'6\""""    |    5 |    6 |            +2d8 |        2 |        8 |         175 | ×(2d6) lb.      |        2 |        6 |
| Gnome           | ""2\'11\""""   |    2 |   11 |            +2d4 |        2 |        4 |          35 | ×1 lb.          |          |          |
| Half-elf        | ""4\'9\""""    |    4 |    9 |            +2d8 |        2 |        8 |         110 | ×(2d4) lb.      |        2 |        4 |
| Half-orc        | ""4\'10\""""   |    4 |   10 |           +2d10 |        2 |       10 |         140 | ×(2d6) lb.      |        2 |        6 |
| Tiefling        | ""4\'9\""""    |    4 |    9 |            +2d8 |        2 |        8 |         110 | ×(2d4) lb.      |        2 |        4 |

NOTE: I'm using [[emacs'][emacs]] with [[https://ess.r-project.org/][ess]] in [[https://orgmode.org/][org-mode]], and this allows me to name the
sheet with ~#+TBLNAME:~ so that I can pass it into the header argument of a
codeblock later on with ~:var dat=races~. If you don't use emacs/org-mode but,
e.g. RStudio with RMarkdown, it's easier to save the table as a csv first.

** Sensible units
Now it's time to read in the data and do some simulations!

We also convert everything into sensible units.

#+begin_src R :var dat=races :colnames yes
  races <- dat %>%
    mutate(base_cm = bh_f * 30.48 + bh_i * 2.54,
           base_kg = Base.Weight * 0.4535923) %>%
    as_tibble()
#+end_src

| Race            | Base.Height | bh_f | bh_i | Height.Modifier | n_height | d_height | Base.Weight | Weight.Modifier | n_weight | d_weight | base_cm |    base_kg |
|-----------------+-------------+------+------+-----------------+----------+----------+-------------+-----------------+----------+----------+---------+------------|
| Human           |           4 |    4 |    8 |           +2d10 |        2 |       10 |         110 | ×(2d4) lb.      |        2 |        4 |  142.24 |  49.895153 |
| Dwarf, hill     |           3 |    3 |    8 |            +2d4 |        2 |        4 |         115 | ×(2d6) lb.      |        2 |        6 |  111.76 | 52.1631145 |
| Dwarf, mountain |           4 |    4 |    0 |            +2d4 |        2 |        4 |         130 | ×(2d6) lb.      |        2 |        6 |  121.92 |  58.966999 |
| Elf,  high      |           4 |    4 |    6 |           +2d10 |        2 |       10 |          90 | ×(1d4) lb.      |        1 |        4 |  137.16 |  40.823307 |
| Elf, wood       |           4 |    4 |    6 |           +2d10 |        2 |       10 |         100 | ×(1d4) lb.      |        1 |        4 |  137.16 |   45.35923 |
| Elf,  drow      |           4 |    4 |    5 |            +2d6 |        2 |        6 |          75 | ×(1d6) lb.      |        1 |        6 |  134.62 | 34.0194225 |
| Halfling        |           2 |    2 |    7 |            +2d4 |        2 |        4 |          35 | ×1 lb.          |      nil |      nil |   78.74 | 15.8757305 |
| Dragonborn      |           5 |    5 |    6 |            +2d8 |        2 |        8 |         175 | ×(2d6) lb.      |        2 |        6 |  167.64 | 79.3786525 |
| Gnome           |           2 |    2 |   11 |            +2d4 |        2 |        4 |          35 | ×1 lb.          |      nil |      nil |    88.9 | 15.8757305 |
| Half-elf        |           4 |    4 |    9 |            +2d8 |        2 |        8 |         110 | ×(2d4) lb.      |        2 |        4 |  144.78 |  49.895153 |
| Half-orc        |           4 |    4 |   10 |           +2d10 |        2 |       10 |         140 | ×(2d6) lb.      |        2 |        6 |  147.32 |  63.502922 |
| Tiefling        |           4 |    4 |    9 |            +2d8 |        2 |        8 |         110 | ×(2d4) lb.      |        2 |        4 |  144.78 |  49.895153 |

* Simulate weight and height dice-rolls
Now let's simulate some dice-rolls! We're creating some new list-columns, using
~purrr::map~ and then unnesting them for easier calculations.

First we define a new function that replicates the analysis:
#+begin_src R
  rep_dice <- function(n, d, n_sim = 1e5) {
    replicate(n_sim, roll_dice(n, d))
  }
#+end_src

#+RESULTS:

And then we run it for all the Races.

#+begin_src R
  races_stats  <- races %>%
    mutate(height_rolls = map2(n_height, d_height, possibly(rep_dice, otherwise = 1)),
           weight_rolls = map2(n_weight, d_weight, possibly(rep_dice, otherwise = 1))) %>%
    unnest(cols = c(height_rolls, weight_rolls)) %>%
    mutate(height = base_cm + height_rolls * 2.54,  # convert roll from inches to cm
           weight = base_kg + height_rolls * weight_rolls * 0.4535923)  # convert rolls from lbs to kg
#+end_src

#+RESULTS:

Note the ~tidyr::possibly~ here, which allows me to ignore the weight rolls for
the Halfling and Gnome and instead set their value to 1.

* Averages
Then we calculate median height and weight and append them back to the original data.

We also convert Race to a factor, which is sorted by the average height.

#+begin_src R :colnames yes
  races_sum <- races_stats %>%
    group_by(Race) %>%
    summarize(height_med = median(height),
              weight_med = median(weight)) %>%
    left_join(races, by = ""Race"") %>%
    arrange(height_med) %>%
    mutate(Race = factor(Race, levels = Race),
           lab_kg = paste0(Height.Modifier, Weight.Modifier))
#+end_src

#+RESULTS:

| Race            | height_med |  weight_med | Base.Height | bh_f | bh_i | Height.Modifier | n_height | d_height | Base.Weight | Weight.Modifier | n_weight | d_weight | base_cm |    base_kg | lab_kg          |
|-----------------+------------+-------------+-------------+------+------+-----------------+----------+----------+-------------+-----------------+----------+----------+---------+------------+-----------------|
| Halfling        |      91.44 |   18.143692 |           2 |    2 |    7 |            +2d4 |        2 |        4 |          35 | ×1 lb.          |      nil |      nil |   78.74 | 15.8757305 | +2d4×1 lb.      |
| Gnome           |      101.6 |   18.143692 |           2 |    2 |   11 |            +2d4 |        2 |        4 |          35 | ×1 lb.          |      nil |      nil |    88.9 | 15.8757305 | +2d4×1 lb.      |
| Dwarf, hill     |     124.46 |  66.6780681 |           3 |    3 |    8 |            +2d4 |        2 |        4 |         115 | ×(2d6) lb.      |        2 |        6 |  111.76 | 52.1631145 | +2d4×(2d6) lb.  |
| Dwarf, mountain |     134.62 |  73.9355449 |           4 |    4 |    0 |            +2d4 |        2 |        4 |         130 | ×(2d6) lb.      |        2 |        6 |  121.92 |  58.966999 | +2d4×(2d6) lb.  |
| Elf,  drow      |      152.4 |  43.5448608 |           4 |    4 |    5 |            +2d6 |        2 |        6 |          75 | ×(1d6) lb.      |        1 |        6 |  134.62 | 34.0194225 | +2d6×(1d6) lb.  |
| Elf,  high      |      165.1 |  51.7095222 |           4 |    4 |    6 |           +2d10 |        2 |       10 |          90 | ×(1d4) lb.      |        1 |        4 |  137.16 |  40.823307 | +2d10×(1d4) lb. |
| Elf, wood       |      165.1 |  56.2454452 |           4 |    4 |    6 |           +2d10 |        2 |       10 |         100 | ×(1d4) lb.      |        1 |        4 |  137.16 |   45.35923 | +2d10×(1d4) lb. |
| Half-elf        |     167.64 |  68.9460296 |           4 |    4 |    9 |            +2d8 |        2 |        8 |         110 | ×(2d4) lb.      |        2 |        4 |  144.78 |  49.895153 | +2d8×(2d4) lb.  |
| Tiefling        |     167.64 |  68.9460296 |           4 |    4 |    9 |            +2d8 |        2 |        8 |         110 | ×(2d4) lb.      |        2 |        4 |  144.78 |  49.895153 | +2d8×(2d4) lb.  |
| Human           |     170.18 |  73.4819526 |           4 |    4 |    8 |           +2d10 |        2 |       10 |         110 | ×(2d4) lb.      |        2 |        4 |  142.24 |  49.895153 | +2d10×(2d4) lb. |
| Half-orc        |     175.26 |  96.1615676 |           4 |    4 |   10 |           +2d10 |        2 |       10 |         140 | ×(2d6) lb.      |        2 |        6 |  147.32 |  63.502922 | +2d10×(2d6) lb. |
| Dragonborn      |      190.5 | 106.5941905 |           5 |    5 |    6 |            +2d8 |        2 |        8 |         175 | ×(2d6) lb.      |        2 |        6 |  167.64 | 79.3786525 | +2d8×(2d6) lb.  |

* Plot of Heights
Great! Now let's create a plot of the average height by race, with a violin
plot to illustrate the distribution.

I further annotate the plot with base height points and which modifier was used
to get the distribution of heights.

#+begin_src R :results graphics file :file raceheights.png :width 600
  pl_h <- races_sum %>%
    ggplot(aes(x = Race, y = height_med)) +
    geom_bar(stat=""identity"", alpha = .5) +
    geom_violin(aes(y = height), bw = 2.54, scale= ""width"", colour = NA, fill = ""cornflowerblue"", alpha = .8, data = races_stats) +
    geom_text(aes(y = base_cm + 2, hjust = 0, label = paste0(Height.Modifier, ""'"")), angle = 90) +
    geom_point(aes(y = base_cm)) +
    ylim(c(0, NA)) +
    labs(y = ""Height (cm)"") # +
    ## coord_flip()
  pl_h
#+end_src

#+RESULTS:

[[file:raceheights.png]]

Notice the ~bw~ argument to ~geom_violin~: this is used to adjust the smoothing
kernel a bit. I've used the value to convert my units in cm back to inches,
because with lower values we get artificial jittering.

* Plot of Weights
Now we do the same for weight:
#+begin_src R :results graphics file :file raceweights.png :width 600
  pl_w <- races_sum %>%
    ggplot(aes(x = Race, y = weight_med)) +
    geom_bar(stat=""identity"", alpha = .5) +
    geom_violin(aes(y = weight), bw = 1 / 0.4535923, scale= ""width"", colour = NA, fill = ""cornflowerblue"", alpha = .8, data = races_stats) +
    geom_point(aes(y = base_kg)) +
    geom_text(aes(y = base_kg, label = lab_kg), hjust = -.05, angle = 90) +
    ylim(c(0, NA)) +
    labs(y = ""Weight (kg)"") # +
  pl_w
#+end_src

#+RESULTS:

[[file:raceweights.png]]

(Again, we set ~bw~ to the value to convert kg to lbs.)

* Combined Plot
To ultimately combine the two into one figure using ~patchwork~.

#+begin_src R :results graphics file :file races_stats.png :width 600 :height 600
  library(patchwork)
  pl <- (pl_h + labs(title = ""D&D 5e Race size and weight distributions based on rolls"") &
         theme(axis.title.x = element_blank(),
               axis.text.x = element_blank(),
               axis.ticks.x = element_blank())) /
    (pl_w & theme(axis.text.x = element_text(size = 10, angle = 30, hjust = 1, face = ""bold"")))
  pl
#+end_src

#+RESULTS:

[[file:races_stats.png]]

* Body Mass Index
Okay now for some more mental picturing, let's calculate the average BMI for
these races. BMI is a troublesome indicator for humans alone already, and will
certainly be wrong for the heavy-boned dwarfs, but it's nice to give us a
little bit more of a mental picture.

I found these BMI categories [[https://en.wikipedia.org/wiki/Body_mass_index#Categories][on the WikiPedia article on BMI]].

#+TBLNAME: bmi
| category                              | from |   to |
|---------------------------------------+------+------|
| Very severely underweight             |      |   15 |
| Severely underweight                  |   15 |   16 |
| Underweight                           |   16 | 18.5 |
| Normal (healthy weight)               | 18.5 |   25 |
| Overweight                            |   25 |   30 |
| Obese Class I (Moderately obese)      |   30 |   35 |
| Obese Class II (Severely obese)       |   35 |   40 |
| Obese Class III (Very severely obese) |   40 |      |

#+begin_src R :var categories=bmi :results graphics file :file races_bmi.png :width 600
  # clean up the categories
  cat <- categories %>%
    mutate(from = ifelse(is.na(from), -Inf, from),
           to = ifelse(is.na(to), Inf, to),
           category = factor(category, levels = rev(category), ordered = TRUE))

  # calculate average bmi
  bmi_avg <- races_sum %>% mutate(bmi = weight_med / (height_med/100)^2)

  # calculate all bmi's
  bmi <- races_stats %>%
    mutate(bmi = weight / (height / 100)^2)

  # plot them
  bmi_avg %>%
    ggplot(aes(x = Race, y = bmi)) +
    # annotate the categories
    geom_point() + # It looks like this is necessary to keep the factor levels in
                   # the right order
    geom_rect(aes(xmin = -Inf, xmax = Inf,
                  ymin = from,
                  ymax = to,
                  fill = category),
              inherit.aes = FALSE, data = cat) +
    scale_fill_brewer(palette = ""RdBu"") +
    geom_violin(data = bmi, bw = .8, fill = ""gray"", draw_quantiles = c(.25, .5, .75)) +
    labs(fill = ""BMI category\nif they would have been human"", y = ""BMI (kg /""~m^2*"")"") +
    geom_point() +
    theme(axis.text.x = element_text(angle = 30, hjust = 1, face = ""bold""))
#+end_src

#+RESULTS:

[[file:races_bmi.png]]

So most dwarves are, according to the human BMI, very severely obese 😉.

And that's it! A quick dive into some simulations with R! Any feedback on how
to improve this workflow is welcome.
* Other Dice roll simulations
** Rolling With Advantage
In D&D-land you sometimes get to roll with advantage. This means that you roll
a d20 twice, and take the higher of the two. I also wanted to study what
happens when we do that, so we add a new function!

#+begin_src R
  roll_with_advantage <- function(d = 20) {
    max(sample(seq_len(d), size = 2, replace = TRUE))
  }
#+end_src

#+RESULTS:

#+begin_src R :results graphics file :file d20_advantage.png :height 300
  replicate(1e5, roll_with_advantage(20)) %>%
    make_hist() + labs(title = ""Histogram of 1e5 simulations of d20 rolls with advantage"")
#+end_src

#+RESULTS:

[[file:d20_advantage.png]]

** Rolling With Disadvantage
When you're particularly unskilled at something, your DM may ask you to roll
with disadvantage. This means: roll 2d20 and take the lower.

#+begin_src R
  roll_with_disadvantage <- function(d = 20) {
    min(sample(seq_len(d), size = 2, replace = TRUE))
  }
#+end_src

#+begin_src R :results graphics file :file d20_disadvantage.png :height 300
  replicate(1e5, roll_with_disadvantage(20)) %>%
    make_hist() + labs(title = ""Histogram of 1e5 simulations of d20 rolls with disadvantage"")
#+end_src

#+RESULTS:

[[file:d20_disadvantage.png]]

** Rolling for Stats
Some DM's let you roll for stats. The common way of doing so is by rolling 4d6
and dropping the lower. Then repeating this 6 times.

When I did this the first time, I got some pretty high rolls and I wondered
what the odds are. So again, time to simulate!

#+begin_src R :results none
   stat_roll <- function() {
     # roll 4d6, drop the lowest
     rolls <- sample(1:6, 4, replace = TRUE)
     # use the highest two values only
     sum(sort(rolls)[-1])
   }
#+end_src

So if you want to roll for stats without rolling any dice (BOOOOO!):
#+begin_src R
  replicate(6, stat_roll())
#+end_src

#+RESULTS:
| 15 |
| 11 |
| 13 |
| 16 |
| 11 |
| 11 |

comparison to pointbuy and standard array
#+begin_src R :colnames ""yes""
  comparison <- tribble( ~ name, ~ array,
                        ""standard"", c(8, 10, 12, 13, 14, 15),
                        ""pointbuy 3 high 3 low"", c(15, 15, 15, 8, 8, 8),
                        ""pointbuy all medium"", c(13, 13, 13, 12, 12, 12),
                        ) %>%
    mutate(sum = map_dbl(array, sum))
  comparison %>% select(-array)
#+end_src

#+RESULTS:
| name                  | sum |
|-----------------------+-----|
| standard              |  72 |
| pointbuy 3 high 3 low |  69 |
| pointbuy all medium   |  75 |

Let's visualize the likelihood of all the total values:
#+begin_src R :results graphics file :file stat_rolls.png :height 300
  replicate(1e5, stat_roll()) %>%
    make_hist() + labs(title = ""Rolling for stats using the roll 4d6 drop lowest method"") +
    geom_bar(aes(x = array, y = stat(count) / sum(stat(count)), group = name, fill = name),
             data = comparison %>% unnest(array),
             width = .5,
             alpha = .6)
#+end_src

#+RESULTS:
[[file:stat_rolls.png]]

[[file:stat_rolls.png]]
#+begin_src R :colnames yes
  sr <- replicate(1e5, stat_roll()) %>%
    as_tibble() %>%
    mutate(name = ""roll for stats"") %>%
    bind_rows(comparison %>% unnest(array) %>% rename(value=array)) %>%
    group_by(name) %>%
    summarize(min = min(value),
              mean = mean(value),
              median = median(value),
              max = max(value))
#+end_src

#+RESULTS:
| name                  | min |     mean | median | max |
|-----------------------+-----+----------+--------+-----|
| pointbuy 3 high 3 low |   8 |     11.5 |   11.5 |  15 |
| pointbuy all medium   |  12 |     12.5 |   12.5 |  13 |
| roll for stats        |   3 | 12.24506 |     12 |  18 |
| standard              |   8 |       12 |   12.5 |  15 |

let's calculate when the sum of ability scores is highest

simulate 1e5 sets of 6 stats, take the sum
#+begin_src R
  sr <- replicate(1e5, sum(replicate(6, stat_roll()))) %>%
    as_tibble()
#+end_src

#+begin_src R :results output graphics file :file stat_sum_vs_stdarray.png :width 700 :height 300
  make_hist(sr$value) +
   geom_vline(aes(xintercept = sum, colour = name), data = comparison, size = 2, alpha = .5)
#+end_src

#+RESULTS:

[[file:stat_sum_vs_stdarray.png]]
do some calculations
#+begin_src R
  sb <- sr %>%
    mutate(
      rbs = value > comparison$sum[[1]],
      rbp1 = value > comparison$sum[[2]],
      rbp2 = value > comparison$sum[[3]],
    )
#+end_src

how often is rolling better than pointbuy or standard array?
#+begin_src R :colnames ""yes""
  tribble(~ name, ~ `P roll better than X`,
    ""standardarray"", sum(sb$rbs) / nrow(sb),
    ""pointbuy 3 high 3 low"", sum(sb$rbp1) / nrow(sb),
    ""pointbuy all medium"", sum(sb$rbp2) / nrow(sb)
  )
#+end_src

#+RESULTS:
| name                  | P roll better than X |
|-----------------------+----------------------|
| standardarray         |              0.56215 |
| pointbuy 3 high 3 low |              0.71725 |
| pointbuy all medium   |              0.39306 |
",2022-08-12
https://github.com/japhir/DnD_mobs,"* roll_dnd_mobs

We're running a massive final D&D 5e battlefield, and would like to roll the
dice! but having to roll 20d20 to determine if your army of archers hits is a
pain so here is some R code to simulate dice-rolls for large groups of mobs.

It shows you exactly what was rolled so you can still cheer on your army of
champions! (except for individual damage dice, but I don't care too much about
that).

Check out the [[examples.md]] file for what you can expect!

* Run it locally
Save [[file:roll_dnd_mobs.R]] and open it in your local R session.

* Run this interactively online via binder [[https://mybinder.org/v2/gh/japhir/DnD_mobs/HEAD][https://mybinder.org/badge_logo.svg]]
1. wait for a long time for it to launch
2. Click on the RStudio button
3. In the bottom right click on ~roll_dnd_mobs.R~ to open it
4. In the top-right of the newly opened file, click Source
5. you can now enter my functions in the console (bottom left)!
   - ~roll_dice(1, 8)~ will roll 1d8
   - ~d20(advantage = TRUE)~ will roll a d20 with advantage
   - ~attack(19, +8, 2, 8, +4, AC = 15, adv = TRUE)~ will attack with 19
     archers that have a +8 bonus to hit. They are also colossus slayers so
     they roll 2d8+4 damage against an AC of 15, with advantage
   - ~save(20, +4, DC = 13)~ to make 20 DC 13 saving throws or skill checks with a +4 bonus.
",2022-08-12
https://github.com/japhir/GeologicTimeScale,"#+title: Make a nice Geologic Time Scale
#+property: header-args:R  :session *R:GTS* :exports both :results output :eval no-export

In this repository I create a nice PDF on A3 size to print the Geologic Time Scale on a linear scale, so that it doesn't seem like the recent time periods took the same amount of time as the Precambrian.

I had to copy over all the colours and age ranges, because the official tools only provide a PDF.

If you want to use this figure for your own plots you can either:

1. load in the ~GTS_widths.csv~ file and plot it yourself
   #+begin_src R
     library(readr)
     GTS <- read_csv(""https://github.com/japhir/GeologicTimeScale/raw/master/GTS_widths.csv"")
     # create plot yourself
   #+end_src

2. load in the ~gts_plot.rds~ file and tweak the plot itself.
   #+begin_src R
     library(ggplot2)
     library(readr)
     gts <- read_rds(""https://github.com/japhir/GeologicTimeScale/raw/master/out/gts_plot.rds"")
   #+end_src

I prefer to read in the data, filter out what I want (e.g., only Epochs), then make the plot full-sized and add it to the data with [[https://patchwork.data-imaginist.com/][patchwork]].

*NOTE: it's probably much better to use a full-fledged package to add the Geologic Time Scale to your plots, e.g. using the [[https://github.com/willgearty/deeptime/][deeptime]] package!*

* full workflow
load libraries
#+begin_src R
  library(dplyr)
  library(ggplot2)
  library(readr)
#+end_src

Read in my weird data frame
#+begin_src R
  GTS <- read_csv(""https://github.com/japhir/GeologicTimeScale/raw/master/GTS_widths.csv"")
#+end_src

Create plot of ""data""
#+begin_src R :results output graphics file :file imgs/iris.png :width 800 :height 500
  dataplot <- iris |>
    # create fake ages
    mutate(age = rep(seq(0, 44, length.out = 50), 3)) |>
    ggplot(aes(x = age, y = Petal.Length, colour = Species)) +
    geom_point() +
    geom_line()
  dataplot
#+end_src

#+RESULTS:

[[file:imgs/iris.png]]

Now create the desired subset of the GTS plot
#+begin_src R :results output graphics file :file imgs/gts_data.png :width 800 :height 80
  gtsplot <- GTS |>
    # subset it to only show Periods
    filter(type == ""Period"") |>
    # filter to our time range
    filter(top < 55) |>
    # rectangles for each period
    ggplot() +
    geom_rect(aes(ymin = start, ymax = end, xmin = top, xmax = bot, fill = col),
              show.legend = FALSE, col = ""black"") +
    # make sure that the fill colour is given by our hex colours
    scale_fill_identity() +
    # add period names
    geom_text(aes(x = meanage, y = meanwidth, label = name, size = fontsize * .5,
                  ## angle = fontangle,
                  col = fontcolor, fontface = fontface)) +
    # make sure the font size is set to your liking
    scale_size_identity() +
    scale_colour_identity() +
    # add axis label for age axis
    labs(x = ""Age (millions of years ago)"") +
    theme(
      # remove gray panel
      panel.grid = element_blank(),
      panel.background = element_blank(),
      # remove y axis entirely
      axis.line.y = element_blank(),
      axis.title.y = element_blank(),
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank()) +
    coord_cartesian(xlim = c(0, 40))
  gtsplot
#+end_src

#+RESULTS:

[[file:imgs/gts_data.png]]

And to combine the two, make sure they have precisely the same x axis and then join them together using patchwork.

#+begin_src R :results output graphics file :file imgs/gts_plus_data.png :width 800 :height 800
  library(patchwork)
  (dataplot +
   # make sure that it has precisely the same x axis range
   coord_cartesian(xlim = c(0, 40)) +
   # then remove the redundant x-axis from the data
   theme(axis.text.x = element_blank(), axis.title.x = element_blank())) /
    # and add the gts beneath the data, at a smaller size
    gtsplot + plot_layout(heights = c(1, .05))
#+end_src

#+RESULTS:

[[file:imgs/gts_plus_data.png]]

* contributing

If you spot any errors, please feel free to file an issue or write a pull request!

* copying

I've licensed the code with GPL-3, but the underlying data is copyright of the Geologic Time Scale at https://timescalefoundation.org/
",2022-08-12
https://github.com/japhir/geosurvey,"#+title: data steward survey 2021
#+author: Ilja Kocken

* description
This is the code to generate a report for the survey we sent out in november 2021 as the team of data stewards.

The goals of the survey were:
1. establish the data literacy of the different departments
2. put open science on the agenda of researchers
3. find open science champions to learn from

* requirements
This document is written in [[https://www.gnu.org/software/emacs/][emacs']] [[https://orgmode.org/][org-mode]], and is a [[https://en.wikipedia.org/wiki/Literate_programming][literate programme]].

We use a recent (>4.0.0) version of R with many packages in the tidyverse, most importantly ggplot2.

The ggsankey development package is installed from github.

We also tried out ggalluvial, but did not like it as much as the sankey graphs.

We use tidylog in some cases to make sure the transformations are doing what we hope they do. Commands that use this package are prepended with ~tidylog::.~
",2022-08-12
https://github.com/japhir/iodp-site-info,"# read_site_info

Easily create a plot of Site, Core, Section, Sample information

In this script, we create an overview of IODP site information, with core
recovery, hole, section information, as well as what samples have been
requested.

![example](https://github.com/japhir/iodp-site-info/blob/master/example.png?raw=TRUE)

## Download the necessary files

Go to the IODP-core repostory
[web-interface](http://dis.bremen.wdc-mare.org/BCRDIS/) to download the ASCII
`.txt` file of the core, section, and samples of interest. Place the files
inside their respective folders (""cores"", ""sections"", and ""samples"") in the
project folder (e.g. ""208-1264"") in your base directory
(e.g. ""data""). It doesn't matter what you name the `.txt` files.

Also take a look at the [Core-section
summary](http://web.iodp.tamu.edu/janusweb/coring_summaries/coresumm.shtml) to
figure out why there are certain holes in the sampling scheme. Unfortunately,
you have to copy/paste this into a file. Make sure to set the options to `.csv`
and save it into the project folder as `coresumm.csv`.

### Example setup

You should have the below directory structure:

```
iodp_read_info
├── data                   # base
│   ├── 105-647            # project 1
│   │   ├── cores
│   │   │   └── 647_all_cores.txt
│   │   ├── coresumm.csv
│   │   ├── samples
│   │   │   └── 647_all_samples.txt
│   │   └── sections
│   │       └── 647_all_sections.txt
│   ├── 208-1264           # project 2
│   │   ├── cores
│   │   │   ├── 208-1264A_cores.txt
│   │   │   ├── 208-1264B_cores.txt
│   │   │   └── 208-1264C_cores.txt
│   │   ├── coresumm.csv
│   │   ├── samples
│   │   │   ├── 208-1264A_samples.txt
│   │   │   ├── 208-1264B_samples.txt
│   │   │   └── 208-1264C_samples.txt
│   │   └── sections
│   │       ├── 208-1264A_sections.txt
│   │       ├── 208-1264B_sections.txt
│   │       └── 208-1264C_sections.txt
├── imgs
│   ├── 1264_overview.pdf
│   ├── 647_29:31.pdf
│   └── 647_overview.pdf
├── out
└── scripts
    └── read_site_info.R
```

Great! Now we can easily get an overview of what samples are available!

## Dependencies

The script uses ggplot2, dplyr, readr from the [tidyverse](https://www.tidyverse.org/).

If any of them is missing/not working, install the package with,
e.g. `install.packages(""ggplot2"")`.

## Example application

Here we apply the functions on the data from Walvis Ridge, IODP Leg 208, Site
1264.

``` r
source(""scripts/read_site_info.R"")
s1264 <- read_site_info()
s1264 %>% plot_site_info() %>%
  ggsave(""imgs/1264_overview.pdf"", .,
    width = 5, height = 10)
```

And now to the Newfoundland, with a nice zoom on the region of interest:

``` r
s647 <- read_site_info(""105-647"")
s647 %>%
  plot_site_info() %>%
  ggsave(""imgs/647_overview.pdf"", ., width = 5, height = 10)
s647 %>%
  plot_site_info() + coord_cartesian(ylim = c(270, 300))
ggsave(""imgs/647_29:31.pdf"", width = 5, height = 10)
ggsave(""example.png"", width = 5, height = 10)
```
",2022-08-12
https://github.com/japhir/minimallatex,"# minimallatex
",2022-08-12
https://github.com/japhir/rpackagesetup,"#+TITLE: Setting up an R package from scratch

Let's build a package for [[cran.r-project.org/][R]]! [[https://r-pkgs.org/man.html][Here]] is a nice book to explain what we're going to do!

(I use this single [[emacs.org/][Emacs]] [[https://orgmode.org/][org-mode]] file to do everything! But you can just work on the separate files as well)

* R libraries needed
We use [[https://devtools.r-lib.org/][devtools]] and [[https://usethis.r-lib.org/][usethis]] for the setup, [[https://roxygen2.r-lib.org/][roxygen2]] for the documentation,

If you haven't installed them yet, run the following:
#+begin_src R :eval never
  install.packages(c(""devtools"", ""usethis"", ""roxygen2""))
#+end_src

* we need to load the libraries for the session
# If you're also running this from org-mode, this bit of code is run interactively (hence the ~:session~ argument to the org-src block)
#+begin_src R :session
  library(devtools)
  library(usethis)
#+end_src

* set your desired usethis options
Run the below code to set up the author and package info metadata.
#+begin_src R :session :results none
  options(
   usethis.full_name = ""Ilja Kocken"",
   usethis.description = list(
     `Authors@R` = 'person(""Ilja"", ""Kocken"", email = ""i.j.kocken@uu.nl"", role = c(""aut"", ""cre""),
     comment = c(ORCID = ""0000-0003-2196-8718""))',
     Version = ""0.0.0.9000"")
  )
#+end_src

* write package functions
** utility functions
This is an example function.
Here we use ~:tangle~ to export the file to ~R/utils.R~, but you can of course also create a new file directly in the R subdirectory.
#+BEGIN_SRC R :tangle R/utils.R
  #' Moving average
  #'
  #' @importFrom stats filter
  #' @param x A vector to create the moving average for.
  #' @param n The number of samples to create the moving average for.
  #' @param sides the
  #' @export
  ma  <- function(x, n = 5, sides = 2) {
     filter(x, rep(1/n, n), sides = sides)
  }
#+END_SRC
** your cool function
Here's the basic syntax for roxygen function documentation directly next to the function definition.
This is one of the examples from [[https://r-pkgs.org/man.html#man-workflow][the documentation workflow section of the r-pkgs book]].
#+begin_src R :tangle R/add.R
  #' Add together two numbers
  #'
  #' @param x A number.
  #' @param y A number.
  #' @return The sum of \code{x} and \code{y}.
  #' @examples
  #' add(1, 1)
  #' add(10, 1)
  add <- function(x, y) {
    x + y
  }
#+end_src

** zzz
There's often a zzz file to put several options in, like a startup message.
#+BEGIN_SRC R :tangle R/zzz.R
  .onAttach  <- function(libname, pkgname) {
      packageStartupMessage(""Welcome to ‘clumpedr’"")
  }
#+END_SRC

* make the package and add dependencies interactively
Now we run below options one by one to interactively set up the package skeleton!

Be sure to answer the questions and to update the new files as they are created.

#+BEGIN_SRC R :session
  create_package(""~/SurfDrive/PhD/programming/rpackagefromorg/"") # example path to your package!
  use_gpl3_license()
  use_roxygen_md()
  use_package_doc()
  use_vignette(""my-vignette"")
  use_testthat() # write tests for your functions!
  use_test(""add"")
  use_pipe()
  use_tibble()
  document()
#+END_SRC

* add package data
Raw data that needs to be accessible can be stored in ~inst/~. This is best if you want to demonstrate how to load the data in your vignette.

If you want the data available as a dataset, loadable with ~data(nameofdata)~, put it in ~data/~ as a ~.rds~ (R data structure) file.
In this case it's probably a good idea to include the source code + raw data in ~data-raw/~.

See [[https://r-pkgs.org/data.html#documenting-data][this section on how to document data]].

* This results in the following file structure
#+begin_src sh :results output
  tree
#+end_src

#+RESULTS:

#+begin_example
.
├── DESCRIPTION
├── LICENSE.md
├── man
│   ├── add.Rd
│   ├── ma.Rd
│   ├── pipe.Rd
│   └── rpackagefromorg-package.Rd
├── NAMESPACE
├── R
│   ├── add.R
│   ├── rpackagefromorg-package.R
│   ├── utils-pipe.R
│   ├── utils.R
│   ├── utils.Rmd
│   └── zzz.R
├── README.org
├── tests
│   ├── testthat
│   │   └── test-add.R
│   └── testthat.R
└── vignettes
    └── my-vignette.Rmd

5 directories, 17 files
#+end_example

* Now feel free to edit all those files separately!
",2022-08-12
https://github.com/japhir/R_AUR_sync,"#+title: Scripts for managing local and root level R packages
#+author: Ilja Kocken

I use [[https://archlinux.org][Arch Linux]] and [[https://www.r-project.org/about.html][R]]. Recently, many R packages have been made available on the [[aur.archlinux.org/][AUR]], which means that my linux package manager can take care of upgrading packages, and that I don't have to do so myself from within R.

Unfortunately, this made a bit of a mess, resulting in some packages installed into ~/usr/lib/R/~---also by me using R as root (bad!)---and most others into ~/R/x86_64-pc-linux-gnu-library~. I'd prefer to have all the available packages loaded from the AUR, and installed in the root directory. There are some niche or development packages for which an AUR package doesn't make a lot of sense, those will remain installed in my user directory.

So I wrote some scripts to help manage the whole jumble. If you want to use them, put them in your ~$PATH~ (I put them in ~~/bin~) and run ~find_r_available~.

| script                | function                                                                                                       |
|-----------------------+----------------------------------------------------------------------------------------------------------------|
| [[file:find_r_available]] | finds locally installed R packages and tries find them in the AUR, then installs them using my AUR helper ~paru~ |
| [[file:find_r_dups]]      | finds R packages that have been installed in both ~~/R/x86_64-pc-linux-gnu-library~ and ~/usr/lib/R~               |
| [[file:remove_r_local]]   | remove all duplicated packages from ~~/R~                                                                        |

If you're not using the same [[https://wiki.archlinux.org/index.php/AUR_helpers][AUR helper]] that I'm using, [[https://github.com/morganamilo/paru][paru]], then make sure to change the last line in ~find_r_available~.
",2022-08-12
https://github.com/japhir/stdsim,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

[![CRAN
status](https://www.r-pkg.org/badges/version/stdsim)](https://cran.r-project.org/package=stdsim)

# stdsim

The goal of stdsim is to simulate sample and standard measurements to
allow optimisation of the Empirical Transfer Function, the calibration
from the machine scale of standards to an absolute reference frame.

## Installation

You can install the released version of stdsim from
[GitHub](https://github.com/japhir) with:

``` r
devtools::install_github(""japhir/stdsim"")
```

## Example

This shows how to run one simulation with some input parameters:

``` r
options(genplot=TRUE, verbose=TRUE)
library(stdsim)
#> now dyn.load(""/usr/lib/R/library/grid/libs/grid.so"") ...
#> now dyn.load(""/usr/lib/R/library/colorspace/libs/colorspace.so"") ...
#> now dyn.load(""/usr/lib/R/library/glue/libs/glue.so"") ...
#> now dyn.load(""/usr/lib/R/library/ellipsis/libs/ellipsis.so"") ...
#> now dyn.load(""/usr/lib/R/library/fansi/libs/fansi.so"") ...
#> now dyn.load(""/usr/lib/R/library/utf8/libs/utf8.so"") ...
#> now dyn.load(""/usr/lib/R/library/vctrs/libs/vctrs.so"") ...
#> now dyn.load(""/usr/lib/R/library/tibble/libs/tibble.so"") ...
#> now dyn.load(""/usr/lib/R/library/purrr/libs/purrr.so"") ...
#> now dyn.load(""/usr/lib/R/library/dplyr/libs/dplyr.so"") ...
#> now dyn.load(""/usr/lib/R/library/lattice/libs/lattice.so"") ...
#> now dyn.load(""/usr/lib/R/library/nlme/libs/nlme.so"") ...
sim_stds(stdfreqs=c(1, 1, 9, 0, 0), stdn=50, smpn=30, stdev=25, smpt=5, out=""pl"")
#> starting simulation
#> simulating sample measurements
#> simulating standard measurements
#> 2 standard measurements not simulated due to roundoff.
#> calculating empirical transfer function (ETF)
#> applying ETF to sample
#> calculating summary statistics of raw values
#> calculating 95% confidence intervals of regression at computed raw value in raw space
#> calculating 95% confidence intervals of sample in expected space
#> creating plots
#> now dyn.load(""/usr/lib/R/library/Rcpp/libs/Rcpp.so"") ...
#> now dyn.load(""/usr/lib/R/library/ggrepel/libs/ggrepel.so"") ...
#> `geom_smooth()` using formula 'y ~ x'
#> now dyn.load(""/usr/lib/R/library/splines/libs/splines.so"") ...
#> now dyn.load(""/usr/lib/R/library/Matrix/libs/Matrix.so"") ...
#> now dyn.load(""/usr/lib/R/library/mgcv/libs/mgcv.so"") ...
#> now dyn.load(""/usr/lib/R/library/farver/libs/farver.so"") ...
```

<img src=""man/figures/README-example-1.png"" width=""100%"" />

    #> `geom_smooth()` using formula 'y ~ x'

<img src=""man/figures/README-example-2.png"" width=""100%"" />
",2022-08-12
https://github.com/japhir/stdstats,"#+TITLE: Simulation and plotting code for ""Optimizing the use of carbonate standards to minimize uncertainties in clumped isotope data""
#+PROPERTY: header-args:R :session *R:standardstats* :tangle standardstats.R :comments org :eval no-export
#+OPTIONS: ^:{}

#+AUTHOR: Ilja J. Kocken
#+DATE: 2019-08-23

The supplementary code to the manuscript ""Optimizing the use of carbonate
standards to minimize uncertainties in clumped isotope data"".

Copyright (C) 2019 Ilja J. Kocken

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.


See https://github.com/japhir/stdstats for potentially updated versions and Emacs org source files.

Please note that it takes quite some time to run all of the code, so if you
want to evaluate it, make sure to do so line-by-line!

We set the seed here in an attempt to make these exact simulations
reproducible, but this only works for the next random sampling call. Thus, you
will not get identical results. Please load the included simulation output =.rds=
files to generate identical plots (see below).

#+begin_src R :results none
  set.seed(1563435)
#+end_src

** install required packages
Install all below packages. Some of them (e.g. =furrr=) rely on the dev version
available on GitHub. Install them with
#+begin_src R :eval never
  devtools::install_github(""DavisVaughan/furrr"")
#+end_src
Install the new package =stdsim= similarly, from
#+begin_src R :eval never
  devtools::install_github(""japhir/stdsim"")
#+end_src
or point to the directory in which you downloaded the stdsim folder with
#+begin_src R :eval never
  devtools::install_local(""/path/to/stdsim"")
#+end_src

** optional packages
I use this for my theme.

#+begin_src R :eval never
 devtools::install_github(""baptiste/egg"")
#+end_src

# For using markdown in axis labels, I only use it once.
# #+begin_src R :eval never
#   devtools::install_github(""clauswilke/ggtext"")
# #+end_src

** load libraries
We use the following packages for this project:
#+begin_src R :results output
  library(dplyr)     # for piping (%>%), mutate, and nice data manipulation
  library(tidyr)     # for making tibbles tidy, (gather, spread etc.)
  library(tibble)    # a better thing than a dataframe
  library(ggplot2)   # plotting
  library(tictoc)    # to keep track of how long the functions took
  library(purrr)     # functional programming to map variables from lists of dataframes
  library(magrittr)  # only used once for extract, but original package of the pipe
  library(patchwork) # combine plots
  library(furrr)     # furrr allows parallell purr functions w/ progress bars!

  library(stdsim)    # the new R package created just for this paper!
#+end_src

#+RESULTS:

** multiple cores
run simulations on as many cores as possible. Note that I read somewhere that
the random number generator gets less random when using multiple cores, but it
is much faster and since I perform all simulations often this shouldn't effect
the results.

#+begin_src R :results none
  plan(multiprocess)
#+end_src

** set up plotting theme
We can use a different theme, but I like the one in the dev package =egg=.
Install it or ignore this section of code.

#+begin_src R :results none
  theme_set(egg::theme_article(base_size = 11, base_family=""Helvetica""))
#+end_src

** set up stdinfo etc.
This uses the default functions in stdsim to generate a tibble with standard
and sample information. It doesn't add \delta_{47} values by default since they differ
between labs and are not important for these simulations.

#+begin_src R :results none
  eth.info <- make_std_table()
  smpinfo <- make_smp_info(c(0, 40))
  stdev <- 14
  # append d47 values based on actual measurement results for our MOTU
  eth.info$d47 <- c(15.6, -13.2, 16.2, -13.1, NA_real_)
#+end_src

*** setup axes
#+begin_src R :exports none
  ## manual tweaks to axes ticks
  # kele is valid between 6 and 95 °C
  temp_labs <- c(""6"", """", ""15"", """", """", ""30"", """", """", """", ""50"", """", """", """", ""70"", """", """", """", """", ""95"")
  temp_breaks <- c(6, seq(10, 95, 5))
  ## err_breaks <- c(14, 25, 50, 100, 300)
  err_breaks <- c(seq(-20, 30, 5), 100, 200)
#+end_src

#+RESULTS:
| -20 |
| -15 |
| -10 |
|  -5 |
|   0 |
|   5 |
|  10 |
|  15 |
|  20 |
|  25 |
|  30 |
| 100 |
| 200 |

** calculate temperature sensitivity as a function of temperature
Now we are interested in calculating the rate of change as a function of D47,
so that we can calculate the change in temperature. So we take the derivative
of the original T(D47) function.

#+begin_src R :results none
  tempcal_simplified  <- function(Tc, slp=0.0449, int=0.167, kkelvin=273.15) {
      (slp * 1e6) / (Tc + kkelvin)^2 + int
  }

  tempcal_derivative  <- function(Tc, slp=0.0449, int=0.167, kkelvin=273.15) {
    -((2 * slp * 1e6) / ((kkelvin + Tc) ^ 3))
  }

  revcal_simplified <- function(D47, slp=0.0449, int=0.167, kkelvin=273.15) {
      sqrt((slp * 1e6) / (D47 - int)) - kkelvin
  }

  revcal_derivative <- function(D47, slp=0.0449, int=0.167) {
    (sqrt(-(slp * 1e6) / (int - D47))) / (2 * int - 2 * D47)
  }
#+end_src

See
#+begin_src R :eval never :results none
  ?revcal
  ?tempcal
#+end_src

for the actual function documentation.

*** temp_sens_pot
Calculate the sensitivity of the temperature calibration at the relevant
temperature range, so that we can add an estimate of uncertainty in the
temperature domain to plots.

#+begin_src R :exports none :results none
  rng <- seq(-10, 1000, .01)     # for all temperatures between -10 and 1000 °C
  sensdf <- tempcal(rng) %>%
    mutate(sens=tempcal_derivative(rng))
#+end_src

The citeA:Kele2015 temperature calibration is only valid between 6 and 95\us\celsius, so

#+begin_src R
  # guo 2009 eqn. 18
  # takes temperature in degrees celsius, converts to D47
  guo_cal <- function(temp) {
    # convert degrees celsius to kelvin
    x <- temp + kkelvin
    # apply polynomial fit
    -3.33040e9 / x^4 + 2.32415e7 / x^3 - 2.91282e3 / x^2 - 5.54042 / x + 0.23252
  }

  # takes D47, converts to temperature in degrees celsius
  guo_deriv <- function(temp) {
    x <- temp + kkelvin
    (5.54042 * x^3 + 5825.64 * x^2 - 69724500 * x + 13321600000)/x^5 * 1000
  }
#+end_src

#+RESULTS:

Update standards to use Guo if ETH-1 or ETH-2. We hack it together by
numerically solving it.

#+begin_src R
  guo_temp <- tibble(Tc = rng, D47 = guo_cal(Tc))
  eth1_new_temp <- guo_temp$Tc[[which(near(guo_temp$D47, eth.info$D47[[1]], tol = .0000005))]]
  eth2_new_temp <- guo_temp$Tc[[which(near(guo_temp$D47, eth.info$D47[[2]], tol = .0000005))]]

  guo_std_temp <- bind_rows(eth.info, smpinfo) %>%
    mutate(temp = case_when(id == ""ETH-1"" ~ eth1_new_temp, #802.812 - kkelvin,
                            id == ""ETH-2"" ~ eth2_new_temp, #822.2 - kkelvin,
                            TRUE ~ temp))
#+end_src

#+RESULTS:
| ETH-1  | orange  |             0.258 |             0.196 |  -0.65390744892801 |           529.66 |  15.6 |
| ETH-2  | purple  |             0.256 |             0.194 | -0.655731852151515 |           549.05 | -13.2 |
| ETH-3  | #00B600 |             0.691 |             0.629 | -0.258924151039187 | 19.5734580636874 |  16.2 |
| ETH-4  | blue    |             0.507 |             0.445 | -0.426769247601643 | 90.2489866928797 | -13.1 |
| UU     | #FFCD00 | 0.751543149422801 | 0.689543149422801 | -0.203696592555137 |                4 |   nil |
| sample | black   | 0.768788565206388 | 0.706788565206388 | -0.187965296482007 |                0 |   nil |
| sample | black   | 0.624869282857647 | 0.562869282857647 | -0.319248697802789 |               40 |   nil |

#+begin_src R :file imgs/sensplot_full.png :results output graphics :output graphics :exports both
  plot_temp <- sensdf %>%
    filter(Tc >= 6, Tc <= 95) %>%
    ggplot(aes(y = D47, x = Tc)) +
    geom_ribbon(aes(ymin = lwr, ymax = upr), fill = ""skyblue"", alpha = .4) +
    geom_line(colour = ""blue"", linetype = 2, alpha = .5, data = sensdf) +
    geom_line(colour = ""blue"") +
    ## geom_line(colour = ""black"", data = guo_temp) +
    stat_function(fun = guo_cal, colour = ""black"") +
    geom_segment(aes(x = -Inf, xend = temp, y = D47, yend = D47, col = id),
                 alpha=.5,
                 inherit.aes=FALSE,
                 data = guo_std_temp) +
    geom_segment(aes(x = temp, xend = temp, y = -Inf, yend = D47, col = id),
                 alpha=.5,
                 inherit.aes=FALSE,
                 data = guo_std_temp) +
    annotate(""text"", x = 60, y = .65, label = ""Kele et al., 2015, \nrecalculated by Bernasconi et al., 2018"", colour = ""darkblue"", hjust = 0) +
    annotate(""text"", x = 450, y = .3, label = ""Guo et al., 2009"") +
    scale_colour_manual(values = c(eth.info$col[-5], smpinfo$col[[1]], eth.info$col[[5]])) +
    labs(x = ""Temperature (°C)"", y = Delta[47] ~ ""CDES (\u2030)"") +
    coord_cartesian(ylim = c(.2, .8), xlim = c(0, 550)) +
    theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          legend.pos=c(.75, .6), legend.title = element_blank())

  plot_sens <- sensdf %>%
    filter(Tc >= 6, Tc <= 95) %>%
    ggplot(aes(y = sens * 1e3, x = Tc)) +
    geom_line(colour = ""blue"", linetype = 2, alpha = .5, data = sensdf) +
    geom_line(colour = ""blue"") +
    stat_function(fun = guo_deriv, xlim = c(-10, 1000)) +
    coord_cartesian(ylim = c(-5, 0), xlim = c(0, 550)) +
    ## geom_vline(xintercept=c(0, 40), col=""#ededed"") +
    labs(x = ""Temperature (°C)"", y = ""Sensitivity"" ~ ""("" * Delta[47] / ""°C, ppm)"")

  temp_sens_pl <- plot_temp + plot_sens + plot_layout(nrow=2, heights=c(3, 1))
  temp_sens_pl
#+end_src

#+RESULTS:
[[file:imgs/sensplot_full.png]]

** create standard intro plot
The standards as a function of composition
#+begin_src R :results none
  lims <- c(.15, .71)
  standards_plot <- ggplot(eth.info, aes(x = d47, y = D47.noacid, col = id, label = id)) +
    geom_point(size = 2, show.legend = F) +
    ## geom_label(show.legend = F) +
    ggrepel::geom_label_repel(size = 2.5, show.legend = FALSE) +
    # add UU1 standard
    geom_hline(yintercept = eth.info$D47.noacid[[5]], linetype = 2, col = eth.info$col[[5]]) +
    annotate(""label"", x = 1.225, y = eth.info$D47.noacid[[5]], label = eth.info$id[[5]], col = eth.info$col[[5]], size = 2.5) +
    scale_colour_manual(values = eth.info$col) +
    labs(colour = """", x = delta^{47}~""(SG vs WG PBL \u2030)"",
      y = Delta[47] ~ ""CDES"" - ""AFF (\u2030)"") +
    scale_y_continuous(sec.axis = sec_axis(~ sqrt((0.0449 * 1e+6)/(. + kaff - 0.167)) - 273.15,
                                          ""Sample temperature (°C)"", temp_breaks,
                                          temp_labs)) +
    coord_flip(ylim=lims) +
    theme(axis.text.x.bottom=element_blank(), axis.title.x.bottom=element_blank())
    ## coord_cartesian(clip = ""off"") +
    ## theme(legend.pos = c(.15, .85))
#+end_src

And the simulation input conditions illustrating the ETF.
#+begin_src R :results none
  stdevs <- c(14, 25, 50) / 1e3
  xs <- .54 + c(0, .04, .08)
  ys <- rep(-.6, 3)

  standard_sample_data <- make_smp_info(c(0, 40)) %>%
    mutate(id=ifelse(temp == 40, ""sample 1"", ""sample 2"")) %>%
    ## mutate(id=paste(id, temp)) %>%
    bind_rows(eth.info)

  base_plot <-  standard_sample_data %>%
    ggplot(aes(x = D47.noacid, y = rawcat, col = id, label = id)) +
    # add etf
    geom_abline(intercept = kintercept, slope = kslope,
      linetype = 1, size = 1, col = ""gray"") +
    # 50 ppm uncertainty pointrange
    geom_linerange(aes(ymin = rawcat - 50 * kslope / 1e3,
      ymax = rawcat + 50 * kslope / 1e3),
      size = 1, linetype = 1, alpha = .1) +
    # 25 ppm uncertainty pointrange
    geom_linerange(aes(ymin = rawcat - 25 * kslope / 1e3,
      ymax = rawcat + 25 * kslope / 1e3),
      size = 1, linetype = 1, alpha = .4) +
    # 14 ppm uncertainty pointrange
    geom_linerange(aes(ymin = rawcat - 14 * kslope / 1e3,
      ymax = rawcat + 14 * kslope / 1e3),
      size = 1, linetype = 1) +
    geom_point(size=2) +
    # create a manual legend with the different input uncertainties
    annotate(""segment"",
             x = xs, xend = xs,
             y = ys, yend = ys + stdevs * kslope,
      alpha = c(1, .4, .1)) +
    annotate(""segment"",
             x=xs, xend=xs + stdevs,
             y=ys, yend=ys,
             alpha=c(1, .4, .1)) +
    annotate(""text"",
             x = xs,
             y = ys - .02,
      label = c(""14"", ""25"", ""50""), size = 2) +
    # add the input sample measurements
    ggrepel::geom_text_repel(force = 3, hjust = 1, nudge_y = .05, nudge_x = -.01, size=2.5, segment.color = NA) +
    # make it pretty, manual colour scale, samples are black
    scale_colour_manual(
        limits = standard_sample_data$id,
        values = standard_sample_data$col) +
    # nice axis labels
    labs(
      colour = """",
      x = Delta[47] ~ ""CDES"" - ""AFF (\u2030)"",
      y = Delta[47] ~ ""raw (\u2030)""
    ) +
    scale_x_continuous(limits=lims) +
    theme(legend.pos = ""none"")
#+end_src

In the text we combine them using patchwork, to create figure 1.
#+begin_src R :file imgs/standards.png :results output graphics :width 400 :height 400 :exports both
  standards_pl <- standards_plot + base_plot + plot_layout(nrow=2, heights = c(.4, .6))
  standards_pl
#+end_src

#+RESULTS:
[[file:imgs/standards.png]]

** micro benchmark
Calculate how long it takes for one simulation.
#+begin_src R
  options(genplot = FALSE, verbose = FALSE)
  tpersim <- microbenchmark::microbenchmark(sim_stds(out = ""cis"",
    stdtable = eth.info)) %>%
    summary() %>%
    pull(mean) / 100
  tpersim
#+end_src

#+RESULTS:
: 0.0653846856

** example sims
We create some example simulations for fig. 2.
#+begin_src R :results none
  options(genplot=F, verbose=F)

  ## set up small inputs dataframe
  example_sims <- tibble(
    name=rep(c(""Equal\nproportions"", ""Optimal\ndistribution"", ""Optimal\ndistribution + UU1""), 2),
    stdfreqs=rep(list(c(1, 1, 1, 1, 0), c(1, 1, 9, 0, 0), c(1, 1, 0, 0, 9)), 2),
    smpt=c(rep(0, 3), rep(40, 3))) %>%
    ## run sims with inputs dataframe
    mutate(res = purrr::pmap(select(., -name), sim_stds, stdev=25, out=""all"", stdn=50, smpn=50),
           # extract the default plots
           pl=purrr::map(res, plot_sim, graylines=F, point_alpha=.2, pointrange=T, labs=F, fixed=F),
           # add a row number for the next step
           exprow=1:n() %>% as.character())

  # combine the smp and std outputs of each experiment, based on the row number
  six_example_sims <- example_sims$res %>%
    map_dfr(""smp"", .id=""exprow"") %>%
    bind_rows(example_sims$res %>% map_dfr(""std"", .id=""exprow"")) %>%
    left_join(example_sims, by=""exprow"")
#+end_src

We create the example plot
#+begin_src R :file imgs/exmp.png :results output graphics
  exmp_plot <- ggplot(six_example_sims, aes(x=D47.noacid, y=raw, col=id, fill=id)) +
    geom_smooth(aes(group=paste0(name, smpt)), method=""lm"", size=.1,
                fullrange=TRUE, data=filter(six_example_sims, id != ""sample"")) +
    geom_violin(alpha=.3, colour=NA, scale=""count"", width=.5, position=position_identity()) +
    geom_point(shape=1, alpha=.2, size=.3) +
    facet_grid(rows=vars(name), cols=vars(paste(smpt, ""°C""))) +
    ## coord_fixed(xlim=c(.1, .8)) +
    coord_cartesian(xlim=c(.14, .75)) +
    scale_colour_manual(""ID"",
                        ## limits = c(out$cond$stdtable$id, out$cond$smpinfo$id),
                        limits = c(example_sims$res[[1]]$cond$stdtable$id, example_sims$res[[1]]$cond$smpinfo$id),
                        ## values = c(out$cond$stdtable$col, out$cond$smpinfo$col)) +
                        values = c(example_sims$res[[1]]$cond$stdtable$col, example_sims$res[[1]]$cond$smpinfo$col)) +
    scale_fill_manual(""ID"",
                      limits = c(example_sims$res[[1]]$cond$stdtable$id, example_sims$res[[1]]$cond$smpinfo$id),
                      values = c(example_sims$res[[1]]$cond$stdtable$col, example_sims$res[[1]]$cond$smpinfo$col)) +
    labs(x = Delta[47] ~ ""CDES"" - ""AFF (\u2030)"",
         y = Delta[47] ~ raw ~ ""(\u2030)"") +
    theme(legend.pos=""top"", legend.key.size=unit(3, ""mm""), legend.text = element_text(size = 6),
          strip.text.y = element_text(size = 8, angle = 90))
  exmp_plot
#+end_src

#+RESULTS:
[[file:imgs/exmp.png]]

and the table for in the text:
#+begin_src R :results value table :colnames yes :exports both
  tbl_exmp <-
    forplot_0 %>%
    bind_rows(forplot_40) %>%
    filter(expname %in% c(""1:1:1:1:0"", ""1:1:9:0:0"", ""1:1:1:0:9"")) %>%
      select(-stdfreqs, -exprow, -meanerr, -hascoldstandard) %>%
    group_by(expname, smpt, stdev) %>%
    ## nest() %>%
    summarize(err_mean = mean(smp, na.rm = TRUE) * 1e3,
              err_ci = qt((1 - .05), length(smp) - 1) * sd(smp, na.rm = TRUE) / sqrt(length(smp) - 1) * 1e3) %>%
    mutate(err_temp = err_mean / 1e3 / tempcal_derivative(smpt) %>% abs,
           err_temp_ci = err_ci / 1e3 / tempcal_derivative(smpt) %>% abs,
     ) %>%
    arrange(stdev, -err_mean) %>%
    ## bind_cols(map_dfr(.$data, ~ mean(smp))) %>%
    pivot_wider(id_cols = c(expname, stdev), names_from = smpt, values_from = c(err_mean, err_ci, err_temp, err_temp_ci)) %>%
    # change order of things
    mutate(Name = case_when(expname == ""1:1:1:1:0"" ~ ""Equal proportions"",
                            expname == ""1:1:9:0:0"" ~ ""Optimal distribution"",
                            expname == ""1:1:1:0:9"" ~ ""Optimal distribution + UU1"",
                            TRUE ~ ""wth"")) %>%
    select(Name, expname, stdev, ends_with(""_0""), ends_with(""_40"")) %>%
    as_tibble() %>%
    mutate(
      out_0_ppm = glue::glue(""{round(err_mean_0, 2)} \\pm {round(err_ci_0, 2)}""),
      ## improv_0 = round(err_mean_0 / lead(err_mean_0), 2),
      ## improv_40 = round(err_mean_0 / lead(err_mean_0), 2),
      out_0_deg = glue::glue(""{round(err_temp_0, 2)} \\pm {round(err_temp_ci_0, 2)}""),
      out_40_ppm=glue::glue(""{round(err_mean_40, 2)} \\pm {round(err_ci_40, 2)}""),
      out_40_deg = glue::glue(""{round(err_temp_40, 2)} \\pm {round(err_temp_ci_40, 2)}""),
      ) %>%
    ## rename for latex output
    select(Name,
           `Standard distribution`=expname,
           `\\sigma` = stdev,
           `0\\us\\celsius (ppm)` = out_0_ppm,
           ## `\\times` = improv_0,
           `40\\us\\celsius (ppm)`=out_40_ppm,
           ## `\\times` = improv_40,
           ## `0\\u
           `0\\us\\celsius (\\celsius)`=out_0_deg,
           `40\\us\\celsius (\\celsius)`=out_40_deg)
#+end_src

#+RESULTS:
| Name                       | Standard distribution |  \sigma | 0\us\celsius (ppm)    | 40\us\celsius (ppm)   | 0\us\celsius (\celsius)     | 40\us\celsius (\celsius)    |
|----------------------------+-----------------------+----+--------------+--------------+-------------+-------------|
| Equal proportions          |             1:1:1:1:0 | 14 | 9.37 \pm 0.12  | 7.21 \pm 0.09  | 2.13 \pm 0.03 | 2.46 \pm 0.03 |
| Optimal distribution       |             1:1:9:0:0 | 14 | 6.8 \pm 0.09   | 5.58 \pm 0.07  | 1.54 \pm 0.02 | 1.91 \pm 0.03 |
| Optimal distribution + UU1 |             1:1:1:0:9 | 14 | 6 \pm 0.07     | 5.63 \pm 0.07  | 1.36 \pm 0.02 | 1.93 \pm 0.02 |
| Equal proportions          |             1:1:1:1:0 | 25 | 17.03 \pm 0.24 | 12.89 \pm 0.16 | 3.86 \pm 0.05 | 4.41 \pm 0.06 |
| Optimal distribution       |             1:1:9:0:0 | 25 | 12.45 \pm 0.18 | 10.24 \pm 0.12 | 2.82 \pm 0.04 | 3.5 \pm 0.04  |
| Optimal distribution + UU1 |             1:1:1:0:9 | 25 | 10.86 \pm 0.12 | 10.11 \pm 0.1  | 2.46 \pm 0.03 | 3.46 \pm 0.04 |
| Equal proportions          |             1:1:1:1:0 | 50 | 35.15 \pm 0.57 | 26.35 \pm 0.46 | 7.98 \pm 0.13 | 9.01 \pm 0.16 |
| Optimal distribution       |             1:1:9:0:0 | 50 | 25.08 \pm 0.44 | 20.1 \pm 0.27  | 5.69 \pm 0.1  | 6.87 \pm 0.09 |
| Optimal distribution + UU1 |             1:1:1:0:9 | 50 | 22.09 \pm 0.33 | 20.03 \pm 0.28 | 5.01 \pm 0.08 | 6.85 \pm 0.1  |

** stddis
To simulate the distribution of the standards we create a tibble of inputs for
=sim_stds()=.
*** create a list of all possible stddis combinations
#+begin_src R :results none
  # the proportions we want each standard to get
  pr <- c(0, 1, 3, 9)

  # each standard gets these proportions in all possible combinations
  props <- expand.grid(
    `ETH-1` = pr,
    `ETH-2` = pr,
    `ETH-3` = pr,
    `ETH-4` = pr,
    UU1 = pr) %>%
    # we need at least 1|2 & 3|4|UU1 to be able to calculate an ETF
    # we need at least 1|2|4 & 3|4|UU1 to be able to calculate an ETF
    ## filter(`ETH-1` + `ETH-2` + `ETH-4` > 0 & `ETH-3` + `ETH-4` + `UU1` > 0) %>%
    # we need at least 1 standard
    filter(`ETH-1` + `ETH-2` + `ETH-4` + `ETH-3` + `ETH-4` + `UU1` > 0) %>%
    # calculate relative abundances
    mutate(sums = rowSums(.),
      f1 = `ETH-1` / sums,
      f2 = `ETH-2` / sums,
      f3 = `ETH-3` / sums,
      f4 = `ETH-4` / sums,
      fu = UU1 / sums) %>%
    # filter out the redundant ones
    distinct(f1, f2, f3, f4, fu, .keep_all = TRUE) %>%
    arrange(`ETH-1`, `ETH-2`, `ETH-3`, `ETH-4`, UU1)

  # convert the proportions to a list we can put in our experimental matrix
  props_list <- props %>%
    select(-c(sums, starts_with(""f""))) %>%
    as.matrix() %>%
    split(seq(nrow(.)))
#+end_src

*** expand the whole grid
#+begin_src R :results none
  stddis <- expand.grid(
      smpt = c(0, 40),
      stdfreqs = props_list,  # this uses the list created previously
      stdev = c(50, 25, 14)) %>%
    # we add an experiment name for plot labels and easy overview
    mutate(expname = map_chr(stdfreqs, paste, collapse = "":"")) %>%
    # add cold standard logical for later filtering
    mutate(hascoldstandard = grepl(""[[139]]"", expname))

  # repeat each experiment a hundred times
  megastddis <- stddis[rep(stddis %>% nrow() %>% seq_len(), 100), ] %>%
    # and add a row character for later merging of results
    mutate(exprow = as.character(seq_len(n())))
#+end_src

*** stddis runall
Note that we use the package =future= so that we can show a progress bar and use
multiple cores. One could also use =purrr::pmap_dfr()= which it's based on, here.

#+begin_src R :eval never
  ## I turn off plotting and info messages
  options(genplot = FALSE, verbose = FALSE)

  ## keep track of how long it takes
  message(nrow(megastddis), "" simulations started at "", Sys.time())
  ## very rough expected finish time (it's usually faster)
  message(""expected to take until "", Sys.time() + tpersim * nrow(megastddis) / 4)

  ## track actual duration with tictoc
  tic(""stddis total time"")
  ## run sim_stds with parameters from mgstddis and global parameters after it
  stddis_cnf <- furrr::future_pmap_dfr(
                         select(megastddis, smpt, stdfreqs, stdev),
                         sim_stds, stdtable = eth.info,
                         out = ""cis"", stdn = 50, smpn = 50,
                         .id = ""exprow"", # append a row name id
                         .progress=TRUE   # show a progress bar
                       ) %>%
    filter(id == ""smp"") %>%                        # filter output
    select(exprow, id, cv) %>%                     # select output
    spread(id, cv) %>%                             # make it wide format
    right_join(megastddis, by=""exprow"")            # join it with experimental df
  toc()
  message(""simulations ran until "", Sys.time())
#+end_src

#+RESULTS:
#+begin_example

460200 simulations started at 2019-06-12 11:10:52

expected to take until 2019-06-12 13:04:06

stddis total time: 1434.973 sec elapsed

simulations ran until 2019-06-12 12:06:18
#+end_example

Save the results so that we don't have to run the simulations every time.

#+begin_src R :eval never :results none
  saveRDS(stddis_cnf, ""stddis_cnf_2019-06-17.rds"")
#+end_src

Restore the results from previous simulations.

#+begin_src R :results none
  stddis_cnf <- readRDS(""stddis_cnf_2019-06-17.rds"")
#+end_src

*** arrange results for plot
We re-organize the dataframes and make a selection of the best results for 0
and 40 degrees.

#+begin_src R :results none
  # the mean error of equal proportions at 0 degrees
  normerr_0  <- stddis_cnf %>%
    filter(smpt == 0, expname == ""1:1:1:1:0"") %>%
    summarize(meanerr=mean(smp)) %>%
    pull(meanerr)

  idealerr_0 <- stddis_cnf %>%
    filter(smpt == 0, expname == ""1:1:9:0:0"") %>%
    summarise(meanerr = mean(smp)) %>%
    pull(meanerr)

  normerr_40  <- stddis_cnf %>%
    filter(smpt == 40, expname == ""1:1:1:1:0"") %>%
    summarize(meanerr=mean(smp)) %>%
    pull(meanerr)

  idealerr_40 <- stddis_cnf %>%
    filter(smpt == 40, expname == ""1:1:9:0:0"") %>%
    summarise(meanerr = mean(smp)) %>%
    pull(meanerr)

  forplot_0 <-
    stddis_cnf %>%
    filter(smpt == 0) %>%
    # append mean error per treatment to original dataframe
    group_by(expname) %>%
    summarise(meanerr = mean(smp, na.rm=TRUE)) %>%
    left_join(filter(stddis_cnf, smpt == 0), by = ""expname"") %>%
    # if there is no cold standard, it should be at least as good as normerr
    filter((!hascoldstandard & (meanerr <= normerr_0)) |
    # if there is a cold standard, it should be at least as good as idealerr
             (hascoldstandard & (meanerr <= idealerr_0))) %>%
    arrange(meanerr, expname) %>%
    ungroup() %>%
    # this is a hack to order the labels of a factor in a plot
    mutate(expname = factor(expname, unique(expname)))

  forplot_40 <-
    stddis_cnf %>%
    filter(smpt == 40) %>%
    # append mean error per treatment to original dataframe
    group_by(expname) %>%  # note that we do not take into account sample temp..
    summarise(meanerr = mean(smp, na.rm=TRUE)) %>%
    left_join(filter(stddis_cnf, smpt == 40), by = ""expname"") %>%
    # if there is no cold standard, it should be at least as good as normerr
    filter((!hascoldstandard & (meanerr <= normerr_40)) |
    # if there is a cold standard, it should be at least as good as idealerr
             (hascoldstandard & (meanerr <= idealerr_40))) %>%
    arrange(meanerr, expname) %>%
    mutate(expname = factor(expname, unique(expname)))
#+end_src

*** create barcharts for proportion axes
#+begin_src R
  forplot_0_x <- forplot_0 %>%
    distinct(expname, .keep_all = TRUE) %>%
    unnest(cols=stdfreqs) %>%
    mutate(std = rep(c(paste0(""ETH-"", 1:4), ""UU""), n()/5))

  x_0 <- forplot_0_x %>%
    ggplot(aes(x = expname, y = stdfreqs, fill = std)) +
    geom_col(position=""fill"") +
    ## geom_text(aes(label = lab), y = .5, size = 2,
    ##           data = mutate(forplot_0_x,
    ##                         lab = ifelse(expname %in% c(""1:1:1:1:0"", ""1:1:9:0:0"", ""1:1:0:0:9""), expname, """"))) +
    scale_fill_manual(values=eth.info$col,
                      guide = guide_legend(label.position = ""top"",
                                           direction = ""horizontal"",
                                           label.theme = element_text(size = 8, angle = 90, hjust = 0))) +
    scale_y_reverse(position = ""right"", expand = c(0, 0)) +
    labs(x = ""Standard distribution"", y = ""ETH-1:2:3:4:UU1"", fill = """") +
    ## labs(x = ""Standard distribution"", y = ""ETH-<span style='color:orange'>1</span>:<span style='color:purple'>2</span>:<span style='color:#00B600'>3</span>:<span style='color:blue'>4</span>:<span style='color:#FFCD00'>UU1</span>"") +
    coord_flip() +
    theme(axis.text = element_blank(),
          axis.ticks = element_blank(),
          plot.margin = margin(r = 0),
          axis.title.x = element_blank(),
          legend.pos = ""top"",
          legend.justification = 12,
          legend.spacing.x = unit(1, ""mm""),
          ## legend.margin = margin(2, 0, 2, 0),
          legend.key.size = unit(2, ""mm""))
#+end_src

#+RESULTS:

#+begin_src R
    forplot_40_x <- forplot_40 %>%
      distinct(expname, .keep_all = TRUE) %>%
      unnest(cols=stdfreqs) %>%
      mutate(std = rep(c(paste0(""ETH-"", 1:4), ""UU""), n()/5))

    x_40 <- x_0 %+% forplot_40_x +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank(),
            legend.pos = ""none"")
#+end_src

#+RESULTS:

*** stddis_pl
#+begin_src R
  sub_vjust <- -7
  sds <- tibble(expname = ""1:0:9:1:0"", sd = c(14, 25, 50), smp = c(.006, .012, .027) + .003, hascoldstandard = FALSE, smpt = 0)

  # create a plot
  stddis_pl_0 <- forplot_0 %>%
    ggplot(aes(x = expname, y = smp * 1e3,  # error in ppm
      colour = factor(smpt), fill = factor(smpt))) +
    # theming
    labs(x = ""Standard distribution"",
      y = ""Combined error of sample and ETF (95% CI, ppm)"",
      fill = ktit_smpid,
      colour = ktit_smpid,
      shape = ""Total standards"") +
    scale_fill_manual(values = kcols[[1]], labels = ""0 °C"") +
    scale_colour_manual(values = kcols[[1]], labels = ""0 °C"") +
    scale_y_continuous(breaks = seq(0, 65, 10), lim = c(5, 40),
      sec.axis = sec_axis(""Combined error of sample and ETF (95% CI, °C)"",
        trans = ~. / abs(tempcal_derivative(0) * 1e3), breaks = seq(0, 20, 2))) +
    theme(
      plot.title = element_text(hjust = 0.5, vjust = -10),
      plot.margin = margin(l = 0),
      ## plot.subtitle = element_text(size = 8, hjust = -.1, vjust = -8),
      axis.title.x.top = element_text(hjust = 7),
      axis.title.x.bottom = element_text(hjust = 7),
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      axis.title.y = element_blank(),
      strip.text = element_blank(),
      legend.position = ""none"") +
    # manual legend for input stdevs (sigma)
    # the actual data
    geom_point(alpha = .03) +
    ## annotate(""text"", x = -Inf, y = Inf, label = paste0(""\u3c3 = "", c(14, 25, 50))) +
    geom_text(aes(label = sd), data = sds, colour = ""black"") +
    coord_flip(clip = ""off"") +
    stat_summary(aes(group=stdev), geom=""ribbon"", fun.data=mean_cl_normal, fun.args=list(conf.int=0.95), alpha=.4, colour=NA) +
    stat_summary(aes(group=stdev), geom=""line"", fun.data=mean_cl_normal) +
    labs(title=""0 °C sample"") #, subtitle = ""ETH-1:2:3:4:UU1"")
  # add the average lines +- 95% CIs for all the sample temperatures

  stddis_pl_40 <- forplot_40 %>%
    filter(smpt == 40) %>%
    ggplot(aes(x = expname, y = smp * 1e3,  # error in ppm
      colour = factor(smpt), fill = factor(smpt))) +
    # theming
    labs(x = ""Standard distribution"",
      y = ""Combined error of sample and ETF (95% CI, ppm)"",
      fill = ktit_smpid,
      colour = ktit_smpid,
      shape = ""Total standards"") +
    scale_fill_manual(values = kcols[[2]], labels = ""40 °C"") +
    scale_colour_manual(values = kcols[[2]], labels = ""40 °C"") +
    scale_y_continuous(breaks = seq(0, 65, 10), lim = c(5, 30),
      sec.axis = sec_axis(""Combined error of sample and ETF (95% CI, °C)"",
        trans = ~. / abs(tempcal_derivative(40) * 1e3), breaks = seq(0, 20, 2))) +
    theme(
      plot.title = element_text(hjust = 0.5, vjust = -10),
      plot.margin = margin(l = 0),
      plot.subtitle = element_blank(),
      axis.title = element_blank(),
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      axis.title.y = element_blank(),
      strip.text = element_blank(),
      legend.position = ""none"") +
    labs(title=""40 °C sample"") +
    coord_flip(clip = ""off"") +
    # the actual data
    geom_point(alpha = .03) +
    stat_summary(aes(group=stdev), geom=""ribbon"", fun.data=mean_cl_normal,
                 fun.args=list(conf.int=0.95), alpha=.4, colour=NA) +
    stat_summary(aes(group=stdev), geom=""line"", fun.data=mean_cl_normal)

  ## stddis_pl  <- stddis_pl_0 + stddis_pl_40 + plot_layout(widths=c(35 / 25 , 1))
  stddis_pl <- x_0 + stddis_pl_0 + x_40 + stddis_pl_40 + plot_layout(widths = c(.15, 35/25, .15, 1), ncol = 4)
#+end_src

#+RESULTS:

#+begin_src R :file imgs/stddis.png :results output graphics :exports both
  stddis_pl
#+end_src

#+RESULTS:
[[file:imgs/stddis.png]]


*** prep text
This generates some dataframes so we can easily extract relevant averages for
use in-text.
#+begin_src R :results none
  stddis_exmp_0 <- forplot_0 %>%
    filter(smpt == 0) %>%
    group_by(expname) %>%
    summarize(err=mean(smp, na.rm=TRUE), cnf = confidence(smp, n(), alpha=0.05)) %>%
    arrange(err)

  stddis_exmp_40 <- forplot_40 %>%
    filter(smpt == 40, stdev == 25) %>%
    group_by(expname) %>%
    summarize(err=mean(smp,na.rm=TRUE), cnf=confidence(smp, n(), alpha= 0.05)) %>%
    arrange(err)
#+end_src

** stdvssmp
Here we simulate the standards versus samples based on three input distributions,
*** setup
#+begin_src R :results none
  stdvssmp <- expand.grid(
    smpt = c(0, 40), stdn = as.integer(seq(12, 88, 4)),
    dist = c(""equal proportions"", ""optimal proportions"", ""optimal proportions including UU1""),
    stdev = c(50, 25, 14)) %>%
    as_tibble() %>%
    mutate(smpn = as.integer(100 - stdn),
           n = as.integer(smpn + stdn),
           smpf = smpn / n,
           stdf = stdn / n,
           stdfreqs = case_when(dist == ""equal proportions"" ~ list(c(1, 1, 1, 1, 0)),
                                dist == ""optimal proportions"" ~ list(c(1, 1, 9, 0, 0)),
                                dist == ""optimal proportions including UU1"" ~ list(c(1, 1, 1, 0, 9))))

  # repeat each experiment a hundred times
  megastdvssmp <- stdvssmp[rep(stdvssmp %>% nrow() %>% seq_len(), 100), ] %>%
    mutate(exprow = as.character(seq_len(n())))  # row number
#+end_src

*** run sims
#+begin_src R :eval never
  # again, I turn off plotting and info messages
  options(genplot = FALSE, verbose = FALSE)

  # keep track of time
  message(nrow(megastdvssmp), "" simulations started at "", Sys.time())
  message(""expected to take until "", Sys.time() + tpersim * nrow(megastdvssmp) / 4)

  # run duration
  tic(""overview"")
  # run sim_stds with parameters from megastdvssmp and global parameters after it
  stdvssmp_cnf <- furrr::future_pmap_dfr(
                           select(megastdvssmp,
                                  smpt, stdn, stdev, smpn, stdfreqs),
                           sim_stds,
                           stdtable = eth.info, out = ""cis"",
                           .id = ""exprow"",          # append a row name id
                           .progress=TRUE
                           ) %>%
    filter(id %in% c(""etf"",""sample"", ""smp"")) %>%    # filter output
    select(exprow, id, cv) %>%                      # select output
    spread(id, cv) %>%                              # make it wide format
    right_join(megastdvssmp)                        # join it with experimental df
  toc()
  message(""simulations ran until "", Sys.time())
#+end_src

#+RESULTS:
#+begin_example

36000 simulations started at 2019-06-13 16:05:54

expected to take until 2019-06-13 16:14:42


 Progress: ─                                                                100%
 Progress: ─                                                                100%
 Progress: ──                                                               100%
 Progress: ───                                                              100%
 Progress: ───                                                              100%
 Progress: ────                                                             100%
 Progress: ─────                                                            100%
 Progress: ─────                                                            100%
 Progress: ──────                                                           100%
 Progress: ──────                                                           100%
 Progress: ───────                                                          100%
 Progress: ────────                                                         100%
 Progress: ────────                                                         100%
 Progress: ─────────                                                        100%
 Progress: ─────────                                                        100%
 Progress: ──────────                                                       100%
 Progress: ──────────                                                       100%
 Progress: ───────────                                                      100%
 Progress: ───────────                                                      100%
 Progress: ────────────                                                     100%
 Progress: ────────────                                                     100%
 Progress: ─────────────                                                    100%
 Progress: ─────────────                                                    100%
 Progress: ──────────────                                                   100%
 Progress: ──────────────                                                   100%
 Progress: ───────────────                                                  100%
 Progress: ───────────────                                                  100%
 Progress: ────────────────                                                 100%
 Progress: ─────────────────                                                100%
 Progress: ─────────────────                                                100%
 Progress: ──────────────────                                               100%
 Progress: ──────────────────                                               100%
 Progress: ───────────────────                                              100%
 Progress: ────────────────────                                             100%
 Progress: ────────────────────                                             100%
 Progress: ─────────────────────                                            100%
 Progress: ─────────────────────                                            100%
 Progress: ──────────────────────                                           100%
 Progress: ──────────────────────                                           100%
 Progress: ───────────────────────                                          100%
 Progress: ────────────────────────                                         100%
 Progress: ────────────────────────                                         100%
 Progress: ─────────────────────────                                        100%
 Progress: ─────────────────────────                                        100%
 Progress: ──────────────────────────                                       100%
 Progress: ──────────────────────────                                       100%
 Progress: ───────────────────────────                                      100%
 Progress: ────────────────────────────                                     100%
 Progress: ────────────────────────────                                     100%
 Progress: ─────────────────────────────                                    100%
 Progress: ─────────────────────────────                                    100%
 Progress: ──────────────────────────────                                   100%
 Progress: ──────────────────────────────                                   100%
 Progress: ───────────────────────────────                                  100%
 Progress: ───────────────────────────────                                  100%
 Progress: ────────────────────────────────                                 100%
 Progress: ─────────────────────────────────                                100%
 Progress: ─────────────────────────────────                                100%
 Progress: ──────────────────────────────────                               100%
 Progress: ──────────────────────────────────                               100%
 Progress: ───────────────────────────────────                              100%
 Progress: ───────────────────────────────────                              100%
 Progress: ────────────────────────────────────                             100%
 Progress: ────────────────────────────────────                             100%
 Progress: ─────────────────────────────────────                            100%
 Progress: ──────────────────────────────────────                           100%
 Progress: ──────────────────────────────────────                           100%
 Progress: ───────────────────────────────────────                          100%
 Progress: ───────────────────────────────────────                          100%
 Progress: ────────────────────────────────────────                         100%
 Progress: ────────────────────────────────────────                         100%
 Progress: ─────────────────────────────────────────                        100%
 Progress: ─────────────────────────────────────────                        100%
 Progress: ──────────────────────────────────────────                       100%
 Progress: ───────────────────────────────────────────                      100%
 Progress: ───────────────────────────────────────────                      100%
 Progress: ────────────────────────────────────────────                     100%
 Progress: ────────────────────────────────────────────                     100%
 Progress: ─────────────────────────────────────────────                    100%
 Progress: ─────────────────────────────────────────────                    100%
 Progress: ──────────────────────────────────────────────                   100%
 Progress: ──────────────────────────────────────────────                   100%
 Progress: ──────────────────────────────────────────────                   100%
 Progress: ───────────────────────────────────────────────                  100%
 Progress: ───────────────────────────────────────────────                  100%
 Progress: ────────────────────────────────────────────────                 100%
 Progress: ────────────────────────────────────────────────                 100%
 Progress: ────────────────────────────────────────────────                 100%
 Progress: ─────────────────────────────────────────────────                100%
 Progress: ─────────────────────────────────────────────────                100%
 Progress: ──────────────────────────────────────────────────               100%
 Progress: ──────────────────────────────────────────────────               100%
 Progress: ───────────────────────────────────────────────────              100%
 Progress: ────────────────────────────────────────────────────             100%
 Progress: ────────────────────────────────────────────────────             100%
 Progress: ─────────────────────────────────────────────────────            100%
 Progress: ─────────────────────────────────────────────────────            100%
 Progress: ──────────────────────────────────────────────────────           100%
 Progress: ──────────────────────────────────────────────────────           100%
 Progress: ───────────────────────────────────────────────────────          100%
 Progress: ───────────────────────────────────────────────────────          100%
 Progress: ────────────────────────────────────────────────────────         100%
 Progress: ────────────────────────────────────────────────────────         100%
 Progress: ─────────────────────────────────────────────────────────        100%
 Progress: ─────────────────────────────────────────────────────────        100%
 Progress: ──────────────────────────────────────────────────────────       100%
 Progress: ──────────────────────────────────────────────────────────       100%
 Progress: ───────────────────────────────────────────────────────────      100%
 Progress: ───────────────────────────────────────────────────────────      100%
 Progress: ────────────────────────────────────────────────────────────     100%
 Progress: ────────────────────────────────────────────────────────────     100%
 Progress: ─────────────────────────────────────────────────────────────    100%
 Progress: ─────────────────────────────────────────────────────────────    100%
 Progress: ──────────────────────────────────────────────────────────────   100%
 Progress: ──────────────────────────────────────────────────────────────   100%
 Progress: ───────────────────────────────────────────────────────────────  100%
 Progress: ───────────────────────────────────────────────────────────────  100%
 Progress: ───────────────────────────────────────────────────────────────  100%
 Progress: ───────────────────────────────────────────────────────────────  100%
 Progress: ───────────────────────────────────────────────────────────────  100%
 Progress: ───────────────────────────────────────────────────────────────  100%
 Progress: ───────────────────────────────────────────────────────────────  100%
 Progress: ───────────────────────────────────────────────────────────────  100%
 Progress: ───────────────────────────────────────────────────────────────  100%
 Progress: ───────────────────────────────────────────────────────────────  100%
 Progress: ──────────────────────────────────────────────────────────────── 100%

Joining, by = ""exprow""

overview: 98.938 sec elapsed

simulations ran until 2019-06-13 16:07:33
#+end_example

*** save results
#+begin_src R :eval never :results none
  saveRDS(stdvssmp_cnf, ""stdvssmp_cnf_2019-06-13.rds"")
#+end_src

#+begin_src R :results none
  stdvssmp_cnf <- readRDS(""stdvssmp_cnf_2019-06-13.rds"")
#+end_src

*** tidy it up
#+begin_src R :results none
  tidy_stdvssmp_results <- stdvssmp_cnf %>%
    gather(errortype, error, sample, etf, smp)
#+end_src

*** stdvssmp_pl
#+begin_src R :file imgs/stdvssmp.png :exports both :results output graphics
  ## create label annotation because the legend with opacity was unclear
  smpn=c(76, 44, 30)
  error=c(8, 4, 25)
  labels = c(""ETF"", ""Sample"", ""Combined"")

  leg <- tibble(
    ## x=c(58, 67, 28,
    ##     53, 62, 63),
    ## y=c(14.5, 5.5, 16,
    ##     10, 5.5, 12.5),
    x=rep(smpn, 2), y=rep(error, 2),
    xend=c(58, 67, 35,
           53, 67, 26),
    yend=c(15, 5.5, 16,
           10, 5.5, 13),
    smpt=c(rep(0, 3), rep(40, 3)),
    stdev = 25,
    ## lab=paste(labels, ""uncertainty at"", smpt, ""°C""),
    lab=rep(labels, 2),
    errortype = rep(c(""etf"", ""sample"", ""smp""), 2),
    dist=rep(""equal proportions"", 6))
                ## smpt=0)

  stdvssmp_pl <- ggplot(tidy_stdvssmp_results %>% filter(stdev == 25),
    aes(x = smpn, y = error * 1e3, fill = as.factor(smpt),
      colour = as.factor(smpt), alpha = as.factor(errortype))) +
    # draw the points for all simulations, but make them very vague
    geom_point(alpha = .05, size = .3) +
    # draw an error range through the different experiments
    stat_summary(geom = ""ribbon"", colour = NA,  # no border
      fun.data = mean_cl_normal,
      fun.args = list(conf.int = .95, na.rm = TRUE)) +
    # draw an average through the different experiments
    stat_summary(geom = ""line"", fun.data = mean_cl_normal) +
    # three standard distributions on the x-facets, 3 standard deviations on y
    ## we add some ugly labels here because they are more clear than a legend in this case
    geom_segment(aes(x=x, xend=xend, y=y, yend=yend), data=leg, size=2, show.legend=F) +
    geom_label(aes(x=x, y=y, label=lab), data=leg, colour=""black"", fill=""white"", alpha=1, show.legend=F) +
    facet_grid(#rows = vars(stdev),
      ## rows = vars(stdev),
      cols = vars(dist),
      as.table = FALSE,
      ## shrink = TRUE,
      ## scales = ""free_y"",
      ## space = ""free_y""
      ) +
    # x-axes
    scale_x_continuous(""Number of sample replicates"", lim = c(10, 90),
      breaks = seq(12, 88, 8),
      sec.axis = sec_axis(~ 100 - ., name = ""Number of standard replicates"",
        breaks = seq(88, 12, -8))) +
    scale_y_continuous(""95% CI (ppm)"",
                       ## trans=""log10"",
                       limits = c(3, 35),
                       breaks = c(seq(0, 50, 5), seq(60, 100, 10)),
                       ## sec.axis = sec_axis(~. / abs(tempcal_derivative(0) * 1e3), name = ""Approximate error (°C)"",
                                           ## breaks=c(seq(0, 20, 1), seq(25, 40, 5)))
                       ) +
    ## coord_trans(y = ""log10"") +
    # colours
    scale_colour_manual(ktit_smpid, labels = klab_smpid, values = kcols) +
    scale_fill_manual(ktit_smpid, labels = klab_smpid, values = kcols) +
    scale_alpha_manual(""Source of Error"",
      labels = c(""ETF"", ""Sample"", ""Combined""),
      values = c(.5, .2, .9), guide=FALSE) +
    # theming
    ## annotation_logticks(sides=""l"") +
    theme(legend.position = c(.85, .8), strip.text=element_text(size=8), strip.placement=""outside"")
   stdvssmp_pl
#+end_src

#+RESULTS:
[[file:imgs/stdvssmp.png]]

*** supplementary figure plot with all input standard deviations
we use a new package that allows different scales on different facets
#+begin_src R
  devtools::install_github(""zeehio/facetscales"")
#+end_src

#+begin_src R :file imgs/stdvssmp_all.png :exports both :results output graphics
  stdvssmp_pl_all <- ggplot(tidy_stdvssmp_results,
    aes(x = smpn, y = error * 1e3, fill = as.factor(smpt),
      colour = as.factor(smpt), alpha = as.factor(errortype))) +
    # draw the points for all simulations, but make them very vague
    geom_point(alpha = .05, size = .3) +
    # draw an error range through the different experiments
    stat_summary(geom = ""ribbon"", colour = NA,  # no border
      fun.data = mean_cl_normal,
      fun.args = list(conf.int = .95, na.rm = TRUE)) +
    # draw an average through the different experiments
    stat_summary(geom = ""line"", fun.data = mean_cl_normal) +
    # three standard distributions on the x-facets, 3 standard deviations on y
    ## we add some ugly labels here because they are more clear than a legend in this case
    geom_segment(aes(x=x, xend=xend, y=y, yend=yend), data=leg, size=2, show.legend=F) +
    geom_label(aes(x=x, y=y, label=lab), data=leg, colour=""black"", fill=""white"", alpha=1, show.legend=F) +
    facetscales::facet_grid_sc(
      rows = vars(stdev),
      cols = vars(dist),
      as.table = FALSE,
      shrink = TRUE,
      scales = list(
        y = list(`14` = scale_y_continuous(""95% CI (ppm)"", lim = c(2, 15)),
                 `25` = scale_y_continuous(""95% CI (ppm)"", lim = c(3, 30)),
                 `50` = scale_y_continuous(""95% CI (ppm)"", lim = c(9, 55)))
      ),
      ## space = ""free_y""
      ) +
    # x-axes
    scale_x_continuous(""Number of sample replicates"", lim = c(10, 90),
      breaks = seq(12, 88, 8),
      sec.axis = sec_axis(~ 100 - ., name = ""Number of standard replicates"",
        breaks = seq(88, 12, -8))) +
    ## scale_y_continuous(,
    ##                    ## trans=""log10"",
    ##                    ## limits = c(NA, 50),
    ##                    breaks = c(seq(0, 50, 5), seq(60, 100, 10)),
    ##                    ## sec.axis = sec_axis(~. / abs(tempcal_derivative(0) * 1e3), name = ""Approximate error (°C)"",
    ##                                        ## breaks=c(seq(0, 20, 1), seq(25, 40, 5)))
    ##                    ) +
    ## coord_trans(y = ""log10"") +
    # colours
    scale_colour_manual(ktit_smpid, labels = klab_smpid, values = kcols) +
    scale_fill_manual(ktit_smpid, labels = klab_smpid, values = kcols) +
    scale_alpha_manual(""Source of Error"",
      labels = c(""ETF"", ""Sample"", ""Combined""),
      values = c(.5, .2, .9), guide=FALSE) +
    # theming
    ## annotation_logticks(sides=""l"") +
    theme(legend.position = c(.85, .85), strip.text=element_text(size=8), strip.placement=""outside"")
   stdvssmp_pl_all
#+end_src

#+RESULTS:
[[file:imgs/stdvssmp_all.png]]

** prop-eth3
In the discussion we create a new set of simulations.
*** prop-eth3 for continuous sample range
#+begin_src R :results none
  new_smp_info2 <- tibble(smpid = ""smp"", smp_D47 = seq(0.18, 0.9, 0.0025)) %>%
    mutate(smp_D47.noacid = smp_D47 - kaff,
           rawcat = smp_D47.noacid * kslope + kintercept,
           smpt = revcal(smp_D47, ignorecnf = TRUE))
  prop_eth3 <- seq(.02, .98, length.out = 500)
#+end_src

*** prop-eth3 expand experimental matrices and run simulations
#+begin_src R :results none
  mat <- expand.grid(smp_D47 = new_smp_info2$smp_D47, prop_eth3 = prop_eth3) %>%
    left_join(new_smp_info2, by = ""smp_D47"") %>%
    mutate(prop_left = 1 - prop_eth3,
           exprow = as.character(seq_along(1:n()))) %>%
    mutate(stdfreqs = select(., prop_eth3, prop_left) %>% as.matrix() %>% split(seq(nrow(.)))) %>%
    select(-prop_left)
#+end_src

**** run many sims
#+begin_src R :eval never
  smp_out <- furrr::future_pmap_dfr(
                      select(mat, stdfreqs, smpt),
                      sim_stds,
                      ## here we subset the standards to ETH-3 and ETH-1, in that order
                      stdtable = make_std_table()[c(3, 1), ],
                      stdev = 25, out = ""cis"", .id = ""exprow"", .progress = TRUE) %>%
    filter(id %in% c(""etf"",""sample"", ""smp"")) %>%   # filter output
    select(exprow, id, cv) %>%                     # select output
    spread(id, cv) %>%                             # make it wide format
    right_join(mat, by = ""exprow"") %>% # join it with experimental df
    mutate(exp=""ETH-1 and ETH-3"")
#+end_src

**** save results
#+begin_src R :eval never :results none
  saveRDS(smp_out, ""smp_out_new_2019-06-12.rds"")
#+end_src

#+begin_src R :results none
  smp_out <- readRDS(""smp_out_new_2019-06-12.rds"")
#+end_src

*** smp_out_uu
#+begin_src R :eval never
  smp_out_uu <-
    furrr::future_pmap_dfr(
             select(mat, stdfreqs, smpt),
             sim_stds,
             # here we subset the standards to ETH-3 and ETH-1, in that order
             stdtable = make_std_table()[c(5, 1), ],
             stdev = 25, out = ""cis"", .id = ""exprow"", .progress = TRUE) %>%
    filter(id %in% c(""etf"",""sample"", ""smp"")) %>%   # filter output
    select(exprow, id, cv) %>%                     # select output
    spread(id, cv) %>%                             # make it wide format
    right_join(mat, by = ""exprow"") %>% # join it with experimental df
    mutate(exp=""ETH-1 and UU1"")
#+end_src

#+begin_src R :eval never :results none
  saveRDS(smp_out_uu, ""smp_out_uu_2019-06-12.rds"")
#+end_src

#+begin_src R :results none
  smp_out_uu <- readRDS(""smp_out_uu_2019-06-12.rds"")
#+end_src

*** best_dat
#+begin_src R :results none
  best_range <- 1:10

  best_dat <- bind_rows(smp_out, smp_out_uu) %>%
    group_by(exp, smp_D47) %>%
    arrange(smp) %>%
    slice(best_range)
#+end_src

*** smp_out_comb
combine the sims for one plot with faceting
#+begin_src R :results none
  smp_out_comb  <- smp_out %>%
    bind_rows(smp_out_uu) %>%
    select(exprow, smp_D47, prop_eth3, exp, smp) %>%
    spread(""exp"", ""smp"") %>%
    mutate(diff=`ETH-1 and UU1` - `ETH-1 and ETH-3`)
#+end_src

*** prop_eth3_pl
**** plot_best
#+begin_src R :exports none :results none
  plot_best <- best_dat %>%
    ggplot(aes(x = smp_D47, y = smp * 1e3, col=exp)) +
    geom_vline(xintercept=smpinfo$D47, col=kcols[1:2]) +
    ## show ALL the points? Just those for smp_out? No, distracting
    ## geom_point(shape = 16, alpha = .01, data=smp_out) +
    geom_point(shape = 0, alpha = .4) +
    geom_smooth(method=""loess"", se = FALSE, span = .3) +
    labs(col=""Selection of standards"",
         x = Sample ~ Delta[47] ~ ""(\u2030)"",
         y = ""Combined uncertainty 95% CI (ppm)"") +
    scale_colour_manual(values=eth.info$col[c(3, 5)]) +
    scale_x_continuous(expand=c(0, 0), lim=c(.18, .9),
            sec.axis = sec_axis(~ sqrt((0.0449 * 1e+6)/(. - 0.167)) - 273.15,
                            ""Sample temperature (°C)"", temp_breaks, temp_labs)) +
    ## scale_y_continuous(expand = c(0, 0), lim=c(NA, 15)) +
    annotate(""segment"",
             arrow = arrow(angle = 20, length = unit(.4, ""cm""), type = ""closed""),
             x = eth.info$D47[c(1, 3, 5)], y = c(-Inf, Inf, -Inf),
             xend = eth.info$D47[c(1, 3, 5)], yend = c(8, 10.5, 8),
             colour = eth.info$col[c(1, 3, 5)],
             alpha = 1,
             size = .4) +
    annotate(""text"", x = eth.info$D47[c(1, 3, 5)], # + c(0.03, -.02),
             y = c(8, 10, 8), label = eth.info$id[c(1, 3, 5)],
             vjust= c(-.5, .5, -.5), size = 2.5) +
    theme(legend.pos=""right"",
          # shared axis with bottom panel
          axis.title.x.bottom=element_blank(), axis.text.x.bottom=element_blank())
  ## facet_grid(cols=vars(exp))
#+end_src

**** plot_prop
#+begin_src R :exports none :results none
  plot_prop <- smp_out %>%
    ggplot(aes(x = smp_D47, y = prop_eth3, fill = smp * 1e3)) +
    geom_raster() +
    geom_smooth(aes(col = exp), se = F, method = ""loess"", span = .3, size = 1,
              data = best_dat) +
    geom_point(aes(col = exp), shape = 0, size = 1, alpha = .5,
               data = filter(best_dat, exp == ""ETH-1 and ETH-3"")) +
    labs(x = Sample ~ Delta[47] ~ ""CDES (\u2030)"",
         y = ""ETH-3 or UU1 / ETH-1"",
         fill = ""Combined 95% CI\n(ppm)"") +
    viridis::scale_fill_viridis(
               ## the rescaler works nicely, but messes up the legend a bit
               ## rescaler = function(x, to = c(0, 1), from = NULL, newmax=30) {
               ##   ifelse(x < newmax,
               ##          scales::rescale(x, to = to, from = c(min(x, na.rm = TRUE), newmax)), 1)},
               ## I'll go back to simple clipping again
               limits = c(NA, 30),
               expand=c(0, 0),
               ## oob = function(x) {x},
               option = ""magma"",
               breaks = err_breaks, #labels = err_ticks
             ) +
    scale_colour_manual(values=eth.info$col[c(3, 5)], guide=F) +
    scale_y_continuous(expand = c(0, 0),
                       breaks = c(.05, .1, .25, .5, .75, .9, .95),
                       labels = c(""5 / 90"", ""10 / 90"", ""25 / 75"", ""50 / 50"", ""75 / 25"", ""90 / 10"", ""95 / 5"")) +
    scale_x_continuous(expand = c(0, 0), lim=c(.18, .9)) +
    theme(legend.key.width = unit(.3, ""cm""),
          legend.key.height = unit(2, ""cm""),
          ## axis.line.y = element_line(arrow = arrow(length = unit(.3, ""cm""))),
          strip.placement=""outside"")
#+end_src

**** combine and print
#+begin_src R :file imgs/prop_best.png :results output graphics :output graphics :height 700 :width 500 :exports both
  prop_eth3_pl <- plot_best + plot_prop + plot_layout(nrow=2, heights=c(.2, .8))
  prop_eth3_pl
#+end_src

#+RESULTS:
[[file:imgs/prop_best.png]]


**** 3d rayshader plot
This one is not included in the manuscript or the supplementary information
pdf, but I highly recommend creating one to play around with it!
#+begin_src R :eval never :results none
  # new rayshader 3d option
  # remotes::install_github(""tylermorganwall/rayshader"")
  library(rayshader)

  ray <- smp_out %>%
    ggplot(aes(x = smp_D47, y = prop_eth3, fill = smp * 1e3)) +
    geom_raster() +
    ## geom_smooth(aes(col = exp), se = F, method = ""loess"", span = .3, size = 1,
    ##           data = best_dat) +
    ## geom_point(aes(col = exp), shape = 0, size = 1, alpha = .5,
    ##            data = filter(best_dat, exp == ""ETH-1 and ETH-3"")) +
    labs(x = Sample~Delt[[47]]""CDES (\u2030)"",
         y = ""Proportion of standards"",
         fill = ""Combined 95% CI\n(ppm)"") +
    viridis::scale_fill_viridis(
               ## the rescaler works nicely, but messes up the legend a bit
               ## rescaler = function(x, to = c(0, 1), from = NULL, newmax=30) {
               ##   ifelse(x < newmax,
               ##          scales::rescale(x, to = to, from = c(min(x, na.rm = TRUE), newmax)), 1)},
               ## I'll go back to simple clipping again
               limits = c(NA, 30),
               expand=c(0, 0),
               ## oob = function(x) {x},
               option = ""magma"",
               breaks = err_breaks, #labels = err_ticks
             ) +
    scale_y_continuous(expand = c(0, 0), breaks = c(.05, .1, .25, .5, .75, .9, .95)) +
    scale_x_continuous(expand = c(0, 0), lim=c(.18, .9),
            sec.axis = sec_axis(~ sqrt((0.0449 * 1e+6)/(. - 0.167)) - 273.15,
                            ""Sample temperature (°C)"", temp_breaks, temp_labs)) +
    theme(legend.key.width = unit(.3, ""cm""),
          legend.key.height = unit(2, ""cm""),
          strip.placement=""outside"")

  plot_gg(ray, multicore=TRUE, width=5, height=7, scale=350, raytrace=TRUE,
          sunangle=40)

  ## render_movie(""rayshader_movie_plot.mp4"")
  render_snapshot(""imgs/rayshader_snapshot.png"")
#+end_src

See the screenshot for a preview
[[file:imgs/rayshader_snapshot.png]]

*** best_prop_diff_pl
The difference plot for the supplementary information.

#+BEGIN_SRC R :results none
  best_dat_100 <- bind_rows(smp_out, smp_out_uu) %>%
    group_by(exp, smp_D47) %>%
    arrange(smp) %>%
    slice(1:100)

  best_dat_comb <- best_dat_100 %>%
    ungroup() %>%
    select(smp_D47, prop_eth3, exp, smp) %>%
    spread(""exp"", ""smp"") %>%
    mutate(diff=`ETH-1 and UU1` - `ETH-1 and ETH-3`)
#+END_SRC

#+BEGIN_SRC R :results none
  plot_best_comb <- best_dat_comb %>%
    ggplot(aes(x = smp_D47, y = diff * 1e3)) +
    geom_hline(yintercept=0) +
    annotate(""text"", x = c(.5, .5), y = c(2, -4), label = c(""ETH-3 does better"", ""UU1 does better""), size = 3) +
    geom_vline(xintercept=smpinfo$D47, col=kcols[1:2]) +
    ## show ALL the points?
    ## geom_point(shape = 16, alpha = .1, data=bind_rows(smp_out,smp_out_uu)) +
    geom_point(shape = 1, alpha = .1) +
    geom_smooth(method=""loess"", span = .3) +
    labs(col=""Selection of standards"",
         x = Sample ~ Delta[47] ~ ""(\u2030)"",
         y = ""Difference in combined 95% CI (ppm)"") +
    scale_colour_manual(values=eth.info$col[c(3, 5)]) +
    scale_x_continuous(expand=c(0, 0), lim=c(.18, .9),
                       sec.axis = sec_axis(~ sqrt((0.0449 * 1e+6)/(. - 0.167)) - 273.15,
                                           ""Sample temperature (°C)"", temp_breaks, temp_labs)) +
    scale_y_continuous(expand = c(0, 0)) +
    ## add arrows to ETH-1 and ETH-3 positions
    annotate(""segment"",
             arrow = arrow(angle = 20, length = unit(.4, ""cm""), type = ""closed""),
             x = eth.info$D47[c(1, 3, 5)], y = c(-Inf, Inf, -Inf),
             xend = eth.info$D47[c(1, 3, 5)], yend = c(-4, 2, -5),
             colour = eth.info$col[c(1, 3, 5)],
             alpha = 1,
             size = .4) +
    annotate(""text"", x = eth.info$D47[c(1, 3, 5)], # + c(0.03, -.02),
             y = c(-4, 2, -5), label = eth.info$id[c(1, 3, 5)],
             vjust= c(-.5, .5, -.5), size = 2.5) +
    theme(legend.pos=c(.2, .7),
          ## shared axis with bottom panel
          axis.title.x.bottom=element_blank(), axis.text.x.bottom=element_blank())
#+END_SRC

#+BEGIN_SRC R :results none
  plot_prop_diff <- smp_out_comb %>%
    ggplot(aes(x = smp_D47, y = prop_eth3, fill = diff * 1e3)) +
    geom_raster() +
    geom_smooth(aes(col = exp, fill=smp*1e3), se = F, method = ""loess"", span = .3, size = 1,
                data = best_dat_100) +
    ## geom_point(aes(col=exp), data=best_dat_100, shape = 1) +
    scale_colour_manual(values=eth.info$col[c(3, 5)]) +
    scale_fill_gradient2(low=eth.info$col[[5]], high=eth.info$col[[3]], breaks = err_breaks,
                         limits=c(-5, 5)) +
    labs(x = Sample ~ Delta[47] ~ ""(\u2030)"",
         y = ""Proportion of standards"",
         col = ""Selection of standards"",
         fill = ""Difference in\ncombined 95% CI\n(ppm)"") +
    scale_y_continuous(expand = c(0, 0), breaks = c(.05, .1, .25, .5, .75, .9, .95)) +
    scale_x_continuous(expand = c(0, 0), lim=c(.18, .9)) +
    theme(legend.key.width = unit(.1, ""cm""),
          legend.key.height = unit(1.5, ""cm""),
          strip.placement=""outside"")
#+END_SRC


#+BEGIN_SRC R :file imgs/prop_diff.png :results output graphics :height 700 :width 500 :exports both
   best_prop_diff_pl <- plot_best_comb + plot_prop_diff + plot_layout(nrow=2, heights=c(.2, .8))
   best_prop_diff_pl
#+end_src

#+RESULTS:
[[file:imgs/prop_diff.png]]

*** COMMENT rayshader?
It's too confusing with peaks and troughs right next to each other.
#+begin_src R :eval never
  ray_diff <- smp_out_comb %>%
    ggplot(aes(x = smp_D47, y = prop_eth3, fill = diff * 1e3)) +
    geom_raster() +
    scale_fill_gradient2(low=eth.info$co[[5]], high=eth.info$co[[3]] breaks = err_breaks,
                         limits=c(-5, 5)) +
    labs(x = Sample~Delt[[47]]""(\u2030)"",
         y = ""Proportion of standards"",
         ## col = ""Selection of standards"",
         fill = ""Difference in\ncombined 95% CI\n(ppm)"") +
    scale_y_continuous(expand = c(0, 0), breaks = c(.05, .1, .25, .5, .75, .9, .95)) +
    scale_x_continuous(expand = c(0, 0), lim=c(.18, .9)) +
    theme(legend.key.width = unit(.1, ""cm""),
          legend.key.height = unit(1.5, ""cm""),
          strip.placement=""outside"")

  plot_gg(ray_diff, multicore=TRUE, width=5, height=7, scale=350)
#+end_src

#+RESULTS:
: org_babel_R_eoe

*** calculate some summary statistics for use in-text
#+begin_src R :results none
  # these are some helper functions to calculate the values we put in the text
  # get the confidence value at alpha value alpha from a t-distribution
  confidence <- function(x, n, alpha=.05) {
    qt(1 - alpha / 2, df=n - 1) * sd(x) / sqrt(n)
  }

  # convert x from permil to ppm and round down to dig digits.
  ppmround <- function(x, dig=2) {
    round(x * 1e3, digits=dig)
  }

  interp_eth3 <- best_dat %>%
    ungroup() %>%
    filter(exp==""ETH-1 and ETH-3"", smp_D47 >= eth.info$D47[[1]], smp_D47 <= eth.info$D47[[5]]) %>%
    summarize(mean=mean(smp),
              ci=confidence(smp, n=n()))

  extrap_eth3 <- best_dat %>%
    ungroup() %>%
    filter(exp==""ETH-1 and ETH-3"", smp_D47 >= eth.info$D47[[5]]) %>%
    summarize(mean=mean(smp),
              ci=confidence(smp, n=n()))

  interp_uu <- best_dat %>%
    ungroup() %>%
    filter(exp==""ETH-1 and UU1"", smp_D47 >= eth.info$D47[[1]], smp_D47 <= eth.info$D47[[5]]) %>%
    summarize(mean=mean(smp),
              ci=confidence(smp, n=n()))

  extrap_uu <- best_dat %>%
    ungroup() %>%
    filter(exp==""ETH-1 and UU1"", smp_D47 >= eth.info$D47[[5]]) %>%
    summarize(mean=mean(smp),
              ci=confidence(smp, n=n()))

  eth3_to_uu_eth3 <- best_dat %>%
    ungroup() %>%
    filter(exp==""ETH-1 and ETH-3"", smp_D47 >= eth.info$D47[[3]], smp_D47 <= eth.info$D47[[5]]) %>%
    summarize(mean=mean(smp),
              ci=confidence(smp, n=n()))

  eth3_to_uu_uu <- best_dat %>%
    ungroup() %>%
    filter(exp == ""ETH-1 and UU1"", smp_D47 >= eth.info$D47[[3]], smp_D47 <= eth.info$D47[[5]]) %>%
    summarize(mean=mean(smp, na.rm = TRUE),
              ci=confidence(smp, n=n()))

  eth3_to_uu_tempsens <- seq(eth.info$D47[[3]], eth.info$D47[[5]], .01) %>%
    tempcal_derivative() %>%
    mean()

  uu_to_0_eth3 <- best_dat %>%
    ungroup() %>%
    filter(exp==""ETH-1 and ETH-3"", smp_D47 >= eth.info$D47[[5]], smp_D47 <= smpinfo$D47[[1]]) %>%
    summarize(mean=mean(smp),
              ci=confidence(smp, n=n()))

  uu_to_0_uu <- best_dat %>%
    ungroup() %>%
    filter(exp==""ETH-1 and UU1"", smp_D47 >= eth.info$D47[[5]], smp_D47 <= smpinfo$D47[[1]]) %>%
    summarize(mean=mean(smp),
              ci=confidence(smp, n=n()))

  uu_to_0_tempsens <- seq(eth.info$D47[[5]], smpinfo$D47[[1]], .01) %>%
    tempcal_derivative() %>%
    mean()
#+end_src
** prop-eth3 with a very very very cold and hot standard?
:LOGBOOK:
- State ""SOME""       from              [2019-08-19 Mon 16:58]
:END:
The reviewers requested another set of simulations.
*** prop-eth3 for continuous sample range
same as before
#+begin_src R :results none :eval never
  new_smp_info2 <- tibble(smpid = ""smp"", smp_D47 = seq(0.18, 0.9, 0.0025)) %>%
    mutate(smp_D47.noacid = smp_D47 - kaff,
           rawcat = smp_D47.noacid * kslope + kintercept,
           smpt = revcal(smp_D47, ignorecnf = TRUE))
  prop_eth3 <- seq(.02, .98, length.out = 500)
#+end_src

*** prop-eth3 expand experimental matrices and run simulations
same as before
#+begin_src R :results none :eval never
  mat <- expand.grid(smp_D47 = new_smp_info2$smp_D47, prop_eth3 = prop_eth3) %>%
    left_join(new_smp_info2, by = ""smp_D47"") %>%
    mutate(prop_left = 1 - prop_eth3,
           exprow = as.character(seq_along(1:n()))) %>%
    mutate(stdfreqs = select(., prop_eth3, prop_left) %>% as.matrix() %>% split(seq(nrow(.)))) %>%
    select(-prop_left)
#+end_src

*** run many sims
#+begin_src R :results output :eval never
  brrr <- make_std_table(id = c(""UU2"", ""UU3""), col = c(""darkblue"", ""red""),
                 D47_std = c(0.9252, 0.0266)) # based on Wang 2004

  brrr_smp_out <- furrr::future_pmap_dfr(
                      select(mat, stdfreqs, smpt),
                      sim_stds,
                      ## here we subset the standards to ETH-3 and ETH-1, in that order
                      stdtable = brrr,
                      stdev = 25, out = ""cis"", .id = ""exprow"", .progress = TRUE) %>%
    filter(id %in% c(""etf"",""sample"", ""smp"")) %>%   # filter output
    select(exprow, id, cv) %>%                     # select output
    spread(id, cv) %>%                             # make it wide format
    right_join(mat, by = ""exprow"") %>% # join it with experimental df
    mutate(exp=""UU3 and UU2"")
#+end_src

**** save results
# the heated/eq. gases
#+begin_src R :eval never :results none
  saveRDS(brrr_smp_out, ""brrr_smp_out_2019-08-20.rds"")
#+end_src

the 0.8 U2 standard + ETH-1
#+begin_src R :results none
  brrr_smp_out <- readRDS(""brrr_smp_out_2019-08-19.rds"")
#+end_src

The heated/eq. gas equivalents
#+begin_src R :results none
  brrr_smp_out <- readRDS(""brrr_smp_out_2019-08-20.rds"")
#+end_src

*** brrr_best_dat
#+begin_src R :results none
  best_range <- 1:10

  brrr_best_dat <- bind_rows(smp_out, smp_out_uu, brrr_smp_out) %>%
    group_by(exp, smp_D47) %>%
    arrange(smp) %>%
    slice(best_range)
#+end_src

*** smp_out_comb
combine the sims for one plot with faceting
#+begin_src R :results none
  brrr_smp_out_comb  <- bind_rows(smp_out, brrr_smp_out, smp_out_uu) %>%
    select(exprow, smp_D47, prop_eth3, exp, smp) %>%
    spread(""exp"", ""smp"") %>%
    mutate(diff=`UU3 and UU2` - `ETH-1 and ETH-3`)
#+end_src

*** prop_eth3_pl
**** plot_best
#+begin_src R :exports none :results none
  scman <- c(eth.info$col[[3]], eth.info$col[[5]], ""skyblue"") #brrr$col[[1]])
  brrr_plot_best <- brrr_best_dat %>%
    ggplot(aes(x = smp_D47, y = smp * 1e3, col = exp)) +
    geom_vline(xintercept = smpinfo$D47, col = kcols[1:2]) +
    ## show ALL the points? Just those for smp_out? No, distracting
    ## geom_point(shape = 16, alpha = .01, data=smp_out) +
    geom_point(shape = 0, alpha = .1) +
    geom_smooth(method=""loess"", se = FALSE, span = .3) +
    labs(col=""Selection of standards"",
         x = Sample ~ Delta[47] ~ ""(\u2030)"",
         y = ""Combined 95% CI (ppm)"") +
    scale_colour_manual(values = scman) +
    scale_x_continuous(expand=c(0, 0), lim=c(.18, .9), minor_breaks = seq(0.1, 1, 0.05),
            sec.axis = sec_axis(~ sqrt((0.0449 * 1e+6)/(. - 0.167)) - 273.15,
                            ""Sample temperature (°C)"", temp_breaks, temp_labs)) +
    scale_y_continuous(expand = c(0, 0), lim=c(NA, 15)) +
    annotate(""segment"",
             arrow = arrow(angle = 20, length = unit(.4, ""cm""), type = ""closed""),
             x = c(eth.info$D47[c(1, 3, 5)], brrr$D47),
             y = c(-Inf, Inf, -Inf, -Inf, -Inf),
             xend = c(eth.info$D47[c(1, 3, 5)], brrr$D47),
             yend = c(8, 10.5, 8, 8, 8),
             colour = c(eth.info$col[c(1, 3, 5)], brrr$col),
             alpha = 1,
             size = .4) +
    annotate(""text"",
             x = c(eth.info$D47[c(1, 3, 5)], brrr$D47), # + c(0.03, -.02),
             y = c(8, 10, 8, 8, 8),
             label = c(eth.info$id[c(1, 3, 5)], brrr$id),
             vjust= c(2.5, .5, 2.5, -.5, -.5), size = 2.5) +
    theme(legend.pos=""right"",
          # shared axis with bottom panel
          axis.title.x.bottom=element_blank(), axis.text.x.bottom=element_blank())
  ## facet_grid(cols=vars(exp))
#+end_src

**** plot_prop
#+begin_src R :exports none :results none
  brrr_plot_prop <- smp_out %>%
    ggplot(aes(x = smp_D47, y = prop_eth3, fill = smp * 1e3)) +
    geom_raster() +
    ## geom_smooth(aes(col = exp), se = F, method = ""loess"", span = .3, size = 1,
    ##           data = best_dat) +
    geom_smooth(aes(col = exp), se = F, method = ""loess"", span = .3, size = 1,
              data = brrr_best_dat) +
    geom_point(aes(col = exp), shape = 0, size = 1, alpha = .5,
               data = filter(best_dat, exp == ""ETH-1 and ETH-3"")) +
    labs(x = Sample~Delta[47] ~ ""CDES (\u2030)"",
         y = ""ETH-3 or UU1 / ETH-1"",
         fill = ""Combined 95% CI\n(ppm)"") +
    viridis::scale_fill_viridis(
               ## the rescaler works nicely, but messes up the legend a bit
               ## rescaler = function(x, to = c(0, 1), from = NULL, newmax=30) {
               ##   ifelse(x < newmax,
               ##          scales::rescale(x, to = to, from = c(min(x, na.rm = TRUE), newmax)), 1)},
               ## I'll go back to simple clipping again
               limits = c(NA, 30),
               expand=c(0, 0),
               ## oob = function(x) {x},
               option = ""magma"",
               breaks = err_breaks, #labels = err_ticks
             ) +
    scale_colour_manual(values = scman, guide = FALSE) +
    scale_y_continuous(expand = c(0, 0),
                       breaks = c(.05, .1, .25, .5, .75, .9, .95),
                       labels = c(""5 / 90"", ""10 / 90"", ""25 / 75"", ""50 / 50"", ""75 / 25"", ""90 / 10"", ""95 / 5"")) +
    scale_x_continuous(expand = c(0, 0), lim=c(.18, .9), minor_breaks = seq(0.1, 1, 0.05)) +
    theme(legend.key.width = unit(.3, ""cm""),
          legend.key.height = unit(2, ""cm""),
          ## axis.line.y = element_line(arrow = arrow(length = unit(.3, ""cm""))),
          strip.placement=""outside"")
#+end_src

**** combine and print
#+begin_src R :file imgs/brrr_prop_best.png :results output graphics :output graphics :height 700 :width 600 :exports both
  brrr_prop_eth3_pl <- brrr_plot_best + brrr_plot_prop + plot_layout(nrow=2, heights=c(.2, .8))
  brrr_prop_eth3_pl
#+end_src

#+RESULTS:
[[file:imgs/brrr_prop_best.png]]

*** calculate some summary statistics for use in-text
#+begin_src R :results none
  eth3_to_u2_eth3 <- brrr_best_dat %>%
    ungroup() %>%
    filter(exp==""ETH-1 and ETH-3"", smp_D47 >= eth.info$D47[[3]], smp_D47 <= brrr$D47[[1]]) %>%
    summarize(mean=mean(smp),
              ci=confidence(smp, n=n()))

  eth3_to_u2_u2 <- brrr_best_dat %>%
    ungroup() %>%
    filter(exp==""UU3 and UU2"", smp_D47 >= eth.info$D47[[3]], smp_D47 <= brrr$D47[[1]]) %>%
    summarize(mean=mean(smp),
              ci=confidence(smp, n=n()))

  eth3_to_u2_tempsens <- seq(eth.info$D47[[3]], brrr$D47[[1]], .001) %>%
    tempcal_derivative() %>%
    mean(na.rm=FALSE)

  u2_to_0_eth3 <- best_dat %>%
    ungroup() %>%
    filter(exp==""ETH-1 and ETH-3"", smp_D47 >= smpinfo$D47[[1]], smp_D47 <= brrr$D47[[1]]) %>%
    summarize(mean=mean(smp),
              ci=confidence(smp, n=n()))

  u2_to_0_u2 <- brrr_best_dat %>%
    ungroup() %>%
    filter(exp==""UU3 and UU2"", smp_D47 >= smpinfo$D47[[1]], smp_D47 <= brrr$D47[[1]]) %>%
    summarize(mean=mean(smp),
              ci=confidence(smp, n=n()))

  u2_to_0_tempsens <- seq(smpinfo$D47[[1]], brrr$D47[[1]], .01) %>%
    tempcal_derivative() %>%
    mean()

  eth3_to_0_eth3 <- brrr_best_dat %>%
    ungroup() %>%
    filter(exp==""ETH-1 and ETH-3"", smp_D47 >= eth.info$D47[[3]], smp_D47 <= smpinfo$D47[[1]]) %>%
    summarize(mean=mean(smp),
              ci=confidence(smp, n=n()))

  eth3_to_0_u2 <- brrr_best_dat %>%
    ungroup() %>%
    filter(exp==""UU3 and UU2"", smp_D47 >= eth.info$D47[[3]], smp_D47 <= smpinfo$D47[[1]]) %>%
    summarize(mean=mean(smp),
              ci=confidence(smp, n=n()))

  eth3_to_0_tempsens <- seq(eth.info$D47[[3]], smpinfo$D47[[1]], .01) %>%
    tempcal_derivative() %>%
    mean()
#+end_src

*** quick check on how much it matters if we add two hypothetical very large-range standards
#+begin_src R
 (eth3_to_0_eth3$mean - eth3_to_0_u2$mean) * 1000
#+end_src

#+RESULTS:
: 1.02024401281854

ppm difference, which equates to
#+begin_src R
   # in permil             # in permil          # in permil / degreeC
  (eth3_to_0_eth3$mean - eth3_to_0_u2$mean) / eth3_to_0_tempsens
#+end_src

#+RESULTS:
: -0.233393968655522

so… .3 \celsius improvement for samples between src_R{brrr$temp[[1]]} {{{results(\(-29.8000392923437\))}}}

maybe it's even more at the more extreme end?

#+begin_src R
 (eth3_to_0_eth3$mean - eth3_to_0_u2$mean) / eth3_to_0_tempsens
#+end_src

#+RESULTS:
: -0.233393968655522

degrees improvement for samples between ETH-3 (~20) and 0 \celsius.

or in terms of improvement:
#+begin_src R
  ((eth3_to_0_eth3$mean / eth3_to_0_u2$mean) - 1) * 100
#+end_src

#+RESULTS:
: 11.8018282962414
",2022-08-12
https://github.com/japhir/stratPlot,"WARNING: this package has not been updated in quite a while, since I have 
switched to using ggplot. The still-relevant features from this packages will
at some point be migrated to ggplot.


# StratPlot
The StratPlot function allows you to easily create depth and age profile plots
in R. 

The files that you need are `StratPlot.R`, if you're plotting Magnetochron ages 
`Chronages.csv`, and if you're plotting time GTS2012 time scales `GTS_colours.csv`.

## Usage

Use your spreadsheat editing skills (or R) to create a .csv that holds one
sample per row with the variables of interest as separate columns. Make sure to
include depth or age information in one of the columns. 

Get it into R using `data <- read.csv(""pathtofile.csv"")`

The simplest way to create a single plot is to give depth/age and var vectors of
equal length, and provide a value for `xlab`. 

Note that if you want to use superscipts and subscripts in the x-axis label, you
can do so by providing `xlab` with a formula. For example:
`Label~with~nice~formatting~(H[2]*O~m^{-2}~mu*M)` will result in: Label with
nice formatting (H<sub>2</sub>O m<sup>-2</sup> μM).

You can also provide `var` with a matrix or dataframe with the variables of
interst. The function tries to automatically find a column with `depth` or `age`
information and use that to plot the other variables. You can also specify it
with `depthcol`. They can be added to a single plot with `oneplot = TRUE`, can
be stacked with `stacked = TRUE` (useful for cumulative sum plots) or each
create a new plot (default). The function also tries to figure out whether it
should use ""Age (Ma)"" or ""Age (ka)"" based on the age variable.

When you create multiple plots, make sure there is room for the plots (via
`par(mfrow = c(1,5))` for example). `xlab` and `ylab` can now also be entered as
vector of characters or a list of formulae. Otherwise it will use the variable
names in the dataframe.

Setting the `pol` and `bar` to `TRUE` adds a polygon and/or bar respectively.

Other configurations are also possible, just look at the function arguments and
see what they do :).

## Example calls

```R
# dinocyst data example
set.seed(1)
dinos <- data.frame(code = paste0(""IJK"", 1:10),
                    depth = seq(600, 800, length.out = 10),
	            age = 41:50,
                    Dinospecies1 = rnorm(10, 5, 3),
                    Dinospecies2 = rnorm(10, 10, 5),
                    Dinospecies3 = rnorm(10, 25, 20))
par(mfrow = c(1, 3))
StratPlot(dinos, pol = T, bar = T, xlim = c(0, 60))

# data with known error values
set.seed(1)
temp <- data.frame(age = 41:50, 
	           temp = rnorm(10, 30, 5),
		   error = rnorm(10, 2.5, 1))
StratPlot(temp$age, temp$temp, xlab = Temperature~(degree~C), 
          ylab = ""Age (Ma)"", error = temp$error)
```

## example plot

The supplementary age-model plot to Cramwinckel et al. 2018<sup>[1](https://www.nature.com/articles/s41586-018-0272-2)</sup>
was created using (slightly different versions of) these plotting functions 
(as well as some tweaking of colours in Inkscape).

![Age Model](https://media.springernature.com/lw900/springer-static/esm/art%3A10.1038%2Fs41586-018-0272-2/MediaObjects/41586_2018_272_Fig5_ESM.jpg)

1. Cramwinckel et al. Synchronous tropical and polar temperature evolution in the Eocene. 
   Nature volume 559, pages382–386 (2018) [doi:10.1038/s41586-018-0272-2](https://doi.org/10.1038/s41586-018-0272-2)
",2022-08-12
https://github.com/jbikker/advgrtmpl8,"This template is intended for students of Utrecht University.

**Please refer to ""_ getting started.pdf"" for instructions.**

Code is fully public domain. Use as you please.

Contact me at bikker.j@gmail.com.",2022-08-12
https://github.com/jbikker/AndroidTmpl8,"# AndroidTmpl8
Template for C/C++ Android application development.

# What It Is
This template provides you with a convenient and lean starting point for C/C++ development on Android. The template code compiles as-is in Android Studio 3.6.3. It runs some basic OpenGLES code, and plays a sound file using the included SoLoud library. The included 'howto' text file explains how to quickly add additional source files and assets, how to modify the application icon and how to rename the project: all operations that can easily become major obstacles for a fun Android side project.

# Plus Windows
The template includes a Visual Studio project that compiles the same template source files, but for Windows. This lets you develop right on your desktop, without the need for an emulator, enabling the full debugging capabilities of Visual Studio. This significantly simplifies your development cycle and limits your exposure to Android Studio. Which is a good thing.

# Advanced
The current version of the template already starts a properly initialized full-screen OpenGLES native activity. You also get access to the pen position for basic controls. PNG images can be loaded straight from the apk, as if they are in the main application directory. SoLoud is used to playback audio. You get convenient file access for audio assets and other application data.

# What It Wants To Be
There are other things that are trivial on a desktop machine, but pretty hard on Android, such as accessing the file system and the camera. These will be tackled in later revisions of the template. The idea is to tackle these once and for all, so you don't have to scour the internet for code snippets.

# What You Need
Just install Android Studio 3.6.3 and Visual Studio 2019 (community edition will do). Then, open the template folder in Android Studio to compile for Android, or open vs2019/Tmpl8win.sln in Visual Studio for development. Read howto.txt for advice on common actions.

Jacco Bikker, May 2020
",2022-08-12
https://github.com/jbikker/bvh_article,"Code for ""How to Build a BVH""<br><br>

<b>INSTRUCTIONS:</b> Open the ""_ bvhdemo.sln"" file (all the way at the top!), and make one of the 10 projects the active project. 

<b>Note:</b> project files are for Visual Studio 2019; they will convert without issues to 2022 though.

Each project implements one of the 10 articles:

<b>part 1, basics:</b><br>
https://jacco.ompf2.com/2022/04/13/how-to-build-a-bvh-part-1-basics<br>
<i>...Which explains how to set up a minimal but working BVH in about 300 lines of code.</i><br>
Project: basics.vcxproj, files: basics.cpp, basics.h<br><br>

<b>part 2, faster rays:</b><br>
https://jacco.ompf2.com/2022/04/18/how-to-build-a-bvh-part-2-faster-rays<br>
<i>...in which rays are traced faster, at the expense of BVH build time.</i><br>
Project: faster.vcxproj, files: faster.cpp, faster.h<br><br>

<b>part 3, quick builds:</b><br>
https://jacco.ompf2.com/2022/04/21/how-to-build-a-bvh-part-3-quick-builds<br>
<i>...explains how a high-quality BVH can be constructed rapidly.</i><br>
Project: quickbuild.vcxproj, files: quickbuild.cpp, quickbuild.h<br><br>

<b>part 4, animation:</b><br>
https://jacco.ompf2.com/2022/04/26/how-to-build-a-bvh-part-4-animation<br>
<i>...in which various forms of animation are applied to BVHs.</i><br>
Project: animation.vcxproj, files: animation.cpp, animation.h<br><br>

<b>part 5, TLAS & BLAS:</b><br>
https://jacco.ompf2.com/2022/05/07/how-to-build-a-bvh-part-5-tlas-blas<br>
<i>...describes how a lot of BVHs can be made into one.</i><br>
Project: toplevel.vcxproj, files: toplevel.cpp, toplevel.h<br><br>

<b>part 6, TLAS & BLAS part 2:</b><br>
https://jacco.ompf2.com/2022/05/13/how-to-build-a-bvh-part-6-all-together-now<br>
<i>...which completes the discussion of article 5.</i><br>
Project: alltogether.vcxproj, files: alltogether.cpp, alltogether.h<br><br>

<b>part 7, consolidation:</b><br>
https://jacco.ompf2.com/2022/05/20/how-to-build-a-bvh-part-7-consolidate<br>
<i>...in which the BVH is applied to some nicer renders.</i><br>
Project: pretty.vcxproj, files: pretty.cpp, pretty.h, bvh.cpp, bvh.h<br><br>

<b>part 8, whitted:</b><br>
https://jacco.ompf2.com/2022/05/27/how-to-build-a-bvh-part-8-whitted-style<br>
<i>...completing part 7, with full recursive ray tracing, in real-time.</i><br>
Project: whitted.vcxproj, files: whitted.cpp, whitted.h, bvh.*<br><br>

<b>part 9a, GPGPU:</b><br>
https://jacco.ompf2.com/2022/06/03/how-to-build-a-bvh-part-9a-to-the-gpu<br>
<i>...which is really a GPGPU tutorial, using BVHs as an example.</i><br>
Project: gpgpu.vcxproj, files: gpgpu.cpp, gpgpu.h, bvh.*<br><br>

<b>part 9b, 'massive':</b><br>
https://jacco.ompf2.com/2022/06/15/how-to-build-a-bvh-part-9b-massive<br>
<i>...series finale, with TLAS & BLAS on the GPU. Also: GL/CL interop.</i><br>
Project: massive.vcxproj, files: massive.cpp, massive.h, raytracer.cl.<br><br>

NOTE: All projects share the same template files and build directories.<br>
DISCLAIMER: None of this is supposed to be 'production quality'.<br>
LICENSE: This code is covered by the Unlicense. Feel free, no strings.<br><br>
P.S.: Follow me on Twitter, <a href=""https://twitter.com/j_bikker"">@j_bikker</a>.
",2022-08-12
https://github.com/jbikker/lighthouse2,"# lighthouse2
Lighthouse 2 framework for real-time ray tracing

This is the public repo for Lighthouse 2, a rendering framework for real-time ray tracing / path tracing experiments. 
Lighthouse 2 uses a state-of-the-art wavefront / streaming ray tracing implementation to reach high ray througput on RTX hardware 
(using Optix 7.2) and pre-RTX hardware (using Optix 5 Prime) and soon on AMD hardware (using RadeonRays / OpenCL) and CPUs (using Embree).
A software rasterizer is also included, mostly as an example of a minimal API implementation.

![ScreenShot](/screenshots/lighthouse_cobra.png)

Quick pointers / Important advice:

* Building Lighthouse 2: Since February 2020, Lighthouse requires Visual Studio 2019. The CUDA-based cores require CUDA 11.6. <b>NOTE: CUDA version 11.7 does <u>not</u> seem to work at the moment!</b>
* Lighthouse 2 wiki: https://github.com/jbikker/lighthouse2/wiki (early stages)
* Trouble shooting page on the wiki: https://github.com/jbikker/lighthouse2/wiki/TroubleShooting
* Lighthouse 2 forum: https://ompf2.com/viewforum.php?f=18
* Follow the project on Twitter: @j_bikker

Lighthouse 2 uses a highly modular approach to ease the development of renderers.

The main layers are:

1. The application layer, which implements application logic and handles user input;
2. The RenderSystem, which handles scene I/O and host-side scene storage;
3. The render cores, which implement low-level rendering functionality.

Render cores have a common interface and are supplied to the RenderSystem as dlls. The RenderSystem supplies the cores with scene data 
(meshes, instances, triangles, textures, materials, lights) and sparse updates to this data.

The Lighthouse 2 project has the following target audience:

*Researchers*

Lighthouse 2 is designed to be a high-performance starting point for novel algorithms involving real-time ray tracing. This may include
new work on filtering, sampling, materials and lights. The provided ray tracers easily reach hundreds of millions of rays per second 
on NVidia and AMD GPUs. Combined with a generic GPGPU implementation, this enables a high level of freedom in the implementation of 
new code.

*Educators*

The Lighthouse 2 system implements all the boring things such as scene I/O, window management, user interfaces and access to ray tracing
APIs such as Optix, RadeonRays and Embree; your students can get straight to the interesting bits. The architecture of Lighthouse 2 is
carefully designed to be easily accessible. Very fast scene loading and carefully tuned project files ensure quick development cycles.

*Industry*

Lighthouse 2 is an R&D platform. It is however distributed with the Apache 2.0 license, which allows you to use the code in your
own products. Experimental cores can be shared with the community in binary / closed form, and application development is separated
from core development.

<b>What it is not</b>

The ray tracing infrastructure (with related scene management acceleration structure maintenance) should be close to optimal. 
The implemented estimators however (unidirectional path tracers without filtering and blue noise) are not, and neither is the shading
model (Lambert + speculars). This may or may not change depending on the use cases encountered. This video shows what can be
achieved with the platform: https://youtu.be/uEDTtu2ky3o .

Lighthouse 2 should compile out-of-the-box on Windows using Visual Studio 2019. For the CUDA/Optix based cores CUDA 11.6 is required:

https://developer.nvidia.com/cuda-downloads

Make sure to chose the correct version; 11.7 (the latest version as of May 2022) does not work.

Optix 5.x, 6.0 and 7.2 libraries are included in the Lighthouse 2 download and do not have to be downloaded separately.

For more information on Lighthouse 2 please visit: http://jacco.ompf2.com.

<b>Credits</b>

Lighthouse 2 was developed at the Utrecht University, The Netherlands.

Lighthouse 2 uses the following libraries:<br>
Dear ImGui https://github.com/ocornut/imgui<br>
FreeImage http://freeimage.sourceforge.net<br>
Glad https://glad.dav1d.de<br>
GLFW https://www.glfw.org<br>
half 1.12 http://half.sourceforge.net<br>
tinygltf https://github.com/syoyo/tinygltf<br>
tinyobj https://github.com/syoyo/tinyobjloader<br>
tinyxml2 https://github.com/leethomason/tinyxml2<br>
zlib https://www.zlib.net

<b>Contributions</b>

* The Lighthouse2 Vulkan core (and sharedBSDF) was developed by Mèir Noordermeer (https://github.com/MeirBon).
* A Linux port by Marijn Suijten (https://github.com/MarijnS95) is being incorporated in the main repo.
* Animation code uses low-level optimizations by Alysha Bogaers and Naraenda Prasetya.
* OptixPrime_BDPT core by Guowei (Peter) Lu (https://github.com/pasu).

<b>Previous Work</b>

Lighthouse 2 implements research by (very incomplete):

* Marsaglia: random numbers
* Van Antwerpen, Laine, Karras, Aila: streaming path tracing
* Aila, Laine: persistent kernels
* Schied et al.: Spatiotemporal Variance-Guided Filtering (SVGF)
* Victor Voorhuis: improved SVGF for specular and glossy reprojection
* Eric Heitz: Blue noise distributions
",2022-08-12
https://github.com/jbikker/msx3d,"# msx3d

This project is to be compiled with the excellent MSXgl library by @Aoineko.
Place the source files in a directory in the projects folder and start build to produce a 48KB rom for MSX1/2.

The code is essentially a video player: 64 frames of a 3D animation are split up in 12x12 tiles, which are displayed in VDP_MODE_GRAPHIC1. 
In this mode, chars have just two colors, defined by the color map, which holds a foreground and background color for 32 groups of 8 chars.
The 144 tiles are reduced to an average of 51 unique tiles per frame, which get assigned to one of 6 'slices': a group of chars that share
the same BG/FG color. This way, updating a frame requires only 51 * 8 bytes for patterns, plus 144 bytes for the map.

Tiles are also reused between frames, resulting in about 2400 unique tiles for the entire animation. After 64 frames, red and green colors 
are swapped for the next 64 frames, to produce one full revolution.

In this animation, some tiles have three colors. In this case, the third color is drawn using a sprite. This way, the cube can be drawn
artifact-free on MSX1, using the default palette.

The playback idea is borrowed from @ARTRAG. Adding sprites is my idea, but surely has been done before.

Rapid VDP code has been borrowed from MSXgl, and optimized for this particular use case to minimize frame time.

Greets to everyone on MSX.ORG - it's good to be back doing some sweet MSX coding again. :)

# todo

Ideas to improve this further: obviously, space quickly runs out on MSX1. Tiles may be more frequently reused if a bitshift of 1 or 2 pixels
is allowed; this would hardly increase processing time. Sprites may also be reused; this is currently not attempted. Sprites should also be
combined where possible: they are not limited to the tile grid after all. To maximize the chance of combining sprites, the sprites should
replace the least used color.
",2022-08-12
https://github.com/jbikker/tanks22,"This template is intended for students of Utrecht University.

**Please refer to ""_ getting started.pdf"" for instructions.**

Code is fully public domain. Use as you please.

Contact me at bikker.j@gmail.com.",2022-08-12
https://github.com/jbikker/tmpl8,"# BRIEF INFO ON THE 2020-01 TEMPLATE

*Purpose:*

The template has been designed to make it easy to start coding C++
using games and graphics. It intends to offer the programmer a
simple library with the main purpose of providing a 32-bit graphical
window with a linear frame buffer. Some basic additional functionality
is available, such as sprites, bitmap fonts, basic multi-threading,
and vector math support.

*How to use:*

1. Copy the template folder (or extract the zip) to a fresh folder for
   your project. 
2. Open the .sln file with any version of Visual Studio 2017/2019.
3. Replace the example code in game.cpp with your own code.
You can go further by:
- Expanding the game class in game.h;
- Implementing some of the empty functions for mouse and keyboard
  handling;
- Exploring the code of the template in surface.cpp and template.cpp.

When handing in assignments based on this template, please run
clean.bat prior to zipping the folder. This deletes any intermediate
files created during compilation and reduces SUBMIT problems.

The Template is a 'quickstart' template, and not meant to be elaborate,
performant or complete. 
At some point, and depending on your requirements, you may want to
advance to a more full-fledged library, or you can expand the template
with OpenGL or SDL2 code.

*Credits*

Although the template is small and bare bones, it still uses a lot of
code gathered over the years:
- EasyCE's 5x5 bitmap font (primarily used for debugging);
- EasyCE's surface class (with lots of modifications);
- Nils Desle's JobManager for efficient multi-threading.

*Copyright*

This code is completely free to use and distribute in any form.

*Remplate Naming*

Starting January 2018, the name of the template represents the version.
This version also appears in the title of the window. Make sure you
are using the most recent version.

Utrecht, 2015-2020, Utrecht University - Breda, 2014, NHTV/IGAD - Jacco Bikker - Report problems and suggestions to bikker.j@gmail.com .

*Changelog*

v2020-01:
swapped SDL2 for GLFW.
turned to github for distribution.
added DearImgui.
added Taskflow for threading.

v2019-01:
upgraded to SDL2.0.10.
minor upgrade for INFOMOV2019.

v2019-01:
upgraded to SDL2.0.9.
upgraded to FreeImage 3.18.0.
upgraded to glew-2.1.0.
added aabb class for ADVGR.
added matrix operators for ADVGR.
made cross-platform by Marijn Suijten.

v2017-02:
added a changelog.
debug mode now also emits .exe to project folder.
removed rogue SDL2 folder.
added assert.h to precomp.h.

v2017-01: 
initial DGDARC release.
",2022-08-12
https://github.com/jbikker/WrldTmpl8,"# BRIEF INFO ON THE 2021 WORLD TEMPLATE

Project website: https://jacco.ompf2.com/voxel-world-template/

*Purpose:*

This template has been designed to make it easy to start coding C++
using games and 3D graphics. The world is 3D, but consists of a
finite amount of 'voxels' (3D pixels): exactly 1024x1024x1024 are
available to you. Programming games on this 3D 'screen' closely
resembles how we created games on homecomputers - but without the
now outdated 2D graphics.
At the same time, the template is nothing like large engines, like
Unity and Unreal. You get full control over your machine, and you
are welcome to dive deep into the template code, to change it as
you wish, once you are ready to do so.

*Performance:*

Although the engine is aimed towards basic game development, it comes
with state-of-the-art ray tracing with high performance code for
the latest GPUs as well as older ones. Up to 2 billion rays per
second in non-trivial scenes are possible on a RTX3080Ti GPU - 
but without using RTX. Even a 1080 will do well over half a billion
rays per second. Combined with TAA and global illumination, this
renders your scenes with high fidelity, even at high resolutions.

![ScreenShot](template_scrn2.png)

*Tools:*

Import models created with MagicaVoxel, including animated models.
And if it turns out to be hard to find those .vox files, just
create your own, for example by converting them from .obj files,
using the included tool.

![ScreenShot](template_scrn3.png)

*Copyright*

This code is completely free to use and distribute in any form. Build,
play and sell your game without obstacles.

Utrecht, 2015-2021, Utrecht University<br/>
Breda, 2014 and 2020-2021, NHTV/IGAD/BUAS<br/>
Jacco Bikker<br/>
Report problems and suggestions to bikker.j@gmail.com.

![ScreenShot](template_scrn0.png)


![ScreenShot](template_scrn1.png)
",2022-08-12
https://github.com/jc-oconnor/BORIS_BehaviouralSoftware,"# BORIS_BehaviouralSoftware
Files used to extract event data from BORIS without needing to reopen each project in the program.
",2022-08-12
https://github.com/jc-oconnor/GWdepthStats,"# GWdepthStats
Stats workflow used in research paper. 


This is the Original workflow used to analyse differences between ET, LST and EVI data for deep and shallow WTD classifications

ET,LST, and EVI are subsets from MODIS datasets

WTD data originates from Fan & Miguez Macho dataset for South America. The data were subset into two broad/basic classifications of Depp >10m and Shallow <2m

The data were then compared using wilcoxon rank sum for diffferences in distribution. 
10th and 90th quantiles were also compared using a similar method proposed by Wilcox (2012)  

We randomly selected 1000 deep and 1000 shallow pixels from the classified scene and compared the data from a 12 year period 2001-2012
This random selection and analysis was then replicated 20 times.
",2022-08-12
https://github.com/jc-oconnor/MachineLearningSolutions,"# MachineLearningSolutions
my solutions used in an online machine learning course

These solutions are from the Andrew Ng Machine learning course from Stanford University/CourseEra
The uploaded solutions are my own and not officailly from the course. If you are a student of the course please stick to the code of conduct and do not use my solutions to pass the course. 

This repository contains seven branches one for each assignment in the course. I have only uploaded the files that I had to edit
",2022-08-12
https://github.com/jelletreep/githubpages-docker-demo,"# Git version control for Utrecht University

1. Getting started
2. Usage
3. Learning Git
4. Best practices
5. Cool projects


## 1. Getting Started
### First time connecting to the Utrecht University Github Organization

To be able to use the Utrecht University Github Organization

You need to be a UU Employee AND
You need a personal Github account

1. First time connection: Click [here](https://github.com/orgs/UtrechtUniversity/sso) and authenticate using your solis ID. This will make you a member of the organization and gives you permissions to create repositories and teams.
2. Go to [https://github.com/UtrechtUniversity](https://github.com/UtrechtUniversity) and start working. 
3. The next time you can go straight to [https://github.com/UtrechtUniversity](https://github.com/UtrechtUniversity), now you will see a green button with Single Sign-on

## 2. Usage
#### Code not data
  This Github organization is meant for managing software and not a data repository. See this [link](https://www.uu.nl/en/research/research-data-management/tools-services/tools-for-storing-and-managing-data/storage-solutions) for data storage solutions. If you are a novice make sure to learn the basics of Git version control first in order to manage your projects in a secure way.  

#### Sensitivity
  The Suitability level for this Github organization can be found in the Service Description (link)

#### Creating Repositories
As soon as you have authenticated with solisID using the steps outlined above you will have permission to create Repositories in the [UU github organization](https://github.com/UtrechtUniversity). View [Github Documentation](https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-new-repository) for instructions on how to create a repository.

#### Creating Teams
As soon as you have authenticated with solisID using the steps outlined above you will have permission to create teams in the [UU github organization](https://github.com/UtrechtUniversity). View [Github Documentation](https://docs.github.com/en/organizations/organizing-members-into-teams) for instructions on how to create a team.

#### Inviting colleagues
When you create a repository or team, you will automatically have permission to invite collaborators. When you [invite](https://docs.github.com/en/organizations/organizing-members-into-teams/adding-organization-members-to-a-team) a UU colleague to a team they will automatically receive an invitation to join the UU github organization. When you want to invite a UU colleague to a repository without using github teams, the colleague should first become a member of the organization via the Getting Started steps above. When your colleague is a member you can invite your colleague to collaborate on repositories. 

#### Inviting external collaborators
Non-UU collaborators or UU students can be invited as outside collaborator to repositories [Github Documentation](https://docs.github.com/en/organizations/managing-access-to-your-organizations-repositories/adding-outside-collaborators-to-repositories-in-your-organization).

#### Command line access
When you use the command line (terminal or Git Bash) to interact with Github it is also important to at least 1 time authenticate via [Single Sign-on](https://github.com/orgs/UtrechtUniversity/sso). From the command line you will probably make use of an SSH key or a Personal Access Token. These keys have to be authorized to be used for the github organization UtrechtUniversity. See the following instructions:

- SSH access
  [Creating an SSH key](https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh)
  [Authorizing SSH key for usage in the UU organization](https://docs.github.com/en/github/authenticating-to-github/authorizing-an-ssh-key-for-use-with-saml-single-sign-on)
- PAT access
  [Authorizing Personal Access Token for usage in the UU organization](https://docs.github.com/en/github/authenticating-to-github/authorizing-a-personal-access-token-for-use-with-saml-single-sign-on)

#### Github actions limits
Github Actions minutes and storage are unlimited for public repositories. Whenever possible use public repositories if you are using Github Actions. There are monthly limits for using Github actions in private repos on an organization level. **If this limit is reached Github action minutes will be disabled for private repos for the remaining part of the month**.

#### Github apps and Third party access
The following applications are authorized in the Organization:
- Zenodo
- Codecov
- Microsoft Teams
- Slack
- Travis CI

For other applications, submit a request via Topdesk.



## 3. Learning Git
Using Git version control is key in the Open Science paradigm and helps managing versions of files, collaboration and publication.
A git novice should invest some time to get acquainted with the way of working. Typically a one day course will get you started.

Resources:  
[Software carpentries course documentation](http://swcarpentry.github.io/git-novice/)  
Course calendar


## 4. Best practices
#### Open Science FAIR software
  - https://fair-software.nl/
  - https://www.uu.nl/onderzoek/open-science/themas/fair-data-en-software
#### Managing your github projects 
  - Set repository visibility to public as early as possible
  - Use a project template to automatically create a folder structure (e.g. [this one](https://github.com/UtrechtUniversity/rse-project-templates/tree/master/good-enough-project))
  - Use [Github Pages](https://pages.github.com/) to publish a project overview and link to all relevant repositories and collaborators
  - Use [topics](https://github.com/topics) to increase findability
  - Working with [Github teams](https://docs.github.com/en/organizations/organizing-members-into-teams/about-teams) to organize your projects and create a central location for team discussions and repositories. 
 
## 5. Cool projects





",2022-08-12
https://github.com/jelletreep/patch-dispersal,"
# patch-dispersal

[![DOI](https://zenodo.org/badge/307415967.svg)](https://zenodo.org/badge/latestdoi/307415967)

Version 1.0.0

Matlab code for simulation of plant seed dispersal and population survival in dynamic fragmented landscapes.

![example run](https://github.com/jelletreep/patch-dispersal/blob/master/example.gif)

## Instructions
Clone the matlab folder in this repository and run the main.m script.
To run different scenarios, change the user input variables in lines 23-28 in the script. 
Note the comments in the script for additional usage instructions.

## License

This project is licensed under the terms of the [MIT License](/LICENSE.md)

## Citation

Please [cite this project as described here](/CITATION.md).



",2022-08-12
https://github.com/jelletreep/re-gitbook,"
",2022-08-12
https://github.com/JeroenDMulder/multilevel-state-trait,"# RI-CLPM & Extensions

Here we provide Lavaan-code and Mplus syntax for the increasingly popular Random-Intercept Cross-Lagged Panel Model, as well as some popular extensions.

### Prerequisites

If you want to run the model in Lavaan, you need to install and load the Lavaan-package in R. 

```
install.packages('lavaan', dependencies = T)
require(lavaan)
```
If you want to run the model using Mplus, you can purchase the software package at http://statmodel.com/. 

## Authors

* **Jeroen Mulder** - *PhD Candidate* - [Utrecht University](https://www.uu.nl/staff/JDMulder)
* **Ellen Hamaker** - *Professor Longitudinal Data Analysis* - [Utrecht University](https://www.uu.nl/staff/ELHamaker)

## License

This project is licensed under the MIT License - see the [LICENSE](https://github.com/JeroenDMulder/RI-CLPM/blob/master/LICENSE) file for details
",2022-08-12
https://github.com/JeroenDMulder/portfolio,"# [Hugo Academic Theme](https://github.com/wowchemy/starter-hugo-academic)

[![Screenshot](https://raw.githubusercontent.com/wowchemy/wowchemy-hugo-themes/main/academic.png)](https://wowchemy.com/hugo-themes/)

The Hugo **Academic Resumé Template** empowers you to easily create your job-winning online resumé, showcase your academic publications, and create online courses or knowledge bases to grow your audience.

[![Get Started](https://img.shields.io/badge/-Get%20started-ff4655?style=for-the-badge)](https://wowchemy.com/hugo-themes/)
[![Discord](https://img.shields.io/discord/722225264733716590?style=for-the-badge)](https://discord.com/channels/722225264733716590/742892432458252370/742895548159492138)  
[![Twitter Follow](https://img.shields.io/twitter/follow/wowchemy?label=Follow%20on%20Twitter)](https://twitter.com/wowchemy)

️**Trusted by 250,000+ researchers, educators, and students.** Highly customizable via the integrated **no-code, widget-based Wowchemy page builder**, making every site truly personalized ⭐⭐⭐⭐⭐

Easily write technical content with plain text Markdown, LaTeX math, diagrams, RMarkdown, or Jupyter, and import publications from BibTeX.

[Check out the latest demo](https://academic-demo.netlify.app/) of what you'll get in less than 10 minutes, or [get inspired by our academics and research groups](https://wowchemy.com/creators/).

The integrated [**Wowchemy**](https://wowchemy.com) website builder and CMS makes it easy to create a beautiful website for free. Edit your site in the CMS (or your favorite editor), generate it with [Hugo](https://github.com/gohugoio/hugo), and deploy with GitHub or Netlify. Customize anything on your site with widgets, light/dark themes, and language packs.

- 👉 [**Get Started**](https://wowchemy.com/hugo-themes/)
- 📚 [View the **documentation**](https://wowchemy.com/docs/)
- 💬 [Chat with the **Wowchemy research community**](https://discord.gg/z8wNYzb) or [**Hugo community**](https://discourse.gohugo.io)
- 🐦 Twitter: [@wowchemy](https://twitter.com/wowchemy) [@GeorgeCushen](https://twitter.com/GeorgeCushen) [#MadeWithWowchemy](https://twitter.com/search?q=(%23MadeWithWowchemy%20OR%20%23MadeWithAcademic)&src=typed_query)
- ⬇️ **Automatically import your publications from BibTeX** with the [Hugo Academic CLI](https://github.com/wowchemy/hugo-academic-cli) 
- 💡 [Suggest an improvement](https://github.com/wowchemy/wowchemy-hugo-themes/issues)
- ⬆️ **Updating?** View the [Update Guide](https://wowchemy.com/docs/hugo-tutorials/update/) and [Release Notes](https://github.com/wowchemy/wowchemy-hugo-themes/releases)

## We ask you, humbly, to support this open source movement

Today we ask you to defend the open source independence of the Wowchemy website builder and themes 🐧

We're an open source movement that depends on your support to stay online and thriving, but 99.9% of our creators don't give; they simply look the other way.

### [❤️ Click here to become a GitHub Sponsor, unlocking awesome perks such as _exclusive academic templates and widgets_](https://github.com/sponsors/gcushen)

<p align=""center""><a href=""https://wowchemy.com/templates/"" target=""_blank"" rel=""noopener""><img src=""https://wowchemy.com/uploads/readmes/academic_logo_200px.png"" alt=""Hugo Academic Theme for Wowchemy Website Builder""></a></p>

## Demo image credits

- [Open book](https://unsplash.com/photos/J4kK8b9Fgj8)
- [Course](https://unsplash.com/photos/JKUTrJ4vK00)

## Latest news
<!--START_SECTION:news-->
* [What&#39;s new in v5.2?](https:&#x2F;&#x2F;wowchemy.com&#x2F;blog&#x2F;v5.2.0&#x2F;)
* [What&#39;s new in v5.1?](https:&#x2F;&#x2F;wowchemy.com&#x2F;blog&#x2F;v5.1.0&#x2F;)
* [Version 5.0 (February 2021)](https:&#x2F;&#x2F;wowchemy.com&#x2F;blog&#x2F;v5.0.0&#x2F;)
* [Version 5.0 Beta 3 (February 2021)](https:&#x2F;&#x2F;wowchemy.com&#x2F;blog&#x2F;v5.0.0-beta.3&#x2F;)
* [Version 5.0 Beta 2 (January 2021)](https:&#x2F;&#x2F;wowchemy.com&#x2F;blog&#x2F;v5.0.0-beta.2&#x2F;)
<!--END_SECTION:news-->
",2022-08-12
https://github.com/JeroenDMulder/powRICLPM,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# powRICLPM

<!-- badges: start -->

[![Lifecycle:
experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![CRAN
status](https://www.r-pkg.org/badges/version/powRICLPM)](https://CRAN.R-project.org/package=powRICLPM)
<!-- badges: end -->

`powRICLPM` is an `R` package that performs a power analysis for the
random intercept cross-lagged panel model (RI-CLPM) in a simple and
user-friendly way. It implements the strategy as proposed by Mulder
(under review). Its main functionalities include:

-   [Setting up and performing a basic power
    analysis](https://jeroendmulder.github.io/powRICLPM/articles/start.html):
    Obtain the power to reject the null-hypothesis of no effect (as well
    as other performance measures, such as bias, mean square error,
    etc.) for all parameters in the RI-CLPM given a specific sample
    size, number of repeated measures, and proportion of between-unit
    variance (amongst other things). This can be done across multiple
    experimental conditions simultaneously (i.e., across varying numbers
    of repeated measures, proportions of between-unit variance, etc.).
-   [Extending the basic power analyis
    setup](https://jeroendmulder.github.io/powRICLPM/articles/extensions.html):
    This includes the option to perform bounded estimation, impose
    various (stationarity) constraints over time on parameters of the
    estimation model, and include the estimation of measurement error.
-   [Create Mplus model
    syntax](https://jeroendmulder.github.io/powRICLPM/articles/mplus.html):
    Create syntax for performing a RI-CLPM power analysis using Mplus.

## Documentation

The rationale for the power analysis strategy implemented here can be
found in Mulder (under review). Every user-facing function in the
package is documented, and the documentation can be accessed by running
`?function_name` in the R console, e.g., `?powRICLPM`. These function
references contain explanations on how to use the functions, as well as
some technical details. Furthermore, there are four main vignettes
(accessible via the ‘Vignettes’ tab), describing functionalities and
analysis options of this package more generally. The ‘Example’ vignette
serves as the supplementary material to Mulder (under review), and
contains the R code for an illustrative example using the `powRICLPM`
package. Finally, the
[FAQ](https://jeroendmulder.github.io/powRICLPM/faq.html) contains
answers to frequently asked question that reach me via email.

## Installation

You can install the development version of `powRICLPM` from GitHub with:

``` r
install.packages(""devtools"")
devtools::install_github(""jeroendmulder/powRICLPM"")
```

## Citing `powRICLPM`

You can cite the R-package with the following citation:

> Mulder, J.D., (n.d.). *Power analysis for the random intercept
> cross-lagged panel model using the powRICLPM R-package*

## Contact

If you have ideas, comments, or issues you would like to raise, please
get in touch.

-   Issues and ideas can be raised on GitHub via
    <https://github.com/jeroendmulder/powRICLPM>
-   Pull request can be created on GitHub via
    <https://github.com/jeroendmulder/powRICLPM/pulls>
",2022-08-12
https://github.com/JeroenDMulder/predicting-PTSD-using-LGCM,"# Supplementary materials to Alting van Geusau, Mulder, and Matthijssen (2021)

These online supplementary materials provide the R- and *lavaan*-code for preparing and analyzing the data, respectively. The website can be found at [https://jeroendmulder.github.io/predicting-PTSD-using-LGCM](https://jeroendmulder.github.io/predicting-PTSD-using-LGCM). The main manuscript can be found at [https://doi.org/10.3390/jcm10184152](https://doi.org/10.3390/jcm10184152). 

### Prerequisites

If you want to run the models in *lavaan*, you need to install and load the *lavaan*-package in R. 

```
install.packages('lavaan', dependencies = T)
require(lavaan)
```

## Author of supplementary materials

* **Jeroen Mulder, MSc** - *PhD candidate* - [Utrecht University](https://www.uu.nl/staff/JDMulder)

## Authors of main manuscript

* **Valentijn Alting van Geusau, MSc**
* **Jeroen Mulder, MSc** - *PhD candidate* - [Utrecht University](https://www.uu.nl/staff/JDMulder)
* **Dr. Suzy Matthijssen** - *Inhoudelijk Leidinggevende Altrecht Academisch Angstcentrum* - [altrecht](https://www.altrecht.nl/medewerkers/suzy-matthijssen/)


",2022-08-12
https://github.com/JeroenDMulder/RI-CLPM,"# RI-CLPM & Extensions

Here we provide Lavaan-code and Mplus syntax for the increasingly popular Random-Intercept Cross-Lagged Panel Model, as well as some popular extensions.

### Prerequisites

If you want to run the model in Lavaan, you need to install and load the Lavaan-package in R. 

```
install.packages('lavaan', dependencies = T)
require(lavaan)
```
If you want to run the model using Mplus, you can purchase the software package at http://statmodel.com/. 

## Authors

* **Jeroen Mulder** - *PhD Candidate* - [Utrecht University](https://www.uu.nl/staff/JDMulder)
* **Ellen Hamaker** - *Professor Longitudinal Data Analysis* - [Utrecht University](https://www.uu.nl/staff/ELHamaker)

## License

This project is licensed under the MIT License - see the [LICENSE](https://github.com/JeroenDMulder/RI-CLPM/blob/master/LICENSE) file for details
",2022-08-12
https://github.com/jhnienhuis/ArcticDeltas,"# ArcticDeltas
code and data to reproduce figures from the Arctic River Delta review paper
",2022-08-12
https://github.com/jhnienhuis/BarrierBreach,"# barrier_breach_data
 supplemental data for Nienhuis, Heijkers, Ruessink 2021 JGR-ES
 
 This github folder contains the data and code to reproduce the figures and findings of our study. MIT License
 
 The following files are important:
 
 ModelRun1
 This folder contains the Delft3D simulation setup files.
 
 run_series_of_models.m
 This function uses barrier and storm parameters as input and copies and edits ModelRun1. It then runs the Delft3D simulations. It stores the parameters also as a .mat file.  
 
 analyze_runs.m
 Opens the .mat file generated by run_series_of_models.m It then goes throught the completed Delft3D simulation files and further populates the .mat file with simulation output, such as overwash volumes. Examples are Width.mat, StormPeak.mat, vegdensity.met
 
 Fig3,4,5,etc.
 Matlab code that uses the .mat files generated by analyze_runs.m and creates the figures as shown in the paper.
 
 Nienhuis_BarrierBreach_tables.xslx
 xls file with reduced model data and sandy field data output for easy inspection.
 
 
 
 
 
 
",2022-08-12
https://github.com/jhnienhuis/CriticalBarrierWidth,"# critical_barrier_width
Code to find the critical barrier width for coastal overwash, see Chen et al. (2019)


Critical barrier width can be obtained from two scripts, (1) first a javascript within the Google Earth Engine interface, next (2) postprocessing in Matlab.

1. Google Earth Engine scripts:
The following script generates a kml file of yearly barrier island outlines from 1984 to 2018.
https://code.earthengine.google.com/5ad3eddacd69bafc90ef4db01f77a1ff

2. Matlab scripts:
In the script Critical_width.m you can import a kml file to generate a barrier overwash vs. barrier width plot


",2022-08-12
https://github.com/jhnienhuis/GlobalDeltaChange,"**************
GlobalDeltaChange
**************
.. image:: https://badge.fury.io/gh/jhnienhuis%2FGlobalDeltaChange.svg
    :target: https://badge.fury.io/gh/jhnienhuis%2FGlobalDeltaChange

.. image:: https://app.codacy.com/project/badge/Grade/0ae4939efdcd43b9b70e3ac605619f50
    :target: https://www.codacy.com/gh/jhnienhuis/GlobalDeltaChange/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=jhnienhuis/GlobalDeltaChange&amp;utm_campaign=Badge_Grade
    
*GlobalDeltaChange* is a (1) theoretical framework to predict delta morphology and delta change, and (2) a set of codes to make this predictions on a global scale for ~11,000 deltas. Results and methods are described in `Nienhuis et al., 2020 <https://www.nature.com/articles/s41586-019-1905-9>`_

.. figure:: https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41586-019-1905-9/MediaObjects/41586_2019_1905_Fig1_HTML.png?as=webp
    
    Global delta morphology, as predicted by three sediment fluxes (Qwave, Qtide, and Qriver), within a ternary space and along Earths' coast.

Documentation
#############

Versioning
**********

by Jaap Nienhuis, Utrecht University, 2019, version 1.0
by Jaap Nienhuis, Utrecht University, 2021, version 2.0
(Version 2.0 includes the newest land/water change data from GSW, local wave estimates from local wind fetch, submarine and subaerial elevation, river names, and more.)

Use the data
#############

The data can be viewed interactively in `a GEE App <https://jhnienhuis.users.earthengine.app/view/globaldelta>`_.
Raw data is available here on github, formatted as `MATLAB .mat <https://github.com/jhnienhuis/GlobalDeltaChange/blob/master/GlobalDeltaData.mat>`_, `Shapefiles <https://github.com/jhnienhuis/GlobalDeltaChange/blob/master/export_data/GlobalDeltaMouth_shp.zip>`_, `NetCDF .nc <https://github.com/jhnienhuis/GlobalDeltaChange/blob/master/export_data/GlobalDeltaData.nc>`_, and `.kml <https://github.com/jhnienhuis/GlobalDeltaChange/blob/master/export_data/GlobalDeltaData.kml>`_ files. 

Reproduce the data
#############

To reproduce the GlobalDeltaData.mat file, run the following functions in this order: 

Main functions
**********
(1) find_river_mouth.m
    uses hydrosheds, DIVA, Durr, and SRTM to find all alluvial river mouths globally, furtheron referred to as deltas. Initiates the GlobalDeltaData.mat file

(2) get_QRiver.m
    uses WBMSED to get a pristine and disturbed sediment and water flux to each delta. Optionally you can use get_QRiver_timeseries to get daily QRiver and Discharge output

(3) get_channel_slope.m
    uses SRTM and hydrosheds to extract river elevation profiles for all deltas up to 30 meters elevation
    
(4) get_bathy_profile.m
    uses etopo data to get steepest descent profiles of the underwater basin depths, from the river mouth to -100m
    
(5) get_Qwave.m
    adds wave data to each delta from WaveWatch. For deltas that are (partially) sheltered from wave approach angles, it estimates a fetch based on shoreline orientation.
    It uses the bretschneider fetch formula and WaveWatch wind data to estimate wave heights in sheltered locations. Uses get_global_fetch.m. 
    Optionally you can use get_QWave_timeseries to get daily wave statistics, or get_QWave_future to get estimates of future wave heights (up to 2100).

(6) get_Qtide.m
    adds tide data to each delta, based on TOPEX data
    
(7) get_hydrobasins_id.m
    adds identifiers from the new WWF HydroATLAS, HydroBasins, and HydroRIVERS datasets

(8) add_names_to_deltas.m
    Uses FAO data to find river names for deltas, where available.

Supplemental functions
**********

land_area_change/get_aquamonitor_data
    defines polygons for each river delta, and retrieves aquamonitor and earthsurfacewater explorer data to get delta coastal area land gain and loss within those regions. 
    These data are noisy, so use with caution and with appropriate estimates of data uncertainty. The GEE code can be found at:
    https://code.earthengine.google.com/21dd5f216c625b8696b4d9af6ee55215
    We manually define polygons for the 100 largest deltas (see GlobalDeltaMax100.kml), and use proxies for delta area size for the remaining deltas.
    
export_data/create_kml, create_netcdf, create_shapefile, create_shapefile_deltaland
    various functions to export relevant data to kml, netcdf, and shapefile formats
    
misc/galloway_predictor
    function to plot output in the galloway triangle.
    
Input datasets
#############

Reproducing the data can be done with the following input datasets:

- HydroSheds 15 arcsec drainage direction (DIR), flow accumulation (ACC), and basin outline (BAS) files
source: https://www.hydrosheds.org/

- DIVA typology_coastline
source: AT Vafeidis, G Boot, J Cox, R Maatens, L McFadden, RJ Nicholls, T Spencer, RSJ Tol, (2006) The DIVA database documentation, DINAS-COAST Consortium

- DURR dataset
source: Dürr, H.H., Laruelle, G.G., van Kempen, C.M. et al. Estuaries and Coasts (2011) 34: 441. https://doi.org/10.1007/s12237-011-9381-y

- NOAA vectorized shoreline
source: https://www.ngdc.noaa.gov/mgg/shorelines/

- WBMSed global discharge, pristine, and disturbed sediment fluxes
source: https://sdml.ua.edu/datasets-2/

- Global directional wave statistics (WaveWatch), and global tides (TOPEX)
source: https://jhnienhuis.users.earthengine.app/view/changing-shores

- SRTM, 1 arcsec (30 meter) resolution global topography
source: https://lpdaac.usgs.gov/products/srtmgl1v003/

- River Names, from FAO Aquamaps
source: http://www.fao.org/nr/water/aquamaps/

(note, I don't store these here because of versioning and file size limitations. Please get in touch if you can't find them, I will send them to you)

",2022-08-12
https://github.com/jhnienhuis/GlobalDeltaSeaLevel,"# GlobalDeltaSeaLevel
Model predictions of global delta land area response to sea-level rise

This github repository contains code and datafiles to reproduce findings published in:

Nienhuis, J.H., & van de Wal, R.S.W. (2021). Projections of global delta land loss from sea-level rise in the 21st century. Geophysical Research Letters, 48, e2021GL093368. https://doi.org/10.1029/2021GL093368

## get_sealevel.m
loads data from 
- Past sea level rise (1900-2015): Dangendorf et al (Nature Clim. https://www.nature.com/articles/s41558-019-0531-8)
- Future sea level rise (2007-2100): Oppenheimer et al (SROCC Ch. 4, https://www.ipcc.ch/srocc/)
- Vertical land motion: Shirzaei et al (Nature Rev., https://www.nature.com/articles/s43017-020-00115-x)
and matches it with global delta river mouths. 
output: export_data/GlobalDeltaSeaLevelData

## get_deltadata.m
this file loads cross-sectional profile information from SRTM15+ and extracts delta widths and lengths. It uses a subfunction called ""get_deltaprofile.m"".
additional data is loaded from Edmonds et al., Nature Comm, https://www.nature.com/articles/s41467-020-18531-4. We use their length/width data to check/validate our automatic width extraction and also replace our automatically obtained values where possible
output: export_data/GlobalDeltaProfile

## get_deltaresponse.m
this file loads delta morphology from GlobalDeltaProfile and sea level changes from GlobalDeltaSeaLevelData to produce land area change estimates for global deltas
it uses the function get_deltachange.m and get_deltachange_montecarlo for individual delta estimates incl uncertainty
output: export_data/GlobalDeltaSeaLevelResponse

## export_data
contains .mat and .nc data files that are generated by the scripts mentioned above.

## grl2021_figures
contains scripts for the figures published in Nienhuis & van de Wal, GRL 2021
",2022-08-12
https://github.com/jhnienhuis/LatticeBoltzmannModel,"# lattice_boltzmann_model
Fluid flow model using the lattice boltzmann concepts
see, Nienhuis et al., JGR 2014, https://doi.org/10.1002/2014JF003158

Central function is LBM.m

LBM.m starts a model run using settings described in InitializeStruct.m and InitializeLBM.m

LBM.m then uses these settings to run the model in RunLBM.m 

The user can initialize multiple model runs using LBM_ParameterRun.m

LBM_ParameterRun.m uses a list of settings such as bed geometries or boundary conditions to start a LBM.m model run for each setting.
",2022-08-12
https://github.com/jhnienhuis/SubsidenceDelft3D,"# Delft3D Subsidence
This repository contains a Delft3D Flow and Morphology setup with MATLAB scripts for land subsidence. 

It is described in the paper of Nienhuis, Tornqvist, Esposito, 2018: doi.org/10.1029/2018GL077933

The main function is: ""run_series_of_models.m"". From this function the user can simulate the development of a sediment diversion / crevasse splay.

The function calls a delft3d model (described inside the ""input"" folder) that is looped iteratively w/ subsidence and vegetation dynamics described in matlab (""consolidation_model"").
",2022-08-12
https://github.com/jkorb/cv,"# Johannes Korbmacher - CV

[![made-with-latex](https://img.shields.io/badge/Made%20with-LaTeX-1f425f.svg)](https://www.latex-project.org/)
[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](http://creativecommons.org/licenses/by/4.0/)

These are the source files for my academic CV. Not much to see here.
",2022-08-12
https://github.com/jkorb/KI1V13001-Inleiding-Logica,"<div align=""center"">

# KI1V13001 - Inleiding Logica

[About](#about) • [Contribute](#contribute) • [GitHub](#GitHub) • [Latex](#latex) • [Compiling](#compiling) • [License](#license) • [Contact](#contact)

[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://gitgub.com/UtrechtUniversity/KI1V13001-Inleiding-Logica/graphs/commit-activity)
[![made-with-latex](https://img.shields.io/badge/Made%20with-LaTeX-1f425f.svg)](https://www.latex-project.org/)
[![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](http://creativecommons.org/licenses/by/4.0/)

</div>

## About

This is the source material for the course **Inleiding Logica** (KI1V13001) at Utrecht University.
The material consists of:

  + the syllabus (`lib/syllabus`)
  + the slides (`lib/slides`)
  + the lecture notes (`lib/notes`)

Students currently taking the course can find pdfs of the material under [Course Material](https://github.com/UtrechtUniversity/KI1V13001-Inleiding-Logica/releases/latest).[^1]

## Contribute

We encourage contributions, especially but not exclusively from past and present students of the course.
Two simple ways of contributing are:

  + *open issues*, for example to point out typos, mistakes, or make feature requests
  + *making pull requests*, for example to fix a mistake yourself and become a [contributor](https://github.com/UtrechtUniversity/KI1V13001-Inleiding-Logica/graphs/contributors).

If these things don't tell you anything yet, don't despair! 
Check the next two sections.

## GitHub

If you're here and don't know about GitHub already, then you're probably here because you're currently taking the course.
Note that it's not mandatory to learn git/GitHub, but it's a highly valuable skill and it allows you to contribute to the course material: immortalize yourself in our first-year logic course!

A good place to get started on git/GitHub is [here](https://guides.github.com/).

## LaTeX 

The technical bits of the course material are written in [LaTeX](https://www.latex-project.org/).
If you want to contribute, you'll need to learn it. 
Again, this is not mandatory if you're a student taking the course but again, it's a pretty useful skill.
In fact,
most scientific writing in formal disciplines (math, physics, computer science, ...) is done in LaTeX.
Get started here [here](https://www.overleaf.com/learn/latex/Tutorials).

The non-technical bits of the material are written in markdown (such as this readme and the syllabus), a *very* simple markup language.
Learn more about it [here](https://guides.github.com/features/mastering-markdown/).

## Compiling

If you want to compile the source materials yourself, you need `git`, a standard LaTeX install (e.g. `texlive-core`), `pandoc`, and `make`.
These are easy to get if you're working on Linux (via your package manager) or Mac (via `homebrew`).
I don't know about Windows but that should be possible, too (if you know you're way around GNU Make on Windows, please let me know).

The standard way of downloading and compiling is via the command line as follows (assuming you have installed the above dependencies):

``` shell
  git clone https://github.com/UtrechtUniversity/KI1V13001-Inleiding-Logica.git
  cd KI1V13001-Inleiding-Logica
  ./configure
  make all
```

This will create the folder `KI1V13001-Inleiding-Logica` in your current working directory with the course materials in it, and then compile the pdfs into a folder `pdf` inside.
Don't forget to run the `configure` script, since this will update the source code with the current course specifics (dates, TAs, etc.).

## License

<a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by/4.0/88x31.png"" /></a><br />This work is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/"">Creative Commons Attribution 4.0 International License</a>.

## Contact

For communication about a current version of the course, especially current students of the course, please use [inleiding.logica@uu.nl](mailto:inleiding.logica@uu.nl) to get in touch.

For everybody and everything else, use [j.korbmacher@uu.nl](mailto:j.korbmacher@uu.nl).

[^1]: Everybody else can find them there, too, of course.
",2022-08-12
https://github.com/jkorb/KI1V14005-Tutoraat-KI-Basis,"# KI1V14005 - Tutoraat KI, Basis

This repository hosts the source files for the guideline for tutors and mentors in our first-year tutoring program for artificial intelligence `KI1V14005 - Tutoraat KI, Basis`.

A compiled PDF can be found under `Releases`.

Contributions (PRs, issues, etc.) very welcome, especially from current mentors, tutors, or students who happen to find this.

# Dependencies

To compile, you need `GNU make`, `pandoc`, some standard latex install (`latex-core` should do), and my `uureport`-class (<https://github.com/jkorb/uureport>). 
`uureport` is added as a git submodule for compilation/convenience, so just clone with `git clone --recursive` to get it copied into `src/tex/uureport`.

# License

<a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by/4.0/88x31.png"" /></a><br />This work is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/"">Creative Commons Attribution 4.0 International License</a>.
",2022-08-12
https://github.com/jkorb/mypubs,"# mypubs

[![License: MIT](https://img.shields.io/badge/License-MIT-greenw.svg)](https://opensource.org/licenses/MIT)

This repository hosts a bibliographical database of my academic publications in different formats (`.bib`, `.bibtex`, `.json`, `.yaml`).
You can find a compiled list of my publications in PDF [here](https://github.com/jkorb/mypubs/releases/download/release/mypubs.pdf).


## Syncing

The repository contains a primitive bash script `sync.sh` for syncing between the different formats using `pandoc`.
It's mainly for my personal use, but in case you're interested have a look.

## License

Bibliographies are not copyrightable (luckily).

The rest, is under the MIT license.
",2022-08-12
https://github.com/jkorb/mytalks,"# mytalks

[![License: MIT](https://img.shields.io/badge/License-MIT-greenw.svg)](https://opensource.org/licenses/MIT)

This repository hosts a bibliographical database of my academic talks in different formats (`.bib`, `.bibtex`, `.json`, `.yaml`).
You can find a compiled list of my presentations in PDF [here](https://github.com/jkorb/mytalks/releases/download/release/mytalks.pdf).

## Syncing

The repository contains a primitive bash script `sync.sh` for syncing between the different formats using `pandoc`.
It's mainly for my personal use, but in case you're interested have a look.

## License

Bibliographies are not copyrightable (luckily).

The rest, is under the MIT license.
",2022-08-12
https://github.com/jkorb/uuletter2,"#+TITLE:        =uuletter2.cls= v.0.2.1 Readme
#+AUTHOR:      J. Korbmacher
#+DATE:        <2020-01-29 Wed>
#+EMAIL:       j.korbmacher@uu.nk
#+LANGUAGE:    en

* About

The purpose of this work is to provide a \LaTeX document class
(=uuletter2.cls=) for writing letters that comply with the Utrecht
University (UU) Corporate Idenity.[FN:1]

The class is loaded like any other document class by means of the
command =\documentclass[<options>]{uuletter2}=. For a description of
the options, see below.

By default (i.e. if you select no options), the letter is set in the
standard sans serif font, at 10 pt font size with the usual \LaTeX
justification, on a4 paper, and with an English language header. There
is an option (=dutch=), which typesets the header in Dutch.

Note that this is a beta-version. I haven't tested the class much
beyond the templates and only on my personal Macbook Pro running
Catalina with MacTex 2019. I'd very much appreciate bug reports to
j.korbmacher@uu.nl and will keep this updated. In the next days, I
also put this on code on github.

[FN:1] There exists a class called =uuletter.cls= at
https://svn.science.uu.nl/index.php?RID=657 but it appears a bit dated. 

* Installation and Dependencies

The simplest way to use the class is to include the actual style file
(=uuletter2.cls=), the logos (with their original file names,
#+BEGIN_CENTER
=UU_logo_EN_CMYK.png= 
and
 =UU_logo_NL_CMYK.png=,
#+END_CENTER
available under
https://www.uu.nl/en/organisation/corporate-identity/downloads/logo,
heading ""LOGOS FOR DIGITAL APPLICATIONS""), and the signature file
(which must be named =signature-uuletter.png=, default ""John Hancock"" provided)
in whatever directory you want to compile your letter.

If you like the class and want to make it available for every \LaTeX
file you compile on your computer, you need to put the files somewhere
in the TDS (TeX Directory Structure). For Mac, I recommend:

+ Put =uuletter2.cls= in
#+BEGIN_CENTER
  =Users/<username>/Library/texmf/tex/latex/uuletter2=
#+END_CENTER 
  If that directory doesn't exist, it's safe to create it.
+ Put the logo files (with their original file names,
  =UU_logo_EN_CMYK.png= and =UU_logo_NL_CMYK.png=) in
#+BEGIN_CENTER
  =Users/username/Library/texmf/tex/latex/uuletter2/logos/=
#+END_CENTER
 Again, safe to create.
+ Put the signature file named =signature-uuletter.png= in
#+BEGIN_CENTER
  =Users/username/Library/texmf/tex/latex/uuletter2/signature/=
#+END_CENTER
Also
  safe to create). A default file with a ""John Hancock"" is
  included. If you want to use your own scanned signature, you can use
  any .png file that you rename =signature-uuletter.png=. For a good
  result, I'd recommend not exceeding 4-5 cm of picture width.

Note that =Users/<username>/Library/= is by default a hidden folder in
MacOS. You can use Command+Shift+Dot in the folder =Users/<username>/=
to make it visible. Otherwise, use the internet to figure it out.

For other Unix systems (e.g. Linux) or Windows you're on your own (for
now). 

The class loads the following packages:
+ =inputenc= with the =utf8= option (this might be a bad idea, let me
  know if that messes things up).
+ =babel= with the provided langauge option (see below, default: english)
+ =graphicx= with no options (to include the logo and signature)
+ =hyperref= with =colorlinks=false= to have clickable links for the
  homepage and email in the header (I might also kick that one out).

This means that you don't need to load these packages anymore, unless
you want to change their options. You're free to load other packages,
but I haven't explored any possible compatability issues. Since the
class was written from scratch I /hope/ there are none, but that might
be folly.

* Package options

The package has the following options:
+ =dutch=
Typesets a letter according to the Dutch template. With this option,
the package loads =babel= with the =dutch= option, displays the Dutch
language UU logo, and the labels in the header are printed in Dutch.
+ =english= (Default)
Typesets a letter according to the English template. With this option,
the package loads =babel= with the =english= option (/not/ =british=),
displays the English language UU logo, and the labels in the header
are printed in English.
+ =sans= (Default)
Typesets the letter in the (default) sans-serif font family. Also
loads =fontenc= with the =T1= option.
+ =roman= 
Typesets the letter in the (default) roman font family.  Also loads
=fontenc= with the =T1= option.
+ =verdana=
Typesets the letter in 9 pt Verdana (if the font is found on the local
machine). With this option, the letter corresponds to the Brand Policy
of the University.  *Warning:* This option is implemented via the
=fontspec= package, which requires compilation with XeLaTex (which is
relatively easy in, say, TeX Shop).
+ =ragged= 
Typesets the letter with right-ragged justification. This makes the
result look like the Word templates, but is not recommended for
serious letters.
+ =justified= (Default)
Typsets the letter with standard, \LaTeX justification. Much
preferrable.
+ =10pt= (Default), =11pt=, =12pt=
Changes font size as usual in \LaTeX. These options only work in
combination with the (default) =sans= option and the =roman=
option. They do /not/ work with the =verdana= option, the reasoning
being that the university wants Verdana to be typeset in 9 pt and only
Verdana is actually properly readable at such a small size.

Note that these are /all/ the class options. Some optionns you might
be used to, like, say, =a5paper=, are not available. I'll consider
including other options upon request.

* Commands

To begin simply put =\opening{<Salutation>}=
somewhere after =\begin{document}= and start writing your letter. To
finish, use =\closing{<Greeting>}= when you're done. So do something
like this:

#+BEGIN_EXAMPLE
\documentclass{uuletter2}
\begin{document}
\opening{Hello there,}
This is a really short letter.
\closing{Bye bye,}
\end{document} 
#+END_EXAMPLE

For a more elaborate examples, see the provided templates.

Note that =\closing{<Greeting>}= accepts an optional argument, as in
#+BEGIN_CENTER
=\closing[<Sign as>]{<Greeting>}=
#+END_CENTER
which tells \LaTeX how you'd like
to sign your letter. By default, the class uses the sender name
provided via the =\name{<Name>}= macro (see below), but there are
conveivable situations where you might want to toy around with this:
e.g. the sender is the Department but you want to sign with your name,
the letter head should include your full name and title (prof. dr. dr. so
and so, provided via =\name{prof. dr. dr. so and so}=) but you want to
sign with your first name, etc.

You can invoke these commands multiple times in a single .tex file to
create multiple letters in the same pdf. Note that
=\opening{<Salutation>}= creates the letter head and
=\closing{<Greeting>}= the signature. Please use both in every letter
you want to create. If you want, you can leave their arguments
empty. Note that if you don't include the =\closing= command, the page
numbers will not be typset properly. 

You provide the information \LaTeX needs to compose the letter (your
names, addresses, etc.) via the macros explained in the following
section. These macros fall into two categories: global and
local. This distinction only really matters if you want to write more
than one letter in the same .tex file (if that's not your thing, just
ignore the labels and use the commands as described/desired). The main
difference between the two is that local macros are reset after each
=\closing=, meaning that if you want to write a new letter, you need
to provide these macros again. This is to avoid accidentally including
the wrong information. Global macros are /not/ reset, but /can/ be
manually re-defined.

** Global Commands

+ =\name[<Function>]{<Your Name>}=
This command tells the class which name to use as the sender and,
optionally, your function. Say, =\name[Head of HR]{dr. Lüdenscheidt}=. Since this is the sender's name in the
letter head, you should provide full academic titles as you'd like to
see them in the letter. You can use a more informal address as the
optional argument for =\closing=.
+ =\subsender{<Subsender>}=
Best used to name the Department, Institute, etc. where you work. You
can also use this to make clear that this letter comes from the head
of department, say.
+ =\visitingaddress{<Address>}=
Provides your visiting address. You can use =\\= to insert newlines in
the argument.
+ =\telephone{<phone number>}=
Provides your phone number.
+ =\fax{<fax number>}=
Provides your fax number (if that's your thing).
+ =\email{<email>}=
Provides your email address. This is typeset as a clickable =mailto:=
link.
+ =\website{<website>}=
Provides your website (university or otherwise) as a clickable link.

Of these global commands, only phone, fax, email, and website should
really be left out and only in case you don't want to provide them. If
an argument is empty or not used, the corresponding line (and label)
are not printed. But watch out for stray spaces. E.g. =\fax{ }= will
include the fax label with no fax number following it (a simple space
actually), while =\fax{}= doesn't print the label. This is a small bug
(or rather possibly unexpected behavior), which I might fix in the
future. 

** Local Commands

Note that these are all reset by =\closing=!

+ =\toname{<name>}=
	Defines the recipients name. Used for generating the address
  field. If there should be no address field, the name can be put in
  the opening (see below). Not printed if not defined.
+ =\toaddress{<street + no//zipcode + city//country>}=
  Defines the address of the recipient. Is used together with
  =\recipientname{}= for generating the address field. Not printed if
  not defined.
+ =\pobox{<PO Box 99999>}=
  Defines the PO Box (as per the university template).
+ =\subject{<Letter of Reference for Student>}=
Defines the subject of the letter. Goes into the letter
  details if defined, otherwise not printed.
+ =\reference{<FMTUIT/2011/WK/IDEB>}=
	Defines the sender's reference for the letter. Goes into the
  letter details if defined, otherwise not printed.
+ =\yourreference{<2012/103/MV/ws>}=
	Defines the recipient's reference for the letter. Goes into
  the letter details if defined, otherwise not printed.
+ =\enclosure{<This important document, that important document>}=
Defines the enclosures of the letter. Goes into the letter
  details if defined, otherwise not printed.
+ =\date{<dd-mm-yyyy>}=
  Defines the date of the letter. If undefined or empty, \today is
  printed in UU's preferred <dd-mm-yyyy> style..
+ =\withsignature=
Binary option, prints a digital signature (image) from the path
provide by =\signaturepath{}= (global command). As a local command,
=\withsignature= is reset at the end of each letter.

** Available environments

Many but not all usual \LaTeX environments are available. At the
moment the following are available: =enumerate=, =itemize=,
=description=, =quote=, and =quotation=. Note that
=\footnote{<footnote>}= is currently /not/ available. I'm not sure if
it's needed. If you think it is, let me know and I'll include it in
the next version. Same goes for other commands, environments that I
haven't thought of. Let me know.
",2022-08-12
https://github.com/jkorb/uureport,"#+TITLE:        =uureport.cls=
#+AUTHOR:      J. Korbmacher
#+DATE:        <2020-03-03 Tue>
#+EMAIL:       j.korbmacher@uu.nk
#+LANGUAGE:    en

* About

This class ports the Word template for the UU house style
(https://www.uu.nl/en/organisation/corporate-identity/downloads/word-document)
to a \LaTeX~class. The class is built on the standard report class and
has (almost) all the functionality of that class. Additional
options/commands are explained below.

* Installation and Dependencies

The simplest way to use the class is to include the actual style file
(=uureport.cls=) and the logos (with their original file names,
#+BEGIN_CENTER
=UU_logo_EN_CMYK.png= 
and
 =UU_logo_NL_CMYK.png=,
#+END_CENTER
available under
https://www.uu.nl/en/organisation/corporate-identity/downloads/logo,
heading ""LOGOS FOR DIGITAL APPLICATIONS"") in whatever directory you want to compile your letter.

If you like the class and want to make it available for every \LaTeX
file you compile on your computer, you need to put the files somewhere
in the TDS (TeX Directory Structure). For Mac, I recommend:

+ Put =uureport.cls= in
#+BEGIN_CENTER
  =Users/<username>/Library/texmf/tex/latex/uureport=
#+END_CENTER 
  If that directory doesn't exist, it's safe to create it.
+ Put the logo files (with their original file names,
  =UU_logo_EN_CMYK.png= and =UU_logo_NL_CMYK.png=) in
#+BEGIN_CENTER
  =Users/username/Library/texmf/tex/latex/uureport/logos/=
#+END_CENTER
 Again, safe to create.

Note that =Users/<username>/Library/= is by default a hidden folder in
MacOS. You can use Command+Shift+Dot in the folder =Users/<username>/=
to make it visible. Otherwise, use the internet to figure it out.

For other Unix systems (e.g. Linux) or Windows you're on your own (for
now). 

The class loads the following packages:
+ =babel= with the provided langauge option (see below, default: english)
+ =graphicx= with no options (to include the logo and signature)

This means that you don't need to load these packages anymore, unless
you want to change their options. You're free to load other packages,
but I haven't explored any possible compatability issues. Since the
class was written from scratch I /hope/ there are none, but that might
be folly.

* Options

The package has all options of the report class, *except for*
=twocolumn= and other paper sizes than a4. 

Additionally, the package has two language options =dutch= and
=english= (the letter is loaded by default).

Finally, there are three font options =verdana=, =opensans=, and
=merriweather= (see:
https://www.uu.nl/en/organisation/corporate-identity/brand-policy/fonts). The
=verdana= option requires compilation with XeLaTeX, =opensans= is
loaded by default.

*  Commands

The class modifies the =\title= command to allow for a subtitle to be
passed as an optional argument: =\title[subtitle]{Titel rapport}=

Additionally, the class has the follwoing three new commmands:
+ =\sender[<subsender>]{<sender>}=
+ =\titlepageinfo{<textblock>}=
The previous two commands together create the textblock on the title
page. The subsender is used for the running heads.
+ =\version{<version>}=
If provided is printed in the running footer.

* Projects
+ TODO Implement a decent two-page option

+ TODO Implement two column

Discussion: Maybe, maybe not.

+ TODO Implement different paper sizes

Note: requires logo adjustment.

* Version History
- 0.1.0 \Alpha-release
",2022-08-12
https://github.com/jnwiegers/Integrated-Population-Model-for-the-Mallard,"# Integrated Population Model of the Dutch Mallard population

This repository contains the data and code required to run the Mallard IPM as described by Wiegers et al. 2022. The same data can be found on Dryad as well. 

This dataset and code are the basis of the integrated population model for the Mallard in the Netherlands as described in Wiegers et al (2022).

The data consists of four .csv files, the integrated population model in WinBugs, and the R code required to run this model and the life-stage sensitivity analysis.

The rows of ‘Clutch size.csv’ are individual observations of Mallard nests. The columns indicate the observation number; the clutch size C; and the observation year, where the years 2003-2020 are indicated by Year = 1-18.

The rows of ‘Egg hatch rate.csv’ are individual observations of Mallard nests for which the number of hatched eggs was known. Only successful nests are used to exclude incompletely observed nest. The columns indicate the observation number; the clutch size C; the number of egg hatched; and the year. 

The rows of ‘Nest success.csv’ depict annual data on nest survival of Mallards in the Netherlands. Nest success was calculated using the Mayfield method. The columns indicate the years, where 2003 is year 1; the total number of days that nests were exposed to potential nest failure; the total number of nests monitored; the total number of nests that failed to produce at least one egg; the corresponding Mayfield estimate of daily nest survival; and the total nest success rate of female Mallards, assuming an average incubation time of 37 days.

The rows of ‘Duckling_project.csv’ are repeated observations of Mallard broods. The columns indicate the observation number; the number of ducklings counted during the first observation; the number of ducklings counted during the next observation; the interval (in days) between these observations; the mean estimated age (in days) of the ducklings at these observations; the observation year, where 2003 is year 1; and finally the scaled estimated duckling age. 



",2022-08-12
https://github.com/jnwiegers/jnwiegers,"- 👋 Hi, I’m Yannick!
- 👀  I’m interested in the effect of hunting pressure on wildlife communities, in particular in the tropics.  
- 🌱 I’m currently setting up my PhD research and will start with my field work in September.
- 📫 You can reach me at j.n.wiegers@uu.nl

<!---
jnwiegers/jnwiegers is a ✨ special ✨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
",2022-08-12
https://github.com/joaomcteixeira/dotrc,"# run_commands
My rc files for vim, bash, etc...
",2022-08-12
https://github.com/joaomcteixeira/dropped_TaurenMD,"**!!!!!I have stop developing this repository. It remains open as a register of the past¡¡¡¡¡**

Tauren-MD
=========

**Tauren-MD is an interface that streamlines analysis routines for Molecular Dynamics.**

Description
-----------

Tauren-MD was designed to facilitate the usage of scientific MD analysis libraries to non-developer users, though it can be very useful also for the proficient Pythonistas out there.

Tauren-MD wraps around high performance MD analysis libraries, such as: `MDTraj`_, `MDAnalysis`_, `OpenMM`_ (and implementing others...); and it contains its own routines for data representation and export, such as curated plotting templates through `matplotlib`_.

In this way, Tauren-MD attempts to be an *à la carte* menu where the user can easily chose from a predefined list of *actions* what she/he wants from hers/his trajectories: a configuration file can be easily setup with the desired routines and executed via the ``taurenmd`` executable file (created after installation).

.. _version: https://semver.org/#semantic-versioning-200
.. _MDTraj: https://github.com/mdtraj/mdtraj
.. _MDAnalysis: https://www.mdanalysis.org/
.. _OpenMM: https://github.com/pandegroup/openmm
.. _matplotlib: https://matplotlib.org/

Documentation
-------------

The complete and latest Tauren-MD project description and full documentation is available *online* at the `project's website`_.

If you have cloned this repository or downloaded any of the previous `releases`_ you can access the corresponding version documentation *offline*, simply open the ``index.html`` file stored in ``docs`` folder with your favourite web-browser.

.. _`project's website`: https://joaomcteixeira.github.io/Tauren-MD/
.. _releases: https://github.com/joaomcteixeira/Tauren-MD/releases

Citing
------

Cite **Tauren-MD** project as:

- João M.C. Teixeira. joaomcteixeira/Tauren-MD. Zenodo. http://doi.org/10.5281/zenodo.2579632.

Or, if you want to cite a specific version you have used, refer to the `DOI of that version`_, for example:

- João M.C. Teixeira. (2019, February 28). joaomcteixeira/Tauren-MD: v0.5.2 (Version v0.5.2). Zenodo. http://doi.org/10.5281/zenodo.2580076

You **SHOULD** by all means cite the Molecular Dynamics (MD) analysis libraries upon which Tauren-MD wraps. For example, by selecting ``""mdtraj""`` as ``""traj_type""`` in the `configuration file`_ you are making use of it, therefore you should also cite MDTraj along with Tauren-MD; the same applies for any other library Tauren-MD uses and which you have chosen to use.

.. _`DOI of that version`: https://zenodo.org/record/2580076#.XH_8jYVw30o
.. _`configuration file`: https://joaomcteixeira.github.io/Tauren-MD/taurenhtml/html/rstfiles/configuration_file.html#trajectory-type

`Please read further`_ on more details on how to cite Tauren-MD's dependencies.

.. _`Please read further`: https://joaomcteixeira.github.io/Tauren-MD/taurenhtml/html/rstfiles/citing.html

License
-------

The entire Tauren-MD code comes with no liability and is licensed under the `GPL-3.0`_.

.. image:: https://raw.githubusercontent.com/joaomcteixeira/Tauren-MD/master/docs/img/gpl3_logo.png
    :target: https://www.gnu.org/licenses/gpl-3.0.en.html

.. _GPL-3.0: https://github.com/joaomcteixeira/Tauren-MD/blob/master/LICENSE

",2022-08-12
https://github.com/joaomcteixeira/dummy,"# dummy
Dummy repository to test things around

Testing, Testing

added vim line
",2022-08-12
https://github.com/joaomcteixeira/dummyrepo,"================================
Python Package Skeleton Template
================================

.. image:: https://github.com/joaomcteixeira/python-project-skeleton/workflows/Tests/badge.svg?branch=master
    :target: https://github.com/joaomcteixeira/python-project-skeleton/actions?workflow=Tests
    :alt: Test Status

.. image:: https://github.com/joaomcteixeira/python-project-skeleton/workflows/Package%20Build/badge.svg?branch=master
    :target: https://github.com/joaomcteixeira/python-project-skeleton/actions?workflow=Package%20Build
    :alt: Package Build

.. image:: https://codecov.io/gh/joaomcteixeira/python-project-skeleton/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/joaomcteixeira/python-project-skeleton
    :alt: Codecov

.. image:: https://img.shields.io/codacy/grade/ae042ac066554bfab398024b0beea6a5/master?label=Codacy
    :target: https://app.codacy.com/manual/joaomcteixeira/python-project-skeleton/dashboard
    :alt: Codacy

.. image:: https://api.codeclimate.com/v1/badges/d96cc9a1841a819cd4f5/maintainability
   :target: https://codeclimate.com/github/joaomcteixeira/python-project-skeleton/maintainability
   :alt: Maintainability

.. image:: https://img.shields.io/codeclimate/tech-debt/joaomcteixeira/python-project-skeleton
    :target: https://codeclimate.com/github/joaomcteixeira/python-project-skeleton
    :alt: Code Climate technical debt

.. image:: https://img.shields.io/readthedocs/python-project-skeleton/latest?label=Read%20the%20Docs
    :target: https://python-project-skeleton.readthedocs.io/en/latest/index.html
    :alt: Read the Docs

Summary
=======

This repository is a **skeleton template** for a **Python application/library** compliant with the latest team-development and software deployment practices within a continuous integration (CI) framework. It can be used as a source of information/study to emulate or as a direct template for your repositories. **Note** I can't cover all strategies available in the wild. This repository reflects the setup I like the most and covers the CI needs of my Python projects, which includes:

* A robust Python library/application file hierarchy with packages, modules, clients, and documentation:
    * detailed, yet simple, ``setup.py``
    * retrievable ``README`` and ``CHANGELOG``
    * documentation deployed in `ReadTheDocs`_
    * the unusual adoption of the ``src`` directory layer (love it)
    * examples of packages and modules hierarchy
    * examples of Python command-line interfaces
* A unique testing framework for developers with `tox`_ and `pytest`_
    * guarantees tests are reproducible for all developers
    * ensures same lint rules are always applied (local and remotely)
    * ensures all desired Python versions are covered
    * adopts `hypothesis`_
* Fully automated continuous integration services with `GitHub Actions`_
    * automatic testing on Linux, MacOS, and Windows
    * automatic testing simulated upon deployment with ``tox``
    * test coverage report to Codecov
    * automated version bump with `bump2version`_, git tagging, and Python packaging to PyPI on Pull Request merge

Motivation
==========

To understand and implement the best practices for team-based development and automatic deployment of scientific software in Python. Also, I believe the strategy reviewed here can be applied to most general-purpose Python libraries.

This repository does **not** intend to be a `cookiecutter`_-like repository. There are very well documented cookiecutters, `even for scientific software`_, if you are looking for one of those.

When I started developing Python libraries, I decided that using a cookiecutter as a shortcut would lead me nowhere in the learning process of configuring CI services because I would miss the details of what was actually being implemented. Hence, assembling this *template* from scratch as a full working repository was the only best approach to achieve a minimum understanding of CI. Now, this repository serves as a reference guide for all my projects and hopefully will serve you too. I try to keep it up to date with my needs and the ever-evolving CI ecosystem.

Acknowledgments
===============

I want to acknowledge `ionel`_ discussions about *Packaging a python library*. They are a pillar in my understanding and decisions on this matter, and I really recommend reading his `blog post`_ and references herein.

I configured the CI pipeline to my needs by taking bits and pieces from many places. Kudos to `python-nameless`_ and `cookiecutter-pylibrary`_; two primary sources of information for the *python-project-skeleton* repository, especially in the first versions using Travis and Appveyor. When migrating to GitHub Actions, I fed on the workflows `@JoaoRodrigues <https://python-project-skeleton.readthedocs.io/>`_ assembled for `pdb-tools`_; on the `tox-gh-actions`_ package; and on `structlog`_, which was also a repository I used as a reference to build test latest version here.

I refer to other important sources of information as comments in the specific files. Thanks, everyone, for keeping discussions out there open.

How to use this repository
==========================

The `documentation`_ pages explain how to use this template for your projects and the implementation details adopted here. Use the documentation as a reference to learn the rationale behind this repository and also as a demonstration of how to deploy documentation in ReadTheDocs.

Issues and Discussions
======================

As usual in any GitHub based project, raise an `issue`_ if you find any bug or room for improvement (certainly there are many), or open a `discussion`_ (new feature!!) if you want to discuss or talk :-)

Version
=======

v0.8.0

.. _GitHub Actions: https://github.com/features/actions
.. _PyPI: https://pypi.org
.. _blog post: https://blog.ionelmc.ro/2014/05/25/python-packaging/
.. _bump2version: https://github.com/c4urself/bump2version
.. _cookiecutter-pylibrary: https://github.com/ionelmc/cookiecutter-pylibrary
.. _cookiecutter: https://cookiecutter.readthedocs.io/en/latest/index.html
.. _discussion: https://github.com/joaomcteixeira/python-project-skeleton/discussions
.. _documentation: https://python-project-skeleton.readthedocs.io/
.. _even for scientific software: https://github.com/MolSSI/cookiecutter-cms
.. _hypothesis: https://hypothesis.readthedocs.io/en/latest/
.. _ionel: https://github.com/ionelmc
.. _issue: https://github.com/joaomcteixeira/python-project-skeleton/issues
.. _latest branch: https://github.com/joaomcteixeira/python-project-skeleton/tree/latest
.. _master branch: https://github.com/joaomcteixeira/python-project-skeleton/tree/master
.. _pdb-tools: https://github.com/haddocking/pdb-tools
.. _project's documentation: https://python-project-skeleton.readthedocs.io/en/latest/index.html
.. _pytest: https://docs.pytest.org/en/stable/
.. _python-nameless: https://github.com/ionelmc/python-nameless
.. _structlog: https://github.com/hynek/structlog
.. _test.pypi.org: https://test.pypi.org
.. _tox-gh-actions: https://github.com/ymyzk/tox-gh-actions
.. _tox: https://tox.readthedocs.io/en/latest/
.. _ReadTheDocs: https://readthedocs.org/
",2022-08-12
https://github.com/joaomcteixeira/farseernmr2,"# farseernmr2
experimental repository testing ideas for farseernmr2

- full refactor
- new implementations
- online documentation
- etc

visit the current stable version of the Farseer-NMR Project [here](https://github.com/Farseer-NMR/FarSeer-NMR)
",2022-08-12
https://github.com/joaomcteixeira/HyperSimpleDocstring2Md,"# HyperSimpleDocstring2Md

A Hyper Simple Docstring to Markdown creator. Creates hierarchically indexed [Markdown](https://en.wikipedia.org/wiki/Markdown) files from [Python](https://www.python.org/) libraries' [docstrings](https://www.python.org/dev/peps/pep-0257/). Fully based on Python standard library.

# Why?

It is extremely important to maintain software documentation up to date with the latest releases.

There are many packages out there to efficiently handle the documentation of Python projects, those can extract updated docstrings and setup beautiful web pages accordingly. But, such complex tools require a considerable amount of time and effort to master, and developers should thrive to keep up to date with the most novel routines and styles of documentation representation.

Moreover, in my opinion, these projects fail to address a specific collective of developers: those who thrive, above all, for extreme simplicity, long-term stability and universality of their output, in this case software documentation.

# Solution

**Hyper Simple Docstring 2 Markdown** creator was designed to recursively extract DOCSTRINGS from a target Python package and write them to a simple yet organized and indexed Markdown formatted file. It intents to be exactly that, **hyper simple** in both input and output.

The output Mardown file can be directly used as a GitHub wiki page, for example.

## Implementation

It uses Python +3.7 standard libraries, such as [pydoc](https://docs.python.org/3.7/library/pydoc.html) and the [inspect module](https://docs.python.org/3/library/inspect.html).

## Which docstrings are covered?

In the current version, DOCSTRINGS from packages (`__init__.py`), modules, classes and functions are extracted.

# How to use it

No installation is required and you only need to pass **one** argument, **it's HYPER SIMPLE**:
- the PATH to your Python package

```
python hypersimpledocstring2md.py <PATH TO YOUR LIBRARY>
```

This will recursively read your library and generate a `docs.md` file containing all referred docstrings in an organized manner. All subpackages MUST have an `__init__.py` file.

## but there are also additional features

For example:

- you can add a base web url in order to link the index with the Markdown headers, it will result in `<link>#<header>`, very useful for GitHub pages.

```
$ python hypersimpledocstring2md.py -h
usage: hypersimpledocstring2md.py [-h] [--baselink baselink] [--output OUTPUT]
                                  path

Hyper Simple Docstring 2 Markdown - A routine to create a single Markdown
formatted file containing the documentation DOCSTRINGS of a target Python
library.

positional arguments:
  path                 PATH to Library.

optional arguments:
  -h, --help           show this help message and exit
  --baselink baselink  The base Web URL where the .md will be hosted toallow
                       Index-Header linking. Defaults to no link.
  --toplink toplink    Adds a quick link to the Index bellow each header.
                       Defaults to True.
  --output OUTPUT      The OUTPUT Markdown file. Defaults to 'docs.md'.
```

## examples

HyperSimpleDocstring2Md is used to generate the Tauren-MD's documentation, [take a look](https://github.com/joaomcteixeira/Tauren-MD/wiki/Modules-Documentation). 

# LICENSE

This software is licensed under the [Unlicense](https://github.com/joaomcteixeira/HyperSimpleDocstring2Md/blob/master/LICENSE) as a demonstration of my gratitute to the whole Python community that cheerily and altruistically share knowledge on the Web. Thanks to the community.
",2022-08-12
https://github.com/joaomcteixeira/libfuncpy,"=========
libfuncpy
=========

.. image:: https://github.com/joaomcteixeira/libfuncpy/workflows/tests/badge.svg?branch=main
    :target: https://github.com/joaomcteixeira/libfuncpy/actions?workflow=tests
    :alt: Test Status

.. image:: https://github.com/joaomcteixeira/libfuncpy/workflows/build/badge.svg?branch=main
    :target: https://github.com/joaomcteixeira/libfuncpy/actions?workflow=build
    :alt: Package Build

.. image:: https://codecov.io/gh/joaomcteixeira/libfuncpy/branch/main/graph/badge.svg?token=AOJFM3HPJF
    :target: https://codecov.io/gh/joaomcteixeira/libfuncpy
    :alt: Codecov

.. image:: https://api.codeclimate.com/v1/badges/262a24f8c06d6f27ebd6/maintainability
    :target: https://codeclimate.com/github/joaomcteixeira/libfuncpy/maintainability
    :alt: Maintainability

.. image:: https://readthedocs.org/projects/libfuncpy/badge/?version=latest
    :target: https://libfuncpy.readthedocs.io/en/latest/?badge=latest
    :alt: Documentation Status

.. image:: https://img.shields.io/pypi/pyversions/libfuncpy
    :target: https://pypi.org/project/libfuncpy/
    :alt: PyPI - Python Version

.. image:: https://img.shields.io/pypi/v/libfuncpy
    :target: https://pypi.org/project/libfuncpy/
    :alt: PyPI

Motivation
----------

Functional Programming tools in Python - extending beyond map, filter,
reduce, and partial.

.. _discussion: https://github.com/joaomcteixeira/libfuncpy/discussions
.. _documentation: https://libfuncpy.readthedocs.io/
",2022-08-12
https://github.com/joaomcteixeira/passgen,"An utterly simple password generator made in Python.

By default, a password contains lower and upper case characters, digits,
and punctuation. You can disable these different types separately.

**Usage:**

```bash
    python passgen.py -h
    python passgen.py
    python passgen.py -pu
```

**Example:**

```bash
$ python passgen.py 
eTJ1s.n_p,JBRVn?
```

```bash
$ python passgen.py -pu -l 10
6T0dHq2djO
```
",2022-08-12
https://github.com/joaomcteixeira/presentationplotlib,"# presentationplotlib

![version](https://img.shields.io/static/v1.svg?label=version&message=0.1&color=9cf)

This is an experimental project where I use [matplotlib](https://matplotlib.org/) to create PDF based scientific presentations.

If you want to create PPT presentations with Python please see other libraries like [python-pptx](https://python-pptx.readthedocs.io/en/latest/index.html).

## How to run

Use the `slide_example.py` template file to create your slides.  

Slides scripts should be defined as a function `slide(page_number, figure=None, ax=None)` and decorated with the template of your choice.  

Currently, there is only one template: that of my presentations `¬¬` - `Template1` inside [templates.py](https://github.com/joaomcteixeira/presentationplotlib/blob/master/templates.py). Templates are classes with all kind of attributes and methods that define the template behaviour.  

Prepare a `slide_WHATEVER.py` file for each slide.

You can generate a PDF file for each slide running the _slide_ script independently or, instead, generate a series of slides using `gen_slides.py`, which is useful to insert page numbers in slides (depending on the template).

```
python gen_slides.py slide1.py slide2.py slide3.py ETC...
```

This will generate a PDF file for each slide passed.

Use the program of your choice to concatenate those PDFs. Normally, I generate the full presentation with the help of a *bash* [script](https://github.com/joaomcteixeira/presentationplotlib/blob/master/make_slides.sh) and [pdfunite](http://www.manpagez.com/man/1/pdfunite/) which comes natively with my Linux distribution ([Xubuntu](https://xubuntu.org/) here `:-)`).
",2022-08-12
https://github.com/joaomcteixeira/Protein_Data_Bank_Parameters,"============================
Protein Data Bank Parameters
============================

A Python package hosting the static parameters for the Protein Data Bank file formats.

.. start-badges

.. image:: https://img.shields.io/travis/joaomcteixeira/python-project-skeleton/latest?label=TravisCI
    :target: https://travis-ci.org/joaomcteixeira/python-project-skeleton
    :alt: Travis-CI latest branch

.. image:: https://img.shields.io/readthedocs/python-project-skeleton/latest?label=Read%20the%20Docs
    :target: https://python-project-skeleton.readthedocs.io/en/latest/index.html
    :alt: Read the Docs (latest)

.. image:: https://img.shields.io/pypi/wheel/taurenmd.svg
    :alt: PyPI Wheel
    :target: https://pypi.org/project/taurenmd

.. image:: https://img.shields.io/pypi/pyversions/taurenmd.svg
    :alt: Supported versions
    :target: https://pypi.org/project/taurenmd

.. image:: https://img.shields.io/pypi/dm/taurenmd?label=PyPI%20Downloads
    :alt: PyPI - Downloads
    :target: https://pypistats.org/packages/taurenmd

.. end-badges

Motivation
==========

Handling Protein Data Bank data through Python requires a constantly retyping of the PDB format static parameters,
such as, line parsing slices, atoms names, residue names, etc. This package hosts all those static parameters
required to handle ``.pdb`` files.

Installation
============

```bash
pip install --upgrade pdbparams
```

",2022-08-12
https://github.com/joaomcteixeira/python-bioplottemplates,"========
Overview
========

_this is an experimental project._

.. start-badges

.. list-table::
    :stub-columns: 1

    * - docs
      - |docs|
    * - tests
      - | |travis| |appveyor|
        | |coveralls| |codecov|
        | |codacy| |codeclimate| |technical-debt|
    * - package
      - | |version| |wheel| |supported-versions| |supported-implementations|
        | |commits-since| |PyPI-downloads|
.. |docs| image:: https://readthedocs.org/projects/python-bioplottemplates/badge/?style=flat
    :target: https://readthedocs.org/projects/python-bioplottemplates
    :alt: Documentation Status

.. |travis| image:: https://api.travis-ci.org/joaomcteixeira/python-bioplottemplates.svg?branch=master
    :alt: Travis-CI Build Status
    :target: https://travis-ci.org/joaomcteixeira/python-bioplottemplates

.. |appveyor| image:: https://ci.appveyor.com/api/projects/status/github/joaomcteixeira/python-bioplottemplates?branch=master&svg=true
    :alt: AppVeyor Build Status
    :target: https://ci.appveyor.com/project/joaomcteixeira/python-bioplottemplates

.. |coveralls| image:: https://coveralls.io/repos/joaomcteixeira/python-bioplottemplates/badge.svg?branch=master&service=github
    :alt: Coverage Status
    :target: https://coveralls.io/github/joaomcteixeira/python-bioplottemplates

.. |codecov| image:: https://codecov.io/github/joaomcteixeira/python-bioplottemplates/coverage.svg?branch=master
    :alt: Coverage Status
    :target: https://codecov.io/github/joaomcteixeira/python-bioplottemplates

.. |codacy| image:: https://img.shields.io/codacy/grade/7cbcb86d52e6494f81f5ca0ee377ff44.svg
    :target: https://www.codacy.com/app/joaomcteixeira/python-bioplottemplates
    :alt: Codacy Code Quality Status

.. |codeclimate| image:: https://codeclimate.com/github/joaomcteixeira/python-bioplottemplates/badges/gpa.svg
    :target: https://codeclimate.com/github/joaomcteixeira/python-bioplottemplates
    :alt: CodeClimate Quality Status

.. |technical-debt| image:: https://img.shields.io/codeclimate/tech-debt/joaomcteixeira/python-bioplottemplates
    :target: https://codeclimate.com/github/joaomcteixeira/python-bioplottemplates
    :alt: Code Climate technical debt

.. |version| image:: https://img.shields.io/pypi/v/bioplottemplates.svg
    :alt: PyPI Package latest release
    :target: https://pypi.org/project/bioplottemplates

.. |wheel| image:: https://img.shields.io/pypi/wheel/bioplottemplates.svg
    :alt: PyPI Wheel
    :target: https://pypi.org/project/bioplottemplates

.. |supported-versions| image:: https://img.shields.io/pypi/pyversions/bioplottemplates.svg
    :alt: Supported versions
    :target: https://pypi.org/project/bioplottemplates

.. |supported-implementations| image:: https://img.shields.io/pypi/implementation/bioplottemplates.svg
    :alt: Supported implementations
    :target: https://pypi.org/project/bioplottemplates

.. |commits-since| image:: https://img.shields.io/github/commits-since/joaomcteixeira/python-bioplottemplates/v0.1.1.svg
    :alt: Commits since latest release
    :target: https://github.com/joaomcteixeira/python-bioplottemplates/compare/v0.1.1...master

.. |PyPI-downloads| image:: https://img.shields.io/pypi/dd/bioplottemplates?label=PyPI%20download
    :alt: PyPI - Downloads
    :target: https://pypi.org/project/bioplottemplates

.. end-badges

Plotting templates common in biological sciences.

Installation
============

::

    pip install bioplottemplates

You can also install the in-development version with::

    pip install https://github.com/joaomcteixeira/python-bioplottemplates/archive/master.zip


Documentation
=============


https://python-bioplottemplates.readthedocs.io/


Development
===========

To run the all tests run::

    tox

",2022-08-12
https://github.com/joaomcteixeira/python-project-skeleton,"Python Package Skeleton Template
================================

.. image:: https://github.com/joaomcteixeira/python-project-skeleton/workflows/ci/badge.svg?branch=main
    :target: https://github.com/joaomcteixeira/python-project-skeleton/actions?workflow=ci
    :alt: CI

.. image:: https://codecov.io/gh/joaomcteixeira/python-project-skeleton/branch/main/graph/badge.svg
    :target: https://codecov.io/gh/joaomcteixeira/python-project-skeleton
    :alt: Codecov

.. image:: https://api.codeclimate.com/v1/badges/d96cc9a1841a819cd4f5/maintainability
   :target: https://codeclimate.com/github/joaomcteixeira/python-project-skeleton/maintainability
   :alt: Maintainability

.. image:: https://img.shields.io/codeclimate/tech-debt/joaomcteixeira/python-project-skeleton
    :target: https://codeclimate.com/github/joaomcteixeira/python-project-skeleton
    :alt: Code Climate technical debt

.. image:: https://img.shields.io/readthedocs/python-project-skeleton/latest?label=Read%20the%20Docs
    :target: https://python-project-skeleton.readthedocs.io/en/latest/index.html
    :alt: Read the Docs

Summary
-------

This repository is a **skeleton template** for a **Python application/library**
compliant with the latest team-development and software deployment practices
within a continuous integration (CI) framework. You can use this repository as a
source of information and a resource to study CI. Also, you can use this
repository as a direct template for your repositories.

**Note** that this repository reflects the setup I like the most and that covers
the CI needs for my Python projects, which include:

* A robust Python library/application file hierarchy with packages, modules, clients, and documentation:
    * detailed, yet simple, ``setup.py``
    * retrievable ``README`` and ``CHANGELOG``
    * documentation deployed in `ReadTheDocs`_
    * the unusual adoption of the ``src`` directory layer (love it)
    * examples of packages and modules hierarchy
    * examples of Python command-line interfaces
* A unique testing framework for developers with `tox`_ and `pytest`_
    * guarantees tests are reproducible for all developers
    * ensures same lint rules are always applied (local and remotely)
    * ensures all desired Python versions are covered
    * adopts `hypothesis`_
* Fully automated continuous integration services with `GitHub Actions`_
    * automatic testing on Linux, MacOS, and Windows
    * automatic testing simulated upon deployment with ``tox``
    * test coverage report to Codecov
    * automated version bump with `bump2version`_, git tagging, and Python packaging to PyPI on Pull Request merge

Motivation
----------

I developed this repository to understand how to implement the best practices
for team-based development and automatic deployment of a scientific software
written in Python. Nonetheless, I believe the strategy reviewed here can be
applied to most general-purpose Python libraries.

This repository does **not** intend to be a `cookiecutter`_-like repository.
There are very well documented cookiecutters, `even for scientific software`_,
if you are looking for one of those. However, when I started developing Python
libraries, I decided that blindly using a cookiecutter would not provide me the
learning insights from configuring CI services because I would miss the details
of what was actually being implemented. Hence, assembling this *template* from
scratch as a full working repository was my best approach to obtain a useful
understanding of CI.  Now, this repository serves as a reference guide for all
my projects and hopefully will serve you, too. I keep constantly updating this
repository, expect one to two updates/reviews per year.

Acknowledgments
---------------

I want to acknowledge `ionel`_ discussions about *Packaging a python library*.
They are a pillar in my understanding and decisions on this matter, and I really
recommend reading his `blog post`_ and references herein.

I configured the CI pipeline to my needs by taking bits and pieces from many
places. Kudos to `python-nameless`_ and `cookiecutter-pylibrary`_; two primary
sources of information for the *python-project-skeleton* repository, especially
in the first versions using Travis and Appveyor.

When migrating to GitHub Actions, I based my choices on the version bump and
deploying workflows `@JoaoRodrigues <https://github.com/JoaoRodrigues>`_
assembled for `pdb-tools`_; on the `tox-gh-actions`_ package; and on
`structlog`_. Implementation details have evolved with newest versions.

I refer to other important sources of information as comments in the specific
files. Thanks, everyone, for keeping open discussions on internet.

How to use this repository
--------------------------

The `documentation`_ pages explain how to use this template for your projects
and the implementation details adopted here. The documentation pages also serve
to demonstrate how to compile documentation with Sphinx and deploy it online
with `ReadTheDocs`_.

Issues and Discussions
----------------------

As usual for any GitHub-based project, raise an `issue`_ if you find any bug or
want to suggest an improvement, or open a `discussion`_ if you want to discuss
or chat :wink:

Projects using this skeleton
----------------------------

Below, a list of the projects using this repository as template or as base for
their CI implementations:

* `julie-forman-kay-lab/IDPConformerGenerator <https://github.com/julie-forman-kay-lab/IDPConformerGenerator>`_
* `haddocking/HADDOCK3 <https://github.com/haddocking/haddock3>`_
* `THGLab/MCSCE <https://github.com/THGLab/MCSCE>`_
* `joaomcteixeira/taurenmd <https://github.com/joaomcteixeira/taurenmd>`_
* `MDAnalysis/mdacli <https://github.com/MDAnalysis/mdacli>`_

If you use this repository as a reference for your works, let me know, so I
list your work above, as well.

Version
-------

v0.11.1

.. _GitHub Actions: https://github.com/features/actions
.. _PyPI: https://pypi.org
.. _blog post: https://blog.ionelmc.ro/2014/05/25/python-packaging/
.. _bump2version: https://github.com/c4urself/bump2version
.. _cookiecutter-pylibrary: https://github.com/ionelmc/cookiecutter-pylibrary
.. _cookiecutter: https://cookiecutter.readthedocs.io/en/latest/index.html
.. _discussion: https://github.com/joaomcteixeira/python-project-skeleton/discussions
.. _documentation: https://python-project-skeleton.readthedocs.io/
.. _even for scientific software: https://github.com/MolSSI/cookiecutter-cms
.. _hypothesis: https://hypothesis.readthedocs.io/en/latest/
.. _ionel: https://github.com/ionelmc
.. _issue: https://github.com/joaomcteixeira/python-project-skeleton/issues
.. _latest branch: https://github.com/joaomcteixeira/python-project-skeleton/tree/latest
.. _master branch: https://github.com/joaomcteixeira/python-project-skeleton/tree/master
.. _pdb-tools: https://github.com/haddocking/pdb-tools/blob/2a070bbacee9d6608b44bb6d2f749beefd6a7690/.github/workflows/bump-version-on-push.yml
.. _project's documentation: https://python-project-skeleton.readthedocs.io/en/latest/index.html
.. _pytest: https://docs.pytest.org/en/stable/
.. _python-nameless: https://github.com/ionelmc/python-nameless
.. _structlog: https://github.com/hynek/structlog
.. _test.pypi.org: https://test.pypi.org
.. _tox-gh-actions: https://github.com/ymyzk/tox-gh-actions
.. _tox: https://tox.readthedocs.io/en/latest/
.. _ReadTheDocs: https://readthedocs.org/
",2022-08-12
https://github.com/joaomcteixeira/ScientificPresentations,"# Scientific Presentations

This repository hosts the code for my scientific presentations.

Final PDFs are not stored here, you will find a link in each subfolder to the final PDF which, most likely, will be hosted in my account in [Research Gate](https://www.researchgate.net/profile/Joao_Teixeira12/publications).

You may find of your interest [my other repository](https://github.com/joaomcteixeira/presentationplotlib) where I forge a small library using [matplotlib](https://matplotlib.org/) to draw PDF presentations.
",2022-08-12
https://github.com/joaomcteixeira/taurenmd,"taurenmd
========

.. image:: https://raw.githubusercontent.com/joaomcteixeira/taurenmd/master/docs/img/taurenmd_logo_black_readme.png

.. start-description

.. image:: https://img.shields.io/pypi/v/taurenmd.svg
    :alt: PyPI Package latest release
    :target: https://pypi.org/project/taurenmd

.. image:: https://joss.theoj.org/papers/10.21105/joss.02175/status.svg
    :target: https://doi.org/10.21105/joss.02175
    :alt: joss

.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.3551990.svg
    :target: https://doi.org/10.5281/zenodo.3551990
    :alt: Zenodo

.. image:: https://github.com/joaomcteixeira/taurenmd/workflows/tests/badge.svg?branch=master
    :target: https://github.com/joaomcteixeira/taurenmd/actions?workflow=tests
    :alt: Test Status

.. image:: https://img.shields.io/readthedocs/taurenmd/latest?label=Read%20the%20Docs
    :target: https://taurenmd.readthedocs.io/en/latest/index.html
    :alt: Read the Docs (latest)

.. image:: https://codecov.io/gh/joaomcteixeira/taurenmd/branch/master/graph/badge.svg
    :target: https://codecov.io/gh/joaomcteixeira/taurenmd
    :alt: Codecov master branch

.. image:: https://api.codeclimate.com/v1/badges/d69e2e9866338d88955c/maintainability
   :target: https://codeclimate.com/github/joaomcteixeira/taurenmd
   :alt: Code Climate

.. image:: https://img.shields.io/codeclimate/tech-debt/joaomcteixeira/taurenmd?label=Code%20Climate%20tech%20debt
    :target: https://codeclimate.com/github/joaomcteixeira/taurenmd
    :alt: Code Climate technical debt

.. image:: https://img.shields.io/pypi/wheel/taurenmd.svg
    :alt: PyPI Wheel
    :target: https://pypi.org/project/taurenmd

.. image:: https://img.shields.io/pypi/pyversions/taurenmd.svg
    :alt: Supported versions
    :target: https://pypi.org/project/taurenmd

.. image:: https://img.shields.io/pypi/dm/taurenmd?label=PyPI%20Downloads
    :alt: PyPI - Downloads
    :target: https://pypistats.org/packages/taurenmd

**A command-line interface for analysis routines of Molecular Dynamics data.**

**Taurenmd** provides an easy, flexible and extensible, **command-line** interface for the most common *(and not so common)* routines of analysis and representation of Molecular Dynamics (MD) data.

It bridges the gap between the highly complex (and powerful) Python libraries available for analysis of MD data and the *non-developer* users that lack the programming skills to perform a thorough and proficient use those libraries. *But not only*, **taurenmd** also facilitates high throughput operations, even to those proficient *devs*, because complex executions are reduced to single argument-rich command-lines that can be concatenated or aliased.

**Taurenmd** wraps around feature-rich and powerful MD analysis libraries such as `MDAnalysis <https://www.mdanalysis.org/>`_ and `MDTraj <http://mdtraj.org>`_ *(but not only)*, combining them to extract the best of *those worlds*. We use these libraries to access and extract MD data and calculate observables, and we have also added our own routines of analysis when needed. When using this software, you **should** cite **taurenmd** together with the dependencies used, please read our `Citing <https://taurenmd.readthedocs.io/en/latest/citing.html>`_ page for a detailed explanation.

Though designed to perform as a command-line user-directed interface, all **taurenmd**'s core functions are openly distributed and documented. Currently, there are already `several command-line interfaces available <https://taurenmd.readthedocs.io/en/latest/reference/clients.html>`_, some that perform only single tasks, while others allow complex setups, all are *one-liners*.

With this said, **taurenmd** aims to be a flexible and extensible peace of software, built as simple and modular as we can think of, to *agile* the incorporation of new functionalities as needed.

.. end-description

Documentation
=============

**taurenmd**'s full documentation is available at: https://taurenmd.readthedocs.io, read there:

#. how to install
#. usage examples
#. citing
#. *etc...*
",2022-08-12
https://github.com/joaomcteixeira/Tree-of-Life,"# Tree-of-Life

[![DOI](https://zenodo.org/badge/162114154.svg)](https://zenodo.org/badge/latestdoi/162114154) [![version](https://img.shields.io/static/v1.svg?label=version&message=1.1.4&color=green)](https://zenodo.org/badge/latestdoi/162114154)

**Tree-of-Life** is a _Python-written_ and _stand-alone_ **library** that installs the Python dependencies, configures the _executable files_ and provides an updating interface for Python-based projects oriented for *non-developer* users. Reducing the task of installation and updating to simple *double-click* operations *(something that was so common in the past but which is becoming rare to find in most Scientific softwares today)*.

As the name implies, it provides a _tree_ of routines that give _life_ to your project (inside user's computers) `:-)`.

## Vision

Read [here](https://github.com/joaomcteixeira/Tree-of-Life/blob/master/VISION.md) about the complete motivation and vision behind Tree-of-Life.

## Projects which have adopted Tree-of-Life

Hosting Tree-of-Life in a project does not prevent it being available through other sources, such as, Conda packages or PiPy. Some Scientific projects already use Tree-of-Life to manage their installation and updating routines:

- [Farseer-NMR](https://github.com/Farseer-NMR/FarSeer-NMR)
- [Tauren-MD](https://github.com/joaomcteixeira/Tauren-MD)

## How to implement Tree-of-Life in your projects

If you are a software developer with Python-based or Python-dependent projects, and you wish to know how to implement Tree-of-Life in your project, find all information in our [Wiki page](https://github.com/joaomcteixeira/Tree-of-Life/wiki) or `docs` folder. Implementing Tree-of-Life is *almost* as easy as copy+pasting a folder `;-)`.

## LICENSE

Tree-of-Life is licensed under [LGPL version 3](https://github.com/joaomcteixeira/Tree-of-Life/blob/master/LICENSE), and you are allowed to modify and use it according to this license terms.

![LGPL](https://www.gnu.org/graphics/lgplv3-with-text-154x68.png)

![LOGO](https://github.com/joaomcteixeira/Tree-of-Life/blob/master/docs/ToL_diss_logo.png)",2022-08-12
https://github.com/jobbo90/Landsat-8-pre-processing,"# Landsat imagery
pre-processing, processing and analyzing Landsat 8 imagery in R language format


",2022-08-12
https://github.com/jobbo90/offshore_boundary,"## Multi-decadal coastline dynamics controlled by migrating sub-tidal mudbanks

[![GitHub release](https://img.shields.io/endpoint?color=blue&label=v0.2&url=https%3A%2F%2Fgithub.com%2Fjobbo90%2Foffshore_boundary%2Freleases%2F)](https://github.com/jobbo90/offshore_boundary/releases/)

<img src=""results/GIF/pos230000-animation.gif?raw=true"" width=""800px"">


## Description

The approach is described in detail in the following publications;
1. Data driven approach to derive image end-members for linear spectral unmixing: https://doi.org/10.1016/j.jag.2020.102252
2. Time series analysis of coastline changes in response to migrating mudbanks (submitted 07-2021)

The coastline position estimates for Suriname are available through the Google Earth Engine (GEE) code editor via: 
https://code.earthengine.google.com/?accept_repo=users/jobdevries90/MangroMud

## Project organization

```
.
├── .gitignore
├── CITATION.md
├── LICENSE.md
├── README.md
├── requirements.txt
├── data               
│   ├── processed      
│   ├── raw
├── results
│	├── GIF
│	├── methodology_figures
│	├── temp_maps
│	├── Validation
└── src

```


## usage
In order to create coastline position estimates and quantify annual changes with respect to alongshore migrating mudbanks the following steps are required.
1. Separate land and water by applying Otsu thresholding approach in GEE
2. Define image end-members for the purpose of linear spectral unmixing (LSU) in GEE
3. Intersection of the shorelines with pre-defined shore-normal transects in GEE
4. Extract mud abundance estimates from the LSU for each transect from GEE 
5. post-process the estimated coastline postions in R
6. post-process mud abundances for estimating the pressence or absence of mudbanks in R

steps 1 - 4 are explained in the GEE repository. The data in data/raw/GEE_exports shows the results for Suriname. 
In step 5 - 6 this data is used for post-processing. 

The result is visualized for Suriname in the GEE repository

## License

This project is licensed under the terms of the [MIT License](/LICENSE.md)

## Citation

Please [cite this project as described here](/CITATION.md).
",2022-08-12
https://github.com/JoostGadellaa/bachelors-thesis,"# A Networked Evolutionary Trust Game for the Sharing Economy
```
Joost Gadellaa
Utrecht University
Bachelor Thesis
Economics and Business Economics
```

## Abstract

In the sharing economy, trust is of higher importance than in regular B2C
interactions because there is no transfer of ownership and transactions sometimes
take place in private space. Chica et al. (2017) developed an evolutionary trust game
to unveil occurring dynamics and explain how trust could evolve in the sharing
economy. This thesis adds the variable of network structure for the network of
possible interactions to their model. Using agent-based modelling, the model is ran
on networks with varying levels of community structure, systematically varying
average degree, the community connectedness and game payoffs. We find negative
correlations for degree and it is suggested that having more isolated communities has
a positive effect on trust, but this is highly dependent on the reward for cooperating.
Chica et al. (2017) showed strong interdependence between players of different
strategies and found a strong influence of payoffs on the dynamics. The current
research shows that a strong interaction with network structure should also be
considered. The presented findings progress the field of evolutionary game theory, by
learning from a specific application. Furthermore, findings also suggest that sharing
economy platforms could enhance trust by emphasising or creating communities,
depending on the risk and how clear benefits are to users.

_Keywords_ : sharing economy, trust, evolutionary game theory, agent-based
modelling, simulation, network structure, community structure

## A Networked Evolutionary Trust Game for the Sharing Economy

For a long time, ownership was one of the fundamental constructs of
economics. Although shared consumption is as old as humankind (Belk, 2014), the
internet has removed barriers for bringing providers and consumers together in the
so-called sharing economy. It is estimated that by 2025, the global market size will
reach $335 billion (Lieberman, 2015). Despite this, the concept of the sharing
economy is not a homogeneous concept. It has been argued that the term is a
misnomer (Eckhardt & Bardhi, 2015) and different kinds of C2C transactions with a
social element have been labelled as sharing economy, causing inconsistency in the
literature (Habibi, Kim, & Laroche, 2016). In this paper, the following definition by
ter Hurne, Ronteltap, Corten, & Buskens (2017), will be used: “[The sharing economy
is] an economic model based on sharing under-utilised assets between peers without
the transfer of ownership, ranging from spaces, to skills, to stuff, for monetary or
non‐monetary benefits via an online mediated platform.”

The fact that transactions occur between peers, and that there is no transfer
of ownership make it require a considerable amount of trust (McNight & Chervany,
2001; Botsman 2012), and the ability to create and maintain trust is often identified
as one of the critical factors for a sharing platform’s success (Ufford, 2015; Strader &
Ramaswami, 2002). To examine the role, moderators and antecedents of trust in the
sharing economy, various methods have been used in the past. Mainly survey data,
but also case studies, econometric analysis, field interviews and experiments are
employed (Gefen, Benbasat, & Pavlou, 2014), each with their own limitations.

Chica, Chiong, Adam, Damas & Teubner (2017) were the first to introduce
the usage of evolutionary game theory (EGT) in an agent-based model (ABM) to
explore system dynamics and population end states for the spread of trust in the
sharing economy. They simulated virtual agents having sharing economy interactions
and showed that trust can be formed when benefits are high enough, except if the
ratio of trusting and trustworthy players is low from the start. Even when the reward
values are low, a situation of trust can still emerge as long as the starting population
included enough trusting and trustworthy players. This methodology of combining


EGT with ABM is relatively new to the field of economics, and gives new
possibilities: ABM can generate amounts of data, with exact control over ‘treatments’
simply impossible with experiments, and the fact that a simulation has to be
programmed requires the research to be very precise about assumptions. However,
Chica et al. (2017) limited the agents in the reach of their interactions by putting
them on an empirically observed email network. It is unknown to what extent the
specificities of that network could have influenced results. This thesis will build upon
their model and methods by adding network structure as a model variable, and
creating software which allows for visual inspection of the network over the course of
the simulation. The simulation will be run multiple times, on different network
structures, while keeping the number of nodes constant, extending this application of
EGT on the sharing economy.

This paper’s scientific contribution will thereby be twofold. Firstly, it
contributes to the development of a theoretical framework using EGT and ABM
combined with aspects from network science. Although the perspective from network
science has previously been used in the context of the sharing economy to explain the
spread of information (e.g. Buskens, 1998; Frey, Buskens, & Corten, 2019), research
on how the network structure affects emergence of trust in isolation of reputation and
communication is relatively new. In this line of theoretical research, all the other
methods of enhancing trust are omitted, and the only ‘motivation’ for fictional
players are the payoffs a trusting or trustworthy strategy can bring them. Developing
this experimental application to the sharing economy could advance the method as a
whole.

Secondly, because the game design is tailored to the sharing economy, this
research might result in new insights for sharing economy platforms on how to
enhance the aspect of trust that is influenced by network structure. Although there is
little data available, it can be theorised platforms have different networks of possible
and occurring connections. Determinants of structure could include selection based
on geography (e.g., in the case of tool or other small asset sharing), field of interest
(e.g., book swapping), sharing a destination (e.g., ride or home sharing), or homogeneity effects within the network as shown by Schor, Fitzmaurice, Carfagna,
Attwood-Charles, & Poteat (2016). Testing the effects of network structure on trust
in isolation of other means could help explain why trust spreads differently on
different platforms. Besides explanatory power, knowing about the extent to which
the spread and stability of trust are influenced by network structure could possibly
be used by platforms to enhance it.

To resemble a structure created by the selection and homogeneity described
above, this research will use networks with a community structure, a common
characteristic of networks (Girvan & Newman, 2002; Fortunato, 2010; Porter,
Onnela, & Mucha 2009). A community structure was also observed in the email-
network used by Chica et al. (2017) as foundation for their model (Guimera, Danon,
Diaz-Guilera, Giralt, & Arenas, 2003). In this structure, nodes are primarily
connected to peers in the same community, creating different highly clustered
communities, while a smaller part of the edges is in between communities.^1 There are
two reasons why this structure is particularly useful as a concept. Firstly, it is
relatively simple to comprehend and apply to the real world. Homophily and other
causes of clustering can be theorised or observed by an outsider without knowing the
exact network structure; the concept of linkage between them is also relatively easy
to grasp. The second reason is scalability. Because the number of edges changed from
intra-community to inter-community is a relative concept, unravelled dynamics could
possible apply to different scales. This ratio of edges changed will be referred to as
the variable ‘rewiring’ throughout this paper.

The research will proceed as follows. In section II, some theoretical
background and related work will be presented, followed by the technical details of
the model in section III. Section IV presents the setup of the experiments and their
results. Finally, conclusions are presented in section V, followed by some limitations,
implications, and possible directions for future research.

(^1) In the network science literature, an individual is referred to as a ‘node’. In game theory context the
individual is referred to as a ‘player’, while in the context of simulations it is often called an ‘agent’.
These refer to the same thing. A connection between two nodes is an ‘edge’.


### Background and Motivation

#### Trust in the Sharing Economy

As stated in the introduction, trust is a critical factor for the sharing economy.
Participants need to be sure that transaction partners do not behave
opportunistically and deviate from the agreement made. Because there is no transfer
of ownership, providers risk damage or misuse of their assets. Consumers, on the
other hand, usually do not have an established seller or governing institution to fall
back to in case the asset is not as promised. This uncertainty makes the effects and
likelihood of opportunism in C2C transactions more severe than in traditional B2C
interactions. Additionally, when a transaction involves co-usage (e.g., ride sharing,
renting out part of your house/couch), its quality is dependent on the experience
with the provider, adding an extra level of complexity, and more need for a trust
relationship between provider and consumer (Karlsson & Kemperman, 2017).

Common ways for platforms to create and maintain trust include binary or
scalar ratings (Teubner et al. 2017; Zervas et al. 2015), subsequent reviews about the
transaction (Abramova et al. 2015; Bridges and Vásquez 2016), host self-disclosure
by means of personal descriptions and pictures (Ma et al. 2017; Tussyadiah 2016; Ert
et al. 2016; Fagerstrøm et al. 2017; Teubner et al. 2014), or more formal measures
like insurance and identity verification.

However, even in the absence of these measures, trust and cooperation can
emerge. In the last two decades, the search for models accounting for complex
cooperation behaviour in social, economic and biological systems has inspired a new
body of interdisciplinary research that uses the methods of EGT (Smith & Price,
1973; Smith, 1998). EGT acknowledges the fact that the outcome of a system is more
than the sum of pairwise interactions; it dismisses the assumption of rationality and
introduces the concepts of a player population and player fitness. Fitness in this
context means the sum of payoffs for an individual received during games. In EGT,
successful behaviour spreads throughout the population by evolution, which is not
(necessarily) rational decision making. The evolution rule often entails that players
with unsuccessful strategies, and therefore a low fitness, die off, and get replaced by players with high fitness. The framework can also be used to simulate learning in an
unchanging population, as is done in the presented sharing economy model.

An insight from EGT very much applicable to this model, is that when
strategy distributions change, the relative fitness of the remaining strategies may also
change. The fitness landscape is not static, but it also evolves as the distribution of
strategies changes (Izquierdo, Izquierdo, & Sandholm, in press). On its own, this does
imply that Pareto-optimal strategies, which benefit the whole population (in this
model, being a trusting or trustworthy participant of the sharing economy), will
necessarily survive. Cooperating with another player has a risk, and being
opportunistic is often the best strategy in an isolated game, so a situation of trust
can be highly unstable, and a single player with an opportunistic strategy can make
the whole population move to a situation of distrust (Abbass, Greenwood, & Petraki,
2016). In order to explain how a situation of trust and cooperation can still emerge,
the topology of the community is considered.

#### Structure as a Solution

As explained above, it has become clear that in zero-dimensional systems,
where every player can interact with every other player, cooperation does seldom
emerge without any other incentives. This is why the network structure of
interactions should be considered. This idea already started with Nowak and May
(1992), who were the first to depart from well-mixed populations, and put simulated
agents playing prisoner-dilemma (PD) games in a two-dimensional space, where each
agent could only interact with eight direct neighbours. A phenomenon that became
visible is that the success of cooperative strategies often originated in the corners of
the space, suggesting some protection from the network can be beneficial. Since the
first exploration on a 2D grid, the emergence of network science as a discipline has
provided tools for further research, and it has become clear that being embedded in a
network structure of some sort can lead natural selection to select cooperative
behaviour in game theoretic models (Rand, Arbesman, & Christakis, 2011).


Some more expectations can be inferred from closely related research.
Simulations from Buskens & Snijders (2016), show that more centralisation^2 and less
segmentation increases payoffs in 2x2 coordination games in a limited part of the
reward space. Together with PD games, coordination games are the classic
abstraction used when talking about cooperation in a network structure (e.g., Ellison,
1993; Anderlini & Ianni, 1996). A characteristic of the coordination game is that
success is mostly dependent on the speed with which signals can propagate
throughout the network. This is why models with Small-World characteristics^3 are
successful: they display a shorter average path length and more synchronisability
(Watts and Strogatz, 1998; Watts, 1999). Both centralisation and degree^4 can
contribute to this, see Cassar (2007) for an overview of experimental evidence.

For social dilemmas, the speed with which signals or behaviour can travel
through the network might not be the only determinant of success. Various
theoretical approximations and simulations about the influence of network structure
on PD games have been carried out. Abramson & Kuperman (2001) varied a
structure from a regular ring lattice^5 to a completely random network and found a
positive relationship between the amount of rewiring towards randomness and the
number of defectors. Santos & Pacheco (2005) simulated snowdrift (SD) and PD
games, in which they found the first game to be significantly more successful on a
regular lattice than in a more random network. It is when expectations are inferred
for the sharing economy trust model that the main issue for this kind of research is
encountered: there is no mathematical framework to compare games, nor enough
ways to quantify network structure to approach these problems as an exact science.
Dynamics that occur in PD or SD games cannot be directly translated to the trust
game that is used as an abstraction for the sharing economy.

More complex game simulations that include the concept of individually placed trust instead of simultaneous cooperation like PD and SD games were carried out by Chica, Chiong, Kirley, & Ishibuchi (2018). They use an N-player trust game, closely resembling a public good game. Their first finding, in line with Abbass et al. (2016), is that strategies of trust never survive in an unstructured population. Since an untrustworthy player can interact with anybody, their distrust will spread quickly. Conversely, sparsely connected structures are better for the promotion of trust via social diversity in smaller clusters, in line with ideas from Santos, Santos, & Pacheco (2008). Both these sources use games with different strategy setups, that seem more dependent on the local neighbourhood than the simpler 2-player trust games used for the sharing economy, still, the idea that sparsely connected clusters increase performance is insightful for our application.

(^2) The existence of nodes that are situated on a lot of all possible shortest paths between nodes
(^3) Small World networks are a specific type of network structure with a high clustering, but a low
average path length, as first recognised and described by Watts & Strogatz (1998)
(^4) The number of edges a node has
(^5) A regular network with the highest possible clustering for each node, but also an high average path
length (Watts & Strogatz, 1998)

#### Community Structure

This research will use community structure as a starting point for variations in
the network. This idea is found in research from various disciplines, but not often in
the context of social dilemmas and trust. Fang, Lee, & Schilling (2010) introduced
the concept of community structure, and the rewiring between communities as
community connectedness, to explain why some companies are good at fostering
innovation. They present the subgroups as shelters for ideas, enabling them to
survive and show their success, rather than being extinguished through fierce
competition in the population as a whole. In biology, there are similar reasons to
believe a community structure can benefit cooperation. Theoretical and
computational works by Nowak and May (1992; 1993) and Nowak, Bonhoeffer, &
May (1994) predict and show behaviour where cooperators and defectors coexist in
clusters in the same network.

Prior to Chica et al. (2017), little research went into the specific dynamics
producing these end states, which they showed to be dependent on the payoffs and
starting distributions. It is difficult to draw expectations from other research because
little research combines varying degree, structure and payoffs within the same
simulation, while it could be suspected that these influence each other. This paper might be the first to vary these three factors at the same time. Additionally, a
graphical user interface (GUI) makes it possible to explore the dynamics in even
more detail, by observing them in two-dimensional space, making more intuitive
understanding of observed results possible.

### The Networked Sharing Economy Model
The influence of the network structure of possible connections will be explored
using agent-based modelling (ABM), where agents are programmed to behave within
the framework of Evolutionary Game Theory (EGT). The simulation will run in
NetLogo, a modelling environment especially built for exploring behaviour in complex
systems using an agent-based approach. The basic premise of the simulation is as
follows: a constant population of agents with pure strategies are matched based on the
existence of a direct link within a network to play a trust game. After a certain
number of games, agents will revise their strategy, comparing their payoffs to those of
agents around them.

The details of the model as below are described mostly theoretically. Further
practical and technical details can be found in Appendix 1 and the simulation’s source
code (available upon request).

#### Game Definitions and Payoffs

Since this paper builds upon the first application of EGT for the Sharing Economy
by Chica et al. (2017), the game definitions of the trust game will be the same. This
social dilemma is characterised by a payoff structure in which the cooperative, Pareto-
optimal, combination is unstable because each individual could gain a higher or less
risky payoff when they choose a selfish alternative. The Provider is the trustor, while
the consumer is the trustee. The strategies and their real-world explanation for each
player are as follows:

- TP: a trusting provider who delivers an asset as agreed upon.
- UP: an untrusting provider who does not deliver, causing a small negative
payoff for the consumer and a small positive payoff for themselves.
- TC: a trustworthy consumer who uses the asset decently.
- UC: an untrustworthy consumer, who misuses the asset, e.g., by damaging
or stealing it, causing damage to the owner and a high payoff for themselves.

_Table 1._ Payoffs in the Sharing Economy Trust Game

| Providers | Consumers |           |
| --------- | --------- | --------- |
|           |  TC       |  UC       |
| TP        |   R, R    | -20, 40   |
| UP        |  10, -10  |  10, -10  |

_Note._ When players interact with an agent with the same role (i.e., provider or
consumer), no transaction takes place, and the added payoff is 0.
By definition, the minimum and maximum rewards are 21 and 39 respectively,
because of the game’s definition by Chica et al. (2017)

Different payoffs likely change the evolutionary dynamics via the ‘difficulty’ or
temptation to defect in the game (Chica et al., 2017; Chica et al. 2018), so the
reward for cooperation will be one of the variables in the model. The reward value R
will be varied covering this games’ easy (39, 36), moderate (33, 30, 27) and hard (24,
21) spectrum, as identified by Chica et al. (2017).

It should be noted, however, that the non-sequential play and lack of
communication make the game designed by Chica et. al. (2017) different from most
trust games in the literature, so comparison to other trust games should be done with
care. Further justification can be found in their paper.

#### Strategy Update

Agents adjust their strategies simultaneously after 50 games via an algorithm
that simulates evolution. For this, the proportional imitation rule (Helbing, 1992) will
be used. This update heuristic is in line with the idea of bounded rationality and
incomplete information for an agent since it considers limited memory, and
knowledge only about the payoffs of direct network neighbours (Schlag, 1996).
During the strategy update procedure, an agent will first randomly select a neighbour to compare their strategy and payoffs during the previous rounds. If the
selected neighbour had a higher total payoff, the revising agent copies the selected
agent’s strategy with a certain probability. This probability is equal to the difference
between their payoffs in the previous 50 games, divided by the maximum
theoretically possible difference (in these scenarios this is the difference between 50 *
R and 50 * -20). The arbitrary structure of 50 games before reconsidering the
strategy will be used for comparability with the trust experiment using EGT by
Chica et al. (2017).

#### Generating the Networks

Although some data is available on network structures of occurred
transactions on sharing economy platforms (e.g., Teubner 2018), it is of limited use,
since this paper is about the network of possible connections, as opposed to the
transactions that have occurred in the past. Besides that, the network data in the
literature is often from Airbnb, which has a more traditional provider-consumer
structure (Cox 2017; Wired 2017). This structure is, although not precluded,
inconsistent with the definition of sharing economy used here. Because of this, this
research will use theoretical approximations of real-world networks.

For this paper, a custom algorithm for generating networks with a community
structure was programmed. First, it assigns every node a ‘community’ and randomly
picks the required nodes within the same community to connect. It is important to
note that this random assignment of community is entirely independent of all other
randomnesses in the model, like which edges are rewired or which initial strategy is
assigned to a node. After the edges within the community have been formed, each
edge will be rewired to a random node with the required probability determined by
the level of community connectedness. When connectedness equals one, the result is a
completely random network. In all other cases, the algorithm will result in a network
with community structure, as can be detected with the conventional Girvan &
Newman (2002) algorithm.

![layouts](images/layouts.png)
_Figure 1._ Example network layouts coloured by community with rewiring probability
0.04 (left) and 0.4 (right) and average degree 4 (top) and 14 (bottom).

### Experiments and Results
All results were obtained from simulations with a population of 512 agents,
spread across 16 communities. Preliminary testing and analysis showed that this
network size is large enough to ensure reasonable robustness and representative
results at any combination of variables while being small enough to compute all
simulations within the available timeframe.


This paper focusses on effects of average degree (D) and the amount of intra-
community edges rewired to be inter-community after the initial communities have
been set up (P). This value of P covers the concept of community connectedness. No
available quantification of this concept of connectedness includes a scale, but in these
experiments, a continuous range of network structures is created by varying the
probability of rewiring edges between 0.01 and 1, creating a continuous range from
communities connected to each other with a minimal amount of inter-community
edges to a completely random network.

#### Experimental Setup

For each network, ‘success’ will be measured by summing all the payoffs
gained during the simulation. A network that sustainably supports cooperation will
gain high payoffs for a long time, while payoffs in a network with little trust present
will quickly reach a state of low payoffs, or no transactions at all. From Chica et al.,
it is known that dynamics within this trust game can be vastly different for different
payoff ratios. Because of this, the reward value (R) will be varied in interaction with
the degree and rewiring, covering this games’ easy (39, 36), moderate (33, 30, 27)
and hard (24, 21) spectrum, as identified by them in their 2007 paper.

During preliminary testing, it was discovered that payoffs are most sensitive to
rewiring when rewiring is low, so an exponential scale was chosen, testing relatively
more values close to 0. Degree was started at 2 and tested up until 22 in steps of 2.
Although this is not the limit of the variable space in which simulations end
successful, an arbitrary limit had to be chosen. This leads to the following
combination of variables: Reward (R) is varied [21, 24, 27, 30, 33, 36, 39], degree (D):
[2, 4, 6, 8, 10, 12, 16, 18, 20, 22] and rewiring (P): [0.01, 0.02, 0.04, 0.05, 0.08, 0.1,
0.2, 0.4, 0.5, 0.8, 1] for a total of 770 possible combinations.

Because of randomness in the assignment of communities, rewiring of links
and strategy evaluation, the simulation is non-deterministic. To approach the mean
value for average total payoffs, every combination is repeated for at least 32
independent runs. With a very regular desktop PC, this took multiple days of non-
stop simulations to compute, but with further code optimisation and access to a high performance computing cluster optimised for these kinds of calculations, the time
could be a fraction of that.

#### Analysis of the Results

First, some general remarks about the dynamics will be presented.
Particularly on the ‘balancing effect’ that occurs in this game, presented by Chica et
al. (2017). Thereafter, some specific areas in the tables of results will be highlighted,
and further interpreted by looking at the details of some specific runs. This
‘anecdotal’ evidence is difficult to quantify but is in line with the exploratory nature
of this research.

##### Remarks on dynamics.
The balancing effect seems to be present to a certain extent at almost all variable combinations. This general pattern can be
summarised as follows: at first, players with a UC strategy profit from their
opportunistic behaviour, making their strategy spread. As a ‘response’ to this, UP
players, a safe and risk-less strategy not affected by UC^6 , become a substantial group
within the population, driving out TP and TC, but most importantly UC. Agents
with strategy UC cannot profit from agents with strategy UP. When a particular
distribution (the flipping point is highly dependent on payoffs) is reached, TP and
TC slowly regain ground. The playing field is now less risky with UC mostly or
entirely gone. TP only forms a small risk to them, and they profit highly from the
presence of each other. This balancing effect can be observed both at the community
level or for the whole network at the same time when it is more connected.

![balancing](images/balancing.png)
_Figure 2._ An example run that clearly contains the balancing dynamic as described
by Chica et al. (2017) (D = 8, P = 0.01, R = 27).

Agents switching to an UP strategy could be interpreted as not partaking in
the network’s sharing economy at all; they are not risking to provide their asset and
pick a certain but low payoff. At the micro level, we can see why this is a neutral
context for TP and TC to spread. TP’s influences are not affected by UP because
they are both providers, and a TC can on average handle two to four times more
UC’s than TC’s in order to still have a non-negative payoff. Another important fact is that TC and TP’s payoffs are equal after a successful transaction, ensuring one
does not become too much more successful in the network than the other, while in
‘hostile’ situations TP has slightly higher average payoffs than TC, spreading slightly
more quickly, thereby creating a more favourable context for TC.

However, looking at some of the runs in detail, it is observed that below 27
the success is dependent on complete absence of UC-players in the network. Because
of the chosen strategy update rule, a strategy can never reoccur after having gone
‘extinct’. This is good for the spread of trust, but problematic for drawing
implications, since it could make the simulation end in a state that would
immediately collapse when a single untrustworthy agent would be re-introduced.
Only when R >= 27 some simulations end in situations of high payoffs for the
network without the complete absence of UC players. This same rule also gives issues
in some highly connected and dense structures, but then because TC (or TP)
completely disappears from the network before the strategy distribution is ‘friendly’
enough for them to flourish. This shows that the update rule chosen by Chica et. al.
(2017) might not be suitable in a certain part of the reward space. A possible
solution for this will be discussed in section V.

In general, community structure, compared to complete randomness and other
structures (not covered in tables) spread the risk of a run, as a portfolio of different gambles. Even in low payoffs, one community can develop a situation of trust, which
will always spread as long as other communities are neutral enough. Neutral in this
context means having a majority of UP players, which TP and TC do not lead
significant losses against. Community structure can also protect the network: if one
community has an ‘outbreak’ of opportunism, depending on the other factors, it can
be ‘dealt with’ within the community, i.e, walk through the Balancing effect on a
small scale, while not influencing the rest of the network.

![separation](images/separation.png)
_Figure 3._ An example network visualisation in which the effect of community
separation is visible (D = 8, P = 0.05, R = 30, time-step = 250).

![payoffs](images/payoffs.png)
_Figure 4._ Observed average payoffs for a specific combination of degree D (y-axis)
and rewiring P (x-axis), grouped in blocks by reward value (value in upper-left
corner of each block). The reported numbers are the average payoffs of all agents
together (in millions), over at least 32 runs, coloured relative to the other results
with the same reward R (left) or relative to all results (right).

(^6) Throughout this research, players with a certain strategy will be referred to by the name of their
strategy. Keep in mind, however, that it is not the strategy that evolves, but the players who ‘decide’
to use a strategy or not, based on its relative success.

##### Analysis of end states.
Observed average payoffs for a specific combination of degree D (y-axis) and rewiring P (x-axis), grouped in blocks by reward value
(upper-left corner of each block). The reported numbers are the average payoffs of all
agents together (in millions), over at least 32 runs, coloured relative to the other
results with the same reward R (left) or relative to all results (right).

In figure 4, the observed average payoffs for a specific combination of degree D
(Y-axis) and rewiring P (X-axis) are shown, grouped in blocks by reward value. The
reported numbers are coloured relative to the other results with the same value for R
(left) or relative to all results (right). Higher average payoffs indicate a network had
many agents with TC and TP strategies trading with each other. End states with a
degree of 2 were omitted because the network was too often split up in multiple
networks by rewiring. Networks splitting was also an issue at D = 4, where total
payoffs are lower because disconnected agents did not always have a trading partner.
This is most clearly visible in the tables when the reward is equal to 39.

In the tables, especially the right-hand table where colour is relative to all
achieved payoffs, patterns related to the three variables can be visually identified: In
all reward spaces, degree seems to have a negative correlation with the total average
payoff. Secondly, a negative correlation to the level of rewiring can be distinguished,
most clearly in the moderate-high reward areas. Thirdly, higher rewards expectedly
led to higher payoffs, but at R = 39 the interaction with degree and rewiring
suddenly changes, as is visible in the left-hand column of tables. The last part of this
analysis will dive into these three patterns in more detail.

Looking at the role of degree in more detail, we see a consistent effect at the
level of the individual agent. A high value for D makes it harder for trustworthy
behaviour to survive in thesimulations. As soon as an UC is able to reach one or
more TP’s, their opportunistic behaviour can quickly spread. A lower number of
edges means UC’s cannot spread as effective, and have a higher likelihood of being
stopped somewhere either by ‘running into’ a group of UP’s, or by not being able to
spread to a node because it is highly successful in its trade with other trusting and
trustworthy agents.

The effect of rewiring seems to depend highly on the value of R, i.e., how
‘hard’ the game is. For low rewards, higher connectedness makes for quicker
extinctions, which is the only way to success (if UC goes extinct) or failure (when TC
or TP goes extinct) for this area of variables. In figure 5, two runs with the same
setup conditions are shown, to illustrate the two only occurring courses of a
simulation in this variable space. The complete dependence on the extinction of UC
is a risky dynamic for reaching a situation of complete trust, but apparently, it is not
influenced much by the structural specifics, mostly just the randomness in the initial
placement of strategies and the element of chance in the strategy selection. This area
should not be considered for real-world implications since this extinction-dependency
might be very unrealistic.

![low1](images/low1.png)
![low2](images/low2.png)
_Figure 5._ The two occurring courses of a simulation in the low reward-space (D = 16,
P = 0.01, R = 24).

For medium values of R, the situations become more chaotic, and rewiring
seems to increase this chaos. The network does not go through the balancing effect as
a whole since some communities can be thriving purely based on higher payoffs if the
amount of trust is high from the start by random assignment. Because higher
rewards make complete extinction of UC no longer a necessity, communities can have
successful interactions before the rest of the network does. Balancing happens locally,
which makes the structure more important. This clear distinction for medium
rewards might explain some of the results observed by Chica et al. (2017) around an
R of 27, where end states seemed to be inconsistent with other trends. When
communities are not very connected, they will find a stable situation on their own,
often by complete extinction of UC. The fact that communities change independent
of others is visible in the top pattern in fig. 6, where fluctuations are in steps of
1/16th of the Y axis, corresponding to one or more communities at a time. When a
community is thriving, the low connectedness makes for a protected situation where
opportunistic behaviour cannot reach the community. In more connected
communities, UC’s from a single community can repeatedly spread towards other
communities. This, combined with the fact that communities started to thrive before
UC went extinct, causes continuous fluctuations in strategies across the network.

![medium1](images/medium1.png)
![medium2](images/medium2.png)
![medium3](images/medium3.png)
_Figure 6._ The visible chaos in moderate reward values for three different amounts of
rewiring from low to high (D = 16, P = 0.01 (top) 0.1 (middle) 1 (bottom), R = 24).

In the situation of high rewards, the effect of rewiring becomes more
predictable. The ‘easy’ nature of the game causes the payoffs gained by trusting and
trustworthy behaviour to be the primary driver of success, without much need for the
balancing dynamic for the whole network. Less rewiring benefits the development of
trust by making individual communities ‘find’ the quickest way to success for them.
It seems as though the separation of the network makes this more efficient because
each community goes through the dynamic that is relevant for their initial
distribution of strategies: sometimes a quick balancing dynamic, or a slower decrease
of opportunistic behaviour via higher payoffs gained by TP and TC if possible.
Because the network does not have to do this as a whole, it is quicker.

![high1](images/high1.png)
_Figure 7._ A typical simulation run in with high rewards, with only a small balancing
dynamic visible, and an equilibrium with UC agents present (D = 16, P = 0.1, R =
36).

A seemingly strange pattern is observed when looking at the table for R = 39
in the left-hand column, where payoffs are coloured relative to the other average
payoffs with the same reward. The pattern seems to have ‘flipped’ on the rewiring
axis, compared to the tables for other reward values.^7 When observing the right-hand
version, it can be seen that these differences are minimal, and not noticeable relative
to other reward values. Looking at some simulations in more detail, the following
explanation can be found: both reasoning about extinction or protection by the
community do not apply here. The game is in favour of agents with a strategies of
trust to the point that the only important thing is how fast these strategies can
spread. Confirming to dynamics described in the very first paper on small-world
networks by Watts & Strogatz (1998), signal propagation is quicker on networks
approaching random networks.

(^7) The low payoffs at D = 4 are because of parts of the network disconnecting when rewired, as
mentioned at the beginning of this section.

### Concluding Remarks and Future Work
This paper built upon the novel evolutionary trust game from Chica et al.
(2017) by adding the dimension of network structure, specifically average degree and
connectedness between communities in a community network. It has become clear that the network structure can have substantial effects on the success of a sharing
economy network. Additionally, it was found that the rewards in the game do not
only influence end states, but also influence dynamics and the effect of other
variables. Clearly, this type of exploration cannot be done with many variables kept
constant. Because of the strong interactions between variables, exact quantification of
interaction is necessary. The latter is currently impossible because of the lack of
mathematical frameworks in this area.

The presented sharing economy trust model consists of 512 agents, arranged in
16 communities, where the degree and amount of edges rewired from inter-
community to a random agent were systematically varied together with the reward
for cooperation in the trust game. Each agent could choose to be a provider or
consumer and to be trusting/trustworthy or not, evaluating every 50 games,
simulating the evolution of chosen strategies over time.

These computational experiments suggested that more isolated communities
could be a safe place to start the formation of trusting behaviour. It possibly also
prevents opportunistic behaviour from spreading to the rest of the network quickly.
In line with previous research on social dilemma-games, a higher number of
individual connections makes an agent more vulnerable to opportunism, thereby
decreasing the total payoffs for the network when the average degree is higher.
Looking at the effect of community connectedness, it was discovered that the
simulation is especially sensitive to the amount of rewiring in the lower rewiring
values. The effects of this variable turned out to be highly dependent on the chosen
reward in the trust game. As expected, higher rewards in the trust game create
resilience to opportunistic behaviour, independent of other variables.

The balancing effect discovered by Chica et al. (2017) was present in many of
the simulations and was essential to the development of successful interactions when
rewards were low. This occurrence of the dynamic does, however, create the
unrealistic situation of complete extinction of strategies, which deteriorates their
game’s real-world application value. This problem became visible because of the
visual user-interface available in this paper’s simulation.


Although the value of this application to the sharing economy is mainly in the
advancement of knowledge about the used theoretical frameworks by finding their
limitations in these situations, some of the results could be extrapolated to findings
relevant for sharing economy platforms. Firstly, it is important for platforms to know
how ‘hard’ their interactions are. In the case that transactions are high in risk, it
could be beneficial to limit the amount of different people an individual can interact
with at first. When the benefits of a platform are obvious, and risks are low, the
platform could benefit from bringing together people who wouldn’t normally do so,
to effectively create bridges in the network. Secondly, this research suggests that
fostering the community structure of a platform could be beneficial. Often, platforms
serve a user with multiple possible providers to have a transaction with, and a
‘recommender system’ decides which options to show you first (Grbovic, 2017). Since
recommendation algorithms include deliberate choices by the platform, it can be used
to influence the structure of the occurring transactions network. Another possibility
is explicitly highlighting characteristics of the underlying network structure to the
user (e.g., friends in common, others you both had a transaction with), to increase
the level of trust, like Airbnb is already doing (Airbnb 2011).

This thesis is part of the balance that has to be found between developing
theoretical frameworks on the one hand, while applications on the other hand do not
have to wait until the theory is fully mature, so they can drive the theory forward.
Especially agent-based simulations with a GUI, like the one used here, can have a big
educational value. This does, however, mean that some limitations have to be
considered when interpreting the results. For network structure, it is unknown what
actual networks of possible interactions look like, or how they relate to the
observable networks of occurring transactions. The number of transactions of agents
might very well follow a power-law distribution in terms of weight or centrality
(Teubner, 2017). Other network structures could also include a more rigid distinction
between players who can provide an asset and those who cannot, or the possibility
for players to create or delete links between them, creating a dynamic graph (Rand,
Arbesman, & Christakis, 2011). These options can be considered for future research.


Considering the other components of the game, the chosen game structure and
evolution rule might not be the best abstract representation of reality. This paper’s
findings highlight a major issue with the proportional imitation rule: the fact that
strategies can completely disappear from the simulated world, a very unrealistic
situation. There exists a trade-off between simplicity and realism, but a simple way
to address this issue would be the introduction of noise to the simulation: a small
chance of random mutation for each agent. Other alternatives that can be considered
include evaluation rules in which agents memorise previous payoffs and strategies,
either increasing knowledge about their past state or the network as a whole.

This thesis also presented a unique way of reporting data in a
multidimensional behaviour-space, using heat maps to find areas of interest in an
abundance of data. With the computing power of current consumer hardware, ABM
provides a way to generate amounts of data previously impossible without high-
performance computing clusters (and therefore seldom viable). This requires new
ways of interpreting and visualising results. Future possibilities for this kind of
research include generating data including even more variables. Artificial intelligence
and other methods from so-called ‘big data’ research could be used to find patterns
and dynamics. Taking this even further, simulation as a method could be developed
into a complete representation of both micro- and macroeconomics, creating a form
of Agent-Based Computational Economics (Tesfatsion, 2002).



## References

Abbass, H., Greenwood, G., & Petraki, E. (2016). The N-Player Trust Game and its
Replicator Dynamics. IEEE Transactions on Evolutionary Computation,
20(3), 470–474. https://doi.org/10.1109/TEVC.2015.2484840

Abramova, O., Shavanova, T., Fuhrer, A., Krasnova, H., & Buxmann, P. (2015, May
28). Understanding the Sharing Economy: The Role of Response To Negative
Reviews in the Peer-to-Peer Accommodation Sharing Network. Retrieved from
https://www.researchgate.net/publication/277597462

Abramson, G., & Kuperman, M. (2001). Social games in a social network. Physical
Review E, 63(3), 030901. https://doi.org/10.1103/PhysRevE.63.030901

Airbnb. (2019). What is the Social Connections feature? Retrieved 23 June 2019,
from Airbnb website: https://www.airbnb.com/help/article/198/what-is-the-
social-connections-feature

Anderlini, L., & Ianni, A. (1996). Path Dependence and Learning from Neighbors.
Games and Economic Behavior, 13(2), 141–177. https://doi.org/10.1006/
game.1996.0032

Belk, R. (2014). You are what you can access: Sharing and collaborative consumption
online. Journal of Business Research, 67(8), 1595–1600. https://doi.org/
10.1016/j.jbusres.2013.10.001

Botsman, R. (2012). The currency of the new economy is trust. Retrieved from
https://www.ted.com/talks/
rachel_botsman_the_currency_of_the_new_economy_is_trust

Bridges, J., & Vásquez, C. (2018). If nearly all Airbnb reviews are positive, does that
make them meaningless? Current Issues in Tourism, 21(18), 2057–2075.
https://doi.org/10.1080/13683500.2016.1267113

Buskens, V. (1998). The social structure of trust. Social Networks, 20(3), 265–289.
https://doi.org/10.1016/S0378-8733(98)00005-7


Buskens, V., & Snijders, C. (2016). Effects of Network Characteristics on Reaching
the Payoff-Dominant Equilibrium in Coordination Games: A Simulation study.
Dynamic Games and Applications, 6(4), 477–494. https://doi.org/10.1007/
s13235-015-0144-4

Cassar, A. (2007). Coordination and cooperation in local, random and small world
networks: Experimental evidence. Games Econ. Behav. Retrieved from
https://doi.org/10.1016/j.geb.2006.03.008

Chica, M., Chiong, R., Adam, M. T. P., Damas, S., & Teubner, T. (2017). An
evolutionary trust game for the sharing economy. 2017 IEEE Congress on
Evolutionary Computation (CEC), 2510–2517. https://doi.org/10.1109/CEC.
2017.7969610

Chica, M., Chiong, R., Kirley, M., & Ishibuchi, H. (2018). A Networked ${N}$ -
Player Trust Game and Its Evolutionary Dynamics. IEEE Transactions on
Evolutionary Computation, 22(6), 866–878. https://doi.org/10.1109/TEVC.
2017.2769081

Cox, M. (2019). Inside Airbnb. Adding data to the debate. Retrieved 13 June 2019,
from Inside Airbnb website: [http://insideairbnb.com](http://insideairbnb.com)

Eckhardt, G. M., & Bardhi, F. (2015). The Sharing Economy Isn’t About Sharing at
All. Harvard Business Review. Retrieved from https://hbr.org/2015/01/the-
sharing-economy-isnt-about-sharing-at-all

Ellison, G. (1993). Learning, Local Interaction, and Coordination. Econometrica,
61(5), 1047. https://doi.org/10.2307/2951493

Ert, E., Fleischer, A., & Magen, N. (2016). Trust and reputation in the sharing
economy: The role of personal photos in Airbnb. Tourism Management, 55,
62–73. https://doi.org/10.1016/j.tourman.2016.01.013

Fang, C., Lee, J., & Schilling, M. A. (2010). Balancing Exploration and Exploitation
Through Structural Design: The Isolation of Subgroups and Organizational
Learning. Organization Science, 21(3), 625–642. https://doi.org/10.1287/orsc.
1090.0468

Fortunato, S. (2010). Community detection in graphs. Physics Reports, 486(3), 75–174. https://doi.org/10.1016/j.physrep.2009.11.002

Frey, V., Buskens, V., & Corten, R. (2019). Investments in and returns on network
embeddedness: An experiment with trust games. Social Networks, 56, 81–92.
https://doi.org/10.1016/j.socnet.2018.07.006

Gefen, D., Benbasat, I., & Pavlou, P. A. (2008). A Research Agenda for Trust in
Online Environments. Journal of Management Information Systems, 24(4),
275–286. Retrieved from JSTOR.

Girvan, M., & Newman, M. E. J. (2002). Community structure in social and
biological networks. Proceedings of the National Academy of Sciences, 99(12),
7821–7826. https://doi.org/10.1073/pnas.122653799

Goes, P. B. (2013). Editor’s Comments: Information Systems Research and
Behavioral Economics. MIS Q., 37(3), iii–viii.

Grbovic, M. (2017). Search Ranking And Personalization at Airbnb. Proceedings of
the Eleventh ACM Conference on Recommender Systems, 339–340. https://
doi.org/10.1145/3109859.3109920

Guimera, R., Danon, L., Diaz-Guilera, A., Giralt, F., & Arenas, A. (2003). Self-
similar community structure in organisations. Physical Review E, 68(6),065103. https://doi.org/10.1103/PhysRevE.68.065103

Habibi, M. R., Kim, A., & Laroche, M. (2016). From Sharing to Exchange: An
Extended Framework of Dual Modes of Collaborative Nonownership
Consumption. Journal of the Association for Consumer Research, 1(2), 277–294. https://doi.org/10.1086/684685

Helbing, D. (1992). Interrelations between Stochastic Equations for Systems with
Pair Interactions. Physica A: Statistical Mechanics and Its Applications,
181(1–2), 29–52. https://doi.org/10.1016/0378-4371(92)90195-V

Izquierdo, L. R., Izquierdo, S. S., & Sandholm, W. H. (2018). An Introduction to
ABED: Agent-Based Simulation of Evolutionary Game Dynamics. 58.

Izquierdo, L. R., Izquierdo, S. S., & Sandholm, W. H. (in press). Agent-Based
Evolutionary Game Dynamics. Retrieved from https://wisc.pb.unizin.org/
agent-based-evolutionary-game-dynamics/

Karlsson, L., Kemperman, A., & Dolnicar, S. (2017). May I sleep in your bed?
Getting permission to book. Annals of Tourism Research, 62, 1–12. https://
doi.org/10.1016/j.annals.2016.10.002

Katz, M. (2017). A Lone Data Whiz Is Fighting Airbnb — and Winning. Wired.
Retrieved from https://www.wired.com/2017/02/a-lone-data-whiz-is-fighting-
airbnb-and-winning/

Lieberman, M. (2015). PWC Consumer Intelligence Series: The Sharing Economy (p.
30). Retrieved from PricewaterhouseCoopers website: https://www.pwc.com/
us/en/technology/publications/assets/pwc-consumer-intelligence-series-the-
sharing-economy.pdf

Ma, X., Hancock, J. T., Lim Mingjie, K., & Naaman, M. (2017). Self-Disclosure and
Perceived Trustworthiness of Airbnb Host Profiles. Proceedings of the 2017
ACM Conference on Computer Supported Cooperative Work and Social
Computing - CSCW ’17, 2397–2409. https://doi.org/10.1145/2998181.2998269

McKnight, D. H., & Chervany, N. L. (2001). What Trust Means in E-Commerce
Customer Relationships: An Interdisciplinary Conceptual Typology.
International Journal of Electronic Commerce, 6(2), 35–59. Retrieved from
JSTOR.

Nowak, M. A., Bonhoeffer, S., & May, R. M. (1994). More spatial games.
International Journal of Bifurcation and Chaos, 04(01), 33–56. https://
doi.org/10.1142/S0218127494000046

Nowak, M. A., & May, R. M. (1992). Evolutionary games and spatial chaos. Nature,
359(6398), 826–829. https://doi.org/10.1038/359826a0

Nowak, M. A., & May, R. M. (1993). The spatial dilemmas of evolution.
International Journal of Bifurcation and Chaos, 03(01), 35–78. https://
doi.org/10.1142/S0218127493000040

Pinsonneault, A., & Kraemer, K. L. (1993). Survey Research Methodology in
Management Information Systems: An Assessment. Journal of Management
Information Systems, 10(2), 75–105. Retrieved from JSTOR.

Porter, M. A., Onnela, J.-P., & Mucha, P. J. (2009). Communities in Networks.
ArXiv:0902.3788 [Cond-Mat, Physics:Nlin, Physics:Physics, Stat]. Retrieved
from [http://arxiv.org/abs/0902.3788](http://arxiv.org/abs/0902.3788)

Rand, D. G., Arbesman, S., & Christakis, N. A. (2011). Dynamic social networks
promote cooperation in experiments with humans. Proceedings of the National
Academy of Sciences, 108(48), 19193–19198. https://doi.org/10.1073/pnas.
1108243108

Santos, F. C., & Pacheco, J. M. (2005). Scale-Free Networks Provide a Unifying
Framework for the Emergence of Cooperation. Physical Review Letters, 95(9),098104. https://doi.org/10.1103/PhysRevLett.95.098104

Santos, Francisco C., Santos, M. D., & Pacheco, J. M. (2008). Social diversity
promotes the emergence of cooperation in public goods games. Nature,
454(7201), 213–216. https://doi.org/10.1038/nature06940

Schlag, K. H. (1998). Why Imitate, and If So, How?: A Boundedly Rational
Approach to Multi-armed Bandits. Journal of Economic Theory, 78(1), 130–156. https://doi.org/10.1006/jeth.1997.2347

Schor, J. B., Fitzmaurice, C., Carfagna, L. B., Attwood-Charles, W., & Poteat, E. D.
(2016). Paradoxes of openness and distinction in the sharing economy. Poetics,
54, 66–81. https://doi.org/10.1016/j.poetic.2015.11.001

Smith, J. Maynard, & Price, G. R. (1973). The Logic of Animal Conflict. Nature,
246(5427), 15. https://doi.org/10.1038/246015a0

Smith, John Maynard. (1998). The origin of altruism. Nature, 393(6686), 639.
https://doi.org/10.1038/31383

Strader, T. J., & Ramaswami, S. N. (2002). The value of seller trustworthiness in
C2C online markets. Communications of the ACM, 45(12), 45–49. https://doi.org/10.1145/585597.585600

ter Huurne, M., Ronteltap, A., Corten, R., & Buskens, V. (2017). Antecedents of
trust in the sharing economy: A systematic review. Journal of Consumer Behaviour, 16(6), 485–498. https://doi.org/10.1002/cb.1667

Tesfatsion, L. (2002). Agent-Based Computational Economics: Growing Economies
From the Bottom Up. Artificial Life, 8(1), 55–82. https://doi.org/
10.1162/106454602753694765

Teubner, T. (2018). The web of host–guest connections on Airbnb: A network
perspective. Journal of Systems and Information Technology, 20(3), 262–277.
https://doi.org/10.1108/JSIT-11-2017-0104

Teubner, T., & Adam, M. T. P. (2014, December). Understanding Resource Sharing
in C2C Platforms: The Role of Picture Humanization. 10. Auckland, New
Zealand.

Teubner, T., Hawlitschek, F., & Dann, D. (2017). Price Determinants on Airbnb:
How Reputation Pays Off in the Sharing Economy. Journal of Self-Governance
and Management Economics, 5(4), 53. https://doi.org/10.22381/JSME5420173

Tussyadiah, I. P. (2016). Strategic Self-presentation in the Sharing Economy:
Implications for Host Branding. In A. Inversini & R. Schegg (Eds.),
Information and Communication Technologies in Tourism 2016 (pp. 695–708).
https://doi.org/10.1007/978-3-319-28231-2_50

Ufford, S. (2015, February 10). The Future Of The Sharing Economy Depends On
Trust. Retrieved 23 June 2019, from Forbes website: https://www.forbes.com/
sites/theyec/2015/02/10/the-future-of-the-sharing-economy-depends-on-trust/

Watts, D. J. (1999). Networks, Dynamics, and the Small‐World Phenomenon.
American Journal of Sociology, 105(2), 493–527. https://doi.org/
10.1086/210318

Watts, D. J., & Strogatz, S. H. (1998b). Collective dynamics of ‘small-world’
networks. Nature, 393(6684), 440. https://doi.org/10.1038/30918

Zervas, G., Proserpio, D., & Byers, J. (2015). A First Look at Online Reputation on
Airbnb, Where Every Stay is Above Average. SSRN Electronic Journal.
https://doi.org/10.2139/ssrn.2554500



## Appendix A

The structure of the simulation and some of the specificities are from the
ABED-1pop framework (Izquierdo, Izquierdo, & Sandholm, 2018), published under
the GNU General Public License. The network statistics and layout procedures are
from a code snippet sent by Luis R. Izquierdo, whom I would like to thank for his
encouragement and helpful directions at the start of my programming endeavour.
Many of the methods and my general ability to program in NetLogo came from the
book “Agent-Based Evolutionary Game Dynamics” by Izquierdo, Izquierdo, &
Sandholm (in press). The code is specific to its application, and readability was often
prioritised over flexibility. No warranty of merchantability or fitness for a particular
purpose is implied.


The GUI is further explained below:

![gui1](images/gui1.png)
![gui2](images/gui2.png)
_Figure 7._ An overview of the GUI developed and used for this paper’s simulations.

A. The network settings. Only community structure is used in this paper, but
some of NetLogo’s build in structures like the Small-World network are also
available. With sliders, he number of nodes, the average degree and the
probability for rewiring can be changed.

B. Community settings. Including the number of communities and whether or
not they should always be connected. This optional procedure makes sure all the
communities have at least one link to another community, even when the
community connectedness is zero. This is used in all the simulations in this paper,
to present results about one single network and prevent changes in network size
across variations of the other variables. This does, however, rewire additional
edges, resulting in an increase in intra-community edges corresponding to an
increase of the effective connectedness between 0.0002 and 0.004 for the presented
experimental setup.

C. Payoff settings. Reward can be varied with a slider, all other payoffs can
also manually be adjusted in the 4x4 array.

D. Network actions. Setup sets the initial network up in a basic layout, which
can be further adjusted by ‘relaxing’ the network or manually by dragging and
dropping nodes. The relax-network procedure makes nodes move according to
their amount of edges and how far the connected nodes are away, creating an
organic looking layout.


E. Simulation actions. To start the situation continuously or watch it step by
step. A simulation can also be reset without changing the network. The ‘exit-
code’ reports if a network has reached a stable state (no-trust or only-trust).
While gathering of data, a simulation was only run for 16 more steps after
reaching a stable state to save time. 16 more steps was found to be enough to
estimate the payoffs for the rest of the simulation, since they merely fluctuating
around a mean after an equilibrium has been reached.

F. The main visualisation. Nodes have a colour and label corresponding to
their strategy, all visually updated after every strategy evaluation. This is what
made this program unique, and enabled in depth visual inspection of occurring
dynamics. With the ‘say-cheese’ procedure, nodes can be coloured by community,
and put agains a white background, better for taking screenshots.

G.Reporters of strategy quantity and payoffs gained. The upper graph
reports the presence of strategies, the bottom one does the same, but in a clearer
stacked bar chart. Payoffs during a round and cumulative payoffs are reported to
see changes in ‘success’ over time. All graphs automatically adjust their scale to
fit the data or can have fixed dimensions to export screenshots.

H. Reporters about the network structure. This includes the actual degree,
degree distribution, clustering coefficient, wether or not a network was split up
and the total number of links. Statistics about the nodes reached within one or
more degrees of separation can be seen by adjusting the link radius for which the
statistics are calculated.
",2022-08-12
https://github.com/JoostGadellaa/bank2YNAB,"# bank2YNAB
Automatically convert your bank's transaction csv's to a YNAB-supported format. Currently supports N26, Rabobank, Triodos Bank and ASN Bank.

## Usage

1. Install [Python 3](https://www.python.org/)
2. Download the csv file from [N26](https://support.n26.com/en-de/payments-transfers-and-withdrawals/balance-and-limits/how-to-export-a-list-of-my-transactions), [Rabobank](https://www.rabobank.nl/particulieren/betalen/service/financiele-overzichten/digitaal-rekeningafschrift-downloaden/), [Triodos](https://www.triodos.nl/veelgestelde-vragen/hoe-download-ik-een-overzicht-van-mijn-bij-en-afschrijvingen-in-mijn-boekhoudprogramma?id=126d202a2cba) or [ASN](https://www.asnbank.nl/online/web/onlinebankieren/inloggen/#/inloggen)
3. Convert using:
```
    python3 bank2YNAB.py 20200600000000.csv
```
Make sure you are either in the directory where both the script and the .csv file are, or link the full path of both the script and mutations file. E.g.:
```
    python3 /Users/joostgadellaa/Documents/bank2YNAB-master/bank2ynab.py /Users/joostgadellaa/Downloads/20200600000000.csv
```

4. The converted .csv will be in the same directory as the original one, and you are ready to upload to YNAB!
",2022-08-12
https://github.com/JoostGadellaa/capita-selecta,"# Topical Analysis of Privacy Literature, with an Application on Citation Network Interpretation
```
Joost Gadellaa
Utrecht University
Capita Selecta project
Business Informatics
```

## Abstract
Privacy is a versatile field of research, studied in a multitude of disciplines. A novel way to explore how research concepts like these are embedded in scientific literature is through the use of topic models. This study aims to investigate the usefulness of such modelling techniques. We use a latent Dirichlet allocation (LDA) topic model to analyse the title, abstract and keywords of 83,159 works in the research field of privacy, evaluating the model both on its own as well as in comparison to a previously made citation network analysis. We show that, although the resulting topic model can be meaningfully labeled, interpretation yields little insights. When comparing the model to the citation network analysis, remarkable similarities between the resulting classifications of these very different methods show. However, many methodological caveats pose a threat to conclusion validity, and computational costs limit possibilities for additional exploration. This method needs further research in order to generate insights, instead of just being an accurate representation of the data.

_Keywords_ : privacy, topic model, LDA, network analysis, bibliometrics
",2022-08-12
https://github.com/JorisEggenhuisen/SBE,"# SBE
Sediment Budget Estimator for deep sea depositional systems

## Info

This is version 1.0.0 of the Sediment Budget Estimator (SBE), which quantifies flow structure (velocity & sediment concentration) and sediment flux for turbidity currents flowing down submarine channels and canyons, and integrates this over time to estimate the sediment budget of a parameterised deep sea depositional system. The model is described in the publication:

Eggenhuisen, J.T., Tilston, M.C., Stevenson, C.J., Hubbard, S.M., Cartigny, M.J.B., Heijnen, M.S., de Leeuw, J., Pohl, F., and Spychala, Y.T. (2021) The Sediment Budget Estimator (SBE): a process-model for the stochastic estimation of fluxes and budgets of sediment through submarine channel systems. EarthArXiv, https://doi.org/10.31223/X5FK6K. 

Please cite the code as: 
J.T. Eggenhuisen, & M.C. Tilston. (2022). Sediment Budget Estimator for deep marine depositional systems (1.0.0). Zenodo. https://doi.org/10.5281/zenodo.6635519 
[![DOI](https://zenodo.org/badge/485391707.svg)](https://zenodo.org/badge/latestdoi/485391707)

## Set-up

The SBE is written to be executed in Matlab. Version 1.0.0 has been tested up to MATLAB [R2019a0]. It makes use of the function 'percentile', which is part of the Statistics Toolbox. The SBE consists of 5 -.m files:

-Run_SBE.m Is the script that executes the Sediment Budget Estimator.
-SimulationConditions.m The boundary condition ranges for the desired simulations are specified here.
-SimParamaters.m Declares some standard parameters and constants.
-VelCon.m This function returns the velocity and concentration profiels and the sediment flux.
-FlowDur.m This function returns the sediment budget.

Auxiliary -.m files:

- The folder 'Boundary Conditions' contains versions of 'SimulationConditions.m' that can be used to reproduce the results of Eggenhuisen et al. (2021).
- The folder 'Visualisations' contains example code that was use to produce the figures in Eggenhuisen et al. (2021). 

## Author information 

The Matlab code for the SBE was written by Joris Eggenhuisen & Mike Tilston. See Eggenhuisen et al. (2021) for a complete list of contributors to the scientific process.

## Potential Improvements 

Development potential:
- Facilitate single-flow parameterisation (deterministic instead of stochastic)
- Facilitate non-uniform sampling of boundary condition ranges (e.g. normal or log-normal distributions) 
- Auto-adjust 'steps' parameter to the boundary condition range.

## Changelog

A changelog file is available that documents the changes made between different versions.

## THE END (but hopefully TO BE CONTINUED)
",2022-08-12
https://github.com/JornBosma/AeolianStreamers,"# Aeolian streamers

<p align=""center"">
  <img src=""results/figures/Streamers.png"" width=""800px"" alt=""Streamers"" />
  <br><b>Contrast-enhanced picture of aeolian streamers on a beach</b>
</p>

## Introduction

Trend analysis of aeolian streamer behaviour for various wind conditions by running MATLAB file ""trendAnalysis.m"".

### Repository version

v0.1.3

## Dependencies

- Tested on macOS v10.15.7
- Tested on MATLAB R2019b and later versions
- Image Processing Toolbox (optional)

## Generated figures

Plot 1: Saltation intensity as a function of shear velocity.

Plot 2: Fit of plot 1 (note the logarithmic y-axis).

Plot 3: Determine height of the saltation layer.

Plot 4: Check for sensitivity equality amongst all sensors.

## Project organization

```
.
├── .gitignore
├── CITATION.md
├── LICENSE.md
├── README.md
├── requirements.txt
├── bin                <- Compiled and external code, ignored by git (PG)
│   └── external       <- Any external source code, ignored by git (RO)
├── config             <- Configuration files (HW)
├── data               <- All project data, ignored by git
│   ├── processed      <- The final, canonical data sets for modeling. (PG)
│   ├── raw            <- The original, immutable data dump. (RO)
│   └── temp           <- Intermediate data that has been transformed. (PG)
├── docs               <- Documentation notebook for users (HW)
│   ├── manuscript     <- Manuscript source, e.g., LaTeX, Markdown, etc. (HW)
│   └── reports        <- Other project reports and notebooks (e.g. Jupyter, .Rmd) (HW)
├── results
│   ├── figures        <- Figures for the manuscript or reports (PG)
│   └── output         <- Other output for the manuscript or reports (PG)
└── src                <- Source code for this project (HW)

```

## Report

More specific information on the data acquisition, code and results can be found in the report ""JWB_Aeolian_Streamers_V2"".

## License

This project is licensed under the terms of the [MIT License](/LICENSE.md)

## Citation

Please [cite this project as described here](/CITATION.md).
",2022-08-12
https://github.com/JornBosma/EURECCA-WP2,"# EURECCA WP2

Version 0.1.1

Analysis of grain-size controls on planform nourishment evolution.


## Project organization

```
.
├── .gitignore
├── CITATION.md
├── LICENSE.md
├── README.md
├── requirements.txt
├── bin                <- Compiled and external code, ignored by git (PG)
│   └── external       <- Any external source code, ignored by git (RO)
├── config             <- Configuration files (HW)
├── data               <- All project data, ignored by git
│   ├── processed      <- The final, canonical data sets for modeling. (PG)
│   ├── raw            <- The original, immutable data dump. (RO)
│   └── temp           <- Intermediate data that has been transformed. (PG)
├── docs               <- Documentation notebook for users (HW)
│   ├── manuscript     <- Manuscript source, e.g., LaTeX, Markdown, etc. (HW)
│   └── reports        <- Other project reports and notebooks (e.g. Jupyter, .Rmd) (HW)
├── results
│   ├── figures        <- Figures for the manuscript or reports (PG)
│   └── output         <- Other output for the manuscript or reports (PG)
└── src                <- Source code for this project (HW)

```


## License

This project is licensed under the terms of the [MIT License](/LICENSE.md)

## Citation

Please [cite this project as described here](/CITATION.md).
",2022-08-12
https://github.com/jwgsim/Bayes-Sampler,"A Bayes Sampler by JWG SIMONS.

<br />

This repository includes:
- A pdf file titled ""Bayesian Sampler Report"". This pdf contains a report which tries to answer whether normative price determinants influenced the price of personal computers in the United States during the 1993 – 1995 period with a Bayesian approach.    
- An annotated R file titled ""Bayesian Sampler Code"". This is an R-code file with which the reader can reproduce the results in the report.  
- This README file, with instructions on which pieces of code to run to produce the results. 

<br />

Please select and run the following lines of code to reproduce the results in the report: 

Preparing the data 
- Running lines 10 - 19 installs the package with the data if necessary, loads the data, and executes the necessary data operations. 
- Running lines 22 - 29 grand-mean centers the variables, and gives a summary of the grand-centered data. 
- Running lines 32 - 37 executes an MLE regression analysis of the second model (with all variables included), and stores coefficients 
                        and standard deviations for later use in the MH step.

Estimation - Gibbs sampler & Metropolis-Hastings algorithm
- Running lines 44 - 131 defines the gibbs sampler function.
- Running lines 134 - 275 defines the Metropolis-Hastings algorithm. 
- Running line 278 executes the Gibbs sampler and stores the results.  
- Running line 280 executes the MH algorithm and stores the results.  

Assessing model convergence
- Running lines 286 - 306 installs the required packages when necessary, and defines a function for plotting history plots. 
- Running lines 308 - 311 prints the history plots for the first model. 
- Running lines 313 - 317 prints the history plots for the second model.
- Running lines 322 - 351 defines a function for plotting auto-correlation plots.  
- Running lines 354 - 357 prints the autocorrelation plots for the first model. 
- Running lines 359 - 363 prints the autocorrelation plots for the second model.  
- Running lines 367 - 368 calculates the MC error for the first model on line 367 and the second model on line 368.
- Running lines 370 - 371 provides TRUE/FALE output for whether the MC error is smaller than 5% of sample standard deviation. 
- Running lines 374 - 375 prints the acceptance ratio of the MH step for the parameters ""ads"" and ""premium"". 

Posterior predictive checks
- Running lines 382 - 386 combines the chains in each of the models. 
- Running lines 388 - 440 defines a function for computing the PPC for both models. 
- Running line 443 computes and prints the PPC for the first model, running line 444 computes and prints the PPC for the second model. 

Model comparison and selection with the DIC
- Running lines 451 - 498 defines a function for computing the DIC in both models. 
- Running line 500 computes the DIC for the first model, running line 501 computes the DIC for the second model.  
 
Hypothesis evaluation with the Bayes factor
- Lines 507 - 535 sample from the posterior and prior of the second model (please read the annotation for the specific steps)
- Lines 539 - 543 calculate the Bayes factor for the first hypothesis. 
- Lines 547 - 551 calculate the Bayes factor for the second hypothesis. 
- Lines 554 - 562 calculate the Bayes factor for the third hypothesis. 

Parameter estimates and credible intervals
- Lines 569 and 571 provide the EAP estimates and associated CI's for the second model, respectively. 
- Lines 573 - 590 define a function for plotting the posteriors of the parameters, with EAP estimates and CI's on the x-axis. It also loads the gridExtra package for plotting if necessary.
- Lines 592 - 595 plot the posterior distributions for each of the parameters in the second model. 
",2022-08-12
https://github.com/jwgsim/Master-Thesis-Research-Archive,"### Research Archive Master Thesis JWG Simons.
This repository contains files for replicating the master thesis manuscript ""On the validity of applying the exponential random graph model to a network sample"" by Jan-Willem Simons. It additionally includes statements on data storage, privacy, and permission and access. 

| Folder | Description |
| ----------- | ----------- |
| `Addendum A. Data Storage` | Folder containing files for reproducing the master thesis manuscript. |
| `Addendum B. Privacy` | Privacy statement. |
| `Addendum C. Permission and access` | Permission and access statement. |

For any help with or comments on the files in this research archive, please contact Jan-Willem Simons (j.g.simons@uu.nl).
",2022-08-12
https://github.com/jwgsim/Paper-Meta-Analysis-for-ERGMs,"# Paper-Meta-Analysis-for-ERGMs
",2022-08-12
https://github.com/jwgsim/Paper-Meta-Analysis-Interethnic-Attitudes,# Paper-Meta-Analysis-Interethnic-Attitudes,2022-08-12
https://github.com/jwgsim/Paper-Propinquity-and-Negative-Ties,"# Paper-Propinquity-and-Negative-Ties
",2022-08-12
https://github.com/kaisakajala/RNA-seq-scripts,"# RNA-seq-scripts

### This folder contains some of my scripts that I use for my analyses of RNA-seq data. Data input files and results will be made available upon publication. 
",2022-08-12
https://github.com/katie-barry44/barry-schnitzer2021PlosOne,"# barry-schnitzer2021PlosOne
The data and code from Barry &amp; Schnitzer 2021 which will be published in PloS One can be found here. 

These data were collected by Kathryn Barry and Arielle Hunt in the late Spring and early Summer of 2014 at Powdermill Nature Reserve in Rector, PA. 

Details on the collection of these data can be found in the publication linked here: https://www.biorxiv.org/content/10.1101/2021.01.06.425540v1.abstract

Details on Powdermill Nature Research can be found in these other two publications: 
Murphy, S. J., L. D. Audino, J. Whitacre, J. L. Eck, J. W. Wenzel, S. A. Queenborough, and L. S. Comita. 2015. Species associations structured by environment and land-use history promote beta-diversity in a temperate forest. Ecology 96:705–715.

Spicer, M. E., K. F. Suess, J. W. Wenzel, and W. P. Carson. 2018. Does salvage logging erase a key physical legacy of a tornado blowdown? A case study of tree tip-up mounds. Canadian Journal of Forest Research 48:976–982.

I also would like to apologize in advance for how the code is formatted! I analyzed this data in 2014 and have never gone back to fully revise it and make it sensible. 2021 Katie would write the code very differently. 

If you have any questions, please contact me (Katie Barry) at barry.kt[at]gmail.com
",2022-08-12
https://github.com/katie-barry44/EcologicalTheoryAndApplications,# EcologicalTheoryAndApplications,2022-08-12
https://github.com/kequach/ARM-Paper,"# Repository for the paper assignment in Advanced Research Methods at UU - Team 02: Quach, K., de Rijk, N.J.,  Onyango, S.
## Steps
* Jupyter Notebooks are required for conducting the experiment. It is recommended to install Anaconda. Many required packages are already pre-installed: https://docs.anaconda.com/anaconda/install/
* Download GloVe from http://nlp.stanford.edu/data/glove.6B.zip (warning, link will start 800 MB download) 
* Put glove.6B.100d.txt in root folder
* Afterwards, start the Jupyter Notebook and run the cells
",2022-08-12
https://github.com/kequach/challongeformatting,"# challongeformatting
changes a seeding list sorted as 
1. 
2. 
3. 
4. 
5.
6.
7.
8.
into the format needed to enter groups in challonge. e.g. 
1. 
8. 
2. 
7. 
3. 
6. 
4. 
5.
",2022-08-12
https://github.com/kequach/efellows-tag-blocker,"# efellows-tag-blocker
Note: Current version only blocks the tag ""jura"". You can adjust this on your own in the file _contentscript.js_ and adding the tags you wish to block to the array _blocklist_. Contributions to improve this are welcome! 

An official chrome extension can be found [here](https://chrome.google.com/webstore/detail/e-fellows-tag-blocker/phpledkfijaogamgelgikggoafdmjajo?hl=en-GB&authuser=0).

## Local installation

This is a chrome extension that allows blocking tags on the e-fellows community forum. You can install it yourself by following these steps:

1. Download this repository
2. Open the following URL in chrome:
```
chrome://extensions
```

3. Activate developer mode (top right corner):

![image](https://user-images.githubusercontent.com/18238845/163572099-4dfec487-152b-4de4-bf21-a06949038866.png)

4. Click load unpacked and select the downloaded repository

![image](https://user-images.githubusercontent.com/18238845/163572221-8a66cd97-2ee3-4141-a1df-73d814aa22d6.png)

That's it!

You can check if the tool works as intended through the console:
![](screenshots/example_1.png)
",2022-08-12
https://github.com/kequach/infomsmt-computational-analysis,"# SMT project: Music recommendation and analysis <!-- omit in toc -->
- [General information](#general-information)
  - [Abstract](#abstract)
  - [Citation](#citation)
- [Setup](#setup)
  - [Register Your Application With Spotify](#register-your-application-with-spotify)
  - [Install Dependencies](#install-dependencies)
- [Running](#running)
  - [Example Run](#example-run)

This repository contains the code for the Sound and Music Technology Project in 2021 for group 2. The title of the corresponding paper is as follows: Computational analysis of audio feature information and their impact on emotion regulation.



## General information

The program output includes LaTeX tables with descriptive statistics and Bonferroni corrected t-tests, as well as the actual track recommendations based on the descriptive statistical values of extracted audio features. The spotify API is used in this project.

### Abstract
Emotional self-regulation is acknowledged as one of the most important reasons for musical engagement at all ages. Several studies proved the importance of music in emotion regulation. The aim of this study is to further investigate the audio features of songs that could be used for promoting positive mood and to build a content-based recommender system that produces a list of song recommendations based on the most important features of songs that promote a positive mood. The features with most importance are valence, danceability, energy, loudness, speechiness and tempo which is expected and in line with previous work done in the field and extends the valence-arousal model by Russell with other features that could serve for mood classification. 

### Citation

Use this citation for citing the implementation.
```
@software{Polancec_Computational_analysis_of_2022,
author = {Polancec, Ana and Stolk, Annemik and Hendriks, Geertje and Quach, Keven and Gerritse, Maarten and Ebbertz, Morice},
month = {1},
title = {{Computational analysis of audio feature information and their impact on emotion regulation}},
url = {https://github.com/mebbertz/infomsmt-computational-analysis},
version = {1.0.0},
year = {2022}
}
```

## Setup

### Register Your Application With Spotify

In order to access certain features of the Web API, we need to tell spotify that we're a legitimate app.
To do this, go to https://developer.spotify.com/my-applications and create a new Application.

From that page, copy your ClientId and your ClientSecret and put them into a file called
`.env` in the root of this repo that looks like this:
```
SPOTIPY_CLIENT_ID='YOUR_CLIENT_ID'
SPOTIPY_CLIENT_SECRET='YOUR_CLIENT_SECRET'
```
Here is an example screenshot of how this looks like:

![credentials](assets/spotify_credentials.png)

For details about how the API authenticates your account with this, see
https://developer.spotify.com/web-api/authorization-guide/#authorization_code_flow

### Install Dependencies

In order to run this program, we need to make sure python3 and pip are installed on your system.
To install this stuff, run

```bash
pip install -r requirements.txt
```

## Running

To run the project, navigate to the src folder and execute the following from the command line. Assuming you opened a terminal in the root folder, follow these commands (on Windows):

```bash
cd src
python main.py
```

You need to provide a playlists.csv file in the root directory. See following example:

```
playlist_name,link,category
Mood Booster,https://open.spotify.com/playlist/37i9dQZF1DX3rxVfibe1L0,mood boosting
Songs to Sing in the Shower,https://open.spotify.com/playlist/37i9dQZF1DWSqmBTGDYngZ,mood boosting
Happy Hits!,https://open.spotify.com/playlist/37i9dQZF1DXdPec7aLTmlC,mood boosting
Feelin' Good,https://open.spotify.com/playlist/37i9dQZF1DX9XIFQuFvzM4,mood boosting
Positive Vibes,https://open.spotify.com/playlist/37i9dQZF1DWUAZoWydCivZ,mood boosting
Running Wild,https://open.spotify.com/playlist/37i9dQZF1DX35oM5SPECmN,running
Fun Run,https://open.spotify.com/playlist/37i9dQZF1DXadOVCgGhS7j,running
Running Music Hits,https://open.spotify.com/playlist/0JTaSx9jkW1saMOc6t0vIk,running
Run This Town,https://open.spotify.com/playlist/37i9dQZF1DWWPcvnOpPG3x,running
Retro Running,https://open.spotify.com/playlist/37i9dQZF1DX4osfY3zybD2,running
Running UK,https://open.spotify.com/playlist/37i9dQZF1DWZ2xRu8ajLOe,running
Pop Rock Run,https://open.spotify.com/playlist/37i9dQZF1DWV3VLITCZusq,running
Deep Focus,https://open.spotify.com/playlist/37i9dQZF1DWZeKCadgRdKQ,studying
Instense Studying,https://open.spotify.com/playlist/37i9dQZF1DX8NTLI2TtZa6,studying
All-Nighter,https://open.spotify.com/playlist/37i9dQZF1DX692WcMwL2yW,studying
```

Only link and category are needed. You can leave the playlist_name empty. The program assumes that there are 3 playlist categories provided:
- mood boosting
- running
- studying

At least one playlist in each category is necessary for the program to work. Mood boosting is the category which the recommendations will be based upon.

It is also possible to use other .csv or .xlsx files. Here is an example. You need to specify the file name on the command line:

```bash
python main.py --input ../other_file_name.csv
```

### Example Run

```
**************************************************
Spotify Web API - Computational Analysis
**************************************************

**************************************************
Parsing playlists
**************************************************
Get tracks from list of playlists for mood boosting
Total number of tracks retrieved: 495
Get tracks from list of playlists for running
Total number of tracks retrieved: 502
Get tracks from list of playlists for studying
Total number of tracks retrieved: 505

**************************************************
Calculating statistics
**************************************************
Successfully calculated statistics and exported to tables subfolder.

**************************************************
Get top 5 genres
**************************************************
Total number of artists retrieved: 662
Unique number of artists retrieved: 363
Found 5 genres: pop, soul, edm, funk, disco

**************************************************
Get recommendations
**************************************************
Artists: Duke Dumont; Jax Jones
Name: I Got U
Preview-URL: None
Spotify-URL: https://open.spotify.com/track/4r8hRPbidDIoDPphxi78aY

Artists: Jess Glynne
Name: All I Am
Preview-URL: None
Spotify-URL: https://open.spotify.com/track/5GNjiM8jZCgbqjHklAcT9e

Artists: Thelma Houston
Name: Don't Leave Me This Way - Single Version
Preview-URL: None
Spotify-URL: https://open.spotify.com/track/4IMArXimMttK8tB0UBa0Ue

Artists: Britney Spears
Name: Till the World Ends
Preview-URL: https://p.scdn.co/mp3-preview/fd017ae5c0a60cff3d5f5b4d37c4e8617c93b11f?cid=33046789d13d49dda7597ce0554f1919
Spotify-URL: https://open.spotify.com/track/38iU2jg98IZZEIJPrP7aWD

Artists: Pitbull; Ne-Yo
Name: Time of Our Lives
Preview-URL: https://p.scdn.co/mp3-preview/c7ee72511ef4733508b42f92de3e62ed4753e223?cid=33046789d13d49dda7597ce0554f1919
Spotify-URL: https://open.spotify.com/track/2bJvI42r8EF3wxjOuDav4r

Artists: Flo Rida
Name: I Cry
Preview-URL: https://p.scdn.co/mp3-preview/83ecc26ad34e94df4a61ed9e44cdd3bacda81d6a?cid=33046789d13d49dda7597ce0554f1919
Spotify-URL: https://open.spotify.com/track/3zrYNl1aMdFrQkcOjKVr5u

Artists: Echosmith
Name: Cool Kids
Preview-URL: https://p.scdn.co/mp3-preview/27df0a89036135748fd03fb67114abeb00c529a4?cid=33046789d13d49dda7597ce0554f1919
Spotify-URL: https://open.spotify.com/track/13P5rwmk2EsoFRIz9UCeh9

Artists: Melanie Martinez
Name: Dollhouse
Preview-URL: https://p.scdn.co/mp3-preview/d98a9f944430689505c2ee20aa6180b6949830f0?cid=33046789d13d49dda7597ce0554f1919
Spotify-URL: https://open.spotify.com/track/6wNeKPXF0RDKyvfKfri5hf

Artists: Little Mix
Name: Touch
Preview-URL: None
Spotify-URL: https://open.spotify.com/track/6B7op3kK1kFQp4Ck1UZtK5

Artists: Martha Reeves & The Vandellas
Name: Dancing In The Street
Preview-URL: None
Spotify-URL: https://open.spotify.com/track/6TPl5DQrkBY2XIqIaFmxqi
```
",2022-08-12
https://github.com/kequach/kequach,"#  Hi, I'm `Keven` 👋

I'm a Business Informatics student and web development / data science freelancer. I am also a research assistant in the [Open Science](https://www.uu.nl/en/research/open-science) Program, more specifically in the Track [FAIR data and software](https://www.uu.nl/en/research/open-science/tracks/fair-data-and-software). 

You can find my online CV [here](https://kequach.github.io/). My UU employee page can be found [here](https://www.uu.nl/staff/KQuach).

How to reach me? Send me an email (see bio) 📫 

Or contact me via LinkedIn:

<a href=""https://www.linkedin.com/in/keven-quach-9907b7122/"" target=""_blank"" rel=""noopener noreferrer"">
  <img align=""left"" alt=""Keven Quach | LinkedIn"" width=""30px"" src=""https://github.com/kequach/kequach/blob/main/icons/linkedin.svg"" />
</a>
<!--
**kequach/kequach** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- 🔭 I’m currently working on ...
- 🌱 I’m currently learning ...
- 👯 I’m looking to collaborate on ...
- 🤔 I’m looking for help with ...
- 💬 Ask me about ...
- 📫 How to reach me: ...
- 😄 Pronouns: ...
- ⚡ Fun fact: ...
-->
",2022-08-12
https://github.com/kequach/Mapping-Research-Software-Landscapes-through-Exploratory-Studies-of-GitHub-Data,"# Mapping Research Software Landscapes through Exploratory Studies of GitHub Data

This repo holds the code, latex files, and instructions for my master thesis with the topic **Mapping Research Software Landscapes through Exploratory Studies of GitHub Data**. 

## Prerequisites 

Please refer to the [SWORDS-UU](https://github.com/UtrechtUniversity/SWORDS-UU) framework for necessary prerequisites. For this study, you will need some basic familiarity with Python and Jupyter Notebooks.

## Reproducing data retrieval

The data retrieval is based on the [SWORDS-UU](https://github.com/UtrechtUniversity/SWORDS-UU) framework. Please keep in mind that the steps are not 100% reproducible due to dependencies on external data of Utrecht University and GitHub.

* First, follow the instructions for [phase 1: **Find user profiles associated to organisation**](https://github.com/UtrechtUniversity/SWORDS-UU/blob/main/collect_users/README.md). This will yield a .csv or .xlsx file which can be found in this repository under [data/users_enriched.xlsx](/data/users_enriched.xlsx). This file is already manually labeled to exclude irrelevant users which include non-employee students and persons unaffiliated with UU. Due to formatting issues with .csv files, .xlsx files are chosen as the default.
* Next, we use the [collected information from the UU employee pages](https://github.com/UtrechtUniversity/SWORDS-UU/blob/main/collect_users/methods/profile_pages/results/profile_page_uu.csv) to relate employee information back to the collected GitHub profiles. This file can be found in this repository under [data/profile_page_uu_without_orgs.csv](/data/profile_page_uu_without_orgs.csv).
* Now, we want to annotate the faculty of each GitHub user with the corresponding employee profile. To do this, follow the instructions in the Jupyter Notebook [label_data.ipynb](/label_data.ipynb). This is partly automated through the information from the **profile_page_uu.csv** file mentioned in the previous step, as well as the names users provide themselves on GitHub. The rest of the users and organizations need to be manually annotated. The Jupyter Notebook holds some code to facilitate  this. After this step is done, the first phase of user retrieval, labeling, and annotating is done. The resulting file of this step can be found in this repository under [data/users_labeled.xlsx](/data/users_labeled.xlsx)
* Start [phase 2: Collect relevant repositories](https://github.com/UtrechtUniversity/SWORDS-UU/blob/main/collect_repositories/README.md). As input, use the file [data/users_labeled.xlsx](/data/users_labeled.xlsx). The resulting file can be found in this repository under [data/repositories_filtered.xlsx](/data/repositories_filtered.xlsx). Two additional columns were manually added: **repo_type** and **note**.
* Execute the last part after the title **Label repositories with faculty information** of the Jupyter Notebook [label_data.ipynb](/label_data.ipynb). This will annotate each repository with the corresponding faculty of the user. The resulting file can be found in this repository under [data/repositories_labeled_faculty.xlsx](/data/repositories_labeled_faculty.xlsx). This is also the fully labeled file.

* Next steps TBD


## Contact

In case of questions, don't hesitate to reach out! You can find more information on how to contact me on [my GitHub profile](https://github.com/kequach).

## License

Distributed under the MIT License. See `LICENSE` for more information.
",2022-08-12
https://github.com/kequach/MyAnimeList-Analysis,"# MyAnimeList-Analysis
Assignment for analysis of MyAnimeList dataset from Kaggle: https://www.kaggle.com/azathoth42/myanimelist

The task includes descriptive visualization and application of frequent itemset analysis.

Download the PDF here: https://github.com/beld78/MyAnimeList-Analysis/raw/main/Report.pdf
",2022-08-12
https://github.com/kequach/PrintXDKRequest,"# PrintXDKRequest
Simple REST api to print XDK light sensor data to browser
",2022-08-12
https://github.com/kequach/Seeding,"# Seeding
This is a WIP program which can be used to insert a list of seeded players and get back a list of pools by seed. You can also adjust and swap players.
Download link for current version: https://www.dropbox.com/s/6bgh6s5hep689pn/Seeding%20Tool.jar?dl=0

![alt tag](https://i.gyazo.com/3fcb5ae81def5949ec45168cdcc72a06.png)
![alt tag](https://i.gyazo.com/b49a3eed46f243cf86df084a89ad04a9.png)
![alt tag](https://i.gyazo.com/35d313d1c6c0568a96ae3759f62e6bc6.png)
",2022-08-12
https://github.com/kequach/SpotifySleep,"# SpotifySleep
![feature-graphic](Spotify-Sleep-feature-graphic.png?raw=true ""Title"")
**Currently it's only possible to listen for a maximum of 6 minutes due to Spotify API. This can be fixed by using the Web API to connect instead but I'm too lazy to fix this at the moment**  
App to send Spotify to sleep after X amount of songs  
# Screenshot
![Image of app](Screenshot_v1.1.jpg?raw=true ""Title"")

# Download
https://www.dropbox.com/s/e8nctuzy7tye1ql/SpotifySleep.apk?dl=0
",2022-08-12
https://github.com/kequach/WebDevTraining,"Git repository for web development training

What is included?

- Basic examples on how to use jQuery
- Simple API call to Weather data from https://openweathermap.org/
- Icon weather pack from flaticon.com
- Naked frontend to be designed
- Roboto Google Font

1
Resources for HTML, CSS and JavaScript:

| Used for                      | Link                                                    |
| ----------------------------- | ------------------------------------------------------- |
| General HTML CSS reference    | https://www.w3schools.com/html/default.asp              |
| Quick HTML CSS JS Cheat sheet | https://htmlcheatsheet.com/js/                          |
| Flexbox cheat sheet           | https://css-tricks.com/snippets/css/a-guide-to-flexbox/ |

Tasks for weather app: - API:https://openweathermap.org/current

- Implement functionality so you can press enter to activate the button
- Display time from when the request was made
- Change the added icon depending on the weather conditions
- Design: Make it user-friendly!
- Always display weather for Tokyo upon loading the page
- Display additional information from the API (hint: check the console for the information retrieved)
- Create 16 day forecast (hint: different API request has to be implemented)
",2022-08-12
https://github.com/kordejong/hello_cmake,"hello_cmake
===========
Project for trying out CMake code.

Requirements
------------
* After *building a project*, one must be able to use the targets without have to tweak the environment settings. Other project's and 3rd party shared libraries the project's targets depend on must be found.
* After *installing a project*, and tweaking the environment settings to find other project's and 3rd party shared libraries (setting LD_LIBRARY_PATH on *nix, for example), one must be able to use the targets.
* After *installing a project's package*, one must be able to use the targets with a minimal amount of environment setting tweaks. At most, the user should have to add a single entry to PATH and/or PYTHONPATH. I should not be necessary to set environment variables specifically for the project's exes and dlls to find other dlls.
* It must be possible to move a directory containing an installed package. This should not have an effect on the useability of the software. The package should be self-contained.
* All of the above should work on Linux, Windows and MacOS.

This project is about figuring out how to use CMake to meet these requirements. There is a minimal amount of C++ source code in this project. Just enough to make sure that there are some interesting dependencies between various targets and 3rd party shared libraries.

Targets
-------
The hello_cmake repository contains two projects managed by CMake build scripts. The first project, called `world`, builds targets with these dependencies:
* static library
* shared library
* application -> `world`'s static library and 3rd party shared library.
* application -> `world`'s shared library and 3rd party shared library.
* python extension -> `world`'s shared library and 3rd party shared libraries.

The second project, called `greeter`, builds targets with these dependencies:
* application -> `world`'s static library and 3rd party shared library.
* application -> `world`'s shared library and 3rd party shared library.

The project contains a script called `build_hello_cmake.sh` that builds, installs, unpacks, moves the sources and built targets, and tests whether the above mentioned requirements are met. This script can be called by yet another script as folows (example for Linux, using a bash script):

```bash
hello_cmake_root=<path to>/hello_cmake
build_hello_cmake=$hello_cmake_root/environment/scripts/build_hello_cmake.sh
boost_root=""$<path to boost>""
python_root=""$<path to python>""
ld_library_path=""$boost_root/lib:$python_root/lib""

cmake_options=""
    -DCMAKE_PREFIX_PATH=""$python_root""
    -DBOOST_ROOT=""$boost_root""
    -DHC_ENABLE_FIXUP_BUNDLE:BOOL=OFF
""

HELLO_CMAKE_ROOT=$hello_cmake_root $build_hello_cmake ""$ld_library_path"" \
    $cmake_options
```

To detect whether all shared libraries are found in the different usage scenarios, this script can be called like this:

```bash
build_hello_cmake_on_linux.sh 2>&1 | tee messages.txt
cat messages.txt | grep ""not found""
```

All is well if the last command doesn't print anything.

Notes
-----
* Targets often depend on shared libraries not build by the installed project, like boost, icu, qt, etc. If we can assume these will exist at the same location on the install machine, we can set `CMAKE_INSTALL_RPATH_USE_LINK_PATH` to `TRUE`. Otherwise we may need to ship these libraries ourselves, and set `CMAKE_INSTALL_RPATH_USE_LINK_PATH` to `FALSE`.
* Don't confuse installing a project with packaging a project. During installation, you normally don't install targets of other projects. They should already be installed. During packaging, you may want to put all kinds of stuff in the package, including 3rd party shared libraries and shared libraries from several of your own projects.
* `FIXUP_BUNDLE` can be used to copy prerequisites of exes and dlls into the install area to create a self-contained 'bundle'. This can be used to create self-contained packages.

See also:
* http://www.vtk.org/Wiki/CMake/Tutorials/Exporting_and_Importing_Targets
* CMake RPATH wiki page.
* HDF5 installs manifest file on Windows. We may need that too.
* http://www.cmake.org/Wiki/BundleUtilitiesExample
* http://www.vtk.org/Wiki/CMake/Tutorials/How_to_create_a_ProjectConfig.cmake_file

",2022-08-12
https://github.com/kordejong/hello_hpx,"# hello_hpx
Test build of HPX

Using the scripts in this project I verify whether HPX can be built the
way I like to use HPX in my project.

Start the build using this command:

```
bash ./build_all.sh
```

It will start a number of builds and dump compiler output to text files.

Last time built:

| Date     | Commit / tag | Result                |
|----------|--------------|-----------------------|
| 20210629 | 1.7.0-rc2    | Fail (HPX issue 5412) |
| 20210615 | 1.7.0-rc1    | Fail (HPX issue 5395) |
| 20210202 | 1.6.0-rc1    |                       |
| 20200828 | 1.5.0-rc3    |                       |
| 20200709 | 57f2f7671b9  |                       |
| 20200619 | 001c1bcc936  |                       |
",2022-08-12
https://github.com/kordejong/hello_thesis,"# hello_thesis
Here I show how I used LaTeX for formatting my PhD thesis. It is meant for inspiration for
others. I am not a LaTeX expert, so please don't ask me questions about how to do things in
LaTeX. If you have improvements, then I am happy to receive pull requests.

Software used, apart from LaTeX:
- [CMake](https://www.cmake.org) for generating project files to drive the generation of a
  PDF.
- [UseLATEX](https://gitlab.kitware.com/kmorel/UseLATEX) which contains a CMake function for
  defining a LaTeX PDF target. A copy is available in this repository.
- [ClassicThesis](https://ctan.org/pkg/classicthesis) which is a style specifically made
  for theses. Your LaTeX distribution probably already provides it.
- A build system for driving the generation of the PDF. See
  https://cmake.org/cmake/help/latest/manual/cmake-generators.7.html for a list of supported
  build systems.

To test things out, generate a PDF of the thesis like this:

```bash
git clone https://github.com/kordejong/hello_thesis.git

# Create a directory in which to store the PDF and all intermediate files
# created.
mkdir build
cd build

# Ask CMake to generate project files for the Ninja build too. Change Ninja
# to something else if you want to use another build tool.
cmake -G ""Ninja"" ../hello_thesis

# Tell Ninja to build the PDF
ninja
```

If all went well and you like the result, you can use the scripts and LaTeX sources as inspiration
for your own thesis project.

I made various changes to the default ClassicThesis configuration. Just compare the one in the
repository with the default one from the ClassicThesis zip file.
- Use biber instead of bibtex.
- I prefer the lines to be a little bit wider.
- I prefer to have black fonts, also for headers and links.
- ...
",2022-08-12
https://github.com/kordejong/my_devenv,"# my_devenv
Settings and stuff I find useful
",2022-08-12
https://github.com/kordejong/pcraster_scratch_pad,"pcraster_scratch_pad
====================",2022-08-12
https://github.com/kordejong/track_time,"Given raw information about the amount of time spent on various projects, track_time aggregates this information and creates reports.

This project is very much work in progress.
",2022-08-12
https://github.com/kovvalsky/assigntools,"The script files contain functions that are provided as ready-to-use for the assignments in courses.
",2022-08-12
https://github.com/kovvalsky/DRS2Graph,"The repository contains scripts that help to convert Discourse Representation Structures (DRSs) into Graph formatted as JSON, the format adopted at the [MRP shared task](http://mrp.nlpl.eu/2020), and back from the graph to DRS.     

### Convert DRSs from the Clausal Form (CLF) to Discourse Representation Graphs (DRG)
`clf2graph.py` supports different types of conversion depending on how to represent concept or role clauses (labelled node vs labelled edge), where to place role nodes (between vs as a parent of arguments), and whether to label argument edges (with vs without ARG[12]).  

#### Supported conversion parameters
   - `-ce` treat concepts as labelled edges, otherwise as labelled nodes (default)  
   - `-rle` treat roles as unlabeled nodes with ingoing labelled edges, otherwise as labelled nodes (default)  
   - `-rmid` place roles between their arguments, otherwise as a parent of its arguments (default)  
   - `-noarg` don't place `ARGn` labelled or argument edges, otherwise place them (default)  
   - `-bm [all arg1 role a1]` box membership (`bm`) representation modes:
     - `all` `bm` edges are present;
     - `bm` edges are kept for `role`s but omitted for role arguments if both arguments (excepy constants) have the same membership as their role;
     - `arg1` `bm` edges are kept, but removed for its role if `arg1` has only that `bm` edge what its role has.
     - `a1` further develops `arg1` and also omits the `bm` edge for arg2 if the latter has `bm` edge for the same box as arg1 and teh role.

#### CLF ↦ MRP DRG 
For example, the DRGs used in the [MRP shared task](http://mrp.nlpl.eu/2020/) are produced with (using a sample DRS): 
```
./clf2graph.py --input test_suite/crops/crops.clf --raw test_suite/crops/crops.raw --output test_suite/crops/crops.mrp -noarg -rmid -bm arg1 --sig clf_signature.yaml
```
Note that with this particular combination of parameters, some CLFs might not be convertible as the conversion assumes a single (maximally specific) concept per discourse referent.   

#### MRP DRG ↦ CLF
Clausal forms can be recovered from MRP DRGs by:
```
./mrp2clf.py --mrp test_suite/crops/crops.mrp --clf test_suite/crops/crops.mrp.clf --sig clf_signature.yaml
```

### Requirements
Python 3
",2022-08-12
https://github.com/kovvalsky/GeoGram,"# GeoGram
An HPSG-based Formal Grammar of a Core Fragment of Georgian Implemented in TRALE.

This is a part of an [MSc thesis](docs/Thesis%20-%20HPSG%20for%20Georgian.pdf) ([official page](https://dspace.cuni.cz/handle/20.500.11956/48423)).
The presentation slides of the thesis defence are [here](docs/GeoGram-presentation.pdf).

An animated visualization of the sample parse with the [`Gralej`](https://code.google.com/archive/p/gralej/) graphical interface:

[<img src=""img/chven_vdumvarth.gif"" height=""512""/>](img/chven_vdumvarth.gif) [<img src=""img/[chven,vdumvarth].png"" height=""512""/>](img/[chven,vdumvarth].png)

___
### Table of content
* [Georgram versions](#versions)
* [TRALE installation](#installation)
* [TRALE docs](#trale_docs)
* [Running the GeoGram grammar](#running)
  * [Run TRALE](#run_trale)
  * [Compile the grammar](#compile)
  * [Test on the test suite & explore parses](#test)
  * [Parse a custom input](#parse_1)
  * [Prasing ambiguity](#parse_2)
* [Reference](#reference)

___
# GeoGram versions <a name=""versions""/>
GeoGram is presented as six nested grammars -- starting from the initial grammar `gr1` and ending with the final grammar `gr6`. The next version adds new components to the previous version.
Such organization is mainly motivated for teaching purposes. 

Short descriptions and relations between these versions of GeoGram:
* `gr1`: covers the verb complementation by nouns.
* `gr2`: `gr1` + verb complementation by nouns and pronouns, distinction between explicit and implicit arguments modeling the polypersonal agreement and pro-drop properties.
* `gr3`: `gr2` + adjunction of the noun by quantifiers and adjectives.
* `gr4`: `gr3` + adjunction of the noun by noun and pronoun possessives.
* `gr5`: `gr4` + noun complementation by possessive phrases
* `gr6`: `gr5` + nominalized quantifiers and adjectives, lexical rules for nominals - argument declension, adjunct declension, adjunct nominalization, pluralization and possessivization; and lexical rules for verbal conjugation - conjugation paradigm I, conjugation paradigm II and conjugation paradigm III.

**Use the last version if you are only interested in the most complete version of GeoGram.**


# TRALE installation <a name=""installation""/>
Before you run the grammar, you need to install TRALE.
TRALE is a system for parsing, logic programming and constraint resolution with typed feature structures in a manner roughly consistent with their use in Head-driven Phrase Structure Grammar (HPSG).

There are several ways to install TRALE depending on your machine and availability of a SICStus Prolog:
* [TRALE download page](http://milca.sfs.uni-tuebingen.de/A4/Course/trale/) - this is considered as an official page, where you can find the lattes version of TRALE (at the time of writing this, it needs a Sicstus Prolog installation). This is the most comprehensive but arguably the least easy way of installing TRALE.
* [Grammix](https://hpsg.hu-berlin.de/Software/Grammix/) - a bootable CD Rom that contains TRALE along with some sample grammars (e.g., it can be run on windows OS via Virtual Machines). This installation can be suitable for teaching.
* [Standalone TRALE](https://hpsg.hu-berlin.de/Software/Trale/) - a version that does not need commercial Sicstus Prolog but does need Linux OS. This version of TRALE was used during the development of GeoGram.

More details about the installation (and running grammars) can be found in [this comprehensive instructions](http://utkl.ff.cuni.cz/%7Erosen/public/trale.pdf) by Alexandr Rosen (copy of it is also located in `docs/`). 

# TRALE docs <a name=""trale_docs""/>
If you want to know more about TRALE, have a look at these docs:
* [ALE - The Attribute Logic Engine User's Guide with TRALE extensions](http://www.ale.cs.toronto.edu/docs/man/ale_trale_manual.pdf)
* [TRALE Reference Manual (Draft)](http://www.ale.cs.toronto.edu/docs/ref/ale_ref.pdf)
* [Trale Milca Environment v.2.5.0 User's Manual (Draft)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.517.737&rep=rep1&type=pdf)


# Running the GeoGram grammar <a name=""running""/>

These commands are tested on the standalone TRALE.

Go to the directory where `theory.pl` of the grammar you want to run is located <a name=""run_trale""/> 
and execute TRALE with `s`hell mode, `g`raphical interface, and from stabnd`a`lone executable. 
```
$ cd GeoGram/gr6
$ trale -sag
Using Trale system found in $TRALE_PATH 
Starting Trale (no saved state) from sources and the gralej interface
   Starting gralej interface . started on 5001 of host localhost
   Starting standalone version of Trale (standalone-trale.Linux)
Loading Interface connection specifications (options -g):

Establishing connection to interface on port 5001 of localhost.
Connection established.


TRALE Milca environment (version 2.7.12)
Copyright (C) 2002/3 Project MILCA A4
PIs: Detmar Meurers (OSU), Gerald Penn (Univ. Toronto), Frank Richter (Univ. T�bingen)
All rights reserved

 ! ?-
```

For the meaning of flags, run `trale -h`.
Running TRALE with graphical interface, you should see an additional window. In the screenshot `Gralej` is used for graphical interface:

[<img src=""img/Gralej_empty.png"" height=""100""/>](img/Gralej_empty.png)

___
Compile the grammar: <a name=""compile""/>

```
 ! ?- c.
Reading signature file...
signature
Compiling type unification...
{ALE: Warning: unary branch from arg_npn to arg_noun}
{ALE: Warning: assuming the following types are maximally specific: abl1 abl2 adj_ adj_i adj_lex adv ah_phrase arg_noun attrib ben both cat ch_phrase cnst com comps conj1 conj2 conj3 dat dat_c decl_a decl_e decl_i decl_ou dynamic e_lex e_list erg erg_c frame fut gen hc_phrase ine ins loc minus n_poss na ne_list nom nom_c noun_lex ori per1 per2 per3 plur plus pn_poss pnp_q poss_noun poss_pn prs psc pst qnt_ qnt_i qnt_lex quant sing static stative ter unspec verb verb_lex verb_val word }
Compiling appropriateness...
{ALE: Warning: homomorphism condition fails for case in adjunct and noun}
{ALE: Warning: homomorphism condition fails for case in noun and adjunct}
{ALE: Warning: homomorphism condition fails for case in noun and poss_npn}
{ALE: Warning: homomorphism condition fails for case in poss_npn and noun}
Compiling extensionality declarations...
Compiling subtype covering constraints...
Compiling functional and macro descriptions...
Compiling type constraints...
Compiling most general satisfiers...
Compiling type promotion...
Compiling feature selection...
Compiling unification...
Compiling definite clauses...
Compiling lexical rules...
Compiling lexicon...
Compiling empty categories and phrase structure rules...

yes
```

___
Run the grammar on the test suite. It should pass all the tests (more than 1,000 tests). <a name=""test""/> 
```
 ! ?- test(all).
 (1)    studenti kithxulobs tcigns  % The student reads the book
 ==>    0.010 sec CPU time.        1 solutions (ok); 6 passive edges; residues: 0

 (2)    studentebi kithxuloben tcignebs  % students reads books
 ==>    0.010 sec CPU time.        1 solutions (ok); 6 passive edges; residues: 0
  .
  .
  .
 (1393) chven vdumvarth  
 ==>    0.000 sec CPU time.        1 solutions (ok); 5 passive edges; residues: 5

 (1394) enatrebodath patara lamazi dzaghli  
 ==>    0.000 sec CPU time.        1 solutions (ok); 12 passive edges; residues: 5

yes
```

The screenshot shows all parsed phrases from the test suite, where you can select any phrase from the list and explore it:

[<img src=""img/Gralej_test_all.png"" height=""256""/>](img/Gralej_test_all.png)
[<img src=""img/chven_vdumvarth.gif"" height=""256""/>](img/chven_vdumvarth.gif)

___
Parse a custom phrase. <a name=""parse_1""/>
Note that the `lexicon.pl` should cover the vocabulary of the phrase, otherwise parsing will fail.
In the example, the grammar parses `my dog likes each woman's some kid`.
There is only one parse of the sentence found.

```
 ! ?- rec[chemi,dzaghli,uyvars,yvela,qalis,zogierth,bavshvs].

STRING: 
0 chemi 1 dzaghli 2 uyvars 3 yvela 4 qalis 5 zogierth 6 bavshvs 7

Do you want to try for more solutions? (return for <yes>) yes


no
```

This is the screenshot of the full parse tree:

[<img src=""img/[chemi,dzaghli,uyvars,yvela,qalis,zogierth,bavshvs].png"" height=""300""/>](img/[chemi,dzaghli,uyvars,yvela,qalis,zogierth,bavshvs].png)

___
Now let's see an example of a phrase that has more than one parse. <a name=""parse_2""/>
Parse `all wine's house`:

```
 ! ?- rec[yvela,ghvinis,saxli].

STRING: 
0 yvela 1 ghvinis 2 saxli 3

Do you want to try for more solutions? (return for <yes>) yes


Do you want to try for more solutions? (return for <yes>) yes


no
```

Two parses of the phrase are `(all wine)'s house` meaning *house of all wine* and `all (wine's house)` meaning *all wine houses*:

[<img src=""img/[yvela,ghvinis,saxli]_1.png"" height=""256""/>](img/[yvela,ghvinis,saxli]_1.png)
[<img src=""img/[yvela,ghvinis,saxli]_2.png"" height=""256""/>](img/[yvela,ghvinis,saxli]_2.png)


# Reference <a name=""reference""/>

If you use GeoGram, cite the following work:

```
@MastersThesis{Abzianidze:thesis:2011,
    author  = ""Abzianidze, Lasha"",
    title   = ""An {HPSG}-based Formal Grammar of a Core Fragment of {Georgian} Implemented in {TRALE}"",
    school  = ""{C}harles {U}niversity in {P}rague"",
    address = ""the {C}zech {R}epublic"",
    year    = 2011
    }
```
 
",2022-08-12
https://github.com/kovvalsky/LangPro,"# [LangPro](https://github.com/kovvalsky/LangPro): Natural [Lang](https://github.com/kovvalsky/LangPro)uage Theorem [Pro](https://github.com/kovvalsky/LangPro)ver
LangPro is a tableau-based theorem prover for natural logic and language.
See the [online demo](https://naturallogic.pro/LangPro/) (not the latest version).
<!-- (http://naturallogic.pro/langpro). -->

Given a set of premises and a hypothesis in natural language, LangPro tries to find out semantic relation between them: `entailment` (i.e. `yes`), `contradiction` (i.e. `no`) or `neutral` (i.e. `unknown`).  
For this, LangPro needs CCG (Combinatory Categorial Grammar) derivations of the linguistic expressions in order to obtain Lambda Logical Forms (LLFs) from them via the LLFgen (LLF generator) component. The architecture is depicted below: 
```
____________    ________             ___________      ________________________    __________ 
|Premises &|    | CCG  | derivations |   LLF   | LLFs |Tableau Theorem Prover|    |Semantic|
|Hypothesis|--->|Parser|------------>|Generator|----->|  for Natural Logic   |--->|relation|
‾‾‾‾‾‾‾‾‾‾‾‾    ‾‾‾‾‾‾‾‾             ‾‾‾‾‾‾‾‾‾‾‾      ‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾    ‾‾‾‾‾‾‾‾‾‾ 
```
If you use the theorem prover, please cite [Abzianidze (2017)](https://www.aclweb.org/anthology/D17-2020):
```
@inproceedings{abzianidze-2017-langpro,
    title = ""{L}ang{P}ro: Natural Language Theorem Prover"",
    author = ""Abzianidze, Lasha"",
    booktitle = ""Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"",
    month = sep,
    year = ""2017"",
    address = ""Copenhagen, Denmark"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/D17-2020"",
    doi = ""10.18653/v1/D17-2020"",
    pages = ""115--120""
}
```

For the manual on how to use the prover or how to obtain reported results, consult the [wiki](https://github.com/kovvalsky/LangPro/wiki). 

# Quick links to the [wiki pages](https://github.com/kovvalsky/LangPro/wiki):

* [Using the prover](https://github.com/kovvalsky/LangPro/wiki/Using-the-prover)
* [Producing LLFs](https://github.com/kovvalsky/LangPro/wiki/Producing-LLFs)
* [Learning as abduction](https://github.com/kovvalsky/LangPro/wiki/Learning-as-abduction)
* [References](https://github.com/kovvalsky/LangPro/wiki/References)
",2022-08-12
https://github.com/kovvalsky/prove_SICK_NL,"# prove_SICK_NL
Prove Ducth NLI problems of [SICK-NL](https://github.com/gijswijnholds/sick_nl) with [LangPro](https://github.com/kovvalsky/LangPro).
Note that the current repo contains SICK-NL version that corresponds its English counterpart used for [Semeval-2014 Task 1](https://alt.qcri.org/semeval2014/task1/). SemeEval version contains in total 9927 problems while the original one 9840 problems.

# Prerequisites

The current repo and the [LangPro](https://github.com/kovvalsky/LangPro) repo shoudl be in the same directory.

Get Langpro repo with
`git clone --branch nl git@github.com:kovvalsky/LangPro.git` or `git clone --branch nl https://github.com/kovvalsky/LangPro.git`.
It is used for theorem-proving and converting type-logical terms into simply-typed terms.
Note that `nl` branch is relevant one.
Additionally add `--single-branch` if you want to clone only `nl` branch.


`produce.ini` contains rules how to generate files.
You will need to install [produce](https://github.com/texttheater/produce) if you want to use the rules to build files from scratch.

# HowTo

## Theorem proving SICK-NL problems
Before proving the problems, either enter the prolog interactive mode (recommended for a demo usage):
```
% loading the prover with alpino (or npn_robbert) trees
$ swipl -f prolog/main.pl  SICK_NL/sen.pl  SICK_NL/parses/alpino.pl  WNProlog/wn.pl
% This can be run only in the beginning, to set the global parameters: the part of the dataset, language flag, lexical annotation file, and theorem proving parameters
?- parList([parts([train]), lang(nl), anno_json('SICK_NL/anno/alpino.json'), complete_tree, allInt, aall, wn_ant, wn_sim, wn_der, constchck]).
```
Or run the prolog goals directly from the terminal:
```
$ swipl -g ""PROLOG_PREDICATES_TO_BE_CHECKED"" -t halt -f prolog/main.pl  SICK_NL/sen.pl  SICK_NL/parses/alpino.pl  WNProlog/wn.pl
```

### Prove a particular problem without abductive training

```
% In an interactive mode, prove a problem and pretty display the proof in a separate window
% Run LangPro in the graphical mode with aligned terms (if the prove is found, it is most probably done with aligned terms as this mode is tested first for efficiency reasons.)
?- gentail(aligned, 8502).
Tableau for ""yes"" checking is generated with Ter,6 ruleapps
XP: [isa(man,persoon),isa(meer,water)]
true.
```
An image of the proof: <img src=""img/8502.png"" height=""100""/>

```
# The same but with a terminal command:
$ swipl -g ""parList([parts([train]), lang(nl), anno_json('SICK_NL/anno/alpino.json'), complete_tree, allInt, aall, wn_ant, wn_sim, wn_der, constchck]), gentail(aligned, 8502)."" -f prolog/main.pl  SICK_NL/sen.pl  SICK_NL/parses/alpino.pl  WNProlog/wn.pl
```
Check [LangPro](https://github.com/kovvalsky/LangPro) repo for running the prover for entire data split.


## Generate typed terms in LaTeX/PDF
### For all sentences filtered with a label or a part

The rule uses the corresponding json annotation file and parse terms from a parser (`npn_robbert` or `alpino`) to obtain annotated simply-typed terms and format them in LateX. The `trial` part keeps only those terms whose sentences occur in the TRIAL part of SICK. Usually it is good to use a filter otherwise files tend to be >15MB and its later compilation into PDF will take long time. Other options for filter are `yes` (problems with `entailment` label), `no` (problems with `contradiction` label), `unknown` (problems with `neutral` label), `train`, `test`, and `all` (i.e. no filters).

```
produce -d -f produce.ini  SICK_NL/latex/npn.spacy_sm.trial.tex
```

If you want additionally to `tex` file to create `pdf` from it, run:

```
produce -d -f produce.ini  SICK_NL/latex/npn.spacy_sm.trial.pdf
```
The conversion uses `lualatex` as it is faster than `pdflatex` and can deal with huge files (well, at least on my machine:)).

### For a specific NLI problem
Create a pdf that depicts how initial trees are converted into the final trees for the sentences of an NLI problem with a specific ID (e.g., 1333).
```
produce -d -b -f produce.ini   SICK_NL/latex/npn.spacy_lg.1333.pdf
```
",2022-08-12
https://github.com/kraaijenbrink/earthengine-workshop,"# Google Earth Engine workshop

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4068478.svg)](https://doi.org/10.5281/zenodo.4068478)

This repository contains the manual and JavaScript code for my Google Earth Engine workshop. The exercises are meant to give you an idea of the capabilities of Google Earth Engine.

Workshop material:

-   [Introductory powerpoint slides](https://github.com/kraaijenbrink/earthengine-workshop/raw/main/earth-engine-workshop-intro-slides.pptx)

-   [**Manual**](https://kraaijenbrink.github.io/earthengine-workshop) (Gitbook hosted as a Github page from `./docs` directory, created using `bookdown`)

All workshop code is available in `./code`. My [`earthengine-tools`](https://github.com/kraaijenbrink/earthengine-tools) repository contains additional functions that are called by the scripts in this manual.
",2022-08-12
https://github.com/kraaijenbrink/ERA5-Land-globe-animation,"# ERA5-Land globe animation

Code to export ERA5-Land data from Google Earth Engine and animate it using `R` and `ggplot2`  
  
  
[Animation video](https://youtu.be/hkAlhjrU0qE)


![](http://i3.ytimg.com/vi/hkAlhjrU0qE/hqdefault.jpg)
",2022-08-12
https://github.com/kraaijenbrink/nature-2017,"## Mass balance gradient glacier model

### Scripts

##### kraaijenbrink2017-debris-classification.js
Google Earth Engine script in JavaScript that performs Landsat satellite image analysis and debris-cover classification.

##### kraaijenbrink2017-mbg-model.r
The main model script written in R. Required source data can be found [here](https://doi.org/10.5281/zenodo.3346675).

##### kraaijenbrink2017-regional-aggregation.r
Script to aggregate the model output to (RGI) regions.

### Contact
<p.d.a.kraaijenbrink@uu.nl>


### Reference
[Kraaijenbrink PDA, Bierkens MFP, Lutz AF and Immerzeel WW (2017). Impact of a global temperature rise of 1.5 degrees Celsius on Asia’s glaciers. Nature (doi:10.1038/nature23878)](http://doi.org/10.1038/nature23878)
",2022-08-12
https://github.com/kraaijenbrink/pkrf,"# pkrf
""Philip Kraaijenbrink's R Functions"". An `R` package with a combination of functions and `rmarkdown` templates for my day to day use.

To install use `devtools` package:
```r
devtools::install_github('kraaijenbrink/pkrf')
```

&nbsp;

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3351498.svg)](https://doi.org/10.5281/zenodo.3351498)
",2022-08-12
https://github.com/kraaijenbrink/plotmanageR,"# plotmanageR

An RStudio addin with functions to manage external plot windows, mappable to keyboard shortcuts.


Install using the `devtools` R package using:  
`devtools::install_github('kraaijenbrink/plotmanageR')`
",2022-08-12
https://github.com/kraaijenbrink/themes-and-styles,"# themes-and-styles
semi-custom syntax highlighting styles for different software and other styling files
",2022-08-12
https://github.com/kraaijenbrink/warmingstripes-3d,"# warmingstripes-3d

R code to generate the frames of a 3D animation of Ed Hawkins' [warmingstripes](https://showyourstripes.info/) using the [rayshader](https://github.com/tylermorganwall/rayshader) package.

Release `single-stripe` makes an animation of a single strip of warming stripes, while release `double-stripes-incl-co2` shows a comparison between two strips of stripes with an animated line plot with CO2 concentration below. Default single-stripe code uses homogenized monthly temperature time series of De Bilt obtained from the *Royal Netherlands Meteorological Institute (KNMI)*. Default double stripe code compares global temperature data with that of Kathmandu, Nepal.

&nbsp;


Single stripes:  

[![Animation video](https://img.youtube.com/vi/_O6rq3FAOws/0.jpg)](https://www.youtube.com/watch?v=_O6rq3FAOws)

&nbsp;  
&nbsp;  

Double stripes:  

[![Animation video](https://img.youtube.com/vi/FphWExHvTD4/0.jpg)](https://www.youtube.com/watch?v=FphWExHvTD4)

",2022-08-12
https://github.com/kylelang/burn-data-synthesis,"# burn-data-synthesis
This repository holds the code to synthesize the NL burn data for Nancy van Loey's project.
",2022-08-12
https://github.com/kylelang/cat-code-interpret,"# cat-code-interpret

This repository contains a number of (very ugly) R scripts that I've written to
explore the interpretation of the intercept term in multiple linear regression
models that contain different categorical predictors.

The *code* directory contains several R scripts that each contain a series of 
test cases. Each test case follows the same recipe:
1. Simulate some regression data
1. Estimate the (true) regression model
1. Compare the estimated intercept to its theoretical equivalent computed from
   the raw data
1. Compare the estimated intercept to its theoretical equivalent computed via
   the **emmeans** package
   
The test cases represent a nearly complete crossing of the following design
factors:
1. Type of Code: {Dummy Code, Unweighted Effects Code, Weighted Effects Code}
1. Correlation between Predictors: {Independent Predictors, Correlated
   Predictors}
1. Error Variance in the Outcome: {Deterministic Outcome, Noisy Outcome}
1. Balance of Group Sizes: {Balanced Groups, Unbalanced Groups}

Regardless of the setup, the estimated intercept always matches the respective
marginal mean computed via the **emmeans** package. The estimated intercepts
sometime match, and sometimes do not match, their respective marginal means
computed from the raw data. A summary of the concordance between the intercept
estimates and the raw data-based mean estimates is available in this [Google
Sheet](https://docs.google.com/spreadsheets/d/1mckS-lc754z989uuLWjGnCaFrwU3FB2fpNT4mLNtZUU/edit?usp=sharing).
",2022-08-12
https://github.com/kylelang/impDvSim,"# impDvSim
This repository holds the code for my DV Imputation Simulation.
",2022-08-12
https://github.com/kylelang/Introduction-to-R,"# Introduction to R

This repository will hold all of the materials for the Utrecht University Winter 
School course: *Introduction to R*.

## Preparation

To participate in this course, you will need a few things:

1. You will need access to the contents of this repository on your local machine.
   - If you're familiar with Git, you can clone this repository.
   - If you are not familiar with Git, you can download a ZIP archive of this 
	 repository's current content via the `Download ZIP` option in the drop-down 
	 menu triggered by the green `Code` button at the top of this page.
      - Extract the contents of this ZIP archive to a convenient location on 
		your computer.
	  - We will be running the course out of this directory.
	  - Do not move any of the contents (doing so will break a bunch of relative 
		file paths).
   - Note that I'll probably be pushing some last-minute updates, so you may 
	 want to pull down recent commits (or download a fresh ZIP image) on the 
	 morning of the course.
	 
2. You will also need to have `R` installed on your computer as well as some 
   convenient way of working with `R` (e.g., `RStudio`).
   - It will also be helpful to install the necessary add-on packages before the 
	 course begins.
   
### Installing R & RStudio

- You can obtain a free copy of `R` [here](https://cran.r-project.org). 
- You can download `RStudio` as stand-alone software [here](https://www.rstudio.com/products/rstudio/download/#download). 
  - You want the free, open-source *RStudio Desktop* version.

### Installing Necessary Packages

We will use several add-on packages in this course. You can save yourself some 
time and hassle at the beginning of the course by installing these packages 
ahead of time.

#### Method 1

Open the script saved as [`code/00_install_packages.R`](code/00_install_packages.R)
in `RStudio`, and run the contents.

- You can run the contents of the script by selecting all the text and hitting 
*CTRL-ENTER* on Windows/Linux or *CMD-ENTER* on Mac.

#### Method 2

Copy-paste the following lines of code into the `RStudio` console window to 
execute the necessary command.

- If nothing happens after you paste the code, try hitting the ""Enter/Return"" 
key.

        install.packages(c(""haven"",
                           ""foreign"",
                           ""openxlsx"",
                           ""readxl"",
                           ""dplyr"",
                           ""magrittr"",
                           ""psych"",
                           ""rockchalk"",
                           ""multcomp"",
                           ""ggplot2"",
                           ""gridExtra""),
                         repos = ""http://cloud.r-project.org"",
                         dependencies = TRUE)

If you are asked the following question:

	Do you want to install from sources the package which needs 
    compilation? (Yes/no/cancel)

Type `Yes` in the console, and press the ""Enter/Return"" key (or click the 
corresponding button if the question presents as a dialog box). 
",2022-08-12
https://github.com/kylelang/lavaan-e-learning,"# lavaan-e-learning
This repository will hold the materials for the Lavaan E-Learning course given as part of the Utrecht University Summer School course.
",2022-08-12
https://github.com/kylelang/lecture-slides,"# lecture-slides
This repository contains various sets of lecture slides that I have developed over they years.
",2022-08-12
https://github.com/kylelang/MI-for-Prediction,"# Do We Need Multiple Imputation for Prediction?

When analyzing incomplete data, we must address the missing values in some
way. Missing data imputation is one way in which we can do so. The process of
imputing the missing data entails estimating a statistical model and using
predictions from this model to replace the missing values. Broadly speaking, we
can do missing data imputation in two ways: single imputation (SI) or multiple
imputation (MI). When testing hypotheses or doing inference on estimated
parameters, SI will produce attenuated standard errors that will inflate Type I
error rates and produce overly narrow confidence intervals (CIs; Enders,
2010). MI was developed by Rubin (1978, 1987) to address this limitation and
produce accurate statistical inferences from imputed data. MI is much more
computationally intensive than SI, however, and there are certain circumstances
where the additional computational burden may not be necessary.

Well-implemented SI routines can produce unbiased point estimates of parameters,
and there are certain data analytic contexts wherein point estimation is the
only objective. Point prediction problems are one such context. Computational
efficiency and scalability are often paramount concerns in prediction problems,
so SI applications dominate MI in the prediction literature (e.g.,
García-Laencina, Sancho-Gómez, & Figueiras-Vidal, 2010). Indeed, the
unbiasedness of SI suggests that point predictions should also be unbiased when
generated from a model fit to singly imputed data. Point predictions are not the
whole story, though. In practice, we often want some type of interval estimate
around predictions (e.g., a CI or a prediction interval [PI]). As in the case of
CIs for model parameters, we would also expect CIs and PIs for predicted values
to be too narrow.  

In this project, you will use Monte Carlo simulation methods to explore the
relative influence of SI and MI on point predictions, CIs for predicted values,
and PIs. This project can support up to three students. There are many different
ways to parameterize imputation models and many problem characteristics which
may affect the relative performance of imputation methods. Therefore, each
student will work with the supervisor to define their own operationalization of
the problem. Each student will then run an independent simulation study to
explore their conceptualization of the problem. Of course, students are
encouraged to collaborate on any overlapping portions of their respective
codebases to avoid “reinventing the wheel”. Throughout this project, we will
strive to follow best practices in open science and reproducible research
workflows.
",2022-08-12
https://github.com/kylelang/MIBRR,"# MIBRR: Multiple Imputation with Bayesian Regularized Regression
This repository hosts development of the R package `MIBRR`.

- Licensing information is given in the [LICENSE][] file.
- Built tarballs of the `MIBRR` package are available in the [builds][] 
  directory.
- Stand-alone documentation is available in the [documentation][docs] directory.
- The source files for the most recent stable version of `MIBRR` are available 
  in the [source][src] directory.

`MIBRR` is alpha software, so please expect frequent---and dramatic---changes to 
the package's functionality and user interface. Please report any bugs that you 
encounter in the issues section of the project page. You may also leave requests 
for new features in the issues section.

Thank you for your interest in the MIBRR project! I hope you find my software
useful!

## Installation
The best way to install `MIBRR` is to use the `devtools::install_github` 
function.

1. First, make sure that you have `devtools` installed on your system
2. Next, execute the following lines:

		library(devtools)
		install_github(""kylelang/MIBRR/source/MIBRR"")
    
3. Finally, load `MIBRR` and enjoy:

		library(MIBRR)

If the `devtools`-based approach does not work, you can download one of the
built tar-balls from the [builds][] directory and manually install the package
from source by executing the following lines:

	install.packages(pkgs  = ""/save_path/MIBRR_version.tar.gz"",
	                 repos = NULL,
                     type  = ""source"")

Where *save_path* is replaced by the (relative or absolute) file path to the
location where you saved the tar-ball, and *version* is replaced with the correct
version number for the tar-ball that you downloaded.

## Examples

The `MIBRR` package contains four primary functions: `miben`, `mibl`, `ben`, and 
`bl`.

- The underlying model in each of these four primary functions can be estimated
  using either Markov Chain Expectation Maximization (MCEM) or fully Bayesian
  modeling.
- The `miben` and `mibl` functions do multiple imputation using the Bayesian 
  elastic net and Bayesian LASSO, respectively.
  
    - A list of imputed datasets can be generated using the `getImpData` 
	  function.
	
- Basic missing data treatments using `miben` or `mibl` might look like the 
  following:

		## Load some data:
		data(mibrrExampleData)

		## Estimate the imputation models using MCEM:
		mibenOut <- miben(data       = mibrrExampleData,
                          targetVars = c(""y"", paste0(""x"", c(1 : 3))),
                          ignoreVars = ""idNum"")
			  
		miblOut <- mibl(data       = mibrrExampleData,
                        targetVars = c(""y"", paste0(""x"", c(1 : 3))),
                        ignoreVars = ""idNum"")
						
		## Estimate the imputation models using fully Bayesian modeling:
		mibenOut <- miben(data         = mibrrExampleData,
		                  targetVars   = c(""y"", paste0(""x"", c(1 : 3))),
                          ignoreVars   = ""idNum"",
                          doMcem       = FALSE,
                          sampleSizes  = c(500, 500),
                          lam1PriorPar = c(1.0, 0.1),
                          lam2PriorPar = c(1.0, 0.1)
                          )
			  
		miblOut <- mibl(data         = mibrrExampleData,
		                targetVars   = c(""y"", paste0(""x"", c(1 : 3))),
                        ignoreVars   = ""idNum"",
                        doMcem       = FALSE,
                        sampleSizes  = c(500, 500),
                        lam1PriorPar = c(1.0, 0.1)
                        )
				
		## Extract list of 100 imputed datasets:
		mibenImps <- getImpData(mibrrFit = mibenOut, nImps = 100)
		miblImps  <- getImpData(mibrrFit = miblOut, nImps = 100)
		
- The `ben` and `bl` functions fit Bayesian elastic net and Bayesian LASSO
  models to incomplete data without returning and imputed datasets.
- Use the following to fit models using `ben` or `bl`:

		## Load some data:
		data(predictData)

		trainData <- predictData$train
		testData  <- predictData$test
		
		## Estimate a Bayesian elastic net model using MCEM:
		benOut <- ben(data = trainData,
                      y    = ""agree"",
                      X    = setdiff(colnames(trainData), ""agree"")
                      )
		   
		## Estimate a Bayesian LASSO model using MCEM:
		blOut <- bl(data = trainData,
                    y    = ""agree"",
                    X    = setdiff(colnames(trainData), ""agree"")
                    )

		## Estimate a Bayesian elastic net model using full Bayes:
		benOut <- ben(data         = trainData,
                      y            = ""agree"",
                      X            = setdiff(colnames(trainData), ""agree""),
                      doMcem       = FALSE,
                      sampleSizes  = c(500, 500),
                      lam1PriorPar = c(1.0, 0.1),
                      lam2PriorPar = c(1.0, 0.1)
                      )
		   
		## Estimate a Bayesian LASSO model using full Bayes:
		blOut <- bl(data         = trainData,
                    y            = ""agree"",
                    X            = setdiff(colnames(trainData), ""agree""),
                    doMcem       = FALSE,
                    sampleSizes  = c(500, 500),
                    lam1PriorPar = c(1.0, 0.1)
                    )
					
		## Extract posterior parameter samples:
		benPars <- getParams(mibrrFit = benOut, target = ""agree"")
		blPars  <- getParams(mibrrFit = blOut, target = ""agree"")
		
		## Generate out-of-sample predictions:
	    benPred <- postPredict(mibrrFit = benOut, newData = testData)
		blPred  <- postPredict(mibrrFit = blOut, newData = testData)
		
- Posterior predictions can also be generated from `miben` and `mibl` models:

		## Load some data:
		data(predictData)

		missData <- predictData$incomplete
		testData <- predictData$test
		
		## Estimate the imputation models:
		mibenOut <- miben(data = missData)
		miblOut  <- mibl(data = missData)
		
		## Generate out-of-sample predictions:
	    mibenPred <- postPredict(mibrrFit = mibenOut, newData = testData)
		miblPred  <- postPredict(mibrrFit = miblOut, newData = testData)
		
		
[builds]:  https://github.com/kylelang/MIBRR/tree/master/builds/
[docs]:    https://github.com/kylelang/MIBRR/tree/master/documentation/
[src]:     https://github.com/kylelang/MIBRR/tree/master/source/MIBRR
[LICENSE]: https://github.com/kylelang/MIBRR/blob/master/LICENSE
",2022-08-12
https://github.com/kylelang/mindfulness-intervention,"# mindfulness-intervention
This repository holds the analysis code for L.P. Hulsbosch's mindfulness intervention study.

You can find my notes on the project
[here](https://fresh-cartwheel-b71.notion.site/Mindfulness-Intervention-Study-a2e3d38d98a3413eb2fdc544745653e7).
",2022-08-12
https://github.com/kylelang/Missing-Data-in-R,"# Missing Data in R

This repository will hold all of the materials for the Utrecht University Winter 
School course: *Missing Data in R*.

## Preparation

To participate in this course, you will need a few things:

1. You will need access to the contents of this repository on your local machine.
   - If you're familiar with Git, you can clone this repository.
   - If you are not familiar with Git, you can download a ZIP archive of this 
	 repository's current content via the `Download ZIP` option in the drop-down 
	 menu triggered by the green `Code` button at the top of this page.
      - Extract the contents of this ZIP archive to a convenient location on 
		your computer.
	  - We will be running the course out of this directory.
	  - Do not move any of the contents (doing so will break a bunch of relative 
		file paths).
   - Note that I'll probably be pushing some last-minute updates, so you may 
	 want to pull down recent commits (or download a fresh ZIP image) on the 
	 morning of the course.
	 
2. You will also need to have `R` installed on your computer as well as some 
   convenient way of working with `R` (e.g., `RStudio`).
   - It will also be helpful to install the necessary add-on packages before the 
	 course begins.
   
### Installing R & RStudio

- You can obtain a free copy of `R` [here](https://cran.r-project.org). 
- You can download `RStudio` as stand-alone software [here](https://www.rstudio.com/products/rstudio/download/#download). 
  - You want the free, open-source *RStudio Desktop* version.

### Installing Necessary Packages

We will use several add-on packages in this course. You can save yourself some 
time and hassle at the beginning of the course by installing these packages 
ahead of time.

#### Method 1

Open the script saved as [`code/install_packages.R`](code/install_packages.R)
in `RStudio`, and run the contents.

- You can run the contents of the script by selecting all the text and hitting 
*CTRL-ENTER* on Windows/Linux or *CMD-ENTER* on Mac.

#### Method 2

Copy-paste the following lines of code into the `RStudio` console window to 
execute the necessary command.

- If nothing happens after you paste the code, try hitting the ""Enter/Return"" 
key.

        install.packages(c(""mice"",
                           ""naniar"",
                           ""dplyr"",
                           ""ggplot2"",
                           ""miceadds"",
                           ""mitools"",
                           ""semTools"",
                           ""psych"",
                           ""lavaan"",
                           ""pROC"",
                           ""mvtnorm""),
                         repos = ""http://cloud.r-project.org"",
                         dependencies = TRUE)

If you are asked the following question:

	Do you want to install from sources the package which needs 
    compilation? (Yes/no/cancel)

Type `Yes` in the console, and press the ""Enter/Return"" key (or click the 
corresponding button if the question presents as a dialog box). 
",2022-08-12
https://github.com/kylelang/personal-website,"# personal-website
Repository to hold the source files for my personal website
",2022-08-12
https://github.com/kylelang/plotImps,"# plotImps
This repository is dead. The `plotImps` function has been folded into the 
**SURF** package.

- You can access the `plotImps` function via the **SURF** package here: [SURF][].

[SURF]: http://github.com/kylelang/SURF
",2022-08-12
https://github.com/kylelang/Regression-in-R,"# Regression in R

This repository will hold all of the materials for the Utrecht University Winter 
School course: *Regression in R*.

## Preparation

To participate in this course, you will need a few things:

1. You will need access to the contents of this repository on your local machine.
   - If you're familiar with Git, you can clone this repository.
   - If you are not familiar with Git, you can download a ZIP archive of this 
	 repository's current content via the `Download ZIP` option in the drop-down 
	 menu triggered by the green `Code` button at the top of this page.
      - Extract the contents of this ZIP archive to a convenient location on 
		your computer.
	  - We will be running the course out of this directory.
	  - Do not move any of the contents (doing so will break a bunch of relative 
		file paths).
   - Note that I'll probably be pushing some last-minute updates, so you may 
	 want to pull down recent commits (or download a fresh ZIP image) on the 
	 morning of the course.
	 
2. You will also need to have `R` installed on your computer as well as some 
   convenient way of working with `R` (e.g., `RStudio`).
   - It will also be helpful to install the necessary add-on packages before the 
	 course begins.
   
### Installing R & RStudio

- You can obtain a free copy of `R` [here](https://cran.r-project.org). 
- You can download `RStudio` as stand-alone software [here](https://www.rstudio.com/products/rstudio/download/#download). 
  - You want the free, open-source *RStudio Desktop* version.

### Installing Necessary Packages

We will use several add-on packages in this course. You can save yourself some 
time and hassle at the beginning of the course by installing these packages 
ahead of time.

#### Method 1

Open the script saved as [`code/install_packages.R`](code/install_packages.R)
in `RStudio`, and run the contents.

- You can run the contents of the script by selecting all the text and hitting 
*CTRL-ENTER* on Windows/Linux or *CMD-ENTER* on Mac.

#### Method 2

Copy-paste the following lines of code into the `RStudio` console window to 
execute the necessary command.

- If nothing happens after you paste the code, try hitting the ""Enter/Return"" 
key.

        install.packages(c(""MLmetrics"",
                           ""dplyr"",
                           ""wec"",
                           ""psychTools"",
                           ""DAAG"",
                           ""rockchalk"",
                           ""magrittr"",
                           ""car"",
                           ""sandwich"",
                           ""lmtest""),
                         repos = ""http://cloud.r-project.org"",
                         dependencies = TRUE)

If you are asked the following question:

	Do you want to install from sources the package which needs 
    compilation? (Yes/no/cancel)

Type `Yes` in the console, and press the ""Enter/Return"" key (or click the 
corresponding button if the question presents as a dialog box). 
",2022-08-12
https://github.com/kylelang/simulate-missing-data,"# simulate-missing-data
Routines to simulate missing data in R
",2022-08-12
https://github.com/kylelang/stats-refs,"# stats-refs

This repository contains thematically organized reading lists and BibTex files
for statistical content.

## Directory Structure
In an attempt to make this project as clear, user-friendly, and extensible as
possible, we (the stats-refs core team) have imposed a particular directory
structure.
- This project contains two types of reading lists: *Raw* and *Annotated*.
  - The materials for the raw reading lists are stored in the */raw* directory.
  - The (additional) materials for the annotated reading lists are stored in in
    the */annotated* directory.
- The */bin* directory contains Python and shell scripts to facilitate
  programmatic modification, formatting, and compilation of BibTex and LaTeX
  files.
  - Both the */raw* and */annotated* directories contain thematic subdirectories
  (e.g., */raw/missingData*).
	  - These thematic directories MAY contain additional subdirectories
        dedicated to sub-themes.
- Each thematic directory contains three subdirectories: *latex*, *bibtex*, and
  *pdf*.
  - The *latex* directory contains the LaTeX files for each reading list.
  - The *bibtex* directory contains the BibTex files.
  - The *pdf* directory contains compiled reading lists in PDF format.
  
## Formatting
The core of this project is it's set of BibTex files. One of the primary
motivations for this project was to create a centralized repository of
well-formatted BibTex files that could be used for diverse writing projects. To
keep these files as clean and useful as possible, we impose the following 
formatting rules.
- The BibTex files in the */raw* directory MUST NOT contain annotations.
  - I.e., the entries in these files have no `annote` field.
- The BibTex files in the */annotated* directory MUST contain only annotations.
  - I.e., the only field that the entries in these files have is the `annote`
    field.
- The entries in all BibTex files MUST be sorted in ascending order based on the
  following sequence of keys:
  1. The entry's `key` field, if defined
  1. The surnames of the entry's `author` field
  1. The entry's `year` field
  1. The entry's citation-key
- The first three fields in each entry SHOULD be:
  1. `title`
  1. `author`
  1. `year`
- The remaining fields SHOULD be ordered alphabetically.

### Format of BibTex citation-keys
- The citation-key SHOULD be formatted as *author1Author2:year* where:
  - *author1* is the surname of the first author (with first letter lower-case)
  - *Author2* is one of the following:
	1. An empty string when the publication has exactly one author
    1. Surname of the second author (with first letter upper-case) when the
       publication has exactly two authors
	1. The string-literal *""EtAl""* when the publication has three or more
       authors
- Exceptions to the *author1Author2:year* rule MAY be made for canonical
  citations of software.
  - In these cases, the citation-key can be the name of the software package.
	
#### Citation-key examples

Reference: 
- Ross, B. (2000). A quantum theory of happy little trees. Journal of Quantum
  Art, 3(14). 42.

Key:
- `ross:2000`

Reference: 
- Foo, A., & Bar, B. (2000). Why did the (prairie) chicken cross the road:
  Numerical methods for modeling avian migration patterns. New York: Gotham
  Publishing.

Key:
- `fooBar:2000`

Reference: 
- Cobain, K., Grohl, D., & Novoselic, K. (2000). A game-theoretic analysis of
  early-career academics' research productivity. Journal of Probabilistic
  Existential Dread. 1(2). 1 - 12.

Key:
- `cobainEtAl:2000`

Reference:
- Turing, A. (2000). stopIt: A header-only C++ library to solve the halting
  problem.

Key:
- `turing:2000` *OR* `stopIt`

## Programmatic Workflows
The project's formatting guidelines can be programatically applied to a BibTex
file via the `/bin/formatBibFile.py` Python script.
- For example, running the following code would apply the formatting guidelines
  described above to `myRefs.bib` and return the formatted result as
  `myRefs2.bib`.
  - `python formatBibFile.py /path/to/myRefs/myRefs.bib myRefs2.bib`
  
BibTex files can be annotated (e.g., by merging the BibTex files from the */raw*
and */annotated* directories) via the `/bin/annotateBibFile.py` Python script.
- For example, running the following code would apply the annotations contained
  in `myAnnotations.bib` to `myRefs.bib` and return the results as
  `myRefs2.bib`.
  - `python annotateBibFile.py /path/to/myRefs/myRefs.bib
    /path/to/myAnnotations/myAnnotations.bib myRefs2.bib`
  
LaTeX files can be compiled into PDF files via the `/bin/compileLatex.sh`
shell script.
- For example, running the following code would compile `myDoc.tex` using the
  resources defined in `myPaths.txt` (please refer to the documentation in the
  `compileLatex.sh` script for details on how to format the `myPaths.txt` file).
  - `./compileLatex.sh /path/to/myDoc/myDoc.tex /path/to/myPaths/myPaths.txt`

To execute the above commands, the script (i.e., `formatBibFile.py`,
`annotateBibFile.py`, or `compileLatex.sh`) MUST be in the current working
directory OR be findable via the search path defined by your *$PATH* variable

## Contributing to the Project
We're happy to accept contributions from people outside the core stat-refs
team. All such contributions SHOULD be submitted via a GitHub pull request.

In particular, we welcome any of the following types of contributions:
- Annotations for existing BibTex entries
- New BibTex entries
- New reading lists within an extant theme (i.e., LaTeX source and compiled PDF
  documents)
- New thematic directories
  - Any new thematic directories SHOULD include, at least, a [set of] BibTex
    files.
- Corrections to errors in existing BibTex files
- Improvements to the scripts in the *\bin* directory
- Scripts to make the programmatic workflow described above portable to non-*nix
  style operating systems
",2022-08-12
https://github.com/kylelang/SURF,"# SURF: Some Useful R Functions
This repository hosts development of the R package `SURF`.

- Licensing information is given in the [LICENSE][] file.
- Built tarballs of the `SURF` package are available in the [builds][] 
  directory.
- Stand-alone documentation is available in the [documentation][docs] directory.
- The source files for the most recent stable version of `mibrr` are available 
  in the [source][src] directory.
  
`SURF` is alpha software created largely for my own personal use (i.e., so that 
I don't have to keep re-implementing these functions). If you choose to use 
`SURF`, please expect frequent---and dramatic---changes to the package's 
functionality and user interface. Please report any bugs that you encounter in 
the issues section of the project page.

## Installation
The best way to install `SURF` is to use the `devtools::install_github` 
function.

1. First, make sure that you have `devtools` installed on your system
2. Next, execute the following lines:

		library(devtools)
		install_github(""kylelang/SURF/source/SURF"")
    
3. Finally, load `SURF` and enjoy:

		library(SURF)

If the `devtools`-based approach does not work, you can download one of the
built tar-balls from the [builds][] directory and manually install the package
from source by executing the following lines:

	install.packages(pkgs  = ""/SAVE_PATH/SURF_VERSION.tar.gz"",
	                 repos = NULL,
                     type  = ""source"")

Where *SAVE_PATH* is replaced by the (relative or absolute) file path to the
location where you saved the tar-ball, and *VERSION* is replaced with the correct
version number for the tar-ball that you downloaded.

[builds]:  https://github.com/kylelang/SURF/tree/master/builds/
[docs]:    https://github.com/kylelang/SURF/tree/master/documentation/
[src]:     https://github.com/kylelang/SURF/tree/master/source/SURF
[LICENSE]: https://github.com/kylelang/SURF/blob/master/LICENSE
",2022-08-12
https://github.com/kylelang/TiU-Exam-Merge,"# TiU-Exam-Merge

This repository provides a utility to combine results from the online and
on-campus version of hybrid exams at Tilburg University. The utility will output
a report for instructors as an XLSX workbook.

## Usage

There are multiple ways to use this program:

1. On Windows machines, you can execute the `code/runJob.bat` batch file (e.g.,
   by double-clicking the file from within the `code` directory).
1. On Linux (and Mac?) machines, you can execute the `code/runJob.sh` script
   (e.g., by navigating to the `code` directory and executing `./runJob.sh` from
   the shell).
1. On all platforms, you can source the `code/runJob.R` script from within an
   interactive R session.

Regardless of how the program is executed, the utility operates by
parameterizing the job through a series of interactive dialog boxes. So, usage
amounts to executing the script via your chosen method and responding to the
dialog boxes when prompted.

## Input Notes

The results of the online examination should be provided as either a standard
Canvas gradebook download or a TestVision results file. This file must be in CSV
format. The utility should work with CSV files that use commas `"",""` or
semi-colons `"";""` as the field delimiter.

The results of the on-campus examinations should be provided in the standard
scoring report provided by the Student Administration. These files must be in
XLSX format.

The full set of on-campus results may comprise multiple files. In this case, you
will be prompted to locate each of the pertinent files when setting up the
job. Multiple sets of online results are supported if the online exam was
administered via Canvas. In this case, the program assumes that these results
will be represented as multiple columns in a single input file. So, you may only
read in one file of online results.

The program will also run with only online exam results. After indicating the
location of their online exam results, the user will be asked if they have any
on-campus results to process.

For online exams administered through Canvas, the name of the column containing
the online exam results should have a specific structure and format.

For proctored exam results:

- `exam_date/course_code/exam_name(Remotely Proctored)extra_stuff`
- `YYYY-mm-dd/123456-{M,B}-{1-6}/whatever_name_you_like(Remotely Proctored)does_not_matter`

For unproctored exam results:

- `exam_date/course_code/exam_name OPT-OUTextra_stuff`
- `YYYY-mm-dd/123456-{M,B}-{1-6}/whatever_name_you_like OPT-OUTdoes_not_matter`

When these formatting requirements are not met, the utility should still work,
but some metadata may not be recoverable (i.e., date of the online exam, course
code, name of the online exam).

The TestVision results file is automatically generated with known formatting, so
no special formatting considerations are necessary when processing TestVision
results.

## Output Notes

When running the script on a Windows machine, the lengths of the filepaths for
the report and irregularity checks are compared to the `maxNameLength` variable
defined in `runJob.R`. By default, `maxNameLength` is set to 260 characters,
which is the default maximum file name length on the last several versions of
Windows. On Windows 10, it is possible to enable long file name support and
extend the maximum file length to 32767 characters. 

If any file name is found to be too long (i.e., to contain more than
`maxNameLength` characters), the user is asked if they would like to select a
new output location. If the user declines this offer, the extant file path is
retained.

If your machine has a maximum file name length higher than 260 characters, you
can either adjust the value of `maxNameLength` or decline the offer to select a
new file name when one of your output files is found to be too long.

## Scoring Notes

You will be prompted to select between four different scoring options:

1. The new standard guessing correction formula that will be adopted
   university-wide in 2021
1. The first scoring rule listed on the paper work order form
1. The second scoring rule listed on the paper work order form
1. A custom scoring scheme provided by the instructor as part of the third
   scoring option listed on the paper work order form

When applying any of the first three scoring schemes, you will be prompted to
indicate the number of questions on the exam and the number of response options
(i.e., alternatives) for each question. If you select the first scoring scheme,
you will also be prompted to indicate the passing norm to use in defining the
minimum passing score. 

The passing norm must be a value between 0 and 1 that corresponds to the minimum
guessing-corrected proportional score that a student must achieve to earn a
grade of 5.5. By default, this value is set to 0.55. If you are not sure of what
value to choose, accept the default.

A reference implementation of the new standard guessing correction formula
(provided by the TiU Examination Committee) is available in
`reference/grading.R`, and the formula is documented in
`reference/gradingFormula.pdf`. A more extensive explanation of the scoring
procedure (provided by the TiU Examination Committee) is available in
`reference/calculating_grades_using_guessing_correction.pdf`.

If you opt to apply a custom scoring scheme, you will be asked to supply a
lookup table defining the custom scoring scheme. This lookup table must:

1. Be a CSV file
1. Contain exactly two columns
1. Not have column names

The first column of the lookup table should contain all possible exam scores,
and the second column should contain the corresponding grades. An example lookup
table describing a simple linear scoring scheme for an exam with 50 questions is
available in `data/lookup_table.csv`.

Regardless of how the exam is scored, the final report will contain a sheet
showing the scoring table corresponding to whatever scoring scheme was applied
to the exam. Cells in this table that correspond to failing grades are filled
with light red, and cells corresponding to passing grades are filled with light
blue.

The minimum score is determined by either the scoring rule used to score the
exam or the faculty to which the exam belongs. All scoring rules other than the
post-2020 standard guessing correction formula imply a fixed minimum grade. 

1. First scoring rule listed on the paper work order form: Minimum grade = 1
1. Second scoring rule listed on the paper work order form: Minimum grade = 0
1. Custom scoring scheme provided by the instructor: Minimum grade defined by
   the scheme
   
When using the post-2020 standard guessing correction formula, TiSEM exams will
be scored with a minimum grade of 0, and all other faculties will get a minimum
grade of 1. The faculty is determined by parsing the on-campus results file, so
the user will need to specify the faculty when processing only online exam
results. If the user cannot specify the faculty, the minimum score is set to 1.

## Irregularity Checks

When processing both online and on-campus results, the program will compare the
two grade distributions to check for irregularities. The results of these checks
will be saved as a separate XLSX workbook in the same directory the user chooses
for the final report. The name of this file is automatically generated by
appending the string *""-irregularity_checks""* onto the output file name
specified by the user.

If either the online or on-campus results contain fewer than 5 observations, the
irregularity checks are not conducted.

The distributions are compared via three tests:

### Mean Grades

The mean on-campus grade is compared to the mean online grade using an
independent samples t-test *without* assuming equal variances. The following
statistics are reported for this test:

1. The estimated t-statistic
1. The df of the t-test
1. The p-value of the t-test
1. The standardized mean difference (i.e, Cohen's d)

### Proportion of Grades >= 6 (i.e., Passing Students)

The proportions of the passing grades are compared using a chi-squared test for
independence when all cell counts in the *Exam Version* X *Passing* table
are at least 5. When any cell-count is less than 5, the comparison is skipped.
The following statistics are reported for this test:

1. The ratio of the odds of passing the online exam to the odds of passing the
   on-campus exam.
1. The estimated chi-squared statistic
1. The p-value for the test
1. The standardized difference in proportions (i.e., Cohen's h)

### Proportion of Grades >= 8 (i.e., Cum Laude Students)

The proportions of cum laude students are compared and reported in the same way
as the proportions of passing students.

## Known Issues

The program expects the XLSX file containing the on-campus exam results to be
created on a Windows machine. If this file is created on a Mac, the origin used
to define the date of the on-campus exam will probably differ from the value
expected by the program. Consequently, the date reported for the on-campus exam
(in the final XLSX report) will be incorrect.

The program treats scores of 0 as valid results. Students who did not complete
an exam are expected to have empty cells in the input files (these students are
excluded from the output report). If a student has a score of zero because they
did not take the exam, they will still be assigned the minimum grade in the
final report.

RStudio does not allow multiple file selection, so only one set of on-campus
results can be read in when sourcing the `code/runJob.R` script through RStudio.

The program will automatically append the "".xlsx"" file extension to the output
file when the user does not include the extension. When applying this
functionality, however, duplicate filenames may not be detected, so the user may
not be warned about overwriting existing files. For example, if the user
specifies the output file name as: ""myOuput"", and a file exists with the name
""myOutput.xlsx"", the existing file will be overwritten without a warning.
",2022-08-12
https://github.com/kylelang/uu-beamer-theme,"# uu-beamer-theme
This is a Beamer theme that conforms to the Utrecht University visual identity.
",2022-08-12
https://github.com/LauraGomezNavarro/jupyterlab_pangeo,"# jupyterlab_pangeo

Notebooks, python scripts and resources for pangeo jupyterlab.
",2022-08-12
https://github.com/LauraGomezNavarro/OceanParcels_Lyapunov,"# OceanParcels_Lyapunov
Calculating Finite Time Lyapunov Exponents from OceanParcels outputs.

## Collaborators:

Laura Gomez Navarro (1), Veronica Morales Marquez(2), Ismael Hernandez Carrasco(2), Darshika Manral(1)
<br>
(1) Utrecht University, Utrecht, Netherlands
<br>
(2) IMEDEA (CSIC-UIB), Esporles, Spain

# HISTORY:
- Created : 20/04/2021
- Version 1
- Latest update: ??/??/????
",2022-08-12
https://github.com/LauraGomezNavarro/paper_Gomez-Navarro_etal_2018,"### All files and scripts will be ready soon!

Codes used for the paper: Gómez-Navarro, L.; Fablet, R.; Mason, E.; Pascual, A.; Mourre, B.; Cosme, E.; Le Sommer, J. SWOT Spatial Scales in the Western Mediterranean Sea Derived from Pseudo-Observations and an Ad Hoc Filtering. _Remote Sens._ **2018**, _10_, 599.  

https://www.mdpi.com/2072-4292/10/4/599

Figures:

* ([Figure 1](figures/jpeg/flowchart_nofill_paper.jpeg): Flowchart of the SWOT simulator procedure.)

* [fig_02.ipynb](figures/fig_02.ipynb)

	* In:<br>
		* roms_WMOP_HINDCAST_simplified_avg_20090123.nc
		* rb_1_ma and rb_2_ma.  Obtained from: 2017-11-06_new_vort_calcs_WMOP.py
		
	* Out: <br>
[Figure 02](figures/jpeg/rel_vort_WMOP_evan_BOX_redBlue.jpeg): WMOP relative vorticity normalized by f on 23 January 2009 (left) and 3 February 2009 (right). Black boxes indicate the two regions studied in Section 3: box 1, pass 15 (left) and box 2, pass 168 (right).

* [fig_03.ipynb](figures/fig_03.ipynb)
	
	* In:
		* swot_MED_TRY_c02*.nc
		* 3739box.mat
		* 378398box.mat
		
    * Out: <br>
[Figure 03](figures/jpeg/daily_inputs_cycle2_gradual.jpeg): ![equation](http://latex.codecogs.com/gif.latex?SSH$_{obs}$) (m) obtained for cycle 2. Time increasing from left to right, top to bottom. Days from the beginning of the simulation are shown at the top left corner and the corresponding pass number at the bottom right corner. Outline of the active pass is shown in black. The red boxes show box 1
(pass 15) and box 2 (pass 168).
            
* [fig_04_07_09_11.ipynb](figures/fig_04_07_09_11.ipynb)

	* In: [List of input files](input_files/list_fig_04_07_09_11.md)
		
	* Out: Figures
		* [04](figures/jpeg/3_vars_p015_DEF_redBlue.jpeg): From top to bottom: SSH (m), geostrophic velocity (m/s) and relative vorticity (![equation](http://latex.codecogs.com/gif.latex?\zeta)) normalized by f, on 23 January 2009 corresponding to pass 15 of cycle 2 (box 1). The first, middle and last columns show the data obtained directly from the model (WMOP), from the model interpolated onto the SWOT grid (![equation](http://latex.codecogs.com/gif.latex?SSH$_{model}$)), and with added noise (![equation](http://latex.codecogs.com/gif.latex?SSH$_{obs}$)), respectively.
		* [07](figures/jpeg/adt_p015_zoom_cutoff_DEF.jpeg): SSH (m) on 23 January 2009 corresponding to pass 15 of cycle 2.
		* [09](figures/jpeg/vel_p015_zoom_cutoff_DEF.jpeg): Absolute geostrophic velocity (m/s) on 23 January 2009 corresponding to pass 15 of cycle 2.
		* [11](figures/jpeg/vort_p015_zoom_cutoff_DEF.jpeg): Relative vorticity normalized by f on 23 January 2009 corresponding to pass 15 of cycle 2.

* [fig_05_08_10_12.ipynb](figures/fig_05_08_10_12.ipynb)
	
	* In:  [List of input files](/input_files/list_fig_05_08_10_12.md)
		
	* Out: Figures
		* [05](figures/jpeg/3_vars_p168_DEF_redBlue.jpeg): From top to bottom: SSH (m), geostrophic velocity (m/s) and relative vorticity (![equation](http://latex.codecogs.com/gif.latex?\zeta)) normalized by f, on 3 February 2009 corresponding to pass 168 of cycle 2 (box 2). The first, middle and last columns show the data obtained directly from the model (WMOP), from the model interpolated onto the SWOT
grid (![equation](http://latex.codecogs.com/gif.latex?SSH$_{model}$)), and with added noise (![equation](http://latex.codecogs.com/gif.latex?SSH$_{obs}$)), respectively.
		* [08](figures/jpeg/adt_p168_zoom_cutoff_DEF.jpeg): SSH (m) on 3 February 2009 corresponding to pass 168 of cycle 2.
		* [10](figures/jpeg/vel_p168_zoom_cutoff_DEF.jpeg): Absolute geostrophic velocity (m/s) on 3 February 2009 corresponding to pass 168 of cycle 2.
		* [12](figures/jpeg/vort_p168_zoom_cutoff_DEF.jpeg): Relative vorticity normalized by f on 3 February 2009 corresponding to pass 168 of cycle 2.
		        
* [fig_06_v2.ipynb](figures/fig_06_v2.ipynb)
 
 	* In: 
		* filter_box1_error.mat
		* filter_box2_error.mat
	
	* Out: <br>
	[Figure 06](figures/jpeg/spectra_nofilt_v2.jpeg): (Top) Spectra of the data before filtering, from cycle 1 to 122, corresponding to box 1 (pass 15), (left), and to box 2 (pass 168), (right). Error bars denote 95% confidence intervals. (Bottom) Corresponding Signal to Noise Ratio (SNR), with a horizontal black line indicating where the noise is
more than 15% of the signal, and the vertical line the corresponding wavelength.
           
* [fig_13_14_v2.ipynb](figures/fig_13_14_v2.ipynb)
    	
	* In: 
		* filter_box1.mat
		* filter_box1_error_v2.mat
		* filter_box2.mat
		* filter_box2_error_v2.mat
	
	* Out Figures:
		* [13](figures/jpeg/spectra_p015_v2.jpeg): Box 1 region (pass 15) mean of cycles 1 to 122. (Left) Spectra of (![equation](http://latex.codecogs.com/gif.latex?SSH$_{model}$)) (blue) and (![equation](http://latex.codecogs.com/gif.latex?SSH$_{obs}$)) (red) before filtering and after applying the different cut-off wavelengths shown in the different rows (30, 60 and 200 km) in purple and orange, respectively. Error bars denote 95% confidence intervals. (Right) SNR of (![equation](http://latex.codecogs.com/gif.latex?SSH$_{model}$)) and (![equation](http://latex.codecogs.com/gif.latex?SSH$_{obs}$)), both filtered (solid line) and of (![equation](http://latex.codecogs.com/gif.latex?SSH$_{model}$)) non-filtered and filtered (![equation](http://latex.codecogs.com/gif.latex?SSH$_{obs}$)) (dashed line).
		* [14](figures/jpeg/spectra_p168_v2.jpeg): Box 2 region (pass 168) mean of cycles 1 to 122. (Left) Spectra of (![equation](http://latex.codecogs.com/gif.latex?SSH$_{model}$)) (blue) and (![equation](http://latex.codecogs.com/gif.latex?SSH$_{obs}$)) (red) before filtering and after applying the different cut-off wavelengths shown in the different rows (30, 60 and 200 km) in purple and orange, respectively. Error bars denote 95% confidence intervals.  (Right) SNR of (![equation](http://latex.codecogs.com/gif.latex?SSH$_{model}$)) and (![equation](http://latex.codecogs.com/gif.latex?SSH$_{obs}$)), both filtered (solid line) and of (![equation](http://latex.codecogs.com/gif.latex?SSH$_{model}$)) non-filtered and filtered (![equation](http://latex.codecogs.com/gif.latex?SSH$_{obs}$)) (dashed line).
        
* [fig_15.ipynb](figures/fig_15.ipynb)
    	
	* In: [List of input files](input_files/list_fig_015.md)
	
	* Out: [Figure 15](figures/jpeg/rmse.jpeg): RMSEs of the different variables against the filter’s cut-off wavelengths applied to SSH, for pass 15 (top) and pass 168 (bottom) of cycle 2. Insets show zoom of marked region for better observation of the curves’ minimum points.
	
Appendix figures:

* [fig_A1.ipynb](figures/fig_A1.ipynb)
    	
	* In:
		* wnoises_loop_try2_cut.mat
		* wnoises_loop_try2.mat
    	
	* Out: <br>
	[Figure A1](figures/jpeg/A1.jpeg): Illustration of how the parameterization corresponding to a 15 (16.72) km cut-off wavelength (![equation](http://latex.codecogs.com/gif.latex?\lambda_c)) is estimated. The blue line represents the mean spectra of the 100 non-filtered white noise fields. The black line is the mean spectra of the 100 filtered white noise fields. The horizontal red line shows the half-power spectra of the blue line, and the vertical red line the corresponding wavelength value of the black line, and thus the cut-off wavelength.
	
* [fig_A2.ipynb](figures/fig_A2.ipynb)

	* In:
		* wnoises_loop_try2_cut.mat
		* wnoises_loop_try2.mat
		
	* Out: <br>
	[Figure A2](figures/jpeg/A2.jpeg): Laplacian diffusion cut-off wavelengths (km) for different combinations of the number of iterations and lambdas.
        
* [fig_B.ipynb](figures/fig_B.ipynb)
    
	* In:
		* swot_MED_TRY_c02*.nc
		* swot_MED_TRY_c30*.nc
    
	* Out: Figures
		* [B1](figures/jpeg/NOISE_inst_some_swot_MED_TRY_c02.jpeg): The different instrument noise types (m) for passes 15 pass 168 over cycle 2 are shown. 
		* [B2](figures/jpeg/NOISE_inst_some_swot_MED_TRY_c30.jpeg): The different instrument noise types (m) for passes 15 pass 168 over cycle 30 are shown.
		* [B3](figures/jpeg/NOISE_geo_some_swot_MED_TRY_c02.jpeg): The different geophysical errors (m) for passes 15 and 168 over cycle 2 are shown.
		* [B4](figures/jpeg/NOISE_geo_some_swot_MED_TRY_c30.jpeg): The different geophysical errors (m) for passes 15 and 168 over cycle 30 are shown.

# Calculations:

## Python codes:


## Matlab codes:

* Example of filtering SWOT simulator outputs: [LDfilter_pass_168_example.m](calculations/LDfilter_pass_168_example.m)
	* Needs: [lap_diffusionMask.m](calculations/lap_diffusionMask.m)

",2022-08-12
https://github.com/LauraGomezNavarro/paper_Gomez-Navarro_etal_2020,"# Repository under construction!


Codes used for the paper: Gómez-Navarro, L.; Cosme, E.; Le Sommer, J.; Papadakis, N.; and Pascual, A.  Development of an image de-noising method in preparation for the Surface Water and Ocean Topography satellite mission.  _Remote Sens._ **2020**, _12_, 734. 

https://www.mdpi.com/2072-4292/12/4/734

Figures:

* [fig_01.ipynb](fig_01.ipynb)

	* In:<br>
		* xxxx.nc
		* xxx and xxx.  Obtained from: xxxx.py
		
	* Out: <br>
		* [Figure 1](figures/swot_box_select.png)

* [fig_02.ipynb](fig_02.ipynb)

	* In:<br>
		* MED_1km_nogap_JAS12_swotFastPhase_BOX_c02_p009.nc
		* MED_1km_nogap_JAS12_swotFastPhase_BOX_c06_p009.nc
		
	* Out: <br>
		* [Figure 2a](figures/noise_pass09.png) <br>
		* [Figure 2b](figures/noise_pass22.png)


* [fig_03.ipynb](fig_03.ipynb)

	* In:<br>
		* NATL60MED-CJM165_y2012m06d14-y2013m10d01.1d_gridTsurf.nc
		(data obtained from: http://servdap.legi.grenoble-inp.fr/meom/NATL60MED/NATL60MED-CJM165-S/)
		
	* Out: <br>
		* [Figure 3](figures/norms_intime_subdomain_seasons.png)

* [fig_04.ipynb](fig_04.ipynb)

	* In:<br>
		* xxxx.nc
		* xxx and xxx.  Obtained from: xxxx.py
		
	* Out: <br>
		* [Figure 4](figures/scores_varreg2_K_A_ext_bars_seasons_5_P.png)

* [fig_05.ipynb](fig_05.ipynb)

	* In:<br>
		* xxxx.nc
		* xxx and xxx.  Obtained from: xxxx.py
		
	* Out: <br>
		* [Figure 5a](figures/MED_1km_nogap_JAS12_swotFastPhase_BOX_c02_p009_params_v3_var_reg2_lambd_00455_KA.png) <br>
		* [Figure 5b](figures/MED_1km_nogap_JAS12_swotFastPhase_BOX_c06_p009_params_v3_var_reg2_lambd_00455_KA.png)

* [fig_06.ipynb](fig_06.ipynb)

	* In:<br>
		* xxxx.nc
		* xxx and xxx.  Obtained from: xxxx.py
		
	* Out: <br>
		* [Figure 6a](figures/MED_1km_nogap_FMA13_swotFastPhase_BOX_c02_p009_params_v7_var_reg2_lambd_00105_KA.png) <br>
		* [Figure 6b](figures/MED_1km_nogap_FMA13_swotFastPhase_BOX_c06_p009_params_v7_var_reg2_lambd_00105_KA.png)

* [fig_07.ipynb](fig_07.ipynb)

	* In:<br>
		* xxxx.nc
		* xxx and xxx.  Obtained from: xxxx.py
		
	* Out: <br>
		* [Figure 7](figures/spectra_min_MSR_varreg2_all_new_EB_noise_v3.png)

* [fig_A1.ipynb](fig_A1.ipynb)

	* In:<br>
		* xxxx.nc
		* xxx and xxx.  Obtained from: xxxx.py
		* Needs: [swot_filt_qual_plot_A.py](swot_filt_qual_plot_A.py)
		
	* Out: <br>
		* [Figure A1a](figures/MED_1km_nogap_JAS12_swotFastPhase_BOX_c02_p009_lambd_0000455_bc_ga_A.png) <br>
		* [Figure A1b](figures/MED_1km_nogap_JAS12_swotFastPhase_BOX_c06_p009_lambd_0000455_bc_ga_A.png)

* [fig_A2.ipynb](fig_A2.ipynb)

	* In:<br>
		* xxxx.nc
		* xxx and xxx.  Obtained from: xxxx.py
		* Needs: [swot_filt_qual_plot_A.py](swot_filt_qual_plot_A.py)
		
	* Out: <br>
		* [Figure A2a](figures/MED_1km_nogap_FMA13_swotFastPhase_BOX_c02_p009_lambd_0000105_bc_ga_A.png) <br>
		* [Figure A2b](figures/MED_1km_nogap_FMA13_swotFastPhase_BOX_c06_p009_lambd_0000105_bc_ga_A.png)

# Calculations:

## Python codes:
",2022-08-12
https://github.com/LauraGomezNavarro/SWOTmodule,"# SWOTmodule

New versions of this module can be now found at: https://github.com/meom-group/SWOTmodule 

* Example notebooks:

  * 2018-03-03-ec-discover-SWOTmodule.ipynb

  * 2018-04-18-lgn-discover-SWOTmodule.ipynb : Example of using SWOTdenoise module with the SWOT simulator output netcdfs

  * 2018-04-18-lgn-discover-SWOTmodule_box_dataset.ipynb : Example of using SWOTdenoise module with modified SWOT simulator output netcdfs.  In this case the dataset used in the study Gomez-Navarro et al. (2020). 

* Codes:

  * SWOTdenoise.cfg: Configuration file called by SWOTdenoise.py.  Used to specify the name of the variables of the input file to be filtered.

  * SWOTdenoise.py: Module to read SWOT data, filter it and save it in a new output file or obtain the SSH de-noised variable.

* Example data:
The two examples below are SWOT simulated passes from the NAtl60 model, generated for the fast-sampling phase in the western Mediterranean Sea.
  
   * MED_fastPhase_1km_swotFAST_c01_p009.nc: example SWOT dataset directly out of SWOT simulator (version 2.21)
   
   * MED_1km_nogap_JAS12_swotFastPhase_BOX_c01_p009_v2.nc: example SWOT dataset subregion (box_dataset) used in paper Gomez-Navarro et al. (in review).
",2022-08-12
https://github.com/lauralwd/2OGD_phylogeny,"This repository contains a phylogentic analyses of the [2-OGD super family of enzymes](https://onlinelibrary.wiley.com/doi/10.1111/tpj.12479).
This work aims to place _Azolla filiculoides_ 2-OGD sequences in context of the broader evolution of this group, using data from the 1kP project.
Within this repository, all code, data, and most intermediate files are stored for reproducibility and documentation purposes.
The phylogeny created here is shown in Güngör et al. (in prep).


Quick links:
----------------------
Phylogeny of 2-OGD sequences in plants, firstly of the ANS, FLS and JOX subclades:
 *  [treefile](analyses/v2g5_JOX-ANS-FLS-subset_trees/aligned-mafft-einsi_trim-gt1_prank/v2g5_JOX-ANS-FLS-subset_aligned-mafft-einsi_trim-gt1_prank_iqtree-b200_booster.treefile) 
 * main text figure: 
 [png](figures/v2g5_JOX-ANS-FLS-subset_trees_transfer-bootstrap.png), 
 [pdf](figures/v2g5_JOX-ANS-FLS-subset_trees_transfer-bootstrap.pdf) & 
 [Inkscape_svg](figures/v2g5_JOX-ANS-FLS-subset_trees_transfer-bootstrap.svg)  

Secondly of the entire 2-OGD superfamily
* [treefile](analyses/orthogroup_AtLDOX_AT4g22880_selection-v2_guide-v5_trees/aligned-mafft-einsi_trim-gt3-res.50-seq99-supplmented/orthogroup_AtLDOX_AT4g22880_selection-v2_guide-v5_aligned-mafft-einsi_trim-gt3-res.50-seq99-supplmented_iqtree-bb1000-alrt1000.treefile)
* Supplemental figure: 
  [pdf](figures/v2g5_transfer_bootstrap.pdf) & 
  [Inkscape_svg](figures/v2g5_transfer_bootstrap.svg) 

Main text figure:
----------------------
<img src=""figures/v2g5_JOX-ANS-FLS-subset_trees_transfer-bootstrap.png"" width=""500"" />


Supplemental figure:
----------------------
View the [pdf](figures/v2g5_transfer_bootstrap.pdf) for all details.
![](figures/v2g5_transfer_bootstrap_snapshot.png)

## Guide through directories and files

### Directories
The `data` folder contains (unaligned) fasta files, lists of sequence names, and aligned sequences in both trimmed and untrimmed versions. 
File names tend to be long, but are meant to reflect the history of that specific file. 
For example: `orthogroup_AtLDOX_AT4g22880_selection-v2_guide-v5_aligned-mafft-einsi_trim-gt30.fasta` contains sequences from the 1kP orthogroup retrevied with LDOX from _Arabidopsis thaliana_ from which a manual selection was taken (v2). 
Second, several sequences were added (guide-v5), a set of guide sequences (sequences whose function has been verified), optionally some outgroup sequences, and _Azolla filiculoides_ sequences.
These sequences were then aligned with mafft-einsi and trimmed with trimAL settings `-gt .30`.

The `analyses` folder contains tree inferences. 
These are organised in folders of starting dataset, and then in folders of alignment and trimming strategy.
Still, a folder may contain several tree inferences made with IQTree. 
The final part of the filename summarises the settings used to create a particular tree file. 
Note that intermediate trees are just that, intermediate results. 

The `figures` folder contains the final versions of the figures shown in Güngör _et al_ (in prep). in several formats. 
These were made by importing a `.treefile` in [iToL](https://itol.embl.de/), then adding annotation manually, and downloading these as `.svg` file.
These `.svg` files were then finalised in Inkscape to their published form and exported as pdf or png. 

### Files
The workflows for which data is shared here, are documented in JuPyter notebooks (`*.ipynb`).
The workflow describing the final version of the complete tree is [2OGD_tree_v5](2OGD_tree_v5.ipynb). 
The workflow describing the final version of the subsetted tree is [v2g5_JOX-ANS-FLS-subset](v2g5_JOX-ANS-FLS-subset.ipynb). 
The other workflows are explorative and should be interpreted as such. 
A blank version of the workflow is maintained here: [Laura's phylogeny workflow](https://github.com/lauralwd/lauras_phylogeny_wf).
Note that figures which are embedded in the JuPyter notebooks are not properly displayed online on Github. 
You may download the `.ipynb` files to display them locally, including images. 

Finaly, the `environment.yaml` file details all software names and versions that were used in this project.
This file may be used to recreate the exact software environment for this analysis using [miniconda](https://docs.conda.io/en/latest/miniconda.html).
To do so, issue a command like so `conda env create -f ./environment.yaml`. 

## Data sources used in this project
In building these trees, we have made use of publicly available data, and a novel assembly and annotation of the _Azolla filiculoides_ genome (Azfi v2). 
_Azolla_ automated annotations (Afi v1) are available on [fernbase](ftp://ftp.fernbase.org/Azolla_filiculoides/Azolla_asm_v1.1/)
The novel assembly and annotation will be available publicly soon.
Sequences of relevance to this particular analysis are stored in [data/ANS-likes_Azolla-filiculoides_v4.fasta](data/ANS-likes_Azolla-filiculoides_v4.fasta).

Notably, we have made use of data made available by the [1000 plant transcriptomes project](https://sites.google.com/a/ualberta.ca/onekp/) (1kP).
First, we made use of the [1kP orthogroup extractor](http://jlmwiki.plantbio.uga.edu/onekp/v2/).
Unfortunately this website was taken offline shortly after publication of the 1kP project, and to the best of our knowledge the data is not accessible in any other manner. 
The orthogroups extracted by us are stored in this repository.
Second, we made use of the online [sample list viewer](http://www.onekp.com/samples/list.php) to create a subset of the orthogroup; taking care to sample across the tree of all plants with extra attention to seed-free plants. 
The subset used here is online in [google sheets](https://docs.google.com/spreadsheets/d/1v2igxY_nr7ETMoUdbqpY0QKVxJ-KYiRiO2lLoyOABsw/edit?usp=sharing), and the resulting lists are stored here in the `data` directory.

The 1kP project provides a wealth of sequencing information on taxa of plants for which few sequences are available from genome sequences, let alone sequences of which their function is verified. 
Therefore, we thankfully made use of the sequences collected in literature and online databases; 
most notably so in [Kawai _et al_. 2014: Evolution and diversity of the 2–oxoglutarate-dependent dioxygenase superfamily in plants](https://onlinelibrary.wiley.com/doi/10.1111/tpj.12479).
",2022-08-12
https://github.com/lauralwd/anabaena_nanopore_workflow,"This repo contains analyses for processing nanopore sequencing data of anabaena strains sequencing in the Azolla lab at Utrecht University.
Specifically, we're looking for large in/del variants in sequenced strains created by RNA guided transposition.

The _anabaena_/_nostoc_ reference strain: _Nostoc spec_ pcc7120 
Downloaded from: https://www.ncbi.nlm.nih.gov/assembly/GCF_000009705.1/

The analyses documented here include two main approaches. 
First a denovo approach, assembling the anabaena genomes one by one,
and second a reference based approach.

The denovo approach includes:
1. de-novo assembly with flye (dir `denovo`)
2. assembly polishing with medaka (dir `denovo/sample/polished-medaka`)
3. assembly annotation with both prokka and bakta
4. mapping of sample and reference strain reads with minimap2
5. locating regions of interest with blat
6. visualisation of all generated data with igv

The reference based approach can take various reference-sample combinations and do:
1. mapping to a reference with minimap2, then variant calling with medaka
2. mapping to a reference with ngmlr, then calling structural variants with sniffles
3. extract fasta files with insertion and deletion SVs
4. locating regions of interest with blat
5. visualisation of all generated data with igv

A mapping table should match up sample names with their appropriate reference like so
```
#ref	samples
WT1	sampleA sampleB sampleC
WT2	sampleX sampleY sampleZ
```

IGV snapshots are exported as png and svg images.
High resolution png images are created by importing svg images in inkscape, then exporting as png again at dpi=1000.


",2022-08-12
https://github.com/lauralwd/Azolla_genus_metagenome,"# Azolla genus metagenome
***This project aims do describe and explore the metagenomes of fern species of the genus Azolla.***

## Current workflow
Current workflow specifically filters plant DNA out of a specific set of Azolla genomic sequencing runs to assembly microbial genomes scatered within this dataset. 
The ambition is to generalise the code so it can take any other set of sequencing files and look within these for microbial genomes as well. 
Feel free to fork or contribute, I would be happy to work together.

A consise visual summary of the snakemake workflow would look like this:

![Snakemake rule graph](https://github.com/lauralwd/Azolla_genus_metagenome/blob/master/rulegraph.svg)

## Students
Students willing to travel to-, or currently studying at Utrecht University are most welcome to discuss starting an internship for ECTS as part of this project. 
Naturally, at least some skills in coding and knowledge of metagenomics are required. 
Please find my contact details on my [personel page](https://www.uu.nl/medewerkers/LWDijkhuizen).

## Biological context
Floating ferns of the genus _Azolla_ are known for their productive symbiosis with cyanobacterium _Nostoc azollae_. 
Residing inside specialised _Azolla sp._ leaf pockets, _N. azollae_ fixes N<sub>2</sub> using energy of its own photosynthesis thereby providing the host fern with sufficient nitrogen to maintain optimum growth rates in absence of any nitrogen fertilisation. 
The symbiosis is unique to the _Azolla_ genus dating back to 90M years [(Metzgar,2007)](http://doi.org/doi.org/10.1086/519007); the only other genus within this family of ferns lacks the symbiosis. 
The symbiotic cyanobacteria are systematically transfered through generations of ferns via their megaspores and have eroded genomes; Ran et al. (2010) therefore argued that the boundary between symbiotic partner and plant organelle is thin or even absent in the case of Azolla. 
Co-speciation patterns of the fern (chloroplast) and _N. azollae_ support the close association between the symbiotic partners (Li et al 2018). While early research indicated that _Azolla sp._ has only one symbiont, electron microscopy [(Carrapico 1990)](http://doi.org/10.1007/BF02187448) revealed bacterial cells accompany the cyanobacteria both in the hosts leaf pockets and megaspores. 
My own study [(Dijkhuizen 2018)](https://doi.org/10.1111/nph.14843) describes one of several bacterial genomes which were found as an efficient by-product of the _Azolla_ genome project and confirmed in plants taken from the environment. 
The paper shows that similar bactera are likely present in other _Azolla_ species as well albeit in lower relative abundances than the cyanobacteria. 
Given 
(1) the transfer mechanism of (cyano)bacteria over _Azolla_ generations, 
(2) the high level of specialisation of the Cyanobacteria and the host leaf pocket, and 
(3) the reproducible presence of other bacteria of similar genomic content in the _Azolla_ leaf pockets, 
I theorise that these third parties in the symbiosis have some fittness benefit to the plant-microbe consortium.
Previous work and past student projects suggest that these microbes are endophytic (i.e. living inside the plant) and may share an evolutionary origin (Dijkhuizen 2018). 
A metagenomic approach like the one that is under construction here, may answer questions of similarity of microbes in genome sequence or pathways encoded. 
Some microbes may be passengers, some may be persistent endophytes. 
This approach may shed light on the evolutionary history of these microbes and if these are shared or not.
Finally it could indicate conserved types of functions encoded in these genomes.
Their function however remains elusive.

## Technical context
Recently, a wealth of data was made available to the community including WGS sequencing of multiple _Azolla_ species, the genome of _Azolla filiculoides_, and genomes of several strains of _N. azollae_; strains which are symbionts to different species of host plant [(Li et al. 2018)](https://doi.org/10.1038/s41477-018-0188-8). 
Here I aim to repurpose this data to assemble the individual metagenomes of the host plants, specifically those hypothetical third partners in the symbiosis. 
Figures and results may be included at a later stage when confidence of their correctness allows so. 
These metagenomes may elucidate what, if any, microbes from the different host plants share similarity, origin, and/or functions.

### Reproducibility
This study aims to achieve full reproducibility by using the [anaconda](https://anaconda.org/) and [snakemake](https://snakemake.readthedocs.io/en/stable/) frameworks for reproducible science. 
All versions of the workflow will be version controlled, here on github. 
Data used in the study is already freely available in the Sequencing Read Archive. 
Accesion numbers are detailed in the respository.
",2022-08-12
https://github.com/lauralwd/azolla_MYBs,"This repository contains a phylogenetic tree of R2R3 MYB transcription factors.
Additionally, this repository details all code and intermediate files used in the process towards infering that tree.
Many of these results are intermediate and should be treated as such.
For the final results, please refer to the quick links listed below

Manuscript DOI:  [preprint on bioRXiv](https://www.biorxiv.org/content/10.1101/2020.09.09.289736v2)

Repository DOI: [![DOI](https://zenodo.org/badge/283424814.svg)](https://zenodo.org/badge/latestdoi/283424814)

### Quick links:
 * [treefile](analyses/combi-I-to-VIII-Azfi-Arabidopsis_sequences_linear_trees/aligned-mafft-einsi_trim-gt4/combi-I-to-VIII-Azfi-Arabidopsis_sequences_linear_aligned-mafft-einsi_trim-gt4_iqtree-b1000.treefile)
 * Main text figure 
 [png](figures/myb_subfamiles+RNAseq_normalbootstrap-600dpi.png), 
 [pdf](figures/myb_subfamiles+RNAseq_normalbootstrap.pdf) and 
 [Inkscape_svg](figures/myb_subfamiles+RNAseq_normalbootstrap.svg). 
 * Input sequences [fasta ](data/combi-I-to-VIII-Azfi-Arabidopsis_sequences_linear.fasta) 
 * Aligned input sequences [fasta](data/alignments_raw/combi-I-to-VIII-Azfi-Arabidopsis_sequences_linear_aligned-mafft-einsi.fasta),
or [png](data/alignments_raw/combi-I-to-VIII-Azfi-Arabidopsis_sequences_linear_aligned-mafft-einsi.png)
 * Trimmed input sequences [fasta](data/alignments_trimmed/combi-I-to-VIII-Azfi-Arabidopsis_sequences_linear_aligned-mafft-einsi_trim-gt4.fasta)
or [png](data/alignments_trimmed/combi-I-to-VIII-Azfi-Arabidopsis_sequences_linear_aligned-mafft-einsi_trim-gt4.png)

### Final figure as shown in Dijkhuizen et al. 2021 with added MSA 
The MSA shown below is not included in the manuscript for size limitations. 
It shows the region of R2R3 MYBs used to differentiate the different subfamilies as described by [Jiang & Rao (2020)](https://doi.org/10.1104/pp.19.01082).
The figure actually included in the paper is available [here](figures/myb_subfamiles+RNAseq_normalbootstrap-600dpi.png).

![myb_subfamiles+RNAseq_normalbootstrap+MSA-600dpi.png](figures/myb_subfamiles+RNAseq_normalbootstrap+MSA-600dpi.png)

### Guide through folders and files

The `data` folder contains (unaligned) fasta files, lists of sequence names, and aligned sequences in both trimmed and untrimmed versions. 
File names reflect the history of that specific file and therefore tend to be rather long.
For example `combi-I-to-VIII-Azfi-Arabidopsis_sequences_linear_aligned-mafft-einsi_trim-gt4.fasta` contains a combination of sequences from the subfamilies I to VIII and sequences from _Azolla filiculoides_ and _Arabidopsis thaliana_. 
Those sequences were then aligned with `mafft-einsi` and trimmed with a gap threshold of .4 (40%).

The `analyses` folder contains tree inferences and annotation information for use in iToL.
These are organised in folders of starting dataset, and then in folders of alignment and trimming strategy.
Still, a folder may contain several tree inferences made with IQTree. 
The final part of the filename summarises the settings used to create a particular tree file. 
Note that intermediate trees are just that, intermediate results. 

The `figures` folder contains the final versions of the figures shown in the manuscript in several formats. 
These were made by importing a `.treefile` in [iToL](https://itol.embl.de/), then adding annotation manually, and downloading these as `.svg` file.
Annotation files for use in iToL can be found in the different directories in the `analyses` directory
These `.svg` files were then finalised in Inkscape to their published form and exported as pdf or png. 

### Jupyter notebooks
The workflows shared here are documented in JuPyter notebooks (`*.ipynb`).
Most notebooks contain intermediate work that is shared for transparency and reproducibility purposes and should be treated as such. 
Alternativelly, the git history may be explored for more information.
Note that figures which are embedded in the JuPyter notebooks may not be correctly displayed online on Github. 
You may download the `.ipynb` files to display them locally, including images. 
Alternatively, a html export may be found accompanying the JuPy notebook file.

 * In `step1_differentiate_subfamilies_VI_and_VII` 
 ([html preview](https://htmlpreview.github.io/?https://raw.githubusercontent.com/lauralwd/azolla_MYBs/main/html_step1_differentiate_subfamilies_VI_and_VII.html) & 
 [ipynb preview](https://github.com/lauralwd/azolla_MYBs/blob/main/step1_differentiate_subfamilies_VI_and_VII.ipynb))
 we gather R2R3 MYB sequences of subfamily VI & VII and reproduce findings by [Jiang & Rao (2020)](https://doi.org/10.1104/pp.19.01082).
 * In `step2_classify-Azfi-RNAseq-targets` 
 ([html preview](https://htmlpreview.github.io/?https://raw.githubusercontent.com/lauralwd/azolla_MYBs/main/html_step2_classify-Azfi-RNAseq-targets.html) & 
 [ipynb preview](https://github.com/lauralwd/azolla_MYBs/blob/main/step2_classify-Azfi-RNAseq-targets.ipynb))
 we placed several _Azolla filiculoides_ sequences in the phylogeny of subfamily VI & VII R2R3 MYBs and compare the differentiating domains as described by [Jiang & Rao (2020)](https://doi.org/10.1104/pp.19.01082).
 * In `step3_VI-subfam_in_azolla` 
 ([html preview](https://htmlpreview.github.io/?https://raw.githubusercontent.com/lauralwd/azolla_MYBs/main/html_step3_VI-subfam_in_azolla.html) & 
 [ipynb preview](https://github.com/lauralwd/azolla_MYBs/blob/main/step3_VI-subfam_in_azolla.ipynb))
 missing type VI sequences were identified in the _Azolla filiculoides_ genome with hmms and added to the phylogeny.
 * In `step4_expanding-phylogeny`
 ([html preview](https://htmlpreview.github.io/?https://raw.githubusercontent.com/lauralwd/azolla_MYBs/main/html_step4_expanding-phylogeny.html) & 
 [ipynb preview](https://github.com/lauralwd/azolla_MYBs/blob/main/step4_expanding-phylogeny.ipynb))
 the phylogenetic analysis was expanded with R2R3 MYB sequences from all subfamilies (I to VIII). Sequences were taken from the [Jiang & Rao (2020)](https://doi.org/10.1104/pp.19.01082) paper.
 * Finally, in `step5_supplement-with-arabidopsis-sequences` 
 ([html preview](https://htmlpreview.github.io/?https://raw.githubusercontent.com/lauralwd/azolla_MYBs/main/html_step5_supplement-with-arabidopsis-sequences.html) & 
 [ipynb preview](https://github.com/lauralwd/azolla_MYBs/blob/main/step5_supplement-with-arabidopsis-sequences.ipynb))
 some uninformative and rogue sequences were removed, _Arabidopsis thaliana_ sequences were added, more _Azolla filiculoides_ sequences were added, and the tree was annotated with RNA-seq data for _A. filiculoides_.
 
A template version of the workflow is maintained [here](https://github.com/lauralwd/lauras_phylogeny_wf).

Finally, the `envs` directory contains conda environment export files detailing all software names and versions that were used in this project.
This file may be used to recreate the exact software environment for this analysis using [miniconda](https://docs.conda.io/en/latest/miniconda.html).
To do so, issue a command like so `conda env create -f ./condaenv.yaml`. 

### Data sources used in this project
In building these trees, we have made use of publicly available data exclusively. 
Most notably, we build here upon the work of [Jiang & Rao (2020)](https://doi.org/10.1104/pp.19.01082).
_Azolla_ automated annotations are available on [fernbase](ftp://ftp.fernbase.org/Azolla_filiculoides/Azolla_asm_v1.1/).
The manually re-ananotated _A. filiculoides_ R2R3 MYB sequence is made available in ENA and NCBI under accession number [....] .
This sequence, and all raw RNA-seq reads used in this project are also made availble in ENA and NCBI under project accession number [....] .

All sequences taken from the several databases used here and their original accession numbers are listed in the data folder, organised in files per subfamily type.
These sequences originate from several databases, each with a slightly different naming system.
The [Jiang & Rao (2020)](https://doi.org/10.1104/pp.19.01082) paper lists each of the species used here, and where to find the right database to search for accession numbers. 
Those are predominantly:
 * NCBI nucleotide and protein.
 * [Fernbase](https://www.fernbase.org/) for _Azolla filiculoides_ and _Salvinia cuculata_.
 * [Congenie](http://www.congenie.org/) for _Picea abies_.
 * [marchantia.info](https://marchantia.info/search) for _Marchantia polymorpha_.
 * [uniprot](http://www.uniprot.org) for _Arabidopsis thaliana_ sequences.

## Links
 * [The Azolla lab](https://www.uu.nl/en/research/molecular-plant-physiology/research-topics/azolla-for-the-circular-economy) at Utrecht University
 * [A MIKC phylogeny workflow](https://github.com/lauralwd/MIKC_tree), similar to this one and featured in the same preprint.
 * [A blank version of this workflow](https://github.com/lauralwd/lauras_phylogeny_wf)

## Authors
The analyses in this repository were conceived and executed by 
Dr. Henriette Schluepmann ([orcid](https://orcid.org/0000-0001-6171-3029)
                           [Utrecht University](https://www.uu.nl/staff/hschlupmann)
                          )
and PhD candidate 
Laura Dijkhuizen ([orcid](https://orcid.org/0000-0002-4628-7671) 
                  [Utrecht University](https://www.uu.nl/staff/lwdijkhuizen)
                  [website](https://www.lauradijkhuizen.com))
.
",2022-08-12
https://github.com/lauralwd/focus_stacked_timelapse,"# focus_stacked_timelapse
a bash shell script to make focus stacked timelapses using gphoto2
",2022-08-12
https://github.com/lauralwd/intragenomic_rrna_variability_in_plants,"# intragenomic_rrna_variability_in_plants

## initial reasoning
So I had this notion that intra-genomic variation in the ITS region of the rRNA cluster in plant genomes has to do with genome size, and that ITS in the rRNA should not be used as a marker for phylogenetic analyses. The former is known and published although in pieces (alvarez 2003 & prokopowhich 2003 & plenty more I’m sure). The latter is also commonly accepted, yet the ITS is commonly used in plant phylogenetic studies. What is missing is a bigger integrated study of plant rRNA cistrons including big plant genomes and an assessment of how this realistically can effect phylogenetic studies. I seem to remember I assessed this as a novel thing when a student report pointed me to the intragenomic variability of the ITS, but a quick search through literature indicates this is not the case. My guess is that the student at the time missed out on this quite simple literature search or I missed out on his literature research in his report. The initial students observations were (1) there is a big intra-genome variability in the Azolla ITS region (2) the variability has some pattern (3) this pattern is consistent among species of Azolla. Here I plan an open systematic look into this matter.

## Build-up
Meta-analysis part:
1.	Hypothesis: rRNA copy nr increases with genome size.
    *	Alternative hypothesis: phylogenetic pattern
    *	Yes, Prokopowich (2003) showed this for 68 plants and 94 animals in 2003. 
          *	How many more plant genomes and rRNA copy nrs are available these days, and is it worth redoing this 15 years after date.
          *  Prokopowhich: “It might be expected that the need for ribosomes would increase as genome size increases if the relative proportion of protein-coding genes remains constant. However, the number of protein-coding genes does not increase proportionately (reviewed in Cavalier-Smith 1985).”
2.	Do ITS lengths (and possibly functional content) relate to genome size
    *	Alternative hypothesis: phylogenetic pattern
    *	Or is it all over the place: Rogers 1987
    *	Really, we should be talking about a length distribution per genome
    
Assembly analysis part:

3.	Technical: How many rRNA copies are preserved in draft genomes
    * Sub: differentiate among sequencing strategies
    * Sub/comeback: are these rRNA clusters (or single rRNA cistrons) a consensus, identical, or do they diverge. (per assembly strategy)
           *	How do rRNA clusters end up in genomes (metagenomes) and how do these compare to sanger sequenced results.
    *	Sub: do these differentiate from rRNA PCR fragments sequenced in databases
  
Reads backmapping to rRNA fragments part:

4.	Intra genome variation of
      *	rRNA genes -> should be practically zero
            *	this seems not to be published
      *	ITS regions  
            *	This is published at least in non-plants
5.	Certain pattern in ITS variation
      *	Is there functional regions in ITS regions
      *	Similar in individuals of the same species
      *	Similar in the same genes
      *	Family…
  
Combine:

6.	Overall ITS variation as a function of genome size
    *	Alternative: phylogenetic
7.	Realistic worst case scenario’s when mis-base-calling such fragments.

## Data to be generated collected
List of plants with
* Plant
    * [Name](https://www.ncbi.nlm.nih.gov/genome?term=%28%22Embryophyta%22%5BOrganism%5D%20OR%20%22Embryophyta%22%5BOrganism%5D%29%20AND%20%22Eukaryota%22%5BOrganism%5D&cmd=DetailsSearch)
    * Origin
    * Specimen
* Genome size
    * [Size](https://www.ncbi.nlm.nih.gov/genome/browse/#!/overview/plants)
    * Genome size as [C-values]
    * Technique of size determination
    
* known rRNA copy nr
    * Technique
* raw sequencing data available
    * accession nr
* annotated genomes
    * accession nr
    * fasta
    * gff (including rRNA repeats)
    * assembly technique

## Novelties to be found:
Scripted redo of Prokopowhich with more genomes and more modern data, not truly novel but I will have to do this to get started anyhow.

How do rRNA clusters end up in genomes (metagenomes) and how do these compare to sanger sequenced results. <- implications for placing sequenced results in a tree.

Is there an (explainable pattern) of intragenomic variability in plant ITS regions. 

Simulate how bad variable base calls due to intragenomic variability can mess up a phylogeny.

What I miss… a more evolution central experiment/explanation of the data like Ganley 2007

Conclusion… ITS is not really trustworthy. Use single copy genomic genes. But that’s not a new advise is it. Perhaps we can give a more nuanced advise, how bad is it really in simulations and is it genome size (~copy nr) dependent.

### Questions I still have
Has the ITS some folding function in the self-assembly of the ribosome

Can we do phylogeny and alignment on a variable sequence; multiple sequence alignments of seqlogos/hmms… or is this black magic.

#Literature
alvarez 2003 Ribosomal ITS sequences and plant phylogenetic inference
Ganley 2007 Highly efficient concerted evolution in the ribosomal DNA repeats: Total rDNA repeat variation revealed by whole-genome shotgun sequence data
Prokopowich 2003 The correlation between rDNA copy number and genome size in eukaryotes
Rogers 1987 Ribosomal RNA genes in plants: variability in copy number and in the intergenic spacer


",2022-08-12
https://github.com/lauralwd/LAR_phylogeny_gungor-et-al-2020,"This repository contains a phylogenetic tree of LAR and other PIP family enzymes as shown in [Güngör _et al_. 2020: Azolla ferns testify: seed plants and ferns share a common ancestor for leucoanthocyanidin reductase enzymes](https://doi.org/10.1111/nph.16896). 
Additionally, this repository details all code and intermediate files used in the process towards that tree.

[![DOI](https://zenodo.org/badge/279892407.svg)](https://zenodo.org/badge/latestdoi/279892407)

### Quick links:
Phylogeny of LAR and LAR likes in plants:
 *  [treefile](https://raw.githubusercontent.com/lauralwd/LAR_phylogeny_gungor-et-al-2020/main/analyses/1kP_LAR_selectionv1_guide_v5_trees/aligned-mafft-linsi_trim-gt4-seq95-res7/1kP_LAR_selectionv1_guide_v5_aligned-mafft-linsi_trim-gt4-seq95-res7_iqtree-bb2000-alrt2000.treefile) 
 * main text figure 
 [png](https://media.githubusercontent.com/media/lauralwd/LAR_phylogeny_gungor-et-al-2020/main/figures/main_text/LAR_orthogroup_selectionv1_guide_v5_17cm_8pts_shalrt_circular_v4.2_brackets_600.svg.png), 
 [pdf](https://media.githubusercontent.com/media/lauralwd/LAR_phylogeny_gungor-et-al-2020/main/figures/main_text/LAR_orthogroup_selectionv1_guide_v5_17cm_8pts_shalrt_circular_v4.2_brackets.pdf) & 
 [Inkscape_svg](https://raw.githubusercontent.com/lauralwd/LAR_phylogeny_gungor-et-al-2020/main/figures/main_text/LAR_orthogroup_selectionv1_guide_v5_17cm_8pts_shalrt_circular_v4_brackets.svg)  
 * supplemental figure 
  [pdf](https://media.githubusercontent.com/media/lauralwd/LAR_phylogeny_gungor-et-al-2020/main/figures/supporting_information/LAR_orthogroup_selectionv1_guide_v5_SHaLRT_long_inpage_v3.pdf) & 
  [Inkscape_svg](https://raw.githubusercontent.com/lauralwd/LAR_phylogeny_gungor-et-al-2020/main/figures/supporting_information/LAR_orthogroup_selectionv1_guide_v5_SHaLRT_long_inpage_v3.svg) 

Final alignment: [raw](https://raw.githubusercontent.com/lauralwd/LAR_phylogeny_gungor-et-al-2020/main/data/alignments_raw/1kP_LAR_selectionv1_guide_v5_aligned-mafft-linsi.fasta) 
& [trimmed](https://raw.githubusercontent.com/lauralwd/LAR_phylogeny_gungor-et-al-2020/main/data/alignments_trimmed/1kP_LAR_selectionv1_guide_v5_aligned-mafft-linsi_trim-gt4-seq95-res7.fasta)

Final [complete fasta file ](https://raw.githubusercontent.com/lauralwd/LAR_phylogeny_gungor-et-al-2020/main/data/1kP_LAR_selectionv1_guide_v5.fasta) used for the alignment which consists of: 
 * a [selection from the 1kP orthogroup](https://raw.githubusercontent.com/lauralwd/LAR_phylogeny_gungor-et-al-2020/main/data/1kP_LAR_orthogroup_manual-selection-1.fasta) 
 * a manually composed set of [guide sequences](https://github.com/lauralwd/LAR_phylogeny_gungor-et-al-2020/blob/main/data/Erbils_guide_v3.fasta) to annotate and interpret the tree.
 
### Final figure as shown in Güngör _et al_. 2020
![PIP enzymes and LAR phylogenetic tree](https://media.githubusercontent.com/media/lauralwd/LAR_phylogeny_gungor-et-al-2020/main/figures/main_text/LAR_orthogroup_selectionv1_guide_v5_17cm_8pts_shalrt_circular_v4.2_brackets_600.svg.png)  

### Guide through folders and files
The `data` folder contains (unaligned) fasta files, lists of sequence names, and aligned sequences in both trimmed and untrimmed versions. 
File names tend to be long, but are meant to reflect the history of that specific file. 
For example: `1kP_LAR_orthogroup_manual-selection-1_guidev4_aligned-mafft-linsi_trim-gt6-seq80.fasta` contains sequences from the 1kP LAR orthogroup from which a manual selection was taken. 
Second, a set of guide sequences (sequences whose function has been verified) was added.
These sequences were then aligned with mafft-linsi and trimmed with trimAL settings `-gt .6 and -seq 80.`

The `analyses` folder contains tree inferences. 
These are organised in folders of starting dataset, and then in folders of alignment and trimming strategy.
Still, a folder may contain several tree inferences made with IQTree. 
The final part of the filename summarises the settings used to create a particular tree file. 
Note that intermediate trees are just that, intermediate results. 
The `fernLARclades_analyses` directory contains tree inferences on specifically the fern LAR, WLAR1 and WLAR2 clades as shown in figure 8 of Güngör _et al_. 2020.

The `figures` folder contains the final versions of the figures shown in Güngör _et al_. in several formats. 
These were made by importing a `.treefile` in [iToL](https://itol.embl.de/), then adding annotation manually, and downloading these as `.svg` file.
These `.svg` files were then finalised in Inkscape to their published form and exported as pdf or png. 

The workflows for which data is shared here, are documented in JuPyter notebooks (`*.ipynb`).
The workflow describing the final version of the tree is [tree_building_workflow_v5](https://github.com/lauralwd/LAR_phylogeny_gungor-et-al-2020/blob/main/tree_building_workflow_v5.ipynb). 
The other two workflows are explorative and should be interpreted as such. 
A blank version of the workflow is maintained [here](https://github.com/lauralwd/lauras_phylogeny_wf).
Note that figures which are embedded in the JuPyter notebooks are not properly displayed online on Github. 
You may download the `.ipynb` files to display them locally, including images. 
Alternatively, a html export may be found [here](https://htmlpreview.github.io/?https://github.com/lauralwd/LAR_phylogeny_gungor-et-al-2020/blob/main/opt/tree_building_workflow_v5.html).

Finaly, the `condaenv.yaml` file details all software names and versions that were used in this project.
This file may be used to recreate the exact software environment for this analysis using [miniconda](https://docs.conda.io/en/latest/miniconda.html).
To do so, issue a command like so `conda env create -f ./condaenv.yaml`. 
One specific perl script that is not included in the conda environment, is stored in the `opt` directory.

### Data sources used in this project
In building these trees, we have made use of publicly available data exclusively. 
Except perhaps, for two _Azolla filiculoides_ sequences for which we have manually corrected the automated annotation.
_Azolla_ automated annotations are available on [fernbase](ftp://ftp.fernbase.org/Azolla_filiculoides/Azolla_asm_v1.1/)
The manually annotated sequences used here were submitted to EBI's [ENA](https://www.ebi.ac.uk/ena/browser/home) under study accession number [PRJEB39515](https://www.ebi.ac.uk/ena/browser/view/PRJEB39515).
These sequences are also hosted in this github repository as 
[nucleotide](https://raw.githubusercontent.com/lauralwd/LAR_phylogeny_gungor-et-al-2020/main/figures/AzfiLAR_ENA-submission.fna) 
and 
[protein](https://raw.githubusercontent.com/lauralwd/LAR_phylogeny_gungor-et-al-2020/main/figures/AzfiLAR_ENA-submission.faa) 
fasta files. 

Notably, we have made use of data made available by the [1000 plant transcriptomes project](https://sites.google.com/a/ualberta.ca/onekp/) (1kP).
First, we made use of the [1kP orthogroup extractor](http://jlmwiki.plantbio.uga.edu/onekp/v2/) to extract a LAR orthogroup by providing it with the _Vitis vinifera_ LAR sequence. 
Second, we made use of the online [sample list viewer](http://www.onekp.com/samples/list.php) to create a subset of the 1kP PIP enzyme orthogroup; taking care to sample across the tree of all plants with extra attention to seed-free plants. 
The subset used here is online in [google sheets](https://docs.google.com/spreadsheets/d/1v2igxY_nr7ETMoUdbqpY0QKVxJ-KYiRiO2lLoyOABsw/edit?usp=sharing), and the resulting lists are stored here in the `data` directory.

The 1kP project provides a wealth of sequencing information on taxa of plants for which few sequences are available from genome sequences, let alone sequences of which their function is verified. 
Therefore, we thankfully made use of the sequences collected in literature and online databases; 
most notably so in Koeduka's paper 'Functional evolution of biosynthetic enzymes that produce plant volatiles' published in 'Bioscience, Biotechnology, and Biochemistry' in 2018.
Each of these sequences and their original accession number are listed in [this fasta file](https://raw.githubusercontent.com/lauralwd/LAR_phylogeny_gungor-et-al-2020/main/data/Erbils_guide_v3.fasta).
 
",2022-08-12
https://github.com/lauralwd/lauras_phylogeny_wf,"## Aim of this workflow
This repository contains a template workflow for state of the art molecular phylogeny inference using tools like `mafft`, `trimAl`, and `IQtree`. 
The workflow is a Jupyter notebook; a hands on workflow aimed at the practical steps needed from start to finish.
Instructions guide the user through the code necessary to run these tools and several checks and balances along the way from one starting sequence to a phylogenetic tree.
Secondly, the workflow encourages users to document their choices and output; making science a bit more transparent and reproducible.

This document does not aim to guide the user in interpreting phylogenies, or go into detail on the main considerations when designing your evolutionary inference.
However, luckily [this paper](https://doi.org/10.1002/bies.201900006) does a very good job in doing so and is written by true experts in the field.
Perhaps take a look at their [short video abstract](https://www.youtube.com/watch?v=VCt3l2pbdbQ) as well, explaining a use case.

## Overview
### requirements
To do this workflow, you need a linux environment like a linux computer, MacOS, or the 'windows sublayer for linux'. 
Second, you need the `conda` or `miniconda` framework for installing bioinformatics software. 
Install all required software as detailed in the conda environment included in this repository like so: `conda env create -f ./envs/conda-env-jalview.yaml`.
Third, you need one sequence or sequence ID you are interested in.

### steps
This workflow aims to guide you through the following steps
 1. acquire homologous sequences to your sequence via either ncbi blast, or the 1kP project (only for plant sequences)
 2. subset your input to contain all sequences of a limited number of species
 3. align sequences with `mafft`
 4. trim alignment with `trimAL`
    - Visualise alignment with Jalview
    - Evaluate and optimise
 5. infer a phylogenetic tree with fasttree
 6. infer a phylogenetic tree with IQtree
    - use modelfitting
    - choose a bootstrap method
 7. visualise the phylogenetic tree with iTol
    - annotate the phylogenetic tree in iTol

![workflow sketch](./docs/workflow_sketch.png)


## Published Examples
Please find published examples here:
 - LAR phylogeny [GitHub](https://github.com/lauralwd/LAR_phylogeny_gungor-et-al-2020) [Zenodo](https://doi.org/10.5281/zenodo.3959057)
 - MIKC phylogeny [GitHub](https://github.com/lauralwd/MIKC_tree) [Zenodo](https://doi.org/10.5281/zenodo.4564374)
 - R2R3 MYB phylogeny [GitHub](https://github.com/lauralwd/azolla_MYBs) [Zenodo](https://doi.org/10.5281/zenodo.4564441)
 - 21OGD Phylogeny [GitHub](https://github.com/lauralwd/2OGD_phylogeny)

This workflow is currently under construction, but nontheless citable via zenodo here: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4564464.svg)](https://doi.org/10.5281/zenodo.4564464)
",2022-08-12
https://github.com/lauralwd/metagenomicspractical,"# Metagenomicspractical
This repository contains a practical for learning a metagenomics workflow.
These lessons are aimed at Master's students in Life Sciences with minimal experience in bioinformatics and bachelor level experience in (micro)biology.
This practical starts by discussing a metagenomics workflow from a biological context, acquiring sequencing data and genome assembly.
Metagenome assembly is rather a resource- and time-intensive process; hence I have done this for you already.
Then, you, as a student, take over.
You'll extract individual microbial genomes from the metagenome assembly, check their quality, and annotate genes coded in these genomes.
This practical includes the following steps (and depends on the following tools):
* backmapping (bwa+samtools)
* binning (MetaBAT)
* quality control of bins (CheckM)
* optional: taxonomy of bins (BAT)
* annotation (Prokka)
* phylogeny reconstruction (IQTree)

Answers/prefilled code are available in a separate branch of this github repository called 'example'. 
Renders of the empty and pre-filled workflow are available as a github pages website in the 'gh-pages' branch and online [here](https://lauralwd.github.io/metagenomicspractical/).

## Installation
If you do this practical without any supervision, you will likely need to install the software required and download the data.
I recommend, especially to beginners, to install these tools via conda.
If you have not installed conda, please refer to [the guide for miniconda installation on linux](https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html).
For windows users can install conda too on the windows sublayer for linux.
I made a [youtube video](https://youtu.be/ZWQMADZjMGc) on how to do that for interns in our lab, it may help you to get conda running as well.
I encourage you to learn more about how conda works; it is definitely worth your time. 
Once conda is installed, make sure you have cloned this git repository to your local Linux system.
Next, navigate to that specific folder in a terminal.
If you do not have git installed, you can do so via your system package manager (i.e. `apt install git`) or via conda (`conda install git`). 
Alternativelly, download the whole practical as a zip file by clicking the green 'code' button on GitHub.

```
git clone https://github.com/lauralwd/metagenomicspractical.git
cd metagenomicspractical
```

Next, you can create an environment with all software required via the following command:

`conda env create`

The previous command reads the `environment.yml` file to install the exact combination of software I use when teaching this practical.
You activate the environment by typing:

`conda activate metagenomics_practical`

Now you installed all of the software you need and made it ready for use!

Make sure you are in the `metagenomicspractical` folder, and then you may open the interactive Jupyter notebook pages by typing:

`jupyter-notebook`

This opens a web browser showing you an overview of the `metagenomicspractical` folder.
Here you can open any of the Jupyter notebooks. 
If you need to download the required data for the practical, make sure you follow the commands in `m00-prepare_download_and_subset_reads.ipynb`

In some cases, your software installed in conda is not available in the JuPyter notebook environment.
As far as I'm concerned, this is abug of the bash_kernel installed in the JuPyter environment; the python3 kernel does find the executables from the conda environment.
A possible fix is the following.
First, identify in a terminal the path where your conda environment is installed.
This is likely in your home directory: `/home/your-name/miniconda2/envs/metagenomics_practical/bin`
To double check, activate your conda environment like above and find out where any of the software tools we use are situated.
Doing this for metabat2 this looks like so: `which metabat2`.
Next, we take that path to the `bin`  directory, and add it the the `$PATH` system variable. 
Make sure to adapt the next command to your situation: 

```
export PATH=/home/laura/miniconda3/envs/metagenomics_practical/bin:$PATH
```

Next, re-install the bash kernel with this command:
```
python3 -m bash_kernel.install
```

Now run `jupyter notebook` again and check if the software is available to you.
If the problem returns after you closed your terminal, run the former of these two commands again and start JuPyter after.

# Help along the way
When teaching this course during the corona pandemic, I decided to record walkthroughs of the entire practical and post these on youtube.
You can find all video's used for this particular practical in a [youtube playlist](https://www.youtube.com/playlist?list=PLWmKspMOX1oHOzXmJ1wI2eZinzfGqmu90).
The quality of these video's is not great and the content isn't always as brief as it could be, but I hope this helps you along in case you get stuck.

# Learning goals of this practical
After this practical, you can name and explain the steps of a simple metagenomics workflow. 
Starting at acquiring sequencing data, all the way to annotating individual draft genomes.
* You can highlight the differences between 'regular' genome sequencing data and assemblies versus metagenomic sequencing data and assemblies (lecture).
* You can explain parts of the workflow and their interdependencies from biological and technical perspectives.
* You can replicate the workflow taken during the practical on new data sets.
* You can design similar workflows for different metagenomic questions.
* You can explain what binning signals are (lecture), why they are used and how you used them during the practical.
If not already, you will understand the basics of the bash computer language and be able to run bio-informatic programmes in loops.

# Credits and references
Original practical (version 2017) was made By Margo Schuller and Laura Dijkhuizen. 
The current version was improved thereupon by [Laura Dijkhuizen](https://www.lauradijkhuizen.com). 
The practical is based on a subset of published data. 
The original paper is published open-access in New Phytologist: 
>Dijkhuizen, L. W., Brouwer, P., Bolhuis, H., Reichart, G. J., Koppers, N., Huettel, B., ... & Wong, G. K. S. (2018). Is there foul play in the leaf pocket? The metagenome of floating fern Azolla reveals endophytes that do not fix N2 but may denitrify. New Phytologist, 217(1), 453-466. [https://doi.org/10.1111/nph.14843](https://doi.org/10.1111/nph.14843)

The continuation of this project is documented also on github, find more details here: [github.com/lauralwd/azolla_genus_metagenome](https://github.com/lauralwd/azolla_genus_metagenome)
",2022-08-12
https://github.com/lauralwd/MIKC_tree,"This repository details our work towards placing _Azolla filiculoides_ MIKC genes in a broader picture of MIKC transcription factor evolution.
The phylogeny created here is featured in [this preprint](https://www.biorxiv.org/content/10.1101/2020.09.09.289736v2).
Many results made publicly available here are intermediate results and should be treated as such.
For the final results, please refer to the quick links listed below

manuscript DOI: [preprint](https://www.biorxiv.org/content/10.1101/2020.09.09.289736v2)

repository DOI: [![DOI](https://zenodo.org/badge/270919511.svg)](https://zenodo.org/badge/latestdoi/270919511)


## Quick links
Final input, output and intermediate files:
* [fasta](data/MIKC_orthogroup_selection-basal-v9_guide-v4.fasta)
* alignment [raw](data/alignments_raw/MIKC_orthogroup_selection-basal-v9_guide-v4_aligned-mafft-einsi_prank.fasta) &
          [trimmed](data/alignments_trimmed/MIKC_orthogroup_selection-basal-v9_guide-v4_aligned-mafft-einsi_prank_trim-gt1.fasta)
* tree in [newick](analyses/MIKC_orthogroup_selection-basal-v9_guide-v4_trees/aligned-mafft-einsi_prank_trim-gt1/MIKC_orthogroup_selection-basal-v9_guide-v4_aligned-mafft-einsi_prank_trim-gt1_iqtree-b1000_booster.treefile)
          ,[png](figures/MIKC_orthogroup_selection-basal-v9_guide-v4_aligned-mafft-einsi_prank_trim-gt1_iqtree-b1000_booster_withrnaseq.png)
          , [pdf](figures/MIKC_orthogroup_selection-basal-v9_guide-v4_aligned-mafft-einsi_prank_trim-gt1_iqtree-b1000_booster_withrnaseq.pdf)
          & [inkscape_svg](figures/MIKC_orthogroup_selection-basal-v9_guide-v4_aligned-mafft-einsi_prank_trim-gt1_iqtree-b1000_booster_withrnaseq.svg)

## Final figure as shown in Dijkhuizen et al. 2021
![](figures/MIKC_orthogroup_selection-basal-v9_guide-v4_aligned-mafft-einsi_prank_trim-gt1_iqtree-b1000_booster_withrnaseq.svg)

## Guide through files and directories

### Directories
The five directories in this repository should be reasonably self-explanatory. 
The `data` directory contains input sequences and alignments.
Since I do not consider phylogenetic trees to be data, these are stored in the `analyses` directory.
The `docs` directory contains several Html ""print-outs"" of the Jupyter notebooks, for these may not render on GitHub.
For any questions about software versions used: conda environments used in the analyses documented here are available in the `envs` directory.
If no specific conda environment is listed in the notebooks, then the default environment 'phylogenetics' was used.
Finally, the `figures` directory contains two manually ""polished"" versions of the phylogenetic tree and annotating data for direct inclusion in a journal submission.

### Jupyter notebooks
This repository contains many JuPy notebooks in which my colleagues and I stepwise tried to get a better phylogenetic signal of MIKC evolution and interpretation of the placement of fern sequences.
In this iterative process, MIKC protein sequences were aligned, filtered, a tree was inferred, and we concluded.
Each notebook describes one iteration of this process. 
Here I list the main conclusion or improvement briefly for each of these notebooks in their chronological order.

In `MIKC_tree_workflow-v1.ipynb`
([ipynb](https://github.com/lauralwd/MIKC_tree/blob/master/MIKC_tree_workflow-v1.ipynb)
[html](https://htmlpreview.github.io/?https://github.com/lauralwd/MIKC_tree/blob/master/docs/MIKC_tree_workflow-v1.html)
[iToL](https://itol.embl.de/tree/9421021579373181591777160))
we make a first version of a MIKC tree.
We sample MIKC(-like) genes from specific species across all major groups of plants as described in the 1kP project.

In `MIKC_tree_workflow-v2.ipynb`
([ipynb](https://github.com/lauralwd/MIKC_tree/blob/master/MIKC_tree_workflow-v2.ipynb)
[html](https://htmlpreview.github.io/?https://github.com/lauralwd/MIKC_tree/blob/master/docs/MIKC_tree_workflow-v2.html)
[iToL](https://itol.embl.de/tree/9421021579263431592334615))
we replace some species with others, add bryophytes and lycophytes, and attempt to introduce an outgroup of non-plant sequences.

In `MIKC_tree_workflow-basalclades-v1.ipynb`
([ipynb](https://github.com/lauralwd/MIKC_tree/blob/master/MIKC_tree_workflow-basalclades-v1.ipynb)
[html](https://htmlpreview.github.io/?https://github.com/lauralwd/MIKC_tree/blob/master/docs/MIKC_tree_workflow-basalclades-v1.html)
[iToL](https://itol.embl.de/tree/9421021579288351592221930))
we attempt to improve the phylogenetic signal by sampling fewer sequences in more recent branches; about a third compared to the previous workflow.

In `MIKC_tree_workflow-basalclades-v2.ipynb`
([ipynb](https://github.com/lauralwd/MIKC_tree/blob/master/MIKC_tree_workflow-basalclades-v2.ipynb)
[html](https://htmlpreview.github.io/?https://github.com/lauralwd/MIKC_tree/blob/master/docs/MIKC_tree_workflow-basalclades-v2.html)
[iToL_gt4](https://itol.embl.de/tree/13121158204159901593010248))
I add _Salivinia cuculata_ [sequences](data/salvinia_sequences/salivinia_fernbase_blast_results_uniq.fasta) from [fernbase](https://www.fernbase.org) and remove some sequences that were behaving oddly due to too large horizontal gaps in the alignment.
Finally the _Chara globularis_ MADS1 sequence was added, like in the [1kP capstone paper](https://doi.org/10.1038/s41586-019-1693-2), to serve as an outgroup.

In `algal sequences.ipynb`
([ipynb](https://github.com/lauralwd/MIKC_tree/blob/master/algal_sequences.ipynb)
[html](https://htmlpreview.github.io/?https://github.com/lauralwd/MIKC_tree/blob/master/docs/algal_sequences.html)
)
I extract and align all algal sequences from the MIKC orthogroup from the 1kP project in an effort to identify those algal sequences which truly have all four domains: M, I, K and C.
We found that in this orthogroup, sequences almost always have the highly conserved M domain but often lack the IKC domains or part thereof.
Based on these results, we proceed only with algal sequences that contain all four domains.

In `MIKC_tree_workflow-basalclades-v3.ipynb`
([ipynb](https://github.com/lauralwd/MIKC_tree/blob/master/MIKC_tree_workflow-basalclades-v3.ipynb)
[html](https://htmlpreview.github.io/?https://github.com/lauralwd/MIKC_tree/blob/master/docs/MIKC_tree_workflow-basalclades-v3.html)
[iToL](https://itol.embl.de/tree/13121159166346421593419936))
I aim to identify non MIKC sequences and remove these from the analyses;
hence I'm making a phylogeny of only MIKC genes and not sequences that only have an M domain.
In a separate notebook, I already did so for all algal sequences in the 1kP MIKC orthogroup.
These should confirm the placing of the CgMADS1 sequence and provide a solid root to the tree.

In `MIKC_tree_workflow-basalclades-v4.ipynb`
([ipynb](https://github.com/lauralwd/MIKC_tree/blob/master/MIKC_tree_workflow-basalclades-v4.ipynb)
[html](https://htmlpreview.github.io/?https://github.com/lauralwd/MIKC_tree/blob/master/docs/MIKC_tree_workflow-basalclades-v4.html)
[iToL](https://itol.embl.de/tree/9421021579173651593446746))
I remove some sequences and run the tree with non-parametric bootstraps instead.

In `MIKC_tree_workflow-basalclades-v5.ipynb`
([ipynb](https://github.com/lauralwd/MIKC_tree/blob/master/MIKC_tree_workflow-basalclades-v5.ipynb)
[html](https://htmlpreview.github.io/?https://github.com/lauralwd/MIKC_tree/blob/master/docs/MIKC_tree_workflow-basalclades-v5.html)
[iToL](https://itol.embl.de/tree/9421021579163171593685733))
I added gymnosperms and _Azolla_ sequences.
Since the algal outgroup has proven to be stable but also very big, it's size is trimmed down.

In `MIKC_tree_workflow-basalclades-v6.ipynb`
([ipynb](https://github.com/lauralwd/MIKC_tree/blob/master/MIKC_tree_workflow-basalclades-v6.ipynb)
[html](https://htmlpreview.github.io/?https://github.com/lauralwd/MIKC_tree/blob/master/docs/MIKC_tree_workflow-basalclades-v6.html)
[iToL-UFbootstrap](https://itol.embl.de/tree/942102157910201593716454)
[iToL-nonparametric](https://itol.embl.de/tree/1312115964226201597403301#))
some redundant _Azolla_ sequences were removed again and a big clade of MIKC\* sequences was removed.
MIKC\* sequences are characterised by a longer C domain.

In `MIKC_tree_workflow-basalclades-v7.ipynb` 
([ipynb](https://github.com/lauralwd/MIKC_tree/blob/master/MIKC_tree_workflow-basalclades-v7.ipynb)
[html](https://htmlpreview.github.io/?https://github.com/lauralwd/MIKC_tree/blob/master/docs/MIKC_tree_workflow-basalclades-v7.html))
several rogue taxa were removed; those taxa that were poorly supported and moved around the tree in between different inferences.
Also, the algal outgroup was reduced in size, and I experimented with different extents of column-content trimming.
I varied the miniumum sequence content per column from 10% to 50% and made UFBootstrap tree inferences:
iToL UFbootstrap 
[gt .1](https://itol.embl.de/tree/131211596429601597651059)
[gt .2](https://itol.embl.de/tree/131211596429631597651059)
[gt .3](https://itol.embl.de/tree/131211596429651597651060)
[gt .4](https://itol.embl.de/tree/131211596429721597651060)
[gt .5](https://itol.embl.de/tree/131211596429741597651061)
Based on the alignments and the trees, I coose the 50% sequence content alignment and made a non-parametric tree only to find that bootstrap support broke down completely:
[itol-nonparametric](https://itol.embl.de/tree/1312115999190001598359608).

In `MIKC_tree_workflow-basalclades-v8.ipynb`
([ipynb](https://github.com/lauralwd/MIKC_tree/blob/master/MIKC_tree_workflow-basalclades-v8.ipynb)
[html](https://htmlpreview.github.io/?https://github.com/lauralwd/MIKC_tree/blob/master/docs/MIKC_tree_workflow-basalclades-v8.html)
sequences were removed from the alignment more stringently if they missed too many residues compared to the rest of the alignment.
Also, more algal sequences were added back into the dataset again to provide the tree with a more solid and confident outgroup.
Finally, since [bootstrap values](https://itol.embl.de/tree/13121159254447621597996339) on basal branches remain low and uninformative, I experimented with alternative support assessments. 
Stricter [trimming](https://itol.embl.de/tree/1312115823116101598607728) of alignments and [jackknifing](https://itol.embl.de/tree/9421021579372301598523476) were deemed unsuccessful, and a bayesian method was not considered feasible for deadline considerations. 
Finally, experimentation with [transfer bootstraps](https://itol.embl.de/tree/1312115823161871598452619) proved to provide more informative tree support values.

In `MIKC_tree_workflow-basalclades-v9.ipynb`
([ipynb](https://github.com/lauralwd/MIKC_tree/blob/master/MIKC_tree_workflow-basalclades-v9.ipynb)
[html](https://htmlpreview.github.io/?https://github.com/lauralwd/MIKC_tree/blob/master/docs/MIKC_tree_workflow-basalclades-v9.html)
I tried to keep the best of versions 6, 7 and 8: a solid algal outgroup, strict trimming of sequences, and more informative support assessment.
More notably, I revisited alignment optimisation with `prank`. 
A set of sequences was aligned, and a [ML tree](https://itol.embl.de/tree/13121159214349041611578775) was inferred with IQtree as before
Then this ML tree was used to re-align insertions and deletions with `prank`, ideally making a clear and less noisy [alignment](data/alignments_trimmed/MIKC_orthogroup_selection-basal-v9_guide-v4_aligned-mafft-einsi_prank_trim-gt1.png).
A final tree was then made with IQtree, [1000 non-parametric bootstraps](https://itol.embl.de/tree/9421021579353661611221693) and then [transfer bootstrap support](https://itol.embl.de/tree/9421021579408691611653003) values were calculated with `booster`.

## Data sources
While gathering data to building these trees, we have relied heavily on publicly available data. 
Most sequences used here were generated by the [1kP project](https://sites.google.com/a/ualberta.ca/onekp/).
Specifically, we have used the [1kP orthogroup extractor](http://jlmwiki.plantbio.uga.edu/onekp/v2/) to retrieve an orthogroup of MIKC sequences using query 'At2g45660': [_Arabidopsis thaliana_ SOC1](https://www.arabidopsis.org/servlets/TairObject?accession=locus:2005522).
To guide our way through the phylogeny, we collected a set of guide sequences, as shown in [Zhang et al. (2019)](https://dx.doi.org/10.3390%2Fijms20081926) and supplemented these [guide sequences](data/Zhang-2019-fig4_MIKC_Azfi-gymnosperms_selection-v3.faa) to our data before alignment.

## Links
 * [The Azolla lab](https://www.uu.nl/en/research/molecular-plant-physiology/research-topics/azolla-for-the-circular-economy) at Utrecht University
 * [A MYB phylogeny workflow](https://github.com/lauralwd/azolla_MYBs), similar to this one and featured in the same preprint.
 * [A blank version of this workflow](https://github.com/lauralwd/lauras_phylogeny_wf)

## Authors
The analyses in this repository were conceived and executed by 
Dr. Henriette Schluepmann ([orcid](https://orcid.org/0000-0001-6171-3029)
                           [Utrecht University](https://www.uu.nl/staff/hschlupmann)
                          )
and PhD candidate 
Laura Dijkhuizen ([orcid](https://orcid.org/0000-0002-4628-7671) 
                  [Utrecht University](https://www.uu.nl/staff/lwdijkhuizen)
                  [website](https://www.lauradijkhuizen.com))
.
",2022-08-12
https://github.com/laurensstoop/CapacityFactor-CF,"# CapacityFactor-CF

Version 0.2.0

This Git contains a set of scripts intended to convert climate model parameters into capacity factors fit for use in Energy System Models. 


## Project organization

```
.
├── .gitignore
├── CITATION.md
├── LICENSE.md
├── README.md
├── requirements.txt
├── config             <- Configuration files (HW)
├── docs               <- Documentation notebook for users (HW)
│   ├── manuscript     <- Manuscript source, e.g., LaTeX, Markdown, etc. (HW)
│   └── reports        <- Other project reports and notebooks (e.g. Jupyter, .Rmd) (HW)
├── results
│   ├── figures        <- Figures for the manuscript or reports (PG)
│   └── output         <- Other output for the manuscript or reports (PG)
└── src                <- Source code for this project (HW)

```


## License

This project is licensed under the terms of the [MIT License](/LICENSE.md)

## Citation

Please [cite this project as described here](/CITATION.md).
",2022-08-12
https://github.com/laurensstoop/Downloader-BASE,"# Downloader-BASE

Version 1.2.0

This Git contains a set of scripts intended to download various datasets from various climate modelling projects. 

The project changes the native climate model output format into the NetCDF4 standard for the European region.


## Project organization

```
.
├── .gitignore
├── CITATION.md
├── LICENSE.md
├── README.md
├── requirements.txt
├── config             <- Configuration files (HW)
├── results
│   ├── figures        <- Figures for the manuscript or reports (PG)
│   └── output         <- Other output for the manuscript or reports (PG)
└── src                <- Source code for this project (HW)

```


## License

This project is licensed under the terms of the [MIT License](/LICENSE.md)

## Citation

Please [cite this project as described here](/CITATION.md).
",2022-08-12
https://github.com/laurensstoop/EnergyVariables,"# CapacityFactor-CF

Version 0.1.0

This Git contains a set of scripts intended for energy variables from given distributions and capacity factor datasets. 


## Project organization

```
.
├── .gitignore
├── CITATION.md
├── LICENSE.md
├── README.md
├── requirements.txt
├── config             <- Configuration files (HW)
├── docs               <- Documentation notebook for users (HW)
│   ├── manuscript     <- Manuscript source, e.g., LaTeX, Markdown, etc. (HW)
│   └── reports        <- Other project reports and notebooks (e.g. Jupyter, .Rmd) (HW)
├── results
│   ├── figures        <- Figures for the manuscript or reports (PG)
│   └── output         <- Other output for the manuscript or reports (PG)
└── src                <- Source code for this project (HW)

```


## License

This project is licensed under the terms of the [MIT License](/LICENSE.md)

## Citation

Please [cite this project as described here](/CITATION.md).
",2022-08-12
https://github.com/laurensstoop/Thesis-course,"# Thesis-course

Here you can find the thesis course material. Open the Zip and start TeXing!
Happy bug free code to ya all!

",2022-08-12
https://github.com/laurensstoop/UAV,"# UAV LaTeX cursus
Bestandenmap voor LaTeX cursus UAV

Deze cursus valt onder de GNU public license v3.0
",2022-08-12
https://github.com/lcvriend/flatbread,"# <img src=""static/noun_pita_3216932.svg"" width=""24""> Flatbread <img src=""static/noun_pita_3216932.svg"" width=""24"">

## About
Flatbread is a small library which extends the pivot table functionality in pandas. Flatbread is accessible through the DataFrame using the `pita` accessor.

<img src=""static/pita_table_example_001.svg"" width=""732"">

The library contains functions which will allow you to easily add **totals/subtotals** to one or more axes/levels of your pivot table. Furthermore, flatbread can calculate **percentages** from the totals/subtotals of each axis/level of your pivot table. You can transform the existing values in your table into percentages, but you can also add the percentages neatly next to your data. If the required (sub)totals are not present, then flatbread will add them automatically in order to perform the calculations. By default the (sub)totals are kept but you can drop them too. The library also contains some functionality built on top of matplotlib for plotting your data.

## Name
Initially I planned for this library to be called pita -- short for pivot tables. But as that name was already taken on pypi.org the choice fell on flatbread.

## Install
To install:

```
pip install flatbread
```

## Pivot tables
Let's create a df for testing:

```Python
from random import randint
import pandas as pd
import flatbread as fb

df = pd._testing.makeCustomDataframe(
    nrows=8,
    ncols=4,
    data_gen_f=lambda r,c:randint(1,100),
    c_idx_nlevels=2,
    r_idx_nlevels=3,
    c_ndupe_l=[2,1],
    r_ndupe_l=[4,2,1],
)
```

### Totals and subtotals
Flatbread let's you easily add subtotals to your pivot tables. Here we add totals and subtotals to both axes at once:

```
df.pipe(fb.totals.add, axis=2, level=[0,1])
```

<table border=""1"" class=""dataframe"">
  <thead>
    <tr>
      <th></th>
      <th></th>
      <th>C0</th>
      <th colspan=""3"" halign=""left"">C_l0_g0</th>
      <th colspan=""3"" halign=""left"">C_l0_g1</th>
      <th>Total</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th>C1</th>
      <th>C_l1_g0</th>
      <th>C_l1_g1</th>
      <th>Subtotal</th>
      <th>C_l1_g2</th>
      <th>C_l1_g3</th>
      <th>Subtotal</th>
      <th></th>
    </tr>
    <tr>
      <th>R0</th>
      <th>R1</th>
      <th>R2</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan=""5"" valign=""top"">R_l0_g0</th>
      <th rowspan=""2"" valign=""top"">R_l1_g0</th>
      <th>R_l2_g0</th>
      <td align=""right"">8</td>
      <td align=""right"">76</td>
      <td align=""right"">84</td>
      <td align=""right"">32</td>
      <td align=""right"">55</td>
      <td align=""right"">87</td>
      <td align=""right"">171</td>
    </tr>
    <tr>
      <th>R_l2_g1</th>
      <td align=""right"">21</td>
      <td align=""right"">75</td>
      <td align=""right"">96</td>
      <td align=""right"">15</td>
      <td align=""right"">67</td>
      <td align=""right"">82</td>
      <td align=""right"">178</td>
    </tr>
    <tr>
      <th rowspan=""2"" valign=""top"">R_l1_g1</th>
      <th>R_l2_g2</th>
      <td align=""right"">66</td>
      <td align=""right"">84</td>
      <td align=""right"">150</td>
      <td align=""right"">38</td>
      <td align=""right"">40</td>
      <td align=""right"">78</td>
      <td align=""right"">228</td>
    </tr>
    <tr>
      <th>R_l2_g3</th>
      <td align=""right"">83</td>
      <td align=""right"">94</td>
      <td align=""right"">177</td>
      <td align=""right"">57</td>
      <td align=""right"">31</td>
      <td align=""right"">88</td>
      <td align=""right"">265</td>
    </tr>
    <tr>
      <th>Subtotal</th>
      <th></th>
      <td align=""right"">178</td>
      <td align=""right"">329</td>
      <td align=""right"">507</td>
      <td align=""right"">142</td>
      <td align=""right"">193</td>
      <td align=""right"">335</td>
      <td align=""right"">842</td>
    </tr>
    <tr>
      <th rowspan=""5"" valign=""top"">R_l0_g1</th>
      <th rowspan=""2"" valign=""top"">R_l1_g2</th>
      <th>R_l2_g4</th>
      <td align=""right"">32</td>
      <td align=""right"">82</td>
      <td align=""right"">114</td>
      <td align=""right"">55</td>
      <td align=""right"">87</td>
      <td align=""right"">142</td>
      <td align=""right"">256</td>
    </tr>
    <tr>
      <th>R_l2_g5</th>
      <td align=""right"">68</td>
      <td align=""right"">22</td>
      <td align=""right"">90</td>
      <td align=""right"">100</td>
      <td align=""right"">70</td>
      <td align=""right"">170</td>
      <td align=""right"">260</td>
    </tr>
    <tr>
      <th rowspan=""2"" valign=""top"">R_l1_g3</th>
      <th>R_l2_g6</th>
      <td align=""right"">55</td>
      <td align=""right"">25</td>
      <td align=""right"">80</td>
      <td align=""right"">40</td>
      <td align=""right"">24</td>
      <td align=""right"">64</td>
      <td align=""right"">144</td>
    </tr>
    <tr>
      <th>R_l2_g7</th>
      <td align=""right"">12</td>
      <td align=""right"">80</td>
      <td align=""right"">92</td>
      <td align=""right"">31</td>
      <td align=""right"">79</td>
      <td align=""right"">110</td>
      <td align=""right"">202</td>
    </tr>
    <tr>
      <th>Subtotal</th>
      <th></th>
      <td align=""right"">167</td>
      <td align=""right"">209</td>
      <td align=""right"">376</td>
      <td align=""right"">226</td>
      <td align=""right"">260</td>
      <td align=""right"">486</td>
      <td align=""right"">862</td>
    </tr>
    <tr>
      <th>Total</th>
      <th></th>
      <th></th>
      <td align=""right"">345</td>
      <td align=""right"">538</td>
      <td align=""right"">883</td>
      <td align=""right"">368</td>
      <td align=""right"">453</td>
      <td align=""right"">821</td>
      <td align=""right"">1704</td>
    </tr>
  </tbody>
</table>

### Percentages from totals and subtotals
Flatbread let's you calculate the percentages of the totals or subtotals. You can either transform the data itself or add the percentages into your pivot table as separate columns. When rounding the percentages they will always add up to 100%:

```Python
df.pipe(fb.percs.add, level=1)
```

<table border=""1"" class=""dataframe"">
  <thead>
    <tr>
      <th></th>
      <th></th>
      <th>C0</th>
      <th colspan=""4"" halign=""left"">C_l0_g0</th>
      <th colspan=""4"" halign=""left"">C_l0_g1</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th>C1</th>
      <th colspan=""2"" halign=""left"">C_l1_g0</th>
      <th colspan=""2"" halign=""left"">C_l1_g1</th>
      <th colspan=""2"" halign=""left"">C_l1_g2</th>
      <th colspan=""2"" halign=""left"">C_l1_g3</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th>abs</th>
      <th>%</th>
      <th>abs</th>
      <th>%</th>
      <th>abs</th>
      <th>%</th>
      <th>abs</th>
      <th>%</th>
    </tr>
    <tr>
      <th>R0</th>
      <th>R1</th>
      <th>R2</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan=""5"" valign=""top"">R_l0_g0</th>
      <th rowspan=""2"" valign=""top"">R_l1_g0</th>
      <th>R_l2_g0</th>
      <td align=""right"">8</td>
      <td align=""right"">4.5</td>
      <td align=""right"">76</td>
      <td align=""right"">23.1</td>
      <td align=""right"">32</td>
      <td align=""right"">22.5</td>
      <td align=""right"">55</td>
      <td align=""right"">28.5</td>
    </tr>
    <tr>
      <th>R_l2_g1</th>
      <td align=""right"">21</td>
      <td align=""right"">11.8</td>
      <td align=""right"">75</td>
      <td align=""right"">22.8</td>
      <td align=""right"">15</td>
      <td align=""right"">10.6</td>
      <td align=""right"">67</td>
      <td align=""right"">34.7</td>
    </tr>
    <tr>
      <th rowspan=""2"" valign=""top"">R_l1_g1</th>
      <th>R_l2_g2</th>
      <td align=""right"">66</td>
      <td align=""right"">37.1</td>
      <td align=""right"">84</td>
      <td align=""right"">25.5</td>
      <td align=""right"">38</td>
      <td align=""right"">26.8</td>
      <td align=""right"">40</td>
      <td align=""right"">20.7</td>
    </tr>
    <tr>
      <th>R_l2_g3</th>
      <td align=""right"">83</td>
      <td align=""right"">46.6</td>
      <td align=""right"">94</td>
      <td align=""right"">28.6</td>
      <td align=""right"">57</td>
      <td align=""right"">40.1</td>
      <td align=""right"">31</td>
      <td align=""right"">16.1</td>
    </tr>
    <tr>
      <th>Subtotal</th>
      <th></th>
      <td align=""right"">178</td>
      <td align=""right"">100.0</td>
      <td align=""right"">329</td>
      <td align=""right"">100.0</td>
      <td align=""right"">142</td>
      <td align=""right"">100.0</td>
      <td align=""right"">193</td>
      <td align=""right"">100.0</td>
    </tr>
    <tr>
      <th rowspan=""5"" valign=""top"">R_l0_g1</th>
      <th rowspan=""2"" valign=""top"">R_l1_g2</th>
      <th>R_l2_g4</th>
      <td align=""right"">32</td>
      <td align=""right"">19.2</td>
      <td align=""right"">82</td>
      <td align=""right"">39.2</td>
      <td align=""right"">55</td>
      <td align=""right"">24.3</td>
      <td align=""right"">87</td>
      <td align=""right"">33.5</td>
    </tr>
    <tr>
      <th>R_l2_g5</th>
      <td align=""right"">68</td>
      <td align=""right"">40.7</td>
      <td align=""right"">22</td>
      <td align=""right"">10.6</td>
      <td align=""right"">100</td>
      <td align=""right"">44.3</td>
      <td align=""right"">70</td>
      <td align=""right"">26.9</td>
    </tr>
    <tr>
      <th rowspan=""2"" valign=""top"">R_l1_g3</th>
      <th>R_l2_g6</th>
      <td align=""right"">55</td>
      <td align=""right"">32.9</td>
      <td align=""right"">25</td>
      <td align=""right"">11.9</td>
      <td align=""right"">40</td>
      <td align=""right"">17.7</td>
      <td align=""right"">24</td>
      <td align=""right"">9.2</td>
    </tr>
    <tr>
      <th>R_l2_g7</th>
      <td align=""right"">12</td>
      <td align=""right"">7.2</td>
      <td align=""right"">80</td>
      <td align=""right"">38.3</td>
      <td align=""right"">31</td>
      <td align=""right"">13.7</td>
      <td align=""right"">79</td>
      <td align=""right"">30.4</td>
    </tr>
    <tr>
      <th>Subtotal</th>
      <th></th>
      <td align=""right"">167</td>
      <td align=""right"">100.0</td>
      <td align=""right"">209</td>
      <td align=""right"">100.0</td>
      <td align=""right"">226</td>
      <td align=""right"">100.0</td>
      <td align=""right"">260</td>
      <td align=""right"">100.0</td>
    </tr>
  </tbody>
</table>

### Localize your table formats
Flatbread provides the `format` function for rendering your pivot table according to your preferred locale. Here we use `nl-NL` as an example:

```Python
df = pd._testing.makeCustomDataframe(
    nrows=5,
    ncols=4,
    data_gen_f=lambda r,c:randint(10,1000),
)

df.pipe(fb.percs.add).pipe(fb.format)
```

<table border=""1"" class=""dataframe"">
  <thead>
    <tr>
      <th>C0</th>
      <th colspan=""2"" halign=""left"">C_l0_g0</th>
      <th colspan=""2"" halign=""left"">C_l0_g1</th>
      <th colspan=""2"" halign=""left"">C_l0_g2</th>
      <th colspan=""2"" halign=""left"">C_l0_g3</th>
    </tr>
    <tr>
      <th></th>
      <th>abs</th>
      <th>%</th>
      <th>abs</th>
      <th>%</th>
      <th>abs</th>
      <th>%</th>
      <th>abs</th>
      <th>%</th>
    </tr>
    <tr>
      <th>R0</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>R_l0_g0</th>
      <td align=""right"">702</td>
      <td align=""right"">23,8</td>
      <td align=""right"">57</td>
      <td align=""right"">1,7</td>
      <td align=""right"">579</td>
      <td align=""right"">23,2</td>
      <td align=""right"">908</td>
      <td align=""right"">39,6</td>
    </tr>
    <tr>
      <th>R_l0_g1</th>
      <td align=""right"">791</td>
      <td align=""right"">26,8</td>
      <td align=""right"">839</td>
      <td align=""right"">25,6</td>
      <td align=""right"">687</td>
      <td align=""right"">27,6</td>
      <td align=""right"">333</td>
      <td align=""right"">14,5</td>
    </tr>
    <tr>
      <th>R_l0_g2</th>
      <td align=""right"">579</td>
      <td align=""right"">19,6</td>
      <td align=""right"">777</td>
      <td align=""right"">23,7</td>
      <td align=""right"">633</td>
      <td align=""right"">25,4</td>
      <td align=""right"">553</td>
      <td align=""right"">24,2</td>
    </tr>
    <tr>
      <th>R_l0_g3</th>
      <td align=""right"">571</td>
      <td align=""right"">19,3</td>
      <td align=""right"">699</td>
      <td align=""right"">21,3</td>
      <td align=""right"">108</td>
      <td align=""right"">4,4</td>
      <td align=""right"">439</td>
      <td align=""right"">19,1</td>
    </tr>
    <tr>
      <th>R_l0_g4</th>
      <td align=""right"">310</td>
      <td align=""right"">10,5</td>
      <td align=""right"">908</td>
      <td align=""right"">27,7</td>
      <td align=""right"">484</td>
      <td align=""right"">19,4</td>
      <td align=""right"">59</td>
      <td align=""right"">2,6</td>
    </tr>
    <tr>
      <th>Total</th>
      <td align=""right"">2.953</td>
      <td align=""right"">100,0</td>
      <td align=""right"">3.280</td>
      <td align=""right"">100,0</td>
      <td align=""right"">2.491</td>
      <td align=""right"">100,0</td>
      <td align=""right"">2.292</td>
      <td align=""right"">100,0</td>
    </tr>
  </tbody>
</table>

### Easy configuration
Flatbread let's you control most of its behavior through key-word arguments, but it is also easy to store your settings and use them globally throughout a project:

```Python
from flatbread import CONFIG

# pick your preferred locale and set it (used with `format`)
CONFIG.format['locale'] = 'nl_NL'
CONFIG.set_locale()

# set your own labels
CONFIG.aggregation['totals_name'] = 'Totes'
CONFIG.aggregation['label_rel'] = 'pct'

# define the number of digits to round to (-1 is no rounding)
CONFIG.aggregation['ndigits] = 2

# store your configuration permanently (across sessions)
CONFIG.save()

# restore your settings to 'factory' defaults
CONFIG.factory_reset()
```

## Pivot charts

Use the Trendline object to create trendlines. Compare multiple years:

```Python
tl = fb.TrendLine.from_df(
    sample,
    offset_year = 2019,
    datefield   = 'Date of Application',
    yearfield   = 'Academic Year',
    period      = 'w',
    end         = '2019-09-01',
    grouper     = 'Academic Year',
    focus       = 2019,
)

fig = tl.plot()
```

<img src=""static/2020-12-22.Date_of_Application.line.abs.svg"" width=""630"">

Split your graphs in rows and columns:

```Python
tl = fb.TrendLine.from_df(
    sample,
    offset_year = 2019,
    datefield   = 'Date Processed',
    yearfield   = 'Academic Year',
    period      = 'w',
    end         = '2019-10-01',
    grouper     = 'Faculty',
    focus       = 'Humanities',
)

fig = tl.plot(
    rows   = 'Origin Country',
    cols   = 'Examination Type',
    cum    = True,
    filter = ""`Academic Year` == 2019""
)
```

<img src=""static/2020-12-22.Date_Processed.line.cum.svg"">
",2022-08-12
https://github.com/lcvriend/humannotator,"# Humannotator

**Library for conveniently creating simple customizable annotators 
for manual annotation of your data**  
*Jenia Kim, Lawrence Vriend*

Works well with Jupyter notebooks:

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/lcvriend/humannotator/master?filepath=examples%2Fexamples.ipynb)

## Use case

The humannotator provides an easy way to set up custom annotators.
This tool is for you if manual annotation is part of your workflow 
and you are looking for a solution that is:

- Lightweight
- Customizable
- Easy to set up
- Integrates with Jupyter/pandas/Python

## Quick start

### Install the humannotator

Install with conda:

```
    conda install -c lcvriend humannotator
```

Or use pip:

```
    pip install humannotator
```

### Create a simple annotator

1. [Load the data](#load-data)
2. [Define the tasks](#define-tasks)
3. [Instantiate the annotator](#annotator)

```Python
    import pandas as pd
    from humannotator import Annotator

    # load data
    df = pd.read_csv('examples/popcorn_classics.csv', sep=';', index_col=0)

    # set up the annotator
    ratings = [
        'One bag',
        'Two bags',
        'Three bags',
        'Four bags',
        'Five-bagger',
    ]
    annotator = Annotator(df, name='VFA | Rate my popcorn classics')
    annotator.tasks['Bags of popcorn'] = ratings

    # run annotator
    annotator(user='GT')
```

In Jupyter this gives:

<img src=""examples/popcorn_classics.png"" alt=""Humannotator"" width=""726"">

### Annotate your data

- Use the annotator by calling it: `annotator()`.
- The annotator keeps track of where you were.
- Highlight phrases with the 'phrases' argument.
- The annotator stores user (if provided) and timestamp with the annotation.

### Access your annotations

- The annotations are conveniently stored in a pandas `DataFrame`.
- Access the annotations with the `annotated` attribute.
- Get the indeces of the records without annotation with `unannotated`.
- Return the data merged with its annotations with the `merged` method.

### Store your annotations

- Store the annotator with the `save` method.
- Load the annotator with the `load` method.

## Load data

The annotator accepts `list`, `dict`, `Series` and `DataFrame` objects as data.  
The data will be converted to a dataframe internally.

### Dataframes

- By default, the annotator will use the dataframe's `index` and all `columns`.
- Use `load_data` to easily create a `data` object if you need more control:
    1. `id_col` sets the column to be used as index.
    2. `item_cols` set the column or columns to be displayed.

## Define tasks

Tasks can be set up through subscription or with the `task_factory`.

### Setting up tasks with the task factory
Create a task by passing `task_factory`:

- the `kind` of task
- the `name` of the task
- (optionally) an `instruction`
- (optionally) a list of `dependencies`
- whether it is `nullable` (default is False)
- any [kwargs](#Available-tasks) necessary (depends on the kind of task)

Typically: 
```Python
    task_factory(
        'kind',
        'name',
        instruction='instruction',
        dependencies=dependencies,
        nullable=True/False,
        **kwargs,
    )
```

Passing a dict or list to `kind` will create a categorical task.  
In this case the `categories` kwarg is ignored.

### Setting up tasks through subscription

It is also possible to instantiate an annotator and add tasks through subscription:  

```Python
    a = Annotator()
    a.tasks['topic'] = ['economy', 'politics', 'media', 'other']
    a.tasks['factual'] = bool, ""Is the article factual?"", False
```

To add a task like this, you minimally need to provide the `kind` of task you are trying to create.
Optionally, you can add `instruction`, `nullability`, `dependencies` and any other kwargs (as dictionary).
Change the order in which tasks are prompted to the user with the `order` attribute on `tasks`.

### Available tasks

kind      | kwargs     | dtype            | description
--------- | -----------| ---------------- | ----------------
str       |            | object           | String
regex     | regex      | object           | String validated by regex
int       |            | Int64            | Nullable integer
float     |            | float64          | Float
bool      |            | bool             | Boolean
category  | categories | CategoricalDtype | Categorical variable
date      |            | datetime64[ns]   | Date

### Dependencies

Dependencies consist of a *condition* and a *value*, that can be passed as tuple:

```Python
    (""col1 == 'x'"", False)
```

The condition is a [pandas query statement](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html#pandas.DataFrame.query).
Before prompting the user for input, the condition is evaluated on the current annotation.
If the query evaluates to True then the value will be assigned automatically.

## Annotator

### Calling the annotator

The annotator detects if it is run from Jupyter.
If so, the annotator will render itself in html and css.
If not, the annotator will render itself as text.
You can annotate a selection of records by passing a list of ids to the annotator call. If you want to reannotate ids that have already been annotated, then set `redo` to True when calling the annotator.

### Instantiating the annotator

> arguments
> ---------
> tasks : *Task, list of Task objects, Tasks, Annotations or DataFrame*
>
>     Annotation task(s).
>     If passed a DataFrame, then the tasks will be inferred from it.
>     Annotation data in the dataframe will also be initialized.
>
> data : *data, list-/dict-like, Series or DataFrame, default None*  
>
>     Data to be annotated.
>     If `data` is not already a data object,
>     then it will be passed through `load_data`.
>     The annotator can be instantiated without data,
>     but will only work after data is loaded.
>
> user : *str, default None*  
>
>     Name of the user.
>
> name : *str, default 'HUMANNOTATOR'*  
>
>     Name of the annotator.
>
> save_data : *boolean, default False*  
>
>     Set flag to True if you want to store the data with the annotator.
>     This will ensure that the pickled object, will contain the data.
> 
> other parameters
> ----------------
> **DISPLAY**  
> text_display : *boolean, default None*  
>
>     If True will display the annotator in plain text instead of html.
>
> **HTML**  
>
> markdown : *boolean, default {markdown}*
>
>      If True will pass values through markdown before rendering.
>
> markdown_extensions : *list, default {markdown_extensions}*
>
>      List of markdown extensions to apply.
>
> escape_html : *boolean, default {escape_html}*
>
>     If true will escape html content within items.
>
> maxheight : *str, default '{maxheight_items}'*
>
>     Max height before item gets y-scroll bar.
>     Set to None to have no maximum.
>
> **DATA**  
> item_cols : *str or list of str, default None*  
>
>     Name(s) of dataframe column(s) to display when annotating.
>     By default: display all columns.
>
> id_col : *str, default None*  
>
>     Name of dataframe column to use as index.
>     By default: use the dataframe's index.
> 
> **HIGHLIGHTER**  
> phrases : *str, list of str, default None*  
>
>     Phrases to highlight in the display.
>     The phrases can be regexes.
>     It also to pass in a dict where:
>     - the keys are the phrases
>     - the values are the css styling
>
> escape : *boolean, default False*  
>
>     Set escape to True in order to escape the phrases.
>
> flags : *int, default 0 (no flags)*  
>
>     Flags to pass through to the re module, e.g. re.IGNORECASE.
> 
> **TRUNCATER**  
> truncate : *boolean, default {truncate}*  
>
>     Set to False to not truncate items.
>
> trunc_limit : *int, default {truncate_word_limit}*  
>
>     The number of words beyond which an item will be truncated.
>

The module contains a [configuration file](humannotator/config.ini) in which some of the default behaviour of the humannotator can be configured.
",2022-08-12
https://github.com/lcvriend/pandas_crosstabs,"# PANDAS CROSSTABS

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/lcvriend/pandas_crosstabs/master?filepath=notebooks%2Fexample.ipynb)

Work in progress to make pandas crosstabs more powerful:
- Create crosstabs with absolute and relative frequencies
- Aggregate data on any level in a crosstab
- Add styling to pandas tables

Some examples:
- [dataframe-plus](https://lcvriend.github.io/pandas_crosstabs/examples/style_dataframe-plus.html)
- [Chaplin](https://lcvriend.github.io/pandas_crosstabs/examples/style_chaplin.html)
- [kindofblue](https://lcvriend.github.io/pandas_crosstabs/examples/style_kindofblue.html)
- [minimal](https://lcvriend.github.io/pandas_crosstabs/examples/style_minimal.html)

Works by adding a semantics accessor to the pandas dataframe.
",2022-08-12
https://github.com/lcvriend/responsive_static_site_builder,"# Responsive static site builder
**Tool for building responsive static sites for instruction manuals**

This site builder for instruction manuals was created for the student administration of [Utrecht University](https://www.uu.nl/). It is a tool that helps you create structured responsive static websites that can be used for the documentation of processes, procedures and work instructions. It can be downloaded from [GitHub](https://github.com/lcvriend/responsive_static_site_builder).

## Example website
Check out the [repository website](https://lcvriend.github.io/responsive_static_site_builder/index.html) if you want to get an impression of what this tool can do.

## Getting started
You can find instructions on getting started [here](https://lcvriend.github.io/responsive_static_site_builder/home/getting_started.html).

## Cheatsheet
The [cheatsheet](https://lcvriend.github.io/responsive_static_site_builder/home/cheatsheet.html) gives an overview of the custom html elements that are available.
",2022-08-12
https://github.com/lcvriend/toponym_extraction,"# Toponym extraction

[![Case Study](https://img.shields.io/badge/Repo-case_study-blue)](https://lcvriend.github.io/toponym_extraction/)
[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/lcvriend/toponym_extraction/master?filepath=notebooks%2Fexplore_data.ipynb)  

This repo contains:
1. [Tools](#tools) for extracting toponyms (and lemmata) from newspaper articles downloaded from LexisNexis.
2. The [results](#results) that were collected with these tools for a research on toponyms in news on Brexit in Dutch newspapers.
3. A short write up on this [case study](https://lcvriend.github.io/toponym_extraction/). Check out the interactive map [here](https://lcvriend.github.io/toponym_extraction/map_toponyms.html).

## Workflow

<img src=""docs/illustrations/workflow.svg"" alt=""Workflow"">

## Tools
There are three main scripts that were used to generate the data for this case study. Each script contains further documentation on how they should be used:
- **Build NER model** :[Create a spaCy NER-model for extracting toponyms](scripts/01_create_model.py)
- **Build data set**: [Extract text and meta data from LexisNexis files](scripts/02_textraction.py)
- **Extract toponyms**: [Apply the model to the data set and extract statistics from it](scripts/03_spacify.py)

The `PhraseAnnotator` in [annotation_tools](src/annotation_tools.py) can be used to annotate the NER-results.

## Results
This tool currently extracts two main statistics for each geographical category defined in the [MODEL] chapter of [config.ini](config.ini):
1. Total frequency
2. Article counts

These scripts will generally store results in Python's [pickle](https://docs.python.org/3/library/pickle.html) format. In order to make the results of this study generally available the following data has been added to the repo as csv-files (some have been zipped):
1. The metadata for the [lexisnexis dataset](data/lexisnexis_dataset.csv)
2. The statistics of the [toponym recognition](results/toponym_results.gz)
3. The statistics of the [lemmata recognition](results/lemmata_results.gz)
4. The [annotation data](annotations)

The data and results have been made available through an online jupyter notebook. Access the notebook by clicking this button:  

[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/lcvriend/toponym_extraction/master?filepath=notebooks%2Fexplore_data.ipynb)

Use [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) and [altair](https://altair-viz.github.io/index.html) to explore the data.
",2022-08-12
https://github.com/leonardovida/bi-final-project-2019,"# bi-final-project-2019
Final project for the course Business Intelligence at Utrecht University (2018-2019)
",2022-08-12
https://github.com/leonardovida/decentralized-utrecht-university,"# MBI
Notes, assignments and lectures from the Master in Business Informatics at Utrecht University

Each course has its own page with in-depth explanations about notes and assignments.

## Courses followed
- Data Mining
- Data Science & Society
- Data Analysis and Visualization
- Advanced Research Methods
- Business Intelligence
- Machine Learning for Human Vision and Language
- Mobile Interaction
- ICT Advisory
- (Seminar) Medical Informatics

### Secondary courses
- Introduction to MBI

## Quick links

### Courses
- [Data Science & Society](courses/data-science-and-society) quick links
  - [Course slides]
  - [Notes first part]
  - [Notes second part]
  - [Material]

- [Data Mining](courses/data-mining) quick links
  - [Lecture slides] (courses/data-mining/lectures)
  - [Professor notes] (courses/data-mining/material/professor-notes)
  - [First assignment] (courses/data-mining/assignment/first)
  - [Second assignment] (courses/data-mining/assignment/second)
  - [Material] (courses/data-mining/assignment/second)
  
- [Data Analysis and Visualization](courses/) quick links

- [Advanced Research Methods](courses/) quick links

- [Business Intelligence](courses/) quick links

- [Machine Learning for Human Vision and Language](courses/) quick links

- [Mobile Interaction](courses/) quick links

- [ICT Advisory](courses/) quick links

- [(Seminar) Medical Informatics](courses/) quick links

### Secondary courses
- Introduction to MBI
  - Course slides

### Contributing

Please read [CONTRIBUTING.md](https://gist.github.com/PurpleBooth/b24679402957c63ec426) for details on our code of conduct, and the process for submitting pull requests to us.

## Authors

* **Leonardo Vida** - [Website](https://www.leonardovida.com)

## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details

## Acknowledgments

* Space dedicated to amazing people who will join the shift to fully Open Source education
",2022-08-12
https://github.com/leonardovida/gatsby-starter-netlify-cms,"# Gatsby + Netlify CMS Starter

[![Netlify Status](https://api.netlify.com/api/v1/badges/b654c94e-08a6-4b79-b443-7837581b1d8d/deploy-status)](https://app.netlify.com/sites/gatsby-starter-netlify-cms-ci/deploys)

**Note:** This starter uses [Gatsby v2](https://www.gatsbyjs.org/blog/2018-09-17-gatsby-v2/).

This repo contains an example business website that is built with [Gatsby](https://www.gatsbyjs.org/), and [Netlify CMS](https://www.netlifycms.org): **[Demo Link](https://gatsby-netlify-cms.netlify.com/)**.

It follows the [JAMstack architecture](https://jamstack.org) by using Git as a single source of truth, and [Netlify](https://www.netlify.com) for continuous deployment, and CDN distribution.

## Features

- A simple landing page with blog functionality built with Netlify CMS
- Editabe Pages: Landing, About, Product, Blog-Collection and Contact page with Netlify Form support
- Create Blog posts from Netlify CMS
- Tags: Separate page for posts under each tag
- Basic directory organization
- Uses Bulma for styling, but size is reduced by `purge-css-plugin`
- Blazing fast loading times thanks to pre-rendered HTML and automatic chunk loading of JS files
- Uses `gatsby-image` with Netlify-CMS preview support
- Separate components for everything
- Netlify deploy configuration
- Netlify function support, see `lambda` folder
- Perfect score on Lighthouse for SEO, Accessibility and Performance (wip:PWA)
- ..and more

## Prerequisites

- Node (I recommend using v8.2.0 or higher)
- [Gatsby CLI](https://www.gatsbyjs.org/docs/)
- [Netlify CLI](https://github.com/netlify/cli)

## Getting Started (Recommended)

Netlify CMS can run in any frontend web environment, but the quickest way to try it out is by running it on a pre-configured starter site with Netlify. The example here is the Kaldi coffee company template (adapted from [One Click Hugo CMS](https://github.com/netlify-templates/one-click-hugo-cms)). Use the button below to build and deploy your own copy of the repository:

<a href=""https://app.netlify.com/start/deploy?repository=https://github.com/netlify-templates/gatsby-starter-netlify-cms&amp;stack=cms""><img src=""https://www.netlify.com/img/deploy/button.svg"" alt=""Deploy to Netlify""></a>

After clicking that button, you’ll authenticate with GitHub and choose a repository name. Netlify will then automatically create a repository in your GitHub account with a copy of the files from the template. Next, it will build and deploy the new site on Netlify, bringing you to the site dashboard when the build is complete. Next, you’ll need to set up Netlify’s Identity service to authorize users to log in to the CMS.

### Access Locally

Pulldown a local copy of the Github repository Netlify created for you, with the name you specified in the previous step
```
$ git clone https://github.com/[GITHUB_USERNAME]/[REPO_NAME].git
$ cd [REPO_NAME]
$ yarn
$ netlify dev # or ntl dev
```

This uses the new [Netlify Dev](https://www.netlify.com/products/dev/?utm_source=blog&utm_medium=netlifycms&utm_campaign=devex) CLI feature to serve any functions you have in the `lambda` folder.

To test the CMS locally, you'll need to run a production build of the site:

```
$ npm run build
$ netlify dev # or ntl dev
```

### Media Libraries (installed, but optional)

Media Libraries have been included in this starter as a default. If you are not planning to use `Uploadcare` or `Cloudinary` in your project, you **can** remove them from module import and registration in `src/cms/cms.js`. Here is an example of the lines to comment or remove them your project.

```javascript
import CMS from 'netlify-cms-app'
// import uploadcare from 'netlify-cms-media-library-uploadcare'
// import cloudinary from 'netlify-cms-media-library-cloudinary'

import AboutPagePreview from './preview-templates/AboutPagePreview'
import BlogPostPreview from './preview-templates/BlogPostPreview'
import ProductPagePreview from './preview-templates/ProductPagePreview'
import IndexPagePreview from './preview-templates/IndexPagePreview'

// CMS.registerMediaLibrary(uploadcare);
// CMS.registerMediaLibrary(cloudinary);

CMS.registerPreviewTemplate('index', IndexPagePreview)
CMS.registerPreviewTemplate('about', AboutPagePreview)
CMS.registerPreviewTemplate('products', ProductPagePreview)
CMS.registerPreviewTemplate('blog', BlogPostPreview)
```

Note: Don't forget to also remove them from `package.json` and `yarn.lock` / `package-lock.json` using `yarn` or `npm`. During the build netlify-cms-app will bundle the media libraries as well, having them removed will save you build time.
Example:
```
yarn remove netlify-cms-media-library-uploadcare
```
OR
```
yarn remove netlify-cms-media-library-cloudinary
```
## Getting Started (Without Netlify)

```
$ gatsby new [SITE_DIRECTORY_NAME] https://github.com/netlify-templates/gatsby-starter-netlify-cms/
$ cd [SITE_DIRECTORY_NAME]
$ npm run build
$ npm run serve
```

### Setting up the CMS

Follow the [Netlify CMS Quick Start Guide](https://www.netlifycms.org/docs/quick-start/#authentication) to set up authentication, and hosting.

## Debugging

Windows users might encounter `node-gyp` errors when trying to npm install.
To resolve, make sure that you have both Python 2.7 and the Visual C++ build environment installed.

```
npm config set python python2.7
npm install --global --production windows-build-tools
```

[Full details here](https://www.npmjs.com/package/node-gyp 'NPM node-gyp page')

MacOS users might also encounter some errors, for more info check [node-gyp](https://github.com/nodejs/node-gyp). We recommend using the latest stable node version.

## Purgecss

This plugin uses [gatsby-plugin-purgecss](https://www.gatsbyjs.org/packages/gatsby-plugin-purgecss/) and [bulma](https://bulma.io/). The bulma builds are usually ~170K but reduced 90% by purgecss.

# CONTRIBUTING

Contributions are always welcome, no matter how large or small. Before contributing,
please read the [code of conduct](CODE_OF_CONDUCT.md).
",2022-08-12
https://github.com/leonardovida/leonardovida,"# Hi I'm `Leonardo` - RE @ UU

<!--
**leonardovida/leonardovida** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.-->

- I am a Research Engineer at @[UtrechtUni](https://github.com/UtrechtUniversity) working on large research projects spanning different areas. Currently, I am: 
  - Leading a team of 3 for an NLP project aimed at creating multiple transformer models on millions of Dutch newspaper articles.
  - Engineering the backend of a software for screening academic papers with machine learning techniques @[ASReview](https://github.com/asreview/asreview).
  - Building a complex data pipeline for internal exam-related data retrieval.
- I have a bias towards practice and creating machine learning and data-driven solutions that can have an impact on society. I am very interested in Big Data analysis, putting machine learning models into production, data pipelines and APIs.
",2022-08-12
https://github.com/leonardovida/leonardovida.com,"# leonardovida.com

[![Build Status](https://travis-ci.com/leonardovida/leonardovida.com.svg?branch=master)](https://travis-ci.com/leonardovida/leonardovida.com) [![Netlify Status](https://api.netlify.com/api/v1/badges/d62d658d-2800-4661-a80b-aa7381d2494f/deploy-status)](https://app.netlify.com/sites/elastic-tereshkova-e9c4fa/deploys)

This is a site built with [Gatsby.js](https://www.gatsbyjs.org/) using [gatsby-starter-lumen](https://github.com/alxshelepenok/gatsby-starter-lumen). 


To get started:
```bash
npm install
npm run develop
```

To deploy:
```bash
gatsby clean && gatsby build
```
",2022-08-12
https://github.com/leonardovida/remindo-etl-airflow,"<!-- Parts of this template are inspired by https://github.com/othneildrew/Best-README-Template -->

# ETL on Remindo Toets with Airflow

<!-- Include Github badges here (optional) -->
<!-- e.g. Github Actions workflow status -->
<!-- ![workflow name](https://github.com/UtrechtUniversity/<REPOSITORY>/workflows/<WORKFLOW_NAME_OR_FILE_PATH>/badge.svg)-->

This repository contains two [Airflow](https://airflow.apache.org/) workflows (`dags`) that leverage [Remindo API](https://github.com/UtrechtUniversity/remindo-api) to extract, transform and load Utrecht University data from [Remindo Toets](https://www.paragin.nl/remindotoets/) to the on-premise datawarehouse of Utrecht University. The creation of the database schemas, loading, upserting and merging of data is done leveraging [SQLAlchemy](https://www.sqlalchemy.org/).

Airflow is used to schedule, monitor and start the repeated ETL of the most recent data from Remindo Toets.

<!-- TABLE OF CONTENTS -->
## Table of Contents

- [ETL on Remindo Toets with Airflow](#etl-on-remindo-toets-with-airflow)
  - [Table of Contents](#table-of-contents)
  - [About the Project](#about-the-project)
    - [Overview](#overview)
    - [ETL Flow](#etl-flow)
    - [Built with](#built-with)
    - [License](#license)
  - [Getting Started](#getting-started)
    - [Prerequisites](#prerequisites)
      - [Airflow setup](#airflow-setup)
      - [Hadoop setup](#hadoop-setup)
      - [Set up Spark](#set-up-spark)
    - [Installation](#installation)
    - [Start PostgreSQL](#start-postgresql)
    - [Start Airflow](#start-airflow)
    - [Stopping services](#stopping-services)
  - [TODO: Usage](#todo-usage)
  - [Contributing](#contributing)
  - [Notes](#notes)
    - [TODO: Testing the Limits](#todo-testing-the-limits)
  - [Contact](#contact)

<!-- ABOUT THE PROJECT -->
## About the Project

### Overview

Data is recurringly captured from the Remindo API using the Remindo Python wrapper (View usage - [Fetch Data Module](apiwrapperdocumentationlink)). The data collected from the [Remindo API]([internallink](https://github.com/UtrechtUniversity/remindo-api)) is stored on the local disk and is then moved to the Landing Zone. ETL jobs are written in spark and scheduled in airflow to run every 30 days.  

### ETL Flow

![Pipeline Architecture](docs/images/architecture.png)

- Data Collected from the API is moved to landing zone.
- ETL job has copies data from landing zone to working zone.
- Once the data is moved to working zone, a job is triggered which reads the data from working zone and apply transformation. If Spark is enabled, the dataset is repartitioned and moved to the Processed Zone.
- Warehouse module of ETL jobs picks up data from processed zone and stages it into the Oracle DB staging tables.
- Using the Oracle staging tables and UPSERT (i.e. MERGE) operation is performed on the Data Warehouse tables to update the dataset.
- ETL job execution is completed once the Data Warehouse is updated.
- **TODO**: Airflow DAG runs the data quality check on all Warehouse tables once the ETL job execution is completed.
- **TODO**: Airflow DAG has Analytics queries configured in a Custom Designed Operator. These queries are run and again a Data Quality Check is done on some selected Analytics Table.
- **TODO**: Dag execution completes after these Data Quality check.

**Date**: February 2019

**Research Software Engineer(s)**:

- Leonardo Jaya Vida (l.j.vida@uu.nl)

### Built with

This project mainly leverages

- [Airflow](https://airflow.apache.org/)
- [SQLAlchemy](https://www.sqlalchemy.org/)

<!-- Do not forget to also include the license in a separate file(LICENSE[.txt/.md]) and link it properly. -->
### License

Currently, it is unsure what the license of the code in this project is.

<!-- GETTING STARTED -->
## Getting Started

Guide to get a local copy of this project up and running.

### Prerequisites

To install and run this project you need to have the following prerequisites installed.

- Airflow
- PostgreSQL

If you want to run the ETL jobs using Spark, you will also need to set up the following:

- Hadoop
- Spark

#### Airflow setup

Detailed instruction on how to setup Airflow are available [here](https://towardsdatascience.com/an-apache-airflow-mvp-complete-guide-for-a-basic-production-installation-using-localexecutor-beb10e4886b2) and [here](https://levelup.gitconnected.com/deploying-scalable-production-ready-airflow-in-10-easy-steps-using-kubernetes-4f449d01f47a) if a containarized solution is preferred.

In both recommendations, you need to modify the following when installing Airflow:

```bash
    # from
    pip install apache-airflow['postgresql']
    # to
    pip install 'apache-airflow[crypto, password, oracle]'
```

- Use a PostgreSQL database to manage Airflow's dags.

Finally, copy the dag and plugin [folder](dags) inside the Airflow home directory.

#### Hadoop setup

On `MacOS`:

- Install `brew` if you do not already have it
  - `/usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""`
- Install Java (1.8)
  - `brew cask install homebrew/cask-versions/adoptopenjdk8`
- Install `Hadoop` using `brew`
  - `brew install hadoop`
- **Configure** Hadoop following this [guide from TowardsDataScience](https://towardsdatascience.com/installing-hadoop-on-a-mac-ec01c67b003c)

#### Set up Spark

- From terminal, install `Pyspark`
  - `pip install Pyspark`
- Setup environment variables. From terminal navigate to your `./zshrc` or `./bashrc` or `./bash_profile` depending on your OS.
Insert the following:

```bash
    export JAVA_HOME=<'path-to-apache-spark-bin'>
    export JRE_HOME=<'path-to-apache-spark-bin'>
    export SPARK_HOME=<'path-to-apache-spark-bin'>
    export PATH=<'path-to-apache-spark-bin'>
    export PYSPARK_DRIVER_PYTHON=python
    export PYSPARK_PYTHON=python
```

### Installation

To run the project, ensure to install the project's dependencies.

```sh
pip install -r requirements.txt
```

### Start PostgreSQL

Start the `PostgreSQL` server on which Airflow depends to run and schedule the jobs.

If you used `brew` to install PostgreSQL:

```bash
  brew services start postgres
```

else:

```bash
  services start postgres
```

### Start Airflow

Start Airflow `scheduler`, `webserver` (and `worker` if you set it up) with the following commands on your Terminal.

```python
  airflow start scheduler
  airflow start webserver
  #airflow start worker
```

### Stopping services

```python
  airflow stop scheduler
  airflow stop webserver
  #airflow stop worker
  
  # Now for the Server
  services stop postgres
  # OR
  brew services stop postgres
```

<!-- USAGE -->
## TODO: Usage

This section will describe how to run the project.

_For more examples, please refer to the [Documentation](sphinx-doc-website)_

<!-- CONTRIBUTING -->
## Contributing

Contributions are what make the open source community an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

To contribute:

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

<!-- NOTES -->
## Notes

### TODO: Testing the Limits

The `remindofaker` module in this project generates Fake data which is used to test the ETL pipeline on heavy load.  

<!-- CONTACT -->
## Contact

Leonardo Vida - [@leonardojvida](https://twitter.com/leonardojvida) - contact.rse@uu.nl

Project Link: [https://github.com/UtrechtUniversity/remindo-etl-airflow](https://github.com/UtrechtUniversity/remindo-etl-airflow)
",2022-08-12
https://github.com/leonardovida/vaccinazioni-italia,"# Data Vaccinazioni Italia

Streamlit app to provide an indication of the vaccination date in Italy, using:
* Age
* Region
* Person-specific health attributes

## To-Do

* Create README
* Extrapolate region-specific vaccination growth rate
* Complete calculations
* Fix edge cases
* Deploy using Streamlit share
* Give region-specific recommendations (links) to book vaccinations",2022-08-12
https://github.com/leonardovida/vz-assignment,"# vz-assignment
Assignment for VodafoneZiggo

This repository contains:
  - **Notebooks**: the notebook used in the analysis
  - **Reports**: the reports produced by the analysis.
    - *Main report*: the main report is called Final_Report_Leonardo_Vida
    - *Other reports*: other two automatic reports were created, containing the analysis of the tables used in the assignment
  - **References**: resources used in the analysis
",2022-08-12
https://github.com/LukasvdWiel/SHRIMP,"# SHRIMP
Scalable Highly Recursive Irrelugar Mesh Partitioner

The philosophy behind SHRIMP was to create a simple parallel partitioner that
was memory friendly, in order to partition extremely large meshes.
To accomplish this, there is no single global array,
but everything is kept locally.
Data is passed between partition by passing data around in a circular fashion,
between the threads.

A meshfile is read in parallel and each thread selects the data that belongs to it.
The partitioning is created by splitting the cloud of points in two
in such a way that the interface crosses the weighted midpoint of the cloud,
and having the smallest interface. These created point clouds can be split
again recursively. Once the partitioning of the points is completed,
the elements get the partition of its vertex with the smallest partition number.

This mechanism ensures that a good partitioning is made without the need
to allocate a large graph.

Depending on the output format selected the partitioning is written to file,
to be used by whatever needs it.

Available input formats:  simple, GTECTON

Available output formats: GTECTON, VTU
",2022-08-12
https://github.com/lukavdplas/adjective-order,"# Adjective order

This repository contains my work for my master thesis at Utrecht University.

✨ [go to the the github page of this repository](https://lukavdplas.github.io/adjective-order/) ✨

### What is this about?

_Abstract of my thesis:_

This study investigates the order of English adjective clusters (e.g. _big expensive TV_ vs. _expensive big TV_). Previous research has established that the subjectivity of the adjective is a good predictor for its preferred position, and provided a theoretical motivation for this preference, but has not directly tested a causal link.

This study approaches the relationship between the subjectivity and order of adjectives from an empirical perspective, by setting up an experiment that manipulates the subjectivity of adjectives by presenting different contexts. I then investigate whether this context has an effect on adjective order preference.

Three experiments are conducted, which take a similar approach. The experiments focus on scalar adjectives and present participants with different types of prior distributions, to achieve different levels of uncertainty about the interpretation of the adjective. The effect of this manipulation on subjectivity is verified in a semantic judgement task, by considering inter-subject disagreement and participants' self-reported confidence.

Within this context, adjective order preference is investigated using an acceptability judgement task. This task presents sentences containing adjective clusters, allowing a comparison between the acceptability of different orders.

The results indicate that the context can effectively influence the subjectivity of the adjective. However, there is no evidence that this affects preferences for adjective order. The results show some expected order preferences, but no significant effect of context.

These results suggest that adjective order preference is not sensitive to subtle contextual variation. Further research may reveal whether adjective order is completely independent of context, or whether the distinctions in this experiment are too subtle to detect.

### What is in this repository?

The repository contains data about the experiments I'm conducting, and the code I use for analysis. It's intended as an appendix to my thesis, so it's not self-explanatory.

The [experiment](./experiment) directory has a subdirectory for each experiment.

* [experiment/acceptability](./experiment/acceptability) is the first experiment.
* [experiment/acceptability_with_semantic](./experiment/acceptability) is the second experiment, which adds a semantic judgement task to the procedure.
* [experiment/novel_objects](./experiment/acceptability) is the third experiment. The method is mostly identical to the second experiment, but instead of TVs and couches, the experiment uses fictional objects.

Each experiment folder contains some code to process and inspect the results. There is also a subdirectory ""materials"" with information on stimuli and questions, and a subdirectory ""results"" with the results.

The [modelling](./modelling) directory contains code used for analysis. It contains the code used to calculate disagreement potential, and the following subdirectories:

* [modelling/linear_model](./modelling/linear_model) mainly contains the statistical models to investigate significant factors on acceptability judgements.
* [modelling/order_model](./modelling/order_model) contains the model of adjective order preference.
* [modelling/threshold_model](./modelling/threshold_model) contains the semantic model.
* [modelling/results](./modelling/results) contains various output files.

### How do I view or run the code?

Most of the code is written in Julia using Pluto notebooks. If you want to view the code, you can read the raw code, but I recommend using the [github page](https://lukavdplas.github.io/adjective-order/) of this repository, which contains a static rendered version of each notebook. This is the easiest and quickest way to view the code, complete with output, plots and documentation ✨

If you want to run the code yourself, you will have to install Julia. You can run each notebook as a Julia script, but I recommend installing Pluto. See instructions [here](https://github.com/fonsp/Pluto.jl#lets-do-it).

After installing Pluto, clone the repository and run any of the notebooks in Pluto. Downloading notebooks individually will not work, as they depend on data in the repository, and use the Julia environment specified in [Project.toml](./Project.toml).
",2022-08-12
https://github.com/lukavdplas/CLIN30-presentation,"# CLIN30-presentation
Redoing some of my thesis analysis and figures for my presentation at CLIN 30
",2022-08-12
https://github.com/lukavdplas/crossword-maker,"# crossword-maker
Generate crossword puzzles from a vocabulary.

## Status
The crossworkmaker works! The makecrossword module imports the vocabulary, generates an empty grid, fills it in, extracts clues and exports the result to an XML file. The core modules are the emptycrosswords module, which generates empty crosswords, and the crosswordFiller, which fills an empty crossword with words from the vocabulary. 

The emptycrosswords module specifies a number of constraints on boolean grids (words have a minimum length of 3, the grid should have good ratio of black and white squares, etc.). It then performs a search through all boolean grids, using simulated annealing. The output is boolean grid that specifies which squares can have letters in them.

The crosswordFiller takes an empty crossword and a vocabulary, and fills in the crossword. It performs an exhaustive, depth-first search through all possible crosswords, filling them in word-by-word, reducing computing time by prioritising positions with few options left.

Writing the program so that the black/white squares are determined a priori has some advantages. Most importantly, it reduces the size of the solution space for the search, which helps with complexity.

There are a few minor errors to work out:
* Output from the emptycrosswords should almost always result in a valid empty crossword for the crosswordFiller. However, empty crosswords often lead to the crosswordFiller aborting the search almost immediately, suggesting there may be some problem with compatiblity. For the moment, it does not take too long to start over a few times.
* The crosswordFiller should not use the same word twice in the same crossword, but this constraint does not seem to work. 

As an alternative to the crosswordFiller and hard-coded templates, I have also started on a crosswordMaker, which is similar to the crosswordFiller but does not use an empty crossword as a basis. The adaptation mostly consists of simplifications to the code, which is nice. However, with even a limited vocabulary, it is almost impossible to run the crosswordMaker on my personal computer, since the combinations of words for an entire row increase exponentially, so I'm abandoning the idea for now.
",2022-08-12
https://github.com/lukavdplas/crosswords,"# crosswords
display and fill in crosswords
",2022-08-12
https://github.com/lukavdplas/dutch-dnd,"# Dutch D&D resources

I've been running a Dutch D&D campaign for a while, and I'm trying to see if this is a good way to host some of the content for it.
",2022-08-12
https://github.com/lukavdplas/dutch-verb-matrices,"# Dutch verb matrices
Code used to train and analyse distributional Dutch verb matrices, created for my [bachelor thesis](https://dspace.library.uu.nl/handle/1874/375639).

The trained verb matrices are included, as well as the database of relative clauses used to test them.

Since each python file corresponds to one step in the analysis, it is recommended to keep in mind the order in which these steps were performed when reviewing the code. See the [file overview](./file-overview.pdf) for a comprehensive graph of the dependencies between the python files. 
",2022-08-12
https://github.com/lukavdplas/expl-analysis,"# Data analysis

These files give the analysis done for the final paper of the course Experimentation in Psychology and Linguistics in 2020.

## Files

The files [results_fr.csv](results_fr.csv) and [results_nl.csv](results_nl.csv) give the raw output from Ibex Farm, which was used to run the experiment. [preprocessing.R](preprocessing.R) contains the code to convert the results into a more readable table, which is exported to [exp.data.Rdata](exp.data.Rdata).

To compare responses to the original slogan that the participant saw, the file [slogans.csv](slogans.csv) contains the original slogans. [check_recall_correctness.R](check_recall_correctness.R) adds data on correctness, and exports it to [exp_data_with_correctness.Rdata](exp_data_with_correctness.Rdata).

The [analysis.R](analysis.R) file contains the analysis of French data, while [analysis_dutch.R](analysis_dutch.R) contains the analysis of Dutch data.

Assignment made by:
Luka van der Plas
Valentin Richard
",2022-08-12
https://github.com/lukavdplas/funprog-answers,"My working out of the excercises from the course Functional Pramming 1, taught by Ralf Hinze at the Radboud University in 2017.
",2022-08-12
https://github.com/lukavdplas/historic-dutch-city-names,"# Generate historic Dutch city names

I run a D&D campaign inspired by Dutch history, and I'm often in need of names for people and places. With this generator, I'm trying out the idea of procedurally generating city names.

The generator creates a character-based language model based on a selection of place names from the _Atlas Nouveau des Dix-sept Provinces des Pais-Bas_ by Pierre Mortier, published around 1700.

I have created an ngram model, which seems to work best with fourgrams. I have also done some work on a recurrent neural network, but this still performs worse than the ngram model.
",2022-08-12
https://github.com/lukavdplas/intensifiers,"# intensifiers
Modelling the role of valence in intensifiers.

The model is implemented in Julia. I write all scripts as [Pluto](https://github.com/fonsp/Pluto.jl) notebooks.

If you don't have a Julia installation, I recommend looking at the `static` folder, which contains static HTML versions of each notebook. (I may convert these to PDF later.) These can be opened without a Julia installation and show my explanations for various functions.

If you want to interact with the code, I recommend opening it in Pluto, but they can be read or imported without a Pluto installation.

## Roadmap

* weather.jl - defines the prior probability distribution of temperatures
* quality.jl - defines a quality function on temperatures
* valence.jl - imports data on valence
* experimental_data.jl - import experimental results
* bleached_model.jl - definition of bleached model
* fitting.jl - parameter optimisation for the bleached model
* unbleached_model.jl - definition of the unbleached model
* plots.jl - various plots",2022-08-12
https://github.com/lukavdplas/pluto-notebooks,"# Pluto notebooks

Some examples of [Pluto notebooks](https://github.com/fonsp/Pluto.jl).

Finished:

* **plots:** Demonstration of using plots in Pluto, with some dos and don'ts.=
* **reading-books:** Some basics for handling natural language data.
* **tower-of-hanoi:** An implementation of the tower of hanoi puzzle. There is also a version with a solution.
* **turtles:** An implementation of turtle graphics. More a demonstration than a tutorial.
* **images** A demonstration of how Pluto displays image objects.

Works in progress:

* **ellipses** Some fun stuff about calculating the circumference of an ellipse.
* **svd:** The Singular Value Decomposition of an uploaded image, or your webcam video.
* **file-format:** A demonstration of how Pluto notebooks are encoded as .jl files.
* **text-formatting.jl** Tips for markdown cells.
",2022-08-12
https://github.com/lukavdplas/printi-my-crosswords,"# printi-my-crosswords
Send crosswords to a thermal printer!

### Details
This module imports the xml crosswords as exported by my [crossword generator](https://github.com/lukavdplas/crossword-maker). It then converts the crossword and the clues to a bitmap image using Pillow, and sends it to a thermal printer with printi software, using the [printipigeon](https://github.com/fonsp/printi-pigeon) module.

### Picture!
![thee pictures showing a receipt with a crossword on it coming out of a printer, being held up to the camera and being filled in](docs/demonstration.jpg)

### Future improvements
The program works well right now. Some things I may want to add in the future:
* wrap text for descriptions (most descriptions are so short it is not necessary, but just in case).
* add a solution print function.
",2022-08-12
https://github.com/M-thieu/fastlogitME,"# fastlogitME
Fast but Basic Marginal Effects for Logit Models in R.

Calculates marginal effects based on logistic model objects such as ""glm"" or ""speed.glm"" at the average (default) or at given values indicated by at. It also returns confidence intervals for said marginal effects and the p-values, which can easily be inputed in stargazer. The function only returns the essentials and is therefore much faster but not as detailed as other functions available to calculate marginal effects. As a result, it is highly suitable for large datasets for which other packages may require too much time or calculating power.

Background:
I wrote this function to work with large datasets in R. Estimating logit models and the respective marginal effects can in these cases take a long time and a lot of CPU. Therefore I make use of speed.glm to estimate the logit model and I use this function to calculate the marginal effects, as well as the confidence intervals and p-values of said marginal effects. In the help file of the function I included details on how to export these results with stargazer.
",2022-08-12
https://github.com/MaartenBransen/scm_confocal,"# scm_confocal
Library for importing and dealing with data from the Utrecht University Soft Condensed Matter groups confocal microscopes, including some general utility functions.

**[The documentation can be found here](https://maartenbransen.github.io/scm_confocal/)**

## Info
- created by:     Maarten Bransen
- email:          m.bransen@uu.nl

## Installation
### PIP
This package can be installed directly from GitHub using pip:
```
pip install git+https://github.com/MaartenBransen/scm_confocal
```
### Anaconda
When using the Anaconda distribution, it is safer to run the conda version of pip as follows:
```
conda install pip
conda install git
pip install git+https://github.com/MaartenBransen/scm_confocal
```
### Updating
To update to the most recent version, use `pip install` with the `--upgrade` flag set:
```
pip install --upgrade git+https://github.com/MaartenBransen/scm_confocal
```

## Usage
The classes in this package typically require specific exporting formats from the confocal, and differ in specific implementation details because of this. I have attempted to be consistent in naming functions but each class has its own peculiarities based on personal need.

### SP8
Data from the SP8 can be saved as the native `.lif` files and imported using the [sp8_lif](https://maartenbransen.github.io/scm_confocal/#scm_confocal.sp8_lif) class, which is essentially a wrapper around the [readlif](https://github.com/nimne/readlif) library. This supports most (but not all) functions of the sp8, most notably lacking support for dirext xz imaging.

Alternatively, data can be exported using the Leica LAS software (the microscope operation software), with the check marks for use RAW data checked, and these can be loaded using [sp8_series](https://maartenbransen.github.io/scm_confocal/#scm_confocal.sp8_series) with support for all functions of the sp8. In principle data exported in color (so with a LUT applied) is accepted but not ideal and will return a warning for this reason.

### Visitech Infinity
Two classes are available:

* [visitech_series](https://maartenbransen.github.io/scm_confocal/#scm_confocal.visitech_series) for normal multy-dimensional acquisitions using MicroManager
* [visitech_faststack](https://maartenbransen.github.io/scm_confocal/#scm_confocal.visitech_faststack) for xyzt data recorded using our custom `faststack` driver, which bypasses the z-stage feedback loop and metadata in favor of acquisition speed.

### Utility functions
Additionally, some [utility functions](https://maartenbransen.github.io/scm_confocal/#scm_confocal.util) for stacks on multidimensional microscopy data are included such as binning, rescaling, etc.
",2022-08-12
https://github.com/MaartenBransen/scm_electron_microscopes,"# scm_electron_microscopes
Set of functions for dealing with data from the electron microscopes at Utrecht University

**[The full API documentation can be found here](https://maartenbransen.github.io/scm_electron_microscopes/)**

## Info
- Created by: Maarten Bransen
- Email: m.bransen@uu.nl
- Version: 3.0.0

## Installation

### PIP
This package can be installed directly from GitHub using pip:
```
pip install git+https://github.com/MaartenBransen/scm_electron_microscopes
```
### Anaconda
When using the Anaconda distribution, it is safer to run the conda version of pip as follows:
```
conda install pip
conda install git
pip install git+https://github.com/MaartenBransen/scm_electron_microscopes
```
### Updating
To update to the most recent version, use `pip install` with the `--upgrade` flag set:
```
pip install --upgrade git+https://github.com/MaartenBransen/scm_electron_microscopes
```

## Usage
### Tecnai 12, Tecnai 20, Tecnai 20feg, Talos120, Talos200 using the TIA software

**Note that since version 2.0.0 Pytesseract is no longer required as dependency**

For these microscopes use the [tia](https://maartenbransen.github.io/scm_electron_microscopes/#scm_electron_microscopes.tia) class (aliasses `tecnai` and `talos` are just provided for convenience, they are identical). In TIA, data must be exported as a .tif file. To import data in python, create a class instance using the filename of the TEM image. This automatically loads the image, which is available as numpy.array as the `image` attribute:
```
from scm_electron_microscopes import tia
import matplotlib.pyplot as plt

em_data = tia('myimage.tif')
image = em_data.image

plt.figure()
plt.imshow(image,cmap='Greys_r')
plt.axis('off')
plt.show()
```
Note that this is only the image data, with the scalebar stripped off. The imagedata for the scale bar is available through the `scalebar` attribute, but more likely you are interested in the pixel size which can be determined semi or fully automatically using `tia.get_pixelsize()`:
```
pixelsize,unit = em_data.get_pixelsize()
```
Other information about the microscope is read from the file and can be printed with `tia.print_metadata()`. Files can be exported with a nicer and customizable scalebar using the `tecnai.export_with_scalebar()` function.

### Talos 120, 200 and Spectra microscopes using the Velox software
The native .emd file format used by Velox can be opened using the [velox](https://maartenbransen.github.io/scm_electron_microscopes/#scm_electron_microscopes.velox) class, and images / videos contained within can be extracted using the `get_image` method. By default, the actual image data is not loaded into memory (as this would be ineffecient for large datasets). Instead, a `get_frame` method is available to extract specific video frames as well as a `get_data` method for explicitely loading the full image data:
```
from scm_electron_microscopes import velox

em_data = velox('myimage.emd').get_image(0)

myarray = em_data.load_data()
```

### Helios
For the Helios SEM use the [helios](https://maartenbransen.github.io/scm_electron_microscopes/#scm_electron_microscopes.helios) class, which unlike the `tecnai` and `talos` classes does not load the image into memory by default, such that it is possible to quickly read out metadata of e.g. a slice-and-view series without having to load all the image data. Image data is available through the `load_image()` function:
```
from scm_electron_microscopes import helios

em_data = helios('myimage.tif')
image = em_data.get_image()
```

Pixel sizes written in the metadata and available as a shortcut through a function:
```
(pixelsize_x,pixelsize_y),unit  = em_data.get_pixelsize()
```

Other information about the microscope is read from the file and can be printed in a human-readable format with `helios.print_metadata()` or parsed directly through `helios.load_metadata()`. Files can be exported with a nicer and customizable scalebar using the `helios.export_with_scalebar()` function.

### Phenom
For any of the Phenom SEMs use the [phenom](https://maartenbransen.github.io/scm_electron_microscopes/#scm_electron_microscopes.phenom) class, which works similar as described above for the `helios` class.

### XL30sFEG
This microscope is no longer around, but for older data you can use the `xl30sfeg` class

### Utility functions
Some utility functions for e.g. plotting a histogram are included in the [util](https://maartenbransen.github.io/scm_electron_microscopes/#scm_electron_microscopes.util) class


## Changelog

### Version 3.0.0
- `tecnai` and `talos` class names are deprecated as they are renamed to the `tia` class to prevent confusion with data recorded with the TIA vs the Velox software
- a new velox class is available that reads the native .emd file format of the more modern Velox software installed on the Talos microscopes

### Version 2.0.0
Note that this version has some backwards incompatible changes:
- `load_image` functions have been renamed to `get_image`
- `load_metadata` functions have been renamed to `get_metadata`
- `tecnai.get_pixelsize` no longer uses text recognition to read the scale bar. This is faster and removes Pytesseract and the Tesseract OCR as dependencies, but yields slightly different (and more accurate) values. The old scaling method is available through `get_pixelsize_legacy`.
- all `get_metadata` functions now return an xml.etree.Elementtree object by default, rather than a dictionary which was used for some classes.
",2022-08-12
https://github.com/MaartenBransen/stackscroller,"# stackscroller
Library for visualizing 2-dimensional and 3-dimensional time series with optionally including particle tracking results

**[The documentation can be found here](https://maartenbransen.github.io/stackscroller/index.html)**

## Info
- created by:     Maarten Bransen
- email:          m.bransen@uu.nl

## Installation
### PIP
This package can be installed directly from GitHub using pip:
```
pip install git+https://github.com/MaartenBransen/stackscroller
```
### Anaconda
When using the Anaconda distribution, it is safer to run the conda version of pip as follows:
```
conda install pip
conda install git
pip install git+https://github.com/MaartenBransen/stackscroller
```
### Updating
Updating to the most recent version can be done by running pip with the `--upgrade`  flag:
```
pip install --upgrade git+https://github.com/MaartenBransen/stackscroller
```

## Usage
There are two classes:
- [stackscroller](https://maartenbransen.github.io/stackscroller/index.html#stackscroller.stackscroller): for visualizing a 3-dimensional stack (or a time series of 3-dimensional stacks)
- [videoscroller](https://maartenbransen.github.io/stackscroller/index.html#stackscroller.videoscroller): for visualizing a 2-dimensional time series

When creating a class instance, it *must* be stored to a global variable, otherwise the python garbage collector comes and deletes all of our information needed to scroll through the data. Additionally, when using the Spyder IDE, it must be set up to open figures in a separate window. 

Example usage:
```
from stackscroller import stackscroller
import numpy as np

#create example data with 10 time steps, and 50x128x512 voxels in z,y,x respectively
data = np.random.rand(10,50,128,512)

#create the scroller object with a z-pixel size 3 times that of the x and y pixel size
myscroller1 = stackscroller(data,pixel_aspect=(3,1,1))
```

you can now scroll through z and time using the arrow keys, and switch the viewing direction with 1, 2 or 3 for xy, xz and yz respectively.
",2022-08-12
https://github.com/MarcelRobeer/ContrastiveExplanation,"# Contrastive Explanation (Foil Trees)
> **Contrastive and counterfactual explanations for machine learning (ML)**
> 
> Marcel Robeer (2018-2020), *TNO/Utrecht University*

![Travis (.org)](https://img.shields.io/travis/MarcelRobeer/ContrastiveExplanation?style=flat-square) ![License](https://img.shields.io/github/license/marcelrobeer/ContrastiveExplanation?style=flat-square) ![Python version](https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8%20%7C%203.9-blue?style=flat-square)

#### Contents
1. [Introduction](#introduction)
2. [Publications: citing this package](#publications)
3. [Example usage](#example)
4. [Documentation: choices for problem explanation](#documentation)
5. [License](#license)

<a name=""introduction""></a>
## Introduction 
Contrastive Explanation provides an explanation for why an instance had the current outcome (*fact*) rather than a targeted outcome of interest (*foil*). These *counterfactual* explanations limit the explanation to the features relevant in distinguishing fact from foil, thereby disregarding irrelevant features. The idea of contrastive explanations is captured in this Python package `ContrastiveExplanation`. Example facts and foils are:

Machine Learning (ML) type | Problem | Explainable AI (XAI) question | Fact | Foil
---|---|---|---|---
Classification | Determine type of animal | *Why is this instance a cat rather than a dog?* | Cat | Dog
Regression analysis | Predict students' grade | *Why is the predicted grade for this student 6.5 rather than higher?* | 6.5 | More than 6.5 
Clustering | Find similar flowers | *Why is this flower in cluster 1 rather than cluster 4?* | Cluster 1 | Cluster 4

## Publications <a name=""publications""></a>
One scientific paper was published on Contrastive Explanation / Foil Trees:
* J. van der Waa, M. Robeer, J. van Diggelen, M. Brinkhuis, and M. Neerincx, ""Contrastive Explanations with Local Foil Trees"", in _2018 Workshop on Human Interpretability in Machine Learning (WHI 2018)_, 2018, pp. 41-47. \[Online\]. Available: [http://arxiv.org/abs/1806.07470](http://arxiv.org/abs/1806.07470)

It was developed as part of a Master's Thesis at Utrecht University / TNO:
*  M. Robeer, ""Contrastive Explanation for Machine Learning"", MSc Thesis, Utrecht University, 2018. \[Online\]. Available: [https://dspace.library.uu.nl/handle/1874/368081](https://dspace.library.uu.nl/handle/1874/368081)

#### Citing this package
```
@inproceedings{vanderwaa2018,
  title={{Contrastive Explanations with Local Foil Trees}},
  author={van der Waa, Jasper and Robeer, Marcel and van Diggelen, Jurriaan and Brinkhuis, Matthieu and Neerincx, Mark},
  booktitle={2018 Workshop on Human Interpretability in Machine Learning (WHI)},
  year={2018}
}
```

<a name=""example""></a>
## Example usage  
As a simple example, let us explain a Random Forest classifier that determine the type of flower in the well-known *Iris flower classification* problem. The data set comprises 150 instances, each one of three types of flowers (setosa, versicolor and virginica). For each instance, the data set includes four features (sepal length, sepal width, petal length, petal width) and the goal is to determine which type of flower (class) each instance is.

#### Steps
First, train the 'black-box' model to explain
```python
from sklearn import datasets, model_selection, ensemble
seed = 1

# Train black-box model on Iris data
data = datasets.load_iris()
train, test, y_train, y_test = model_selection.train_test_split(data.data, 
                                                                data.target, 
                                                                train_size=0.80, 
                                                                random_state=seed)
model = ensemble.RandomForestClassifier(random_state=seed)
model.fit(train, y_train)
```

Next, perform contrastive explanation on the first test instance (`test[0]`) by wrapping the tabular data in a `DomainMapper`, and then using method `ContrastiveExplanation.explain_instance_domain()`
```python
# Contrastive explanation
import contrastive_explanation as ce

dm = ce.domain_mappers.DomainMapperTabular(train, 
                                           feature_names=data.feature_names,
					   contrast_names=data.target_names)
exp = ce.ContrastiveExplanation(dm, verbose=True)

sample = test[0]
exp.explain_instance_domain(model.predict_proba, sample)
```
[OUT] *""The model predicted 'setosa' instead of 'versicolor' because 'sepal length (cm) <= 6.517 and petal width (cm) <= 0.868'""*

The predicted class using the `RandomForestClassifier` was 'setosa', while the second most probable class 'versicolor' may have been expected instead. The difference of why the current instance was classified 'setosa' is because its sepal length is less than or equal to 6.517 centimers and its petal width is less than or equal to 0.868 centimers. In other words, if the instance would keep all feature values the same, but change its sepal width to more than 6.517 centimers and its petal width to more than 0.868 centimers, the black-box classifier would have changed the outcome to 'versicolor'.

#### More examples
For more examples, check out the attached [Notebook](https://nbviewer.jupyter.org/github/MarcelRobeer/ContrastiveExplanation/blob/master/Contrastive%20explanation%20-%20example%20usage.ipynb).

<a name=""documentation""></a>
## Documentation
Several choices can be made to tailor the explanation to your type of explanation problem.

### Choices for problem explanation 
##### FactFoil
Used for determining the current outcome (fact) and the outcome of interest (foil), based on a `foil_method` (e.g. second most probable class, random class, greater than the current outcome). Foils can also be manually selected by using the `foil=...` optional argument of the `ContrastiveExplanation.explain_instance_domain()` method.

FactFoil | Description | foil_method
---------|-------------|---
`FactFoilClassification` (*default*) | Determine fact and foil for classification/unsupervised learning | `second`, `random`
`FactFoilRegression` | Determine fact and foil for regression analysis | `greater`, `smaller`

##### Explanators
Method for forming the explanation, either using a Foil Tree (`TreeExplanator`) as described in the [paper](http://arxiv.org/abs/1806.07470), or using a prototype (`PointExplanator`, not fully implemented). As multiple explanations hold, one can choose the `foil_strategy` as either 'closest' (shortest explanation), 'size' (move the current outcome to the area containing most samples of the foil outcome), 'impurity' (most informative foil area), or 'random' (random foil area)

Explanator | Description | foil_strategy
-----------|-------------|---
`TreeExplanator` (*default*) | __Foil Tree__: Explain using a decision tree  | `closest`, `size`, `impurity`, `random`
`PointExplanator` | Explain with a representatitive point (prototype) of the foil class | `closest`, `medoid`, `random`

##### Domain Mappers
For handling the different types of data:
- Tabular (rows and columns)
- Images (rudimentary support)

Maps to a general format that the explanator can form the explanation in, and then maps the explanation back into this format. Ensures meaningful feature names.

DomainMapper | Description
-------------|-------------
`DomainMapperTabular` | Tabular data (columns with feature names, rows)
`DomainMapperPandas` | Uses a `pandas` dataframe to create a `DomainMapperTabular`, while automatically inferring feature names
`DomainMapperImage` | Image data

<a name=""license""></a>
## License
ContrastiveExplanation is [BSD-3 Licensed](https://github.com/MarcelRobeer/ContrastiveExplanation/blob/master/LICENSE).
",2022-08-12
https://github.com/MarcelRobeer/CounterfactualGAN,"# CounterfactualGAN
> Code accompanying the paper [_Generating Realistic Natural Language Counterfactuals_](https://aclanthology.org/2021.findings-emnlp.306.pdf) (Marcel Robeer, Floris Bex and Ad Feelders, 2021).

### Abstract
Counterfactuals are a valuable means for understanding decisions made by machine learning (ML) systems. However, the counterfactuals generated by the methods currently available for natural language text are either unrealistic or introduce imperceptible changes. We propose `CounterfactualGAN`: a method that combines a conditional GAN and the embeddings of a pretrained BERT encoder to model-agnostically generate realistic natural language text counterfactuals for explaining regression and classification tasks. Experimental results show that our method produces perceptibly distinguishable counterfactuals, while outperforming four baseline methods on fidelity and human judgments of naturalness, across multiple datasets and multiple predictive models.

### Software
Download the software directly at https://aclanthology.org/2021.findings-emnlp.306, under [Software](https://aclanthology.org/attachments/2021.findings-emnlp.306.Software.zip). Details on the used hyperparameters and hyperparameter tuning are included in [Appendix A of the paper](https://aclanthology.org/2021.findings-emnlp.306.pdf).

### Method
`CounterfactualGAN` aims to find **targeted counterfactuals for explaining black-box NLP classifiers and regressors** in a model-agnostic manner. It assumes (1) access to the _training set_ a black-box was trained on (or a similar sufficiently large domain-specific dataset) and (2) the ability to _query_ the predictive function of the black-box. Generator `G`, discriminator `D`, encoder `Enc` and decoder `Dec` are trained in a two-phase process:

<img src=""counterfactualgan.png"" alt=""drawing"" style=""width: 70%; max-width: 700px; !important""/>

### Datasets
CounterfactualGAN is compared against three baseline methods on three datasets (accessible through `dataset.py`):

Dataset | Class | Task | URL | Folder format
---|---|---|---|---
_Hatespeech_ | `Hatespeech()` | Regresssion | [\[url\]](https://data.world/thomasrdavidson/hate-speech-and-offensive-language) | `hatespeech_data.csv`
_SST-2_ | `SST()` | Binary classification | [\[url\]](https://github.com/clairett/pytorch-sentiment-classification/tree/master/data/SST2) | `SST2/*.tsv`
_SNLI_ | `SNLI()` | Three-class classification | [\[url\]](https://archive.nyu.edu/handle/2451/41728) | `snli_1.0/snli_1.0_*.txt`

### Citation
```bibtex
@inproceedings{robeer-etal-2021-generating-realistic,
    title = {Generating Realistic Natural Language Counterfactuals},
    author = {Robeer, Marcel and Bex, Floris and Feelders, Ad},
    booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
    month = nov,
    year = {2021},
    address = {Punta Cana, Dominican Republic},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2021.findings-emnlp.306},
    pages = {3611--3625},
}
```
",2022-08-12
https://github.com/MarcelRobeer/explabox,"*<p align=""center"">
  <img src=""https://github.com/MarcelRobeer/explabox/blob/main/img/explabox.png?raw=true"" alt=""explabox logo"">*
</p>

**<h3 align=""center"">
""{`Explore` | `Examine` | `Expose` | `Explain`} your model with the *explabox*!""**
</h3>

---

| Status | |
|:-----------------|:------------------
| _Latest release_ | [![PyPI](https://img.shields.io/pypi/v/explabox)](https://pypi.org/project/explabox/)  [![Downloads](https://pepy.tech/badge/explabox)](https://pepy.tech/project/explabox)  [![Python_version](https://img.shields.io/badge/python-3.8%20%7C%203.9%20%7C%203.10-blue)](https://pypi.org/project/explabox/)  [![License](https://img.shields.io/pypi/l/explabox)](https://www.gnu.org/licenses/lgpl-3.0.en.html)
| _Development_ | [![Lint, Security & Tests](https://github.com/MarcelRobeer/explabox/actions/workflows/check.yml/badge.svg)](https://github.com/MarcelRobeer/explabox/actions/workflows/check.yml)  [![codecov](https://codecov.io/gh/MarcelRobeer/explabox/branch/main/graph/badge.svg?token=7XVEUE5PDM)](https://codecov.io/gh/MarcelRobeer/explabox)  [![Documentation Status](https://readthedocs.org/projects/explabox/badge/?version=latest)](https://explabox.readthedocs.io/en/latest/?badge=latest)  [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

---

The `explabox` aims to support data scientists and machine learning (ML) engineers in explaining, testing and documenting AI/ML models, developed in-house or acquired externally. The `explabox` turns your **ingestibles** (AI/ML model and/or dataset) into **digestibles** (statistics, explanations or sensitivity insights)!

*<p align=""center"">
  <img src=""https://github.com/MarcelRobeer/explabox/blob/main/img/ingestibles-to-digestibles.png?raw=true"" alt=""ingestibles to digestibles"">*
</p>

The `explabox` can be used to:

- __Explore__: describe aspects of the model and data.
- __Examine__: calculate quantitative metrics on how the model performs
- __Expose__: see model sensitivity to random inputs (_robustness_), test model generalizability (_robustness_), and see the effect of adjustments of attributes in the inputs (e.g. swapping male pronouns for female pronouns; _fairness_), for the dataset as a whole (_global_) as well as for individual instances (_local_).
- __Explain__: use XAI methods for explaining the whole dataset (_global_), model behavior on the dataset (_global_), and specific predictions/decisions (_local_).

A number of experiments in the `explabox` can also be used to provide transparency and explanations to stakeholders, such as end-users or clients.

> :information_source: The `explabox` currently only supports natural language text as a modality. In the future, we intend to extend to other modalities.

&copy; National Police Lab AI (NPAI), 2022

<a name=""quick-tour""/></a>
## Quick tour
The `explabox` is distributed on [PyPI](https://pypi.org/project/explabox/). To use the package with Python, install it (`pip install explabox`), import your `data` and `model` and wrap them in the `Explabox`:

```python
>>> from explabox import import_data, import_model
>>> data = import_data('./drugsCom.zip', data_cols='review', label_cols='rating')
>>> model = import_model('model.onnx', label_map={0: 'negative', 1: 'neutral', 2: 'positive'})

>>> from explabox import Explabox
>>> box = Explabox(data=data,
...                model=model,
...                splits={'train': 'drugsComTrain.tsv', 'test': 'drugsComTest.tsv'})
```

Then `.explore`, `.examine`, `.expose` and `.explain` your model:
```python
>>> # Explore the descriptive statistics for each split
>>> box.explore()
```
<img src=""https://github.com/MarcelRobeer/explabox/blob/main/img/example/drugscom_explore.png?raw=true"" alt=""drugscom_explore"" width=""400""/>

```python
>>> # Show wrongly classified instances
>>> box.examine.wrongly_classified()
```
<img src=""https://github.com/MarcelRobeer/explabox/blob/main/img/example/drugscom_examine.png?raw=true"" alt=""drugscom_examine"" width=""400""/>

```python
>>> # Compare the performance on the test split before and after transforming all tokens to uppercase
>>> box.expose.compare_metrics(split='test', perturbation='upper')
```
<img src=""https://github.com/MarcelRobeer/explabox/blob/main/img/example/drugscom_expose.png?raw=true"" alt=""drugscom_expose"" width=""400""/>

```python
>>> # Get a local explanation (uses LIME by default)
>>> box.explain.box.explain_prediction('Hate this medicine so much!')
```
<img src=""https://github.com/MarcelRobeer/explabox/blob/main/img/example/drugscom_explain.png?raw=true"" alt=""drugscom_explain"" width=""400""/>


For more information, visit the [explabox documentation](https://explabox.rtfd.io).

# Contents
- [Quick tour](#quick-tour)
- [Installation](#installation)
- [Documentation](#documentation)
- [Example usage](#example-usage)
- [Releases](#releases)
- [Contributing](#contributing)
- [Citation](#citation)

<a name=""installation""/></a>
## Installation
The easiest way to install the latest release of the `explabox` is through `pip`:

```console
user@terminal:~$ pip install explabox
Collecting explabox
...
Installing collected packages: explabox
Successfully installed explabox
```

> :information_source: The `explabox` requires _Python 3.8_ or above.

See the [full installation guide](INSTALLATION.md) for troubleshooting the installation and other installation methods.

<a name=""documentation""/></a>
## Documentation
Documentation for the `explabox` is hosted externally on [explabox.rtfd.io](https://explabox.rtfd.io).

<a name=""example-usage""/></a>
## Example usage
The [example usage guide](EXAMPLE_USAGE.md) showcases the `explabox` for a black-box model performing multi-class classification of the [UCI Drug Reviews](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29#) dataset.

If you want to follow along, simply `pip install explabox-demo-drugreview` and run the lines in the [Jupyter notebook](https://git.science.uu.nl/m.j.robeer/explabox-demo-drugreview/-/blob/main/explabox_demo_drugreview.ipynb) we have prepared for you!

<a name=""releases""/></a>
## Releases
The `explabox` is officially released through [PyPI](https://pypi.org/project/explabox/). The [changelog](CHANGELOG.md) includes a full overview of the changes for each version.

<a name=""contributing""/></a>
## Contributing
The `explabox` is an open-source project developed and maintained primarily by the Netherlands *National Police Lab AI* (NPAI). However, your contributions and improvements are still required! See [contributing](CONTRIBUTING.md) for a full contribution guide.

<a name=""citation""></a>
## Citation
...

```bibtex
```
",2022-08-12
https://github.com/MarcelRobeer/genbase-test-helpers,"# genbase-test-helpers
Test helpers for [`genbase`](https://git.science.uu.nl/m.j.robeer/genbase/) and its dependent packages.

&copy; Marcel Robeer, 2022.
",2022-08-12
https://github.com/MarcelRobeer/MarcelRobeer,"### Welkom! 🇳🇱

🐻 My name is Marcel Robeer, and I am currently pursuing a [PhD in _Explainable Artificial Intelligence_ (XAI) at Utrecht University](https://uu.nl/staff/MJRobeer)!

---

🤖 My thesis projects and scientific research projects have resulted in several open-source Python packages:

- [**Explabox**](https://github.com/MarcelRobeer/explabox): {`Explore` | `Examine` | `Explain` | `Expose` } your AI model with the _explabox_!
- [**text_explainability**](https://git.science.uu.nl/m.j.robeer/text_explainability): A generic explainability architecture for explaining text machine learning models.
- [**text_sensitivity**](https://git.science.uu.nl/m.j.robeer/text_sensitivity): Extension of text_explainability for sensitivity testing (robustness & fairness).
- [**CounterfactualGAN**](https://github.com/marcelrobeer/counterfactualgan): Generating realistic natural language counterfactuals for classifiers and regressors, without requiring explainee intervention [_EMNLP 2021 paper_].
- [**ContrastiveExplanation**](https://github.com/MarcelRobeer/ContrastiveExplanation): Contrastive and counterfactual explanations for machine learning with Foil Trees [_WHI 2018 paper_].
- [**VisualNarrator**](https://github.com/MarcelRobeer/VisualNarrator): Turns user stories into a conceptual model containing entities and relationships [_RE 2016 paper_].

---

💻 Check out [marcelrobeer.github.io](https://marcelrobeer.github.io) for a full overview. See you there!
",2022-08-12
https://github.com/MarcelRobeer/StoryMiner,"# StoryMiner
Part of Visual Narrator
",2022-08-12
https://github.com/MarcelRobeer/User-Story-Statistics,"# User Story Statistics
For reading in comma-separated values file (.csv) with user stories and obtaining heuristics statistics.
",2022-08-12
https://github.com/MarcelRobeer/VisualNarrator,"# Visual Narrator

> Tells Your User Story Graphically

This program turns user stories into a conceptual model containing entities and relationships.

#### Input
* __Text file__ (.txt, .csv, etc.) containing _one user story per line_

#### Output
* __Report__ of user story parsing, and conceptual model creation
* __Manchester Ontology__ (.omn) describing the conceptual model
* (Optional) __Prolog__ (.pl) arguments
* (Optional) __JSON__ (.json) of the user stories parts' information
* (Optional) __Statistics__ about the user stories
* Returns the mined user stories, ontology, prolog and matrix of weights, which can be used by other tools

## Publications
Two scientific papers were published on Visual Narrator:
* M. Robeer, G. Lucassen, J. M. E. M. van Der Werf, F. Dalpiaz, and S. Brinkkemper (2016). Automated Extraction of Conceptual Models from User Stories via NLP. In _2016 IEEE 24th International Requirements Engineering (_RE_) Conference_ (pp. 196-205). IEEE. \[[pdf](https://www.staff.science.uu.nl/~dalpi001/papers/robe-luca-werf-dalp-brin-16-re.pdf)\]
* G. Lucassen, M. Robeer, F. Dalpiaz, J. M. E. M. van der Werf, and S. Brinkkemper (2017). Extracting conceptual models from user stories with Visual Narrator. _Requirements Engineering_. \[[url](https://link.springer.com/article/10.1007/s00766-017-0270-1)\]

## Dependencies
The main dependency for the program is its Natural Language Processor (NLP) [spaCy](http://spacy.io/). To run the program, you need:

* _Python_ >= 3.6 (under development using 3.7.3)
* _spaCy_ >= 2.1.2 (under development using 2.1.4; language model 'en_core_web_md')
* _NumPy_ >= 1.16.2
* _Pandas_ >= 0.24.2
* _Jinja2_ >= 2.10

## Running the Project
Running the program can be done in three ways: (1) from the command line, (2) using method `VisualNarrator().run()`, (3) serving VisualNarrator as a REST API.

### (1) Command line
With the program main directory as current directory, run the program by executing:

```bash
python run.py <INPUT FILE> [<arguments>]
```

### (2) Method VisualNarrator().run()
First, install as a package `pip install git+https://github.com/MarcelRobeer/VisualNarrator` (or download and run `python setup.py install` in the VisualNarrator folder).

Next, import the `VisualNarrator` class from `vn.vn` and run `VisualNarrator().run()`:

```python
from vn.vn import VisualNarrator

visualnarrator = VisualNarrator(<ARGUMENTS>)
visualnarrator.run(<INPUT FILE>, <SYSTEM_NAME>)
```

Arguments may be supplied to `VisualNarrator(**args)` to re-use and for a single run to `run(*args, **kwargs)`. Execute `help(visualnarrator)` to see all (optional) arguments.

### (3) REST API
Allows a user to POST their user story file to `/mine/` and retrieve a JSON response. It requires three dependencies (`fastapi`, `uvicorn` and `python-multipart`), and can then be run using `uvicorn`. Execute the following in your terminal:
```bash
pip install fastapi uvicorn python-multipart
uvicorn vn.ui.api:vn_app --reload
```
The REST API is then accessible at [http://127.0.0.1:8000/](http://127.0.0.1:8000/) where documentation on the API can be found at [http://127.0.0.1:8000/docs/](http://127.0.0.1:8000/docs/).

#### Arguments
For details on arguments, see our [documentation here](vn/documentation.md).

### Example usage

Command line:
```bash
python run.py example_stories.txt -n ""TicketSystem"" -u --json
```

From method:
```python
from vn.vn import VisualNarrator

visualnarrator = VisualNarrator(json = True)
visualnarrator.run(""example_stories.txt"", ""TicketSystem"", print_us = True)
```

## Conceptual Model
The classes in the program are based on the following conceptual model:

![conceptual_model](https://cloud.githubusercontent.com/assets/1345476/12152551/a6b7dca0-b4b5-11e5-8cee-80f463588df2.png)

The `Reader` starts by reading the input file line by line and generates a list of sentences. These sentences are then enriched using Natural Language Processing, adding Part-of-Speech tags, dependencies, named entity recognition, etc. Subsequently, the `StoryMiner` uses these enriched sentences to create _UserStory_ objects. The User Story objects contain all the information that could be mined from the sentence. These are then used to attach weight to each term in the User Story, creating _Term-by-US Matrix_ in the `Matrix` class. The `Constructor` then constructs patterns out of each user story, using the _Term-by-US Matrix_ to attach a weight to each token. The Constructor forms a model for an ontology, which is then used by the `Generator` to generate a Manchester Ontology file (.omn) and optionally a Prolog file (.pl). Finally, these files are printed to an actual file by the `Writer` in the '/ontologies' and '/prolog' folders respectively. Optionally, the _UserStory_ objects can be output to a JSON file so they can be used in other applications.

## Visual Narrator was developed as part of the _Requirements Engineering Lab_ at _Utrecht University_
[https://www.uu.nl/en/research/software-systems/organization-and-information/labs/requirements-engineering](https://www.uu.nl/en/research/software-systems/organization-and-information/labs/requirements-engineering)
",2022-08-12
https://github.com/marnixnaber/Irissometry,"# Irissometry

Thank you for your interest in the irissometry toolbox!

The irissometry implementation does the following:
- Detects the eye's pupil in close-up videos using a starburst-like algorithm (see function detectPupil())
- Tracks points and calculates distances within the iris (see function irissometry())
- Outputs a matrix containing data about the pupil center coordinates, pupil size, iris feature distances, etc.
- Saves the output matrix in .mat file and .csv file
- Plots some graphs with pupil radius and position data over time (see plotResults.m)

See ""example.m"" for an example code. Just run it, select one or more videos, and observe the magic.

For more info about input and output (io), enter ""help irissometry"" in the command.

> Please cite our work in case you use our implementation!

![Example of irissometry output](https://github.com/marnixnaber/Irissometry/blob/main/images/irissometry.png)

## Reference:
Strauch, C., & Naber, M. (Submitted). Irissometry: effects of pupil size on iris elasticity measured with video-based feature tracking. Investigative Ophthalmology and Vision Sciences.

## Info for videos:
Make sure that the videos do not display black borders at the edge of the frame 
because this causes the pupil border detection to fail. This is how a video frame should look like:

![Example of a good video](https://github.com/marnixnaber/Irissometry/blob/main/images/goodVideoForIrissometry.png)

## Required software:
The code has been tested in MATLAB 2019b on a windows 10 machine. 
No guarantees can be provided for other MATLAB versions and operating platforms.

Please note that you need the following toolboxes to get the code to work:
- Computer vision toolbox 
- Image processing toolbox 
- Signal processing toolbox
- Statistics and machine learning toolbox

If you get an error about a missing function, it is likely that you have not installed the required toolboxes.
Please contact marnixnaber@gmail.com in case you cannot get the code to work. 
If you do so, please send along a screenshot of the error, the movie you want to analyze, and details regarding your
operating system, matlab version, etc.

## License:

This work is licensed under a
[Creative Commons Attribution 4.0 International License][cc-by-sa].

[![CC BY 4.0][cc-by-image]][cc-by-sa]

[cc-by-sa]: https://creativecommons.org/licenses/by-nc-sa/4.0/
[cc-by-image]: https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png
<!-- https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png -->

Please contact marnixnaber@gmail.com for a commercial licence.


## Acknowledgments:
Richard Brown (2007) for sharing the circle fit code.

Dongheng, L.,Winfield, D., and Parkhurst, D. J. Starburst:
A hybrid algorithm for video-based eye tracking combining
feature-based and model-based approaches, in 2005
IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR'05) - Workshops, 2005,
pp. 79–79.

## Contact:
For questions, please contact marnixnaber@gmail.com
",2022-08-12
https://github.com/marnixnaber/rPPG,"# Irissometry

Thank you for your interest in the rPPG toolbox for MATLAB!

This rPPG implementation does the following:
- Opens a popup window to select a directory with video files to be analyzed for heart rate
- Detects a person's face
- Detects and tracks unique face points to calculate face rotations over time
- Puts a mask on top of the face to detect only the skin (alternative color-based skin selection procedures are also available)
- Averages color changes across all facial skin pixels for a raw pulse signal
- Applies signal filtering and color space rotation algorithms to filter out movement and other noisy signals unrelated to heart rate pulsations (multiple algorithms available)
- Applies time frequency analysis to extract heart rate as a function of video time 
- Creates several plots to allow inspection of signal and HR detection results
- Saves the results per video file in an excel file

See ""RunMe.m"" for an example code. Just run it, select one or more videos, and observe the magic.

> Please cite our work in case you use our implementation!

![Example of face detection and tracking](https://github.com/marnixnaber/rPPG/blob/master/images/RPPG_image_output.png)

![Example of time frequency analysis](https://github.com/marnixnaber/rPPG/blob/master/images/RPPG_TFA.png)

![Example of heart rate as a function of video time](https://github.com/marnixnaber/rPPG/blob/master/images/RPPG_HR_time.png)

## Reference:

van der Kooij & Naber (2018). Standardized procedures for the testing and reporting of remote heart rate imaging. Behavior Research Methods

http://link.springer.com/article/10.3758/s13428-019-01256-8

If using the POS algorithm, please cite:
Plane-orthogonal-to-skin (pos) by Wang, W., den Brinker, A. C., Stuijk, S., & de Haan, G. (2017). Algorithmic principles of remote-PPG. IEEE. Transactions on Biomedical Engineering, 64(7), 1479-1491.

https://ieeexplore.ieee.org/abstract/document/7565547/


In case you plan to use rppg for  webcam videos collected online, then check out the following paper:
Di Lernia, D., Finotti, G., Tsakiris, M., Riva, G., & Naber, M. (2022, April 11). Remote photoplethysmography (rPPG) in the wild: Remote heart rate imaging via online webcams. https://doi.org/10.31234/osf.io/v89zn

https://psyarxiv.com/v89zn/

Also see the branch ""into the wild"" for an online data collection manual and adaptations to the rppg script to deal with settings and video database analyses more properly (thanks to Gianluca Finotti!).


## Info for videos:
Make sure that the videos have a high framerate (>20 frames per second) and lossless compression. Videos with VP8 and VP9 codecs are probably not supported and need to be converted to a lossless compression format (e.g., use ffmpeg). Resolution of the video is less relevant.


## Settings:

You can modify the parameters for the signal processing steps in the RunMe.m file.
For example, you can choose from multiple rPPG methods, including Naber (which involves an independent component analysis and several other processing steps) or the popular POS algorithm (see acknowledgments).

In the ""extractFaceFromVideo.m"" file you will find more parameters that can be set (e.g., sensitivity to detect faces, number of face points to track, and method to detect skin pixels from detected face).


## Required software:
The code has been tested in MATLAB 2019b on a Microsoft Windows 10 operating system. No guarantees can be provided for other MATLAB versions and operating platforms.

Please note that you probably need the following toolboxes to get the code to work:
- Computer vision toolbox 
- Image processing toolbox 
- Signal processing toolbox
- Statistics and machine learning toolbox

If you get an error about a missing function, it is likely that you have not installed the required toolboxes.

Please contact marnixnaber@gmail.com in case you cannot get the code to work. 
If you do so, please send along a screenshot of the error, the movie you want to analyze, and details regarding your operating system, matlab version, etc.

To get the scripts to work, download the following MATLAB scripts (also see acknowledgments):

- fastica (version 2.5): https://github.com/aludnam/MATLAB/tree/master/FastICA_25
- polyfitweighted: https://nl.mathworks.com/matlabcentral/fileexchange/13520-polyfitweighted



## License:

This updated work is licensed under a
[Creative Commons Attribution 4.0 International License][cc-by-sa].

[![CC BY 4.0][cc-by-image]][cc-by-sa]

[cc-by-sa]: https://creativecommons.org/licenses/by-nc-sa/4.0/
[cc-by-image]: https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png
<!-- https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png -->

Please contact marnixnaber@gmail.com for a commercial licence.


## Acknowledgments:

Plane-orthogonal-to-skin (pos) algorithm by Wang, W., den Brinker, A. C., Stuijk, S., & de Haan, G.

faceica matlab scripts by Hugo Gävert, Jarmo Hurri, Jaakko Särelä, and Aapo Hyvärinen.

polyfitweighted matlab script by Salman Rogers.

For more rPPG algorithms, see:

MATLAB code by Daniel McDuff's in github repository:
https://github.com/danmcduff/iphys-toolbox

PYTHON code by phuselab in github repository:
https://github.com/phuselab/pyVHR



## Contact:
For questions, please contact marnixnaber@gmail.com
",2022-08-12
https://github.com/MartineDeVos/cont-integr-ex,"# cont-integr-ex
",2022-08-12
https://github.com/MartineDeVos/mdevos-exc1,"# mdevos-exc1
",2022-08-12
https://github.com/MartineDeVos/Spreadsheets,"# Agrovoc

From ftp://ftp.fao.org/gi/gil/gilws/aims/kos/agrovoc_formats/current/agrovoc.skos.xml.en.zip

# Installation

  - Install SWI-Prolog 7.1.10 or later
  - Install ODF sheet library (previously known as plsheet):

    ==
    % swipl
    ?- pack_install(odf_sheet).
    ==

  - See usage by running (assumes swipl 7.1.10 is in $PATH)

    ==
    ./sheet-concepts.pl --help
    ==
",2022-08-12
https://github.com/mgiulini/haddock3-antibodies,"# haddock3-antibodies
A repo for assessing haddock3 performances on antibody-antigen complex prediction
",2022-08-12
https://github.com/mgiulini/pymap,"# pymap

A program for describing how different selections of *N* out of *n* degrees of freedom (mappings) affect the amount of information retained about a full data set.

Three quantities are calculated for each low-resolution representation, namely the mapping entropy:

![equ](https://latex.codecogs.com/gif.latex?S_{map}&space;=&space;\sum_{\phi}p(\phi)&space;\ln\left(\frac{p(\phi)}{\overline{p(\phi)}}&space;\right))

the resolution:

![equ](https://latex.codecogs.com/gif.latex?H_{s}&space;=&space;-\sum_{\phi}p(\phi)&space;\ln\left(p(\phi)\right))

and the relevance:

![equ](https://latex.codecogs.com/gif.latex?H_{k}&space;=&space;-\sum_{K}p(k)\ln\left(p(k)\right).)

where K is the set of unique frequencies observed in the sample.

If you use pymap please cite [this paper](https://arxiv.org/abs/2203.00100).


# Setup

A minimal conda environment (see [here](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html)) to run the calculations can be generated from the .yml file pymap.yml using the following command:

```
conda env create --file pymap.yml
```

Once the environment is correctly created, it must be activated via:

```
conda activate pymap
```

# Testing

Pytest is employed to test the correct installation of pymap. In order to do so, run the following command from the main directory:

```
python -m pytest tests
```

Or directly run *pytest* inside the *tests* folder:

```
cd tests
pytest
```

# Contributing

If you like to add a contribution to this open-source project, follow these steps:

**1.** create an issue with a brief explanation of the contribution;

**2.** add a reasonable label to the issue, or create a new one;

**3.** create a new branch *entirely dedicated to this contribution* either on this repo on your fork;

**4.** develop the code

**5.** use *tox* to test the code. In particular, you should run the following commands:

```
tox -e py310
tox -e lint  
``` 

The first command tests the code with a standard python 3.10 environment, while the second checks the code-style.
    
**6.** open a new Pull-Request on this page, correctly linking the issue. Ask for a review from anyone of the contributors. PS: the Pull Request must pass the continuous integration tests to be accepted.

Enjoy!

# Usage

The program must be provided with two command line arguments, namely a task (**-t**) among *measure* and *optimize* and a relative path to a parameter file (**-p**), containing the parameters to be employed. A list of the accepted parameters is provided here:

| Parameter | Description | Type | Mandatory |
| ----------- | ----------- | ---- | ------- |
| *input_filename* | relative path to the input data | str | yes |
| *output_filename* | relative path to the desired output file | str | yes |
| *max_binom* | max number of mappings that must be generated for each degree of coarse-graining | int | no |
| *nsteps* | number of simulated annealing steps in the optimisation | int | no |
| *ncg* | number of degrees of freedom to be used in the optimisation | int | no |


For task measure, *the default choice is to generate all the coarse-grained mappings for each *N*, a prescription that becomes prohibitive when *n > 15*.

Verbosity can be turned on with the *-v* (*--verbose*) flag.

In general, running

```
python src/pymap.py -h
```

shows the available command line arguments.

## non-interacting spin system

The first data set described in [this article](https://arxiv.org/abs/2203.00100) contains 20 non-interacting spins. The variables of interest can be calculated with the following command

```
python3 src/pymap.py -p parameters/parameters_spins.dat -t measure
```

In this context, the mapping space is quite big, and *max_binom* allows one to explore just a portion of it in few minutes: 

```
python3 src/pymap.py -p parameters/parameters_spins_test.dat -t measure
```

## financial market

To obtain the full results for the simple model of the Nasdaq stock market reported [here](https://arxiv.org/abs/2203.00100) one can use the following command:

```
python3 src/pymap.py -p parameters/parameters_m1.dat -t measure
```

and 

```
python3 src/pymap.py -p parameters/parameters_m2.dat -t measure
```
",2022-08-12
https://github.com/mgiulini/SAT,"# SAT
Simulation Analysis Training. A mini-course on the analysis of molecular simulations offered by me for the EUTOPIA COST action academy.

## Create the conda environment

After cloning the repository, open a terminal and go to the repository folder. There you can setup the conda environment with:

```
conda env create -f conda_env_sat.yml
```

## Activate the environment

```
conda activate SAT
```

## Run the notebook

From the terminal, run:

```
jupyter notebook
```

Then you can select the notebook ```simulation_analysis_notebook.ipynb```.

## pdf version

If you are not interested in running the calculations interactively, a pdf version of the notebook is present in this folder.
",2022-08-12
https://github.com/mountainhydrology/pub_dekoketal-vortex,"# pub_dekoketal-vortex
Scripts to reproduce figures in de Kok et al. Publication details TBD.

Files needed to run the scripts can be downloaded from KNMI climate explorer ( http://climexp.knmi.nl/selectfield_rea.cgi?id=someone@somewhere ) for monthly means, or download ERA5 data directly for hourly data (script included here).
",2022-08-12
https://github.com/mountainhydrology/pub_kraaijenbrink-glaciermodel,"## Mass balance gradient glacier model

Note that this repository is a clone of v1.0 from [kraaijenbrink/nature-2017](https://github.com/kraaijenbrink/nature-2017). Development may have continued there.

### Scripts

##### kraaijenbrink2017-debris-classification.js
Google Earth Engine script in JavaScript that performs Landsat satellite image analysis and debris-cover classification.

##### kraaijenbrink2017-mbg-model.r
The main model script written in R. Required source data can be found [here](https://doi.org/10.5281/zenodo.3346675).

##### kraaijenbrink2017-regional-aggregation.r
Script to aggregate the model output to (RGI) regions.

### Contact
<p.d.a.kraaijenbrink@uu.nl> or <w.w.immerzeel@uu.nl>


### Reference
[Kraaijenbrink PDA, Bierkens MFP, Lutz AF and Immerzeel WW (2017). Impact of a global temperature rise of 1.5 degrees Celsius on Asia’s glaciers. Nature (doi:10.1038/nature23878)](http://doi.org/10.1038/nature23878)
",2022-08-12
https://github.com/MycrofD/alice-yale-hfjet,"This repository contains the analysis code for the D-meson jet analysis.

The aim of this analysis is to reconstruct jets containing D mesons and study their properties.
The results will be presented as a double differential yield in jet _p_<sub>T</sub> and momentum fraction 
carried by the D meson in the direction of the jet axis (_z_).
As a first step, the analysis will be carried out for a single differential yield in jet _p_<sub>T</sub>.

The analysis progress is documented in a [JIRA ticket](https://alice.its.cern.ch/jira/browse/PWGHF-108).

The analysis note and the paper draft are available in this [repository](https://gitlab.cern.ch/saiola/PaperD0JetsPP2010).

The analysis note record in aliceinfo is available at this [link](https://aliceinfo.cern.ch/Notes/node/587).
",2022-08-12
https://github.com/MycrofD/alice_Djets,"
- Prepare root files with an output of your analysis: data + MC for efficiencies and response matrix 
- If a separete bkg.fluctuation matrix is needed, prepare it first
- Prepare configs:

	-  config *.h file, examples are: Dzero.h or Dstar.h 
		- adjust variables to your needs: set up the system, D meson specie, D bins that you want to use, used R parameter, signal and SB ranges, if you want reflections (for D0), sigma in, etc ...
		- IMPORTANT -- set up also names (and description) of your non-prompt (needed for the B feed-down subtraction) and prompt (needed only at the last step when comparing the measured x-section to a model, otherwise can be skipped) D-jet simulation files (path to the files needs to be given in diffrerent place)
		- in case of running with reflections for D0, you must already have the reflection files prepared


- running scripts: here you need to define your configuration
	- run.sh: the main script running the whole analysis -- steps to be run -  settings are passed via a script that runs this one, usually this script doesn't have to be modified, unless you want to skip one of the analysis steps
		-- !! you may want to change the jet pT ranges for the efficiency evaluation: jetpteffmin, jetpteffmax; and how the denominator is define.
	- run_analysis.csh: this script runs run.sh, define here paths to data and MC output files, file names, the *.h configuration file name, if reflections are used, if external bkg. fluctuation matrix is used ...
		-- !!!! setup number of POWHEG sim files !
	- run_main.csh: this script runs run_analysis.sh, define here unfolding algorith, reg. parameter for the unfolding, prior, prior type and bkg. fluctuation matrix type (this you need to set once you use an extrenal bkg. fluctuation matrix, then based on this type name of a root file with the corresponding bkg. fluctuation matrix will be set in the run_analysis.csh script - you should set path to your file). Set up also flags and RELEVANT SCRIPTS if you want to run analysis for systematic unc. evaluation.
",2022-08-12
https://github.com/nanoepics/CETgraph,"# CETgraph version 0.1 #
_First public release by_ **Sanli, 14/11/2017**


In its completed form, this package, should enable identifying thermally-diffusing or electro-actuated single (nano)particles (most probably measured in one of the nanoCET setups), track their position and use this tracking information to identify these particles and their temporal dynamics.

Further information about nanoCET: www.nano-EPics.com
## Project Goals ##

*[x] Identify presence of particles and extract their tracks as they pass through the field of view.  
*[x] Use tracking information (position, intensity, point spread function) to determine  diffusion constants and optical scattering cross section of individual particle and distribution of their properties.
*[ ] Where beneficial, can use Trackpy standard package
*[ ] Generate presentable reports of particle size distribution or other characterized properties.
*[ ] For electro-actuated particles determine the mobility of the particles.

## Directories ##
* **tracking**: the core classes used for analysis
* **analyzing**: actual code used for analyzing an specific type of datafile
* **presenting**: all examples and templates that can be used for presenting extracted information
* **users**: file exchange between active users
* **examples**: sandbox examples to start
* **upstream_tests**: standardized test to assure backward compatiblity when the corse classes are updated


_For a list of contributors see_ ./AUTHORS.md 
",2022-08-12
https://github.com/nanoepics/epicsjukebox,"# epicsjukebox
Code jukebox for the nanoEPics projects at the Debye Institute for Nanomaterials, Utrecht University

# Content

root
|__signals: simulated signals and programmed waveforms
|__processing: analysis code for time traces
|__imagepro: processing code for image sequences
|__examples: example of using the functions in the jukebox

# Contributors

+ Sanli Faez (PI)

# bug reports

+ If you have found a bug or like to ask for a new feature, please report an issue.
+ If you have written a piece of code that can be useful for other nanoEPics projects, please send a pull-request.
",2022-08-12
https://github.com/nanoepics/pynta,"# PyNTA
## Particle tracking instrumentation and analysis in Python

![Screenshot of the PyNTA software](docs/media/screenshot_01.png?raw=true ""PyNTA acquiring"")

PyNTA aims at bridging the gap between data acquisition and analysis in nanoparticle tracking experiments. You can read more about the project at [https://python-nta.readthedocs.io](https://python-nta.readthedocs.io).

## Installation
Create a virtual environment in your own computer and run the following command to get the latest PyNTA version:

    pip install pynta

And if you want the development branch, you can run:

    pip install git+https://github.com/nanoepics/pynta@develop
    
Note that the development branch is for testing purposes only. Forward 
compatibility is not ensured and unexpected bugs may be encountered. 

## Starting PyNTA
In order to start the program, you need to run the following command: 

    pynta
    
It will automatically start the program with a default synthetic camera. If you would like to specify your own configuration file for PyNTA, you should run instead:

    pynta -c config.yml
    
where ``config.yml`` has to be replaced by the name of your file. You can explore the [examples folder](https://github.com/nanoepics/pynta/tree/master/examples) in the repository.

## First Steps with PyNTA
By default, PyNTA comes configures to use synthetic data. The images displayed are simulated random movements of particles. You can use that data to test the program regardless of whether you have a camera available or not. 

You can start by aquiring images and movies. Stream the data to the hard drive and do real time tracking and characterization based on the diffusion of the particles. You can find more information on the [online documentation](http://nanoepics.github.io/pynta).

## Supported cameras
We currently support the following cameras:
* **Hamamatsu Orca** (which interface through DCAM-API)
* **Photonic Science** 
* **Basler**

The code has been structured in such a way that adding support for other cameras is straightforward, and also the simulation of experiments is easy to implement. You can read the guide on how to contribute to the code. 

## Features
The key feature of PyNTA is the ability to acquire and analyse images in real time. By leveraging the capabilities of Trackpy, every frame is processed, detecting particles and linking their trajectories. This allows the user to see results about the distribution of particle sizes in close-to real time. 

PyNTA allows the user to stream data directly to the hard drive, both video data and particle location can be saved during the progress of the experiment, making it failsafe against failures. Metadata is included in every generated file, guaranteeing the reproducibility of the experiments. 

## Report Issues
To report a problem with PyNTA, or suggestions for improvement, etc. you can use the [Issue Tracking System](https://github.com/nanoepics/pynta/issues). You can also contact the authors of the program if you have specific needs or would like to collaborate either scientifically or with development of code.

## Wishlist
* Make PyNTA available for data analysis of data already collected
* Simplify the generation of the configuration file
* Encapsulate the experiment and isolate it from the GUI
* Improve response time for calculating size histograms
* Include extra parameters for the tracking, currently supported by trackpy but not implemented in the GUI
* Create an installer easy to distribute

## For Developers
You can find the source code of this project on [Github](https://github.com/nanoepics/pynta). You can check [the documentation](https://nanoepics.github.io/pynta) in order to understand the structure of the code and the main development guidelines. 

In summary, if you want to add or improve the code, the proper workflow is as follows: 

* Fork the repository into your own user space on Github.
* Start a new branch based on develop if you are adding new functionality or on master if you are solving an ASAP bug.
* Improve the code on that branch.
* Once you are done, update your branch with the latest code from develop:

    ```
    git checkout develop
    git pull upstream develop
    git rebase develop my_branch
    ```
    
    where `my_branch` is the name of the branch you have started. More info [here](https://git-scm.com/docs/git-rebase).
* This leaves your branch with a history appended to the end of the history of develop. Then, you can just merge the branch into develop with ``--squash``:
    ```
    git merge --squash my_branch
    git commit -m ""description of your work""
    ```
    **Important**: When you do this, all the work that you have done on your branch will be condensed to a single commit into develop. Make sure you use a clear message. This makes tracking changes much easier, and the history of commits remains safe in your own repository.",2022-08-12
https://github.com/nehamoopen/cran-privacy-packages,"# cran-privacy-packages

A quick script to extract the names and descriptions of all packages published on CRAN using webscraping (rvest package) and extract the ones relevant to data privacy (tidyverse pacakge). Specifically, packages for generating or simulating (synthetic) data, anonymization, pseudnomization.

",2022-08-12
https://github.com/nehamoopen/digital-garden,"# Digital Garden 🌱
",2022-08-12
https://github.com/nehamoopen/doy-bookdown-test,"This is a [demo](https://nehamoopen.github.io/doy-bookdown-test/_book/index.html) of the Handbook to be developed as part of the FAIR Data Project for Dynamics of Youth.
",2022-08-12
https://github.com/nehamoopen/doy-data-project,# fair-data-project,2022-08-12
https://github.com/nehamoopen/fair-data-project,# fair-data-project,2022-08-12
https://github.com/nehamoopen/nehamoopen,"# Neha Moopen-@nehamoopen 

Hello! I'm a [Research Data Manager](https://www.uu.nl/en/research/research-data-management) at Utrecht University Library :wave: <br/> 
I've a background in clinical psychology, and I've previously worked as a university teacher and junior researcher. 

I'm quite enthusiastic about open science & open source projects + communities :rocket: <br/>
I've contributed to [Liberate Science](https://www.libscie.org/), [The Turing Way](https://the-turing-way.netlify.app/welcome), and [The Carpentries](https://carpentries.org/). In 2020, I got to join the second cohort of the [Open Life Science](https://openlifesci.org/) program, which was developed through [Mozilla Open Leaders X (MOLx)](https://foundation.mozilla.org/en/initiatives/mozilla-open-leaders/).

When I'm not working, I'd love to be catching up on my non-fiction book collection - but I usually end up watching Netflix.
On rare occassions, you might find me hanging out at the local bouldering gym :mountain: 

### :speech_balloon: Get in touch:

* Twitter: https://twitter.com/nehamoopen 
* Personal site: coming soon!
",2022-08-12
https://github.com/nehamoopen/personal-website,"# personal-website
repo for my personal website
",2022-08-12
https://github.com/nehamoopen/postcard,# postcard,2022-08-12
https://github.com/nehamoopen/privacy-engineering-tools,"# privacy-engineering-tools

- [Tools To Generate Synthetic Data & Simulated Data](/synthetic-and-simulated-data-tools.md)
- [Deidentification Tools](/deidentification-tools)
",2022-08-12
https://github.com/nehamoopen/rdm-wiki,"## Hello World!

This is a wiki / knowledge base for all things related to Research Data Management. 

This is not a formal project, notes will likely be added and updated at random and will be minimally structured. It's essentially a collection of interesting links and knowledge items that can be revisited.

Moreover, information might be specific to the institution I work in (Utrecht University, The Netherlands). 

-----
",2022-08-12
https://github.com/nehamoopen/worcshop-book,"# worcshop-book

# Writing A Reproducible Manuscript in R with WORCS

[![go the course website](https://img.shields.io/badge/go%20to%20the-course%20website-blue)](https://nehamoopen.github.io/worcshop-book/)
[![go the GitHub repo](https://img.shields.io/badge/go%20to%20the-GitHub%20repo-yellow)](https://github.com/nehamoopen/worcshop-book)
<a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by/4.0/80x15.png"" /></a>

This repository contains materials for the single-day workshop ['Writing a Reproducible Paper in R with WORCS'](https://www.uu.nl/en/events/writing-a-reproducible-paper-in-r-with-worcs) at Utrecht University.  

## Workshop Description

Open Science is becoming increasingly popular and relevant, and a world of opportunity is opening up to make your work fully reproducible. This is not without its challenges: best practices for reproducible science include a number of tools that you may never have used or even heard of before - *Are you using version control? How are you managing your dependencies? Are you writing your manuscript as an executable document?*

For those who would like to get started with an open and reproducible workflow, without dealing with a mountain of new tools and platforms, we introduce [WORCS](https://psyarxiv.com/k4wde/), a *Workflow for Open Reproducible Code in Science*. The workflow is written for R, but you do not need to have prior programming experience to join this workshop. Having the motivation to step out of your comfort zone — and into a new one — is the most important prerequisite.

WORCS is an R package that takes you from data to published paper in a single streamlined workflow, making the entire process of your analysis, up to the submission of your manuscript, reproducible. The WORCS workflow optionally facilitates pre-registration, sharing your code and your data (safely!), and the submission of a pre-print.

This workshop will be taught by [Research Data Management Support](https://www.uu.nl/en/research/research-data-management) in collaboration with [Caspar van Lissa](https://github.com/cjvanlissa), author of the `worcs` package and the WORCS workflow. We will give you an overview of the workflow and introduce you to its use. You will create your first reproducible project by going through all steps of the workflow yourself.

![WORCS workflow](https://github.com/cjvanlissa/worcs/raw/master/paper/workflow_graph/workflow.png)

## Upcoming Workshops
This is a pilot workshop. Upcoming workshops will be featured on RDM Support's [trainings & workshops](https://www.uu.nl/en/research/research-data-management/training-workshops) page and the university's [Events](https://www.uu.nl/en/organisation/current-affairs/events) page, in addition to the Intranet and various (internal) newsletters. 

## Setup & Installation Requirements
Please work through the [Setup & Test](https://nehamoopen.github.io/workshop-worcs-pilot/setup-and-test.html) instructions prior to the workshop. If you run into issues during setup and installation, you can contact your instructors to help resolve them.

## Schedule
This workshop will be given online. Participants will therefore do most exercises independently, using the step-by-step guides provided.
Instructors will be available for 1:1 support during these times, and reconvene in the main channel at stated times.

| Time | Activity |
|---:|:---|
| 9:00 | Walk-in, Tech Support |
| 10:00 | Introductions |
| 10:15 | Introduction to WORCS by Caspar van Lissa|
| 10:45 | Explanation of workshop program and setup |
| 11:00 | Phase 1: _Study Design_ |
| 11:45 | Recap & Questions |
| **12:00** | **Lunch Break** |
| 12:30 | Phase 2: _Writing & Analysis_ |
| 13:15 | Recap & Questions |
| 13:30 | Phase 3: _Submission & Publication_ |
| 14:15 | Recap & Questions |
| 14:30 | General Discussion |
| 15:00 | **End** |

## Data

You are encouraged to use your own data, code, and manuscript text during the workshop.  

If you do not have your own data, you can use [Allison Horst's Penguin dataset](https://github.com/allisonhorst/palmerpenguins).  
You can use this data by entering the following code in the `prepare_data.R` script.

In case you need some code to run with the Penguin data, you can use some of the [example code](https://allisonhorst.github.io/palmerpenguins/articles/examples.html) on the `palmerpenguins` R package website.

``` r
install.packages(""remotes"")
remotes::install_github(""allisonhorst/palmerpenguins"")
data <- palmerpenguins::penguins
```
Alternatively, you can download one of these versions of the data, and load them yourself:
- An SPSS file ([penguins.sav](https://raw.githubusercontent.com/nehamoopen/workshop-worcs-pilot/main/data/penguins.sav))
- An Excel file ([penguins.xlsx](https://raw.githubusercontent.com/nehamoopen/workshop-worcs-pilot/main/data/penguins.xlsx))
- A CSV file ([penguins.csv](https://raw.githubusercontent.com/nehamoopen/workshop-worcs-pilot/main/data/penguins.csv))

## Workshop Program
Links to a step-by-step guide for each part of the workshop are provided below, taken from the [WORCS Workflow Vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html).

| Subject | Link | Additional information |
|:--------|:-------|:------|
| Phase 1: Study Materials | [step-by-step guide](https://cjvanlissa.github.io/worcs/articles/workflow.html#phase-1-study-design) | - |
| Phase 2: Writing & Analysis | [step-by-step guide](https://cjvanlissa.github.io/worcs/articles/workflow.html#phase-2-writing-and-analysis) | [citing references in WORCS](https://cjvanlissa.github.io/worcs/articles/citation.html) |
| Phase 3: Submission & Publication | [step-by-step guide](https://cjvanlissa.github.io/worcs/articles/workflow.html#phase-3-submission-and-publication) | - |

If you're new to R Markdown, here's a handy [cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)!

### Notes for cautious researchers

Some researchers might want to share their work only once the paper is accepted for publication. In this case, we recommend creating a ""Private"" GitHub repository in Step 1, and completing Steps 13-17 upon acceptance by the journal.

## Additional Resources

- This [paper in *Data Science*](https://doi.org/10.3233/DS-210031) introduces the WORCS workflow, explains the underlying tools, and illustrates how the `worcs` package can be used to create a new project that follows the workflow. You can cite WORCS, both the package and paper, using the citation below.

> Van Lissa, C. J., Brandmaier, A. M., Brinkman, L., Lamprecht, A.,
> Peikert, A., , Struiksma, M. E., & Vreede, B. (in press). WORCS: A
> Workflow for Open Reproducible Code in Science. Data Science, 2021.
> Data Science, vol. 4, no. 1, pp. 29-49. DOI: [10.3233/DS-210031](https://doi.org/10.3233/DS-210031).

- This [presentation](https://bvreede.github.io/worcshop/slides/overview_lecture.html) from an earlier workshop introduces WORCS and the concepts + tools its based on.

- Caspar van Lissa has recorded the following videos [introducing WORCS](https://www.youtube.com/watch?v=uKd6HoK_iS0) and [showing WORCS in action](https://www.youtube.com/watch?v=uzjpN_yFeUU). 

- The `worcs` [GitHub repo](https://github.com/cjvanlissa/worcs) & [package documentation](https://cjvanlissa.github.io/worcs/index.html)  

- R Markdown [cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)

**Sample WORCS projects**  
For a list of sample `worcs` projects created by the `worcs` authors and other users, see the [`README.md` file on the WORCS GitHub page](https://github.com/cjvanlissa/worcs). This list is regularly updated.

## Acknowledgements

This workshop was developed by [Research Data Management Support](https://www.uu.nl/en/research/research-data-management) at Utrecht University, in collaboration with [Caspar van Lissa](https://github.com/cjvanlissa). The [workshop repository and documentation](https://github.com/nehamoopen/workshop-worcs-pilot) was set up by [Neha Moopen](https://github.com/nehamoopen) and [Bianca Kramer](https://github.com/bmkramer), based on an [earlier workshop](https://bvreede.github.io/worcshop/) by [Barbara Vreede](https://github.com/bvreede).

**Image attribution**  
The [Git Logo](https://git-scm.com/) by Jason Long is licensed under the Creative Commons Attribution 3.0 Unported License. The [OSF logo](https://osf.io/) is licensed under CC0 1.0 Universal. Icons in the workflow graph are obtained from [Flaticon](https://www.flaticon.com); see [detailed attribution](https://github.com/cjvanlissa/worcs/blob/master/paper/workflow_graph/Attribution_for_images.txt).
",2022-08-12
https://github.com/nehamoopen/workshop-worcs-bookdown,"This is a minimal example of a book based on R Markdown and **bookdown** (https://github.com/rstudio/bookdown). Please see the page ""Get Started"" at https://bookdown.org/home/about/ for how to compile this example.
",2022-08-12
https://github.com/nehamoopen/workshop-worcs-pilot,"# Writing A Reproducible Manuscript in R with WORCS

[![go the course website](https://img.shields.io/badge/go%20to%20the-course%20website-blue)](https://nehamoopen.github.io/workshop-worcs-pilot/)
[![go the GitHub repo](https://img.shields.io/badge/go%20to%20the-GitHub%20repo-yellow)](https://github.com/nehamoopen/workshop-worcs-pilot)
<a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by/4.0/80x15.png"" /></a>

This repository contains materials for the single-day workshop ['Writing a Reproducible Paper in R with WORCS'](https://www.uu.nl/en/events/writing-a-reproducible-paper-in-r-with-worcs) at Utrecht University.  

## Workshop Description

Open Science is becoming increasingly popular and relevant, and a world of opportunity is opening up to make your work fully reproducible. This is not without its challenges: best practices for reproducible science include a number of tools that you may never have used or even heard of before - *Are you using version control? How are you managing your dependencies? Are you writing your manuscript as an executable document?*

For those who would like to get started with an open and reproducible workflow, without dealing with a mountain of new tools and platforms, we introduce [WORCS](https://psyarxiv.com/k4wde/), a *Workflow for Open Reproducible Code in Science*. The workflow is written for R, but you do not need to have prior programming experience to join this workshop. Having the motivation to step out of your comfort zone — and into a new one — is the most important prerequisite.

WORCS is an R package that takes you from data to published paper in a single streamlined workflow, making the entire process of your analysis, up to the submission of your manuscript, reproducible. The WORCS workflow optionally facilitates pre-registration, sharing your code and your data (safely!), and the submission of a pre-print.

This workshop will be taught by [Research Data Management Support](https://www.uu.nl/en/research/research-data-management) in collaboration with [Caspar van Lissa](https://github.com/cjvanlissa), author of the `worcs` package and the WORCS workflow. We will give you an overview of the workflow and introduce you to its use. You will create your first reproducible project by going through all steps of the workflow yourself.

![WORCS workflow](https://github.com/cjvanlissa/worcs/raw/master/paper/workflow_graph/workflow.png)

## Upcoming Workshops
This is a pilot workshop. Upcoming workshops will be featured on RDM Support's [trainings & workshops](https://www.uu.nl/en/research/research-data-management/training-workshops) page and the university's [Events](https://www.uu.nl/en/organisation/current-affairs/events) page, in addition to the Intranet and various (internal) newsletters. 

## Setup & Installation Requirements
Please work through the [Setup & Test](https://nehamoopen.github.io/workshop-worcs-pilot/setup-and-test.html) instructions prior to the workshop. If you run into issues during setup and installation, you can contact your instructors to help resolve them.

## Schedule
This workshop will be given online. Participants will therefore do most exercises independently, using the step-by-step guides provided.
Instructors will be available for 1:1 support during these times, and reconvene in the main channel at stated times.

| Time | Activity |
|---:|:---|
| 9:00 | Walk-in, Tech Support |
| 10:00 | Introductions |
| 10:15 | Introduction to WORCS by Caspar van Lissa|
| 10:45 | Explanation of workshop program and setup |
| 11:00 | Phase 1: _Study Design_ |
| 11:45 | Recap & Questions |
| **12:00** | **Lunch Break** |
| 12:30 | Phase 2: _Writing & Analysis_ |
| 13:15 | Recap & Questions |
| 13:30 | Phase 3: _Submission & Publication_ |
| 14:15 | Recap & Questions |
| 14:30 | General Discussion |
| 15:00 | **End** |

## Data

You are encouraged to use your own data, code, and manuscript text during the workshop.  

If you do not have your own data, you can use [Allison Horst's Penguin dataset](https://github.com/allisonhorst/palmerpenguins).  
You can use this data by entering the following code in the `prepare_data.R` script.

In case you need some code to run with the Penguin data, you can use some of the [example code](https://allisonhorst.github.io/palmerpenguins/articles/examples.html) on the `palmerpenguins` R package website.

``` r
install.packages(""remotes"")
remotes::install_github(""allisonhorst/palmerpenguins"")
data <- palmerpenguins::penguins
```
Alternatively, you can download one of these versions of the data, and load them yourself:
- An SPSS file ([penguins.sav](https://raw.githubusercontent.com/nehamoopen/workshop-worcs-pilot/main/data/penguins.sav))
- An Excel file ([penguins.xlsx](https://raw.githubusercontent.com/nehamoopen/workshop-worcs-pilot/main/data/penguins.xlsx))
- A CSV file ([penguins.csv](https://raw.githubusercontent.com/nehamoopen/workshop-worcs-pilot/main/data/penguins.csv))

## Workshop Program
Links to a step-by-step guide for each part of the workshop are provided below, taken from the [WORCS Workflow Vignette](https://cjvanlissa.github.io/worcs/articles/workflow.html).

| Subject | Link | Additional information |
|:--------|:-------|:------|
| Phase 1: Study Materials | [step-by-step guide](https://cjvanlissa.github.io/worcs/articles/workflow.html#phase-1-study-design) | - |
| Phase 2: Writing & Analysis | [step-by-step guide](https://cjvanlissa.github.io/worcs/articles/workflow.html#phase-2-writing-and-analysis) | [citing references in WORCS](https://cjvanlissa.github.io/worcs/articles/citation.html) |
| Phase 3: Submission & Publication | [step-by-step guide](https://cjvanlissa.github.io/worcs/articles/workflow.html#phase-3-submission-and-publication) | - |

If you're new to R Markdown, here's a handy [cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)!

### Notes for cautious researchers

Some researchers might want to share their work only once the paper is accepted for publication. In this case, we recommend creating a ""Private"" GitHub repository in Step 1, and completing Steps 13-17 upon acceptance by the journal.

## Additional Resources

- This [paper in *Data Science*](https://doi.org/10.3233/DS-210031) introduces the WORCS workflow, explains the underlying tools, and illustrates how the `worcs` package can be used to create a new project that follows the workflow. You can cite WORCS, both the package and paper, using the citation below.

> Van Lissa, C. J., Brandmaier, A. M., Brinkman, L., Lamprecht, A.,
> Peikert, A., , Struiksma, M. E., & Vreede, B. (in press). WORCS: A
> Workflow for Open Reproducible Code in Science. Data Science, 2021.
> Data Science, vol. 4, no. 1, pp. 29-49. DOI: [10.3233/DS-210031](https://doi.org/10.3233/DS-210031).

- This [presentation](https://bvreede.github.io/worcshop/slides/overview_lecture.html) from an earlier workshop introduces WORCS and the concepts + tools its based on.

- Caspar van Lissa has recorded the following videos [introducing WORCS](https://www.youtube.com/watch?v=uKd6HoK_iS0) and [showing WORCS in action](https://www.youtube.com/watch?v=uzjpN_yFeUU). 

- The `worcs` [GitHub repo](https://github.com/cjvanlissa/worcs) & [package documentation](https://cjvanlissa.github.io/worcs/index.html)  

- R Markdown [cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)

**Sample WORCS projects**  
For a list of sample `worcs` projects created by the `worcs` authors and other users, see the [`README.md` file on the WORCS GitHub page](https://github.com/cjvanlissa/worcs). This list is regularly updated.

## Acknowledgements

This workshop was developed by [Research Data Management Support](https://www.uu.nl/en/research/research-data-management) at Utrecht University, in collaboration with [Caspar van Lissa](https://github.com/cjvanlissa). The [workshop repository and documentation](https://github.com/nehamoopen/workshop-worcs-pilot) was set up by [Neha Moopen](https://github.com/nehamoopen) and [Bianca Kramer](https://github.com/bmkramer), based on an [earlier workshop](https://bvreede.github.io/worcshop/) by [Barbara Vreede](https://github.com/bvreede).

**Image attribution**  
The [Git Logo](https://git-scm.com/) by Jason Long is licensed under the Creative Commons Attribution 3.0 Unported License. The [OSF logo](https://osf.io/) is licensed under CC0 1.0 Universal. Icons in the workflow graph are obtained from [Flaticon](https://www.flaticon.com); see [detailed attribution](https://github.com/cjvanlissa/worcs/blob/master/paper/workflow_graph/Attribution_for_images.txt).
",2022-08-12
https://github.com/nimota/quants-compl-degs,"# quants-compl-degs
Code corresponding to a paper I wrote called ""Quantifiers, complexity, and degrees of semantic universals: A large-scale analysis""

In order to reproduce the results from the paper:
- Execute `./generate_quantifiers.py -u 10 -d 6 -v -i data/cfg.txt -e data/quantifiers.txt -b data/quantifiers.bin`, renaming/creating all paths as needed
- Run `./measures.py -u 10 -v -e data/quantifiers.txt -b data/quantifiers.bin -o measures.csv -p data/precomp`
- Open up the notebook to get plots and statistics

Note that this code is **extremely slow**. Even on a modern PC, running generate_quantifiers with depth 6 takes hours, generating them up to depth 7 will take weeks, if not months (it has yet to finish running on my system), and will require gigabytes of RAM and storage (we have combinatorial explosion to thank for that). The precomputation step of measures.py can also take up to a week, while the measure computation itself requires between one and two days.
I can only imagine this code's performance on non-extensional quantifiers...

Feel free to send me an e-mail if you have a question/suggestion - address is in the paper.


",2022-08-12
https://github.com/nlesc-smcm/easybuild,"This repository contains EasyBuild scripts required to build Trilinos 12.14 with SuperLU_DIST support.
Job scripts are available for Peregrine and Cartesius.

Before you run the script, make sure you don't have your ((LD_)LIBRARY_)PATH set in your .bashrc, since this will make the compilation of the X11 module fail.

On Peregrine, an alternate installation path is required due to the inode limit on /home, and the huge amount of files that the compilation of Boost creates.
To make sure the modules are actually found after installation in this installation path, add the path to your module path:

```
module use /data/$USER/software/modules/all
```

or

```
export MODULEPATH=$MODULEPATH:/data/$USER/software/modules/all
```

After this you can load the Trilinos and CMake modules (and all of their dependencies) by using

```
module load Trilinos/12.14.1-intel-2019a-Python-3.7.2
module load CMake/3.13.3-GCCcore-8.2.0
```

You may also need to set the Intel MPI compilers as your default compilers

```
export CC=mpiicc
export CXX=mpiicpc
export FC=mpiifort
```",2022-08-12
https://github.com/nlesc-smcm/eSalsa-POP,"eSalsa-POP
==========

Fork of the Parallel Ocean Program (POP) created during the eSalsa project at the Netherlands eScience center.

Extensions to the original POP include GPU acceleration and an interface for coupling using the OMUSE framework.





",2022-08-12
https://github.com/nlesc-smcm/hymls,"# HYMLS

HYMLS is a hybrid direct/iterative solver for the Jacobian of the incompressible Navier-Stokes equations on structured grids. The method is based on domain decomposition, where the variables in the interior of the subdomains are eliminated using a direct method and the Schur-complement on the separators is solved using a robust iterative method. The method features a grid-independent convergence rate and does not break down at high Reynolds numbers. The structure-preserving preconditioning technique allows recursive application of the algorithm resulting a a multilevel solver. 

The implementation of HYMLS was done based on the Epetra package in Trilinos. For this reason, we can easily interface with other packages that are available in Trilinos that perform continuation, eigenvalue computation, etc. 

# Building HYMLS

To build HYMLS, you can use cmake. This allows you to build HYMLS anywhere you like, not only in the src directory. So let's say we make a directory ~/build/hymls and hymls is located in ~/hymls. Then we can build with

```
cd ~/build/hymls
cmake ~/hymls
make
```

Note that HYMLS has to be built with an mpi compiler, and that Trilinos has to be in your PATH so cmake can find it. Those can be set in for instance .bashrc, but in case they are not, one can build with something like

```
cd ~/build/hymls
PATH=$PATH:$HOME/Trilinos/bin cmake ~/hymls
make
```

Instead of adding Trilinos to your path, you can also set it through TRILINOS_HOME.

Building with PHIST is done in the same way. You just add it to your PATH. After this, you can enable support for the PHIST JDQR solver by setting `-DHYMLS_USE_PHIST=On`. A build command may look something like

```
cd ~/build/hymls
PATH=$PATH:$HOME/Trilinos/bin:$HOME/phist/bin cmake -DHYMLS_USE_PHIST=On ~/hymls
make
```

# Installing HYMLS

HYMLS can be installed by calling

```
make install
```

from the build directory. It will be installed in the `CMAKE_INSTALL_PREFIX`. If you want to install HYMLS in `~/local`, the build process might be like this

```
cd ~/build/hymls
PATH=$PATH:$HOME/Trilinos/bin cmake -DCMAKE_INSTALL_PREFIX=""${HOME}/local"" ~/hymls
make
make test
make install
```

The line ""make test"" will run all unit and integration tests and exit with a non-zero return value if anything fails.
A more verbose variant is ""make check"", which runs the same tests but prints the output. The integration tests are run
with 8 MPI processes.
After installing HYMLS, other packages should be able to find it through the cmake config.",2022-08-12
https://github.com/nlesc-smcm/i-emic,"# I-EMIC: an Implicit Earth system Model of Intermediate Complexity.
The I-EMIC is a parallel Earth system model that allows the use of large-scale dynamical systems techniques. The term  'implicit' refers to this model's main design feature: all model equations and constraints are solved simultaneously. A Jacobian matrix and preconditioning techniques are available for efficient Newton iterations that are involved in implicit time stepping and continuation strategies.

The I-EMIC contains a number of fully implicit submodels coupled through a modular framework. At the center of the coupled model is the implicit primitive equation ocean model THCM [1]. THCM is extended with models of modest complexity that describe varying aspects of the climate, including atmospheric heat and moisture transport, the formation of sea ice and the adjustment of albedo over snow and ice.

For a description of the model equations, see [2].

[1] Dijkstra, H. A., Oksuzoglu, H., Wubs, F. W., and Botta, E. F. F. (2001). A fully implicit model of the three-dimensional thermohaline ocean circulation. Journal of Computational Physics, 173(2):685–715.

[2] Mulder, T. E. Design and bifurcation analysis of implicit Earth System Models. Dissertation (June, 2019),
Utrecht University, https://dspace.library.uu.nl/handle/1874/381139

# Build instructions

## Dependencies:

|                |                                                                                            |
| -------------- | ------------------------------------------------------------------------------------------ |
| Cmake          | (version 2.8.12.2 or higher)                                                               |
| lapack         | (`liblapack-dev` in ubuntu repository, `mkl` on intel)                                     |
| blas           | (`libblas-dev` in ubuntu repository, `mkl` on intel)                                       |
| openmpi        | (`libopenmpi-dev` in ubuntu repository)                                                    |
| hdf5-openmpi   | (`libhdf5-openmpi-dev` in ubuntu repository, `hdf5/impi` on intel)                         |
| metis          | (`wget http://glaros.dtc.umn.edu/gkhome/fetch/sw/metis/metis-5.1.0.tar.gz`)                |
| parmetis       | (`wget http://glaros.dtc.umn.edu/gkhome/fetch/sw/parmetis/parmetis-4.0.3.tar.gz`)          |
| mrilu          | (available in this repository)                                                             |
| Trilinos       | <https://trilinos.org/download/>  (this project is tested to work with 11.12/11.14/12.12)  |
| jdqzpp         | <https://github.com/erik808/jdqzpp>                                                        |
| gtest          | (external project, fetched and installed by cmake)                                         |

### Compilers
Depending on architecture: ifort, gfortran, mpicc, mpicpc, mpic++, etc...

## Installation:

### Manually
  * Install cmake, lapack, blas, openmpi and hdf5-openmpi as described above.

  * Install metis

  * Install parmetis (requires openmpi)

  * Install Trilinos with support for metis and parmetis:
    * Create build directory `{trilinos_base_dir}/build`
    * Create cmake script `build/{something}.cmake`, for examples see `notes/trilinos_cmake_examples`
      * Adjust `METIS_LIBRARY_DIRS`, `TPL_METIS_INCLUDE_DIRS`, `ParMETIS_LIBRARY_DIRS` and `TPL_ParMETIS_INCLUDE_DIRS`.

    * Make cmake script executable and run it, install Trilinos
      * Possible failures: no lapack, blas or hdf5 libs. `hdf5-openmpi` might install in `/usr/include/hdf5/openmpi`, so you could extend `CPATH` and `LD_LIBRARY_PATH` appropriately: e.g.: `export CPATH=$CPATH:/usr/include/hdf5/openmpi` `export LIBRARY_PATH=$LIBRARY_PATH:/usr/lib/x86_64-linux-gnu/hdf5/openmpi`

  * Install JDQZPP

  * Install I-EMIC
    * Create build directory
    * Create cmake script, see for examples `notes/i-emic_cmake_examples`
    * Run cmake script
    * make install -j<#procs>

### Using EasyBuild
  * Make sure no PATHs are set in your `.bashrc`.

  * Clone the EasyBuild repository located at <https://github.com/nlesc-smcm/easybuild>

  * Install the Trilinos module. For Cartesius, a job script is provided.

  * In your `.bashrc`, load the newly installed modules and set the appropriate PATHs and compilers, e.g.
  ```
  module load 2019 CMake Trilinos/12.14.1-intel-2019a-Python-3.7.2

  export PATH=${HOME}/local/bin:${PATH}
  export LIBRARY_PATH=${HOME}/local/lib:${HOME}/local/lib64:${LIBRARY_PATH}
  export LD_LIBRARY_PATH=${HOME}/local/lib64:${HOME}/local/lib:${LD_LIBRARY_PATH}:$EBROOTIMKL/mkl/lib/intel64

  export CC=mpiicc
  export CXX=mpiicpc
  export FC=mpiifort
  ```

  * Install JDQZPP

  * Compile I-EMIC, e.g.
  ```
  mkdir build
  cd build
  cmake ~/i-emic
  make -j<#procs>
  ```

# General remarks
- See the test code for examples ^^
",2022-08-12
https://github.com/nlesc-smcm/iemic-global,"# Global setup for I-EMIC using OMUSE #

### MacOS Installation Instructions ###

XCode commandline tool should be installed, this can be done via the following
terminal command:

    xcode-select --install


The simplest method for installing the prerequisites if via Homebrew
(https://brew.sh/):

    brew tap nlesc/nlesc
    brew install omuse-iemic

After this is done `python-omuse` can be used to run the omuse-iemic code via
`python-omuse iemic_global.py`. Alternatively, you can create a new Python
virtualenv with access to the omuse-iemic install via `omuse-env DIR`

### Ubuntu Linux Installation Instructions ###

The following command can be used to install all prerequisite packages:

    sudo apt-get install gfortran libopenblas-dev libhdf5-openmpi-dev libgsl0-dev \
          cmake libfftw3-3 libfftw3-dev libmpfr6 libmpfr-dev libnetcdf-dev \
          libnetcdff-dev libptscotch-dev trilinos-all-dev libslicot-dev

Create a new virtualenv:

    python3 -m venv DIR

Activate the virtualenv:

    . DIR/bin/activate

Install/update necessary packages:

    python -m pip install --upgrade pip setuptools wheel setuptools_scm
    pip install matplotlib omuse-iemic

The code can then be run using:

    python iemic_global.py

This will produce a data file with the solution grid. An example script to visualize
the solution is provided:

    python read_data.py
",2022-08-12
https://github.com/nlesc-smcm/POP-global,"# Global setup for POP using OMUSE #

The code can be run by  

    python pop_global.py
   
Input parameters are listed in file pop_in_lowres. 

[Full description of input parameters can be found here.](https://www.cesm.ucar.edu/models/cesm2/ocean/doc/users/POPusers_main.html)
",2022-08-12
https://github.com/nlesc-smcm/pop-iemic-examples,"Examples of POP and IEMIC runs
==============================

This repository contains example OMUSE run scripts that illustrate various 
couplings between the POP ocean model and the I-EMIC implicit ocean model.
In addition to this it has a number of utility scripts for plotting and 
setting up the models. Data for a number of I-EMIC grids is included as well
as an initial POP parameter namelist.

Prerequisites
-------------

In order to run the examples OMUSE must be installed.

Examples
--------

The examples included:

  - ```example_pop_iemic_setup.py```: sets up a configuration for POP where both the grid
    and forcings are taken from the I-EMIC (default) setup. The long term evolution
    should be very close to the I-EMIC equilibrium solution.
  - ```example_iemic_continuation.py```: Continuation run for I-EMIC. The final equilibrium 
    solution is written out.
  - ```example_iemic_timestepping.py```: Evolve I-EMIC with the included implicit time 
    stepper.
  - ```example_pop_iemic_state.py```: Restart POP from a I-EMIC equilibrium state.
  - ```example_iemic_equilibrium.py```: Calculate equilibrium starting from a non-zero state.

Other files
-----------
",2022-08-12
https://github.com/nlesc-smcm/spinup-benchmark,"# spinup-benchmark
omuse scripts for spinup benchmark problem
",2022-08-12
https://github.com/nlpsoc/reliability_bias,"# Assessing the Reliability of Word Embedding Gender Bias Measures


**Prepare Folders**
```shell
mkdir data/embed/
mkdir data/results/
```

**Requirements**

Our experiments are performed on ```Python 3.7```. 
```shell
conda create -n reliability_bias python=3.7
conda activate reliability_bias
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

**Download and Preprocess Data**

Follow the instructions in ```data/train_corpora/```.

**Train Word Embeddings** 

1. Skip-gram with Negative Sampling  
We use 48 threads as default. You can change it to fit your own machine.
   
```shell
python train_sgns.py --corpus wikitext --num_threads 48
python train_sgns.py --corpus reddit_ask_science --num_threads 48
python train_sgns.py --corpus reddit_ask_historians --num_threads 48
```

2. GloVe
First clone the repository from GitHub and compile
```shell
git clone https://github.com/stanfordnlp/glove
cd glove && make
```
Then make your own script to train with different corpora, 
and save the embeddings of WikiText-103, r/AskScience, and r/AskHistorians 
at ```EMBEDDING_FOLDER/glove```. 
For ```EMBEDDING_FOLDER``` see ```embed_folders``` in ```paths.py```. 

**Calculate Word Embedding Gender Bias Scores**

After training word embeddings, 
we calculate gender bias scores of words regarding each word embedding model.
```shell
%SGNS
python calc_bias_scores.py --embed_folder data/embed/wikitext-103 --vocab_path data/embed/wikitext-103/vocab.txt --embed_type sgns --bias_score_path data/results/bias_scores/wikitext-103/sgns.pkl
python calc_bias_scores.py --embed_folder data/embed/wikitext-103 --vocab_path data/embed/wikitext-103/vocab.txt --embed_type glove --bias_score_path data/results/bias_scores/wikitext-103/glove.pkl
python calc_bias_scores.py --embed_folder data/embed/reddit/askscience --vocab_path data/embed/reddit/askscience/vocab.txt --embed_type sgns --bias_score_path data/results/bias_scores/reddit/askscience/sgns.pkl
python calc_bias_scores.py --embed_folder data/embed/reddit/askscience --vocab_path data/embed/reddit/askscience/vocab.txt --embed_type glove --bias_score_path data/results/bias_scores/reddit/askscience/sgns.pkl
python calc_bias_scores.py --embed_folder data/embed/reddit/askhistorians --vocab_path data/embed/reddit/askhistorians/vocab.txt --embed_type sgns --bias_score_path data/results/bias_scores/reddit/askhistorians/sgns.pkl
python calc_bias_scores.py --embed_folder data/embed/reddit/askhistorians --vocab_path data/embed/reddit/askhistorians/vocab.txt --embed_type glove --bias_score_path data/results/bias_scores/reddit/askhistorians/glove.pkl
```

**Estimate Reliability and Run Experiments** 

Run ```reliability_analyses.ipynb``` 
after you have calculated word embedding gender bias scores. 

If you want to train your own word embeddings and run reliability estimation and analyses, 
please refer to ```reliability_metrics.py```. 
```ReliabilityEstimator``` can help you get the job done. 

**Regression Analyses**  

See ```mlr/```. 


**Citation**

If you find this repository useful, please consider citing our paper
```
@inproceedings{du-etal-2021-assessing,
    title = ""Assessing the Reliability of Word Embedding Gender Bias Measures"",
    author = ""Du, Yupei  and
      Fang, Qixiang  and
      Nguyen, Dong"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.785"",
    pages = ""10012--10034"",
    abstract = ""Various measures have been proposed to quantify human-like social biases in word embeddings. However, bias scores based on these measures can suffer from measurement error. One indication of measurement quality is reliability, concerning the extent to which a measure produces consistent results. In this paper, we assess three types of reliability of word embedding gender bias measures, namely test-retest reliability, inter-rater consistency and internal consistency. Specifically, we investigate the consistency of bias scores across different choices of random seeds, scoring rules and words. Furthermore, we analyse the effects of various factors on these measures{'} reliability scores. Our findings inform better design of word embedding gender bias measures. Moreover, we urge researchers to be more critical about the application of such measures"",
}
```

**Contact** 

If you have questions/issues, 
either open an issue or contact Yupei Du (y.du@uu.nl) directly. 
",2022-08-12
https://github.com/nlpsoc/STEL,"**NOTE:** With the current code, changing the batch size of RoBERTa changes its performance. To get the highest possible performance, use an eval batch size of 1. This is probably connected to the tokenization call and the used padding in the batch. To make sure you are not affected by this set eval batch size to 1 (performance of ~0.80 for RoBERTa instead of ~0.61), or even ,cleaner use the sentence-bert implementation for encoding sentences, similar to [here](https://github.com/nlpsoc/Style-Embeddings/blob/78ec8cfb3d0493b7df11241897f412a52aee6f46/src/style_embed/utility/trained_similarities.py).


Thank you for your interest in STEL! This is the code going with the EMNLP 2021 main conference paper [Does It Capture STEL? A Modular, Similarity-based Linguistic Style Evaluation Framework](https://aclanthology.org/2021.emnlp-main.569/).

# Quickstart

You can find the raw data for STEL in Data/STEL. You will need to get permission to use the formality data from Yahoo ([L6 - Yahoo! Answers ComprehensiveQuestions and Answers version 1.0 (multi part)](https://webscope.sandbox.yahoo.com/catalog.php?datatype=l)) as this is also the prerequisite for receiving the [GYAFC dataset](https://github.com/raosudha89/GYAFC-corpus). Please e-mail me (a.m.wegmann@uu.nl) with the permission to receive the full data necessary to run STEL. You will need to add the files to the repository as specified in ```to_add_const.py```.

To use it, in the project folder src, call

```python
import eval_style_models

eval_style_models.eval_sim()
```

This will call all implemented style similarity models on the current version of STEL (except for deepstyle and LIWC based models).  Can take a long time. Might run into RAM problems (depending on your machine).

To only call a specific method on style:

```python
import eval_style_models
import style_similarity

eval_style_models.eval_sim(style_objects=[style_similarity.WordLengthSimilarity()])
```

Do not forget the instantiation via '()'. See some further example calls in `example_eval_style_models.py`. You will need to set LOCAL_STEL_DIM_QUAD to `/Data/STEL/dimensions/_quad_stel-dimensions_formal-815_complex-815.tsv'`.

Expected output:

```
INFO : Running STEL framework 
INFO : Filtering out tasks with low agreement ... 
INFO :       on dimensions ['simplicity', 'formality'] using files ['/home/anna/Documents/UU/STEL/src/../Data/STEL/dimensions/_quad_stel-dimensions_formal-815_complex-815.tsv']...
INFO :       on characteristics ['contraction', 'nbr_substitution'] using file ['/home/anna/Documents/UU/STEL/src/../Data/STEL/characteristics/quad_questions_char_contraction.tsv', '/home/anna/Documents/UU/STEL/src/../Data/STEL/characteristics/quad_questions_char_substitution.tsv']
INFO : Evaluating on 1630 style dim and 200 style char tasks ... 
INFO : Evaluation for method WordLengthSimilarity
INFO : random assignments: 156
INFO :   Accuracy at 0.5792349726775956, without random 0.5866188769414575 with 156 questions
INFO :   Accuracy simplicity at 0.5907975460122699 for 815 task instances, without random 0.5943877551020408 with 784 left questions
INFO :   Accuracy formality at 0.5300613496932515 for 815 task instances, without random 0.5313700384122919 with 781 left questions
INFO :   Accuracy contraction at 0.94 for 100 task instances, without random 0.94 with 100 left questions
INFO :   Accuracy nbr_substitution at 0.5 for 100 task instances, without random 0.7777777777777778 with 9 left questions
             Model Name  Accuracy  Accuracy simplicity  Accuracy formality  \
0  WordLengthSimilarity  0.579235             0.590798            0.530061   

   Accuracy contraction  Accuracy nbr_substitution  
0                  0.94                        0.5  
INFO : Saved results to output/STEL-quadruple_WordLengthSimilarity.tsv
INFO : Saved single predictions to output/STEL_single-pred-quadruple_WordLengthSimilarity.tsv
```

When running on the provided sample of STEL only, the output will look different (see below). Keep in mind that this does not include the full variability of STEL though. It is using a sample, i.e., `/Data/STEL/dimensions/quad_stel-dimension_simple-100_sample.tsv` and `Data/STEL/dimensions/quad_stel-dimension_formal-100_sample.tsv`.

```
INFO : Running STEL framework 
INFO : Filtering out tasks with low agreement ... 
INFO :       on dimensions ['simplicity', 'formality'] using files ['/home/anna/Documents/UU/STEL/src/../Data/STEL/dimensions/quad_stel-dimension_simple-100_sample.tsv', '/home/anna/Documents/UU/STEL/src/../Data/STEL/dimensions/quad_stel-dimension_formal-100_sample.tsv']...
INFO :       on characteristics ['contraction', 'nbr_substitution'] using file ['/home/anna/Documents/UU/STEL/src/../Data/STEL/characteristics/quad_questions_char_contraction.tsv', '/home/anna/Documents/UU/STEL/src/../Data/STEL/characteristics/quad_questions_char_substitution.tsv']
INFO : Evaluating on 200 style dim and 200 style char tasks ... 
INFO : Evaluation for method WordLengthSimilarity
INFO : random assignments: 100
INFO :   Accuracy at 0.6425, without random 0.69 with 100 questions
INFO :   Accuracy simplicity at 0.62 for 100 task instances, without random 0.625 with 96 left questions
INFO :   Accuracy formality at 0.485 for 100 task instances, without random 0.4842105263157895 with 95 left questions
INFO :   Accuracy contraction at 0.94 for 100 task instances, without random 0.94 with 100 left questions
INFO :   Accuracy nbr_substitution at 0.5 for 100 task instances, without random 0.7777777777777778 with 9 left questions
INFO : Saved results to output/STEL-quadruple_WordLengthSimilarity.tsv
INFO : Saved single predictions to output/STEL_single-pred-quadruple_WordLengthSimilarity.tsv
             Model Name  Accuracy  Accuracy simplicity  Accuracy formality  \
0  WordLengthSimilarity    0.6425                 0.62               0.485   

   Accuracy contraction  Accuracy nbr_substitution  
0                  0.94                        0.5  
```



Possible models to call upon (compare to the paper):

```
UncasedBertSimilarity(), LevenshteinSimilarity(), CharacterThreeGramSimilarity(), PunctuationSimilarity(), WordLengthSimilarity(), UppercaseSimilarity(), PosTagSimilarity(), CasedBertSimilarity(), UncasedSentenceBertSimilarity(), RobertaSimilarity(), USESimilarity(), BERTCasedNextSentenceSimilarity(), BERTUncasedNextSentenceSimilarity(), DeepstyleSimilarity(), LevenshteinSimilarity(), LIWCStyleSimilarity(), LIWCSimilarity() 
```

To add your own model, implement the abstract similarity class. Either implement the ```similarity(self, sentence_1: str, sentence_2: str) -> float``` or override the ```similarities(self, sentences_1: List[str], sentences_2: List[str]) -> List[float]``` function for a more efficient implementation. Only similarities will be called. 
```python
from style_similarity import Similarity 

class MySimilarity(Similarity):
    def similarity(self, sentence_1: str, sentence_2: str) -> float:
        if len(sentence_1) == 0 and len(sentence_2) == 0:
            return self.SAME  # same style is at 1
        else:
            return self.DISTINCT # distinct style is at 0
```

To call this new method on STEL:

```python
eval_style_models.eval_sim(style_objects=[MySimilarity()])
```

# Structure

When you add all necessary (partly proprietary) data to use ALL functionalities, the folder should look something like this. Files starting with _ include proprietary data (see below). They are not included in the public release but will need to be acquired. The Datasets folder contains files that were used to generate STEL. They were not included because of size. Here, the GYAFC_Corpus also needs permission from Verizon. Everything else in the Datasets folder can be downloaded from different sources, see also ```to_add_const.py```. 

```
.
├── Data
│   ├── Datasets
│   │   ├── GYAFC_Corpus
│   │  	│	└── Entertainment_Music
│   │  	│	    ├── test
│   │  	│	    │   ├── formal
│   │  	│	    │   └── informal.ref0
│   │  	│	    └── tune
│   │  	│	        ├── formal
│   │  	│	        └── informal.ref0
│   │   └── turkcorpus
│   │  	│	└── truecased
│   │  	│	    ├── test.8turkers.organized.tsv
│   │  	│	    └── tune.8turkers.organized.tsv
│   │   ├── enwiki-20181220-abstract.xml
│   │   ├── RC_2007-05.bz2
│   │   ├── RC_2007-06.bz2
│   │   ├── RC_2007-07.bz2
│   │   ├── RC_2007-08.bz2
│   │   ├── RC_2007-09.bz2
│   │   ├── RC_2012-06.bz2
│   │   ├── RC_2016-06.bz2
│   │   └── RC_2017-06.bz2
│   ├── Experiment-Results
│   │   ├── annotations
│   │   │   ├── 00_nsubs_sentences.txt
│   │   │   ├── 00_nsubs_translated_sentences.txt
│   │   │   ├── QUAD-full_agreement_accuracy.log
│   │   │   ├── _QUAD-full_annotations.tsv
│   │   │   ├── QUAD-subsample_agreement_accuracy.log
│   │   │   ├── _QUAD-subsample_annotations.tsv
│   │   │   ├── readme.md
│   │   │   ├── TRIP-subsample_agreement_accuracy.log
│   │   │   └── _TRIP-subsample_annotations.tsv
│   │   └── models
│   │       ├── readme.md
│   │       ├── Significance testing.ipynb
│   │       ├── STEL_all-models.log
│   │       ├── STEL_deepstyle.log
│   │       ├── STEL_single-preds_all-models.tsv
│   │       ├── STEL_single-preds_deepstyle.tsv
│   │       ├── UNFILTERED_all-models.log
│   │       ├── UNFILTERED_deepstyle.log
│   │       ├── UNFILTERED_single-preds_all-models.tsv
│   │       └── UNFILTERED_single-preds_deepstyle.tsv
│   ├── Models
│   │   ├── coord-liwc-patterns.txt
│   │   └── _LIWC2015_Dictionary.dic
│   └── STEL
│       ├── characteristics
│       │   ├── quad_questions_char_contraction.tsv
│       │   └── quad_questions_char_substitution.tsv
│       ├── dimensions
│       │   ├── quad_stel-dimension_formal-100_sample.tsv
│       │   ├── _quad_stel-dimensions_formal-815_complex-815.tsv
│       │   └── quad_stel-dimension_simple-100_sample.tsv
│       └── readme.md
├── src
│   ├── set_for_global.py
│   ├── to_add_const.py
│   ├── generate_pot_quads.py
│   ├── sample_for_annotation.py
│   ├── eval_annotations.py
│   ├── eval_style_models.py
│   ├── output
│   └── utility
│       ├── base_generators.py
│       ├── file_utility.py
│       ├── nsubs_possibilities.py
│       ├── qualtrics_api.py
│       ├── qualtrics_constants.py
│       ├── const_generators.py
│       ├── quadruple_generators.py
│       ├── neural_model.py
│       ├── style_similarity.py
├── LICENSE
├── readme.md
└── .gitignore 
```



# Prerequisites

## Python Libraries

### STEL light

 Code tested on python 3.8.5. If you only want to use STEL with your own models, the relevant packages with the tested versions are:

```
pandas==1.1.3
numpy==1.18.5
scikit-learn==0.23.2
nltk==3.6.2  # for pos tagger 'averaged_perceptron_tagger', 'universal_tagset' and 'punkt' need to be downloaded
typing==3.10.0.0
```

Tested functionality for calling eval_style_models.py only.

### STEL

If you also want to call the neural similarity methods, you will also need the following packages: 

```
transformers==3.5.1
torch==1.7.0
sentence-transformers==2.0.0
scipy==1.7.1
tensorflow==2.3.1
tensorflow-hub==0.11.0 
```

### using deepstyle

see: https://github.com/hayj/DeepStyle

``` h5py==2.10.0 
h5py==2.10.0
transformers==2.4.1
tensorflow-gpu==2.0
```

Also, in ```model.py```, you will need to add the line ```import os```.

## Proprietary data

The file ```to_add_const.py``` contains the paths and information to all files that are not part of the GitHub release but are necessary to run (part) of the code. Some files were not added because of their size and others because the data is proprietary:

For **LIWC-based similarities** the repository expects the [LIWC 2015 dictionary](https://repositories.lib.utexas.edu/bitstream/handle/2152/31333/LIWC2015_LanguageManual.pdf) in ```Data/Models/_LIWC2015_Dictionary.dic```

For the **formal/informal STEL dimension**, you will need to get permission to use the formality data from Yahoo ([L6 - Yahoo! Answers ComprehensiveQuestions and Answers version 1.0 (multi part)](https://webscope.sandbox.yahoo.com/catalog.php?datatype=l)) as this is also the prerequisite for receiving the [GYAFC dataset](https://github.com/raosudha89/GYAFC-corpus). Please e-mail me (a.m.wegmann@uu.nl) with the permission to receive the full data necessary to run STEL and add it to ```Data/STEL/dimensions/_quad_stel-dimensions_formal-815_complex-815.tsv```.  We received a limited waiver to release a  sample of 100 STEL formal/informal tasks with this GitHub release to test the code. It still falls under the Yahoo's Terms of Use and you will need to get permission from them to use Yahoo's data in your own publications.  



# Citation

When using STEL please cite our paper

```
@inproceedings{wegmann-nguyen-2021-capture,
    title = ""Does It Capture {STEL}? A Modular, Similarity-based Linguistic Style Evaluation Framework"",
    author = ""Wegmann, Anna  and
      Nguyen, Dong"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.569"",
    pages = ""7109--7130"",
    abstract = ""Style is an integral part of natural language. However, evaluation methods for style measures are rare, often task-specific and usually do not control for content. We propose the modular, fine-grained and content-controlled similarity-based STyle EvaLuation framework (STEL) to test the performance of any model that can compare two sentences on style. We illustrate STEL with two general dimensions of style (formal/informal and simple/complex) as well as two specific characteristics of style (contrac{'}tion and numb3r substitution). We find that BERT-based methods outperform simple versions of commonly used style measures like 3-grams, punctuation frequency and LIWC-based approaches. We invite the addition of further tasks and task instances to STEL and hope to facilitate the improvement of style-sensitive measures."",
}
```

and the papers that the dataset was (partly) generated from:

```
@inproceedings{rao-tetreault-2018-dear,
    title = ""Dear Sir or Madam, May {I} Introduce the {GYAFC} Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer"",
    author = ""Rao, Sudha  and
      Tetreault, Joel"",
    booktitle = ""Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)"",
    month = jun,
    year = ""2018"",
    address = ""New Orleans, Louisiana"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/N18-1012"",
    doi = ""10.18653/v1/N18-1012"",
    pages = ""129--140"",
}

@article{xu-etal-2016-optimizing,
    title = ""Optimizing Statistical Machine Translation for Text Simplification"",
    author = ""Xu, Wei  and
      Napoles, Courtney  and
      Pavlick, Ellie  and
      Chen, Quanze  and
      Callison-Burch, Chris"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""4"",
    year = ""2016"",
    url = ""https://aclanthology.org/Q16-1029"",
    doi = ""10.1162/tacl_a_00107"",
    pages = ""401--415"",
}
```

We thank Yahoo for granting us the right to upload a sample of 100 task instances from the formal/informal dimension. Please make sure to adhere to their Terms of Use. Especially asking for their permission to reuse any examples via [L6 - Yahoo! Answers ComprehensiveQuestions and Answers version 1.0 (multi part)](https://webscope.sandbox.yahoo.com/catalog.php?datatype=l).

# Comments

Thank you for your comments and questions. You can use GitHub Issues or address me directly (Anna via a.m.wegmann @ uu.nl).
",2022-08-12
https://github.com/nlpsoc/Style-Embeddings,"
Thank you for your interest in Style Embeddings. This is the code that is part of the [Same Author or Just Same Topic? Towards Content-Independent Style Representations](https://aclanthology.org/2022.repl4nlp-1.26/) publication at the 7th RepL4NLP workshop co-located with ACL 2022.

In `Data` you can find the generated training (contrastive) AV tasks with `src/style_embed/generate_dataset.py`.  The best-performing style embedding as trained and described in our publication can be found here: https://huggingface.co/AnnaWegmann/Style-Embedding

# Quickstart

You might just want to use the style embedding model and not fine-tune anything or generate authorship verification tasks. If that is the case it is not necessary to to download anything from the repo. Just use the above [huggingface model](https://huggingface.co/AnnaWegmann/Style-Embedding). The Huggingface Hosted Inference API also allows calculating sentence similarities without downloading anything if you want to just try out a few sentence similarities.

To load the model from the huggingface hub and encode a sentence:
```Python
from sentence_transformers import SentenceTransformer
sentences = [""This is an example sentence"", ""Each sentence is converted""]

model = SentenceTransformer('AnnaWegmann/Style-Embedding')
embeddings = model.encode(sentences)
print(embeddings)
```

```
[[ 1.4103483e-01  2.0874986e-01  1.7136575e-01 ...  3.5445389e-01
   4.6482438e-01 -4.4582412e-06]
 [ 3.9674792e-01  1.5319356e-01  7.7177114e-03 ...  6.6641623e-01
   4.8512089e-01 -3.2561386e-01]]
```

Let's calculate the sentence similarity between two sentences with our new style model. We use a parallel example with formal vs. informal style from [GYAFC](https://aclanthology.org/N18-1012/). See also Figure 2 in our paper. 
```Python
from sentence_transformers import util

emb1 = model.encode(""r u a fan of them or something?"")  # more informal
emb2 = model.encode(""Are you one of their fans?"")  # more formal, similar content to emb1
print(""Cosine-Similarity:"", util.cos_sim(emb1, emb2))
```

```
Cosine-Similarity: tensor([[0.078]])
```

```Python
emb3 = model.encode(""Oh yea and that young dr got a bad haircut"")  # more informal, different content to emb1
print(""Cosine-Similarity:"", util.cos_sim(emb1, emb3))
```

```
Cosine-Similarity: tensor([[0.745]])
```


## Fine-tuning 

If you want to fine-tune a RoBERTa model based on the CAV task, you will need the provided code. 

First step: generate the training task based on [convokit](https://convokit.cornell.edu/). If you want to use your own conversation data, you will need to first load it in convokit corpus format. For example, [via a pandas dataframe](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/corpus_from_pandas.ipynb).

```python
from utility.convokit_generator import TaskGenerator
from global_const import TOPIC_CONVERSATION

# these can take some time depending on your data size ...
cav_gen = TaskGenerator(convokit_data_keys=[""subreddit-ApplyingToCollege""], years=[2018], total=10)
cav_gen._get_data_split(topic_variable=TOPIC_CONVERSATION)
train_path, dev_path, test_path = cav_gen.save_data_split(output_dir=""."", topic_variable=TOPIC_CONVERSATION)
```

Second step: use the generated data as a fine-tuning task.

```python
from utility.neural_trainer import SentenceBertFineTuner
from utility.training_const import TRIPLET_LOSS, TRIPLET_EVALUATOR

tuner = SentenceBertFineTuner(model_path=""roberta-base"", train_filename=train_path, dev_filename=dev_path, loss=TRIPLET_LOSS, evaluation_type=TRIPLET_EVALUATOR)
model_path = tuner.train(epochs=1, batch_size=8)

from sentence_transformers import util
emb1 = tuner.model.encode(""I'm a happy person"")
emb4 = tuner.model.encode(""I'm a happy person."")
print(""Cosine-Similarity:"", util.cos_sim(emb1, emb4))
```

Third step: evaluating the trained model (without the STEL directory)

```python
from eval_model import _evaluate_model
from utility.trained_similarities import TunedSentenceBertSimilarity
from global_const import set_logging
set_logging()

model_w_sim = TunedSentenceBertSimilarity(model=tuner.model)
_evaluate_model(model_w_sim, model_path, [test_path], test_stel=False, test_AV=True)
```

Evaluating WITH STEL (needs the downloaded repo on the same level as style-embeddings)

```python
from global_identifiable import include_STEL_project 
include_STEL_project()

_evaluate_model(model_w_sim, model_path, [test_path], test_stel=True, test_AV=False)
```



# Prerequisites

To run most functionalities you will only need to have the necessary **Python Packages** installed. To run all evaluations (i.e., including the STEL tasks), you will need to have access to **[STEL](https://github.com/nlpsoc/stel)** data and code.

## Python Packages

Python version **3.8.5**

```
pandas==1.1.3
numpy==1.18.5
pytorch==1.7.1
sentence-transformers==2.1.0
transformers==4.12.2
scipy==1.5.2
```



## STEL

To run `eval_model.py` with `--stel True` you need access to some code from the STEL project and the STEL data (which includes partly proprietary data). You will need to download the STEL repo and ask for access to the STEL data (see: https://github.com/nlpsoc/STEL). 



# Structure

Below you find a structure of the repository and some comments.

```python
.
├── Data
│   └── train_data
│       ├── GENERATE_100-SUBS-2018_745453.txt 
│       ├── Task-Statistics.ipynb   
│       ├── author_data.json
│       ├── dev-45000__subreddits-100-2018_tasks-300000__topic-variable-conversation.tsv
│       ├── dev-45000__subreddits-100-2018_tasks-300000__topic-variable-random.tsv
│       ├── dev-45000__subreddits-100-2018_tasks-300000__topic-variable-subreddit.tsv
│       ├── test-45000__subreddits-100-2018_tasks-300000__topic-variable-conversation.tsv
│       ├── test-45000__subreddits-100-2018_tasks-300000__topic-variable-random.tsv
│       ├── test-45000__subreddits-100-2018_tasks-300000__topic-variable-subreddit.tsv
│       ├── train-210000__subreddits-100-2018_tasks-300000__topic-variable-conversation.zip
│       ├── train-210000__subreddits-100-2018_tasks-300000__topic-variable-random.zip
│       └── train-210000__subreddits-100-2018_tasks-300000__topic-variable-subreddit.zip
├── src
│   ├── output
│   ├── style_embed
│   │   ├── cluster.py 	 	 # script to cluster sentences with a rep. model 
│   │   ├── eval_model.py 	 # script to evaluate embeddings on CAV & STEL-Or-Content 
│   │   ├── fine_tune.py 	 # script to fine-tune transformers based on CAV tasks 
│   │   ├── generate_dataset.py 	 # script to generate CAV tasks with different CC variables 
│   │   ├── global_const.py 	 	 # constants & functions that are globally accessible in the project 
│   │   ├── global_identifiable.py 	 # same as const but including paths/names that are local, like dir paths
│   │   └── SD_calc.ipynp 	 # jupyter notebook to calculate stand devation & means from results
│   ├── test
│   │   ├── test_cluster.py 	 	 
│   │   ├── test_eval_model.py 	
│   │   ├── test_fine_tune.py 	
│   │   ├── test_neural_trainer.py 	 	
│   │   └── test_STEL_Or_Content.py 	 
│   └── utility
│       ├── convokit_generator.py 	 	 
│       ├── evaluation_metrics.py 	
│       ├── neural_trainer.py 	
│       ├── plot.py 	 	
│       ├── plot_utility.py 	 	
│       ├── statistics_utility.py 	 	
│       ├── STEL_error_analysis.py 	 	
│       ├── STEL_Or_Content.py 	 	
│       ├── trained_similarities.py 	 	
│       └── plot_utility.py 	 
├── LICENSE
├── Readme.md
└── .gitignore 
```

# Citation

If you find our work or this repository helpful, please consider citing our paper



```
@inproceedings{wegmann-etal-2022-author,
    title = ""Same Author or Just Same Topic? Towards Content-Independent Style Representations"",
    author = ""Wegmann, Anna  and
      Schraagen, Marijn  and
      Nguyen, Dong"",
    booktitle = ""Proceedings of the 7th Workshop on Representation Learning for NLP"",
    month = may,
    year = ""2022"",
    address = ""Dublin, Ireland"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.repl4nlp-1.26"",
    pages = ""249--268"",
    abstract = ""Linguistic style is an integral component of language. Recent advances in the development of style representations have increasingly used training objectives from authorship verification (AV){''}:'' Do two texts have the same author? The assumption underlying the AV training task (same author approximates same writing style) enables self-supervised and, thus, extensive training. However, a good performance on the AV task does not ensure good {``}general-purpose{''} style representations. For example, as the same author might typically write about certain topics, representations trained on AV might also encode content information instead of style alone. We introduce a variation of the AV training task that controls for content using conversation or domain labels. We evaluate whether known style dimensions are represented and preferred over content information through an original variation to the recently proposed STEL framework. We find that representations trained by controlling for conversation are better than representations trained with domain or no content control at representing style independent from content."",
}
```



# Comments

For comments, problems or questions open an issue or contact Anna (a.m.wegmann@uu.nl). 
",2022-08-12
https://github.com/OceanParcels/AntarcticPeninsulaPlastic,"# AntarcticPeninsulaPlastic

This repository holds the code for the publication ""Plastics in sea surface waters around the Antarctic Peninsula"" Lacerda, ALF, LS Rodrigues, E van Sebille, FL Rodrigues, L Ribeiro, ER Secchi, F Kessler, MC Proietti (2019), Scientific Reports, 9, 3977 (https://doi.org/10.1038/s41598-019-40311-4)
",2022-08-12
https://github.com/OceanParcels/bare-bones-parcels,"# bare-bones-parcels
Simplified code mimicking Parcels architecture.
It is not used for particle simulations but to develop technical features easily.
",2022-08-12
https://github.com/OceanParcels/BayesianAnalysis_SouthAtlantic,"### Source Code for article ""Attribution of Plastic Sources Using Bayesian Inference: Application to River-Sourced Floating Plastic in the South Atlantic Ocean""

This repository contains the code to reproduce the results shown in the article ""Attribution of Plastic Sources Using Bayesian Inference: Application to River-Sourced Floating Plastic in the South Atlantic Ocean"". In general, you can find the scripts for running the simulation and the scripts for analysing the output of the simulation.

This code uses [Ocean Parcels](https://oceanparcels.org) to perform the simulations. We recommend creating and environment to run the code as explained [here](https://oceanparcels.org/#installing). For the analysis, make sure to install the packages on the `requirements.txt`.

Here is a description of scripts according to it's function:

#### Analysis
To run the following scripts download data from the simulations from the supplementary material. For the scripts to work make sure to save the Data from the supplementary material within this repository such as `BayesianInference_SouthAtlantic/PierardBassottoMeirervanSebille_AttributionofPlastic`.

We recommend running the scripts in the following order:

0. `python/article_plots.py`: plots the figures shown in the article. The analysis files that you get from step 1, 2, and 3 are in the `PierardBassottoMeirervanSebille_AttributionofPlastic/analysis`. You can plot the plots without running the analysis

1. `python/compute_probability.py`: computes the probabilities in the domain.
2. `python/beached_probability.py`: computes the probabilities
 for beached particles.
3. `python/Bootstrapping.py`: computes the standard deviation by performing bootstrapping. This takes 1 day in a super computer. Avoid running it in the local computer.


#### Simulation
To run the simulations you need to download the SMOC velocity fields from April 2016 to august 2020.

1. Run before simulation:

    - `python/landmask.py`: creates the supporting fields that deal with the land-boundaries.
    - `python/release_positions.py`: creates the initial conditions for the particles.

2. Run Simulation:

    - `parcels_simulations.py`: script that runs the Lagrangian simulation.
    - `local_kernels.py`: contains the kernels used in the `parcels_simulations.py`. You don't need to run it.
",2022-08-12
https://github.com/OceanParcels/BayesianAnalysis_SW_NL,"Repository for paper: Identifying Marine Sources of Beached Plastics Through a Bayesian Framework: Application to Southwest Netherlands.
Including codes used for simulation, post-processing and datafiles to do so. Codes to generate (most of) the used datafiles can be found on https://github.com/OceanParcels/NorthSeaBeaching/
",2022-08-12
https://github.com/OceanParcels/biofouling_3dtransport_2,"## Repository for biofouling study using NEMO-MEDUSA 2.0
This repository contains the code needed to produce the results for ""Modeling submerged biofouled microplastics and their three-dimensional trajectories"".
The study can be done in four steps: **Preprocessing**, **Simulation**, **Postprocessing** and **Analysis**.

### Preprocessing
In this step, vertical diffusivity fields are computed from global tidal mixing climatology (https://www.seanoe.org/data/00619/73082/)

### Simulation
The Lagrangian simulation of virtual particles representing microplastics. Bash scripts are used to run the script *Simulation.py*, which uses functions from *kernels.py* and *utils.py*

### Postprocessing
Computation of the individual oscillations from the particle trajectories. Computation of regional climatology for study locations.

### Analysis
Notebooks to explore the simulation. Scripts to create figures.
Note that numpy version 1.21.0 is needed to run these scripts 
",2022-08-12
https://github.com/OceanParcels/coherent_vortices_OPTICS,"# Code for the paper ""Ordering of trajectories reveals hierarchical finite-time coherent sets in Lagrangian particle data""
David Wichmann, Christian Kehl, Henk A. Dijkstra, and Erik van Sebille

Questions to d.wichmann@uu.nl
",2022-08-12
https://github.com/OceanParcels/ContinuousCascadingFragmentation,"# Continuous Cascading Fragmentation model for Marine Plastics 
GitHub repository for 'Modelling size distributions of marine plastics under the influence of continuous cascading fragmentation' by Mikael L. A. Kaandorp, Henk A. Dijkstra, Erik van Sebille
",2022-08-12
https://github.com/OceanParcels/continuous_integration_example,"# continuous_integration_example
This is a simple example of how to use Travis for continuous integration in code development.
",2022-08-12
https://github.com/OceanParcels/CoralDispersion,"# CoralDispersion
Repository for Reint Fischer's thesis project on dispersion around corals

The code referred to in the methods section of the thesis is located in the following files:
explfunctions.py all functions used in different steps of the process
preprocessing.py evaluating the CFD model output to determine objects and distances and direction to each object in the fluid domain
simulation_repeat.py Parcels particle tracking simulation to analyse the volume fluxes
simulation.py Parcels particle tracking simulation to define the network
postprocessing.py Calculation of the volume fluxes and network

The figures from the results and discussion section of the thesis are from the Jupyter Notebooks.

",2022-08-12
https://github.com/OceanParcels/drifter_trajectories_network,"# Code for ""Detecting flow features in scarce trajectory data using networks derived from symbolic itineraries: an application to surface drifters in the North Atlantic""
David Wichmann, Christian Kehl, Henk A. Dijkstra, Erik van Sebille

Questions to: d.wichmann@uu.nl

# Code description:
particle_and_network_classes.py: Main script to handle trajectory data and for network analysis and

## Preparation
Set up python environment specified in 'env_drifter_network.yml'

## North Atlantic drifters
### Preparation of drifter data
1. Download the data (.dat) from https://www.aoml.noaa.gov/phod/gdp/interpolated/data/all.php
2. Execute 'constrain_drifterdata_to_northatlantic.py', possibly adjusting the name of the .dat files (if updated). This saves a file 'drifterdata_north_atlantic.npz' containing only thos drifters that start in the North Atlantic. Note that this script takes a whil, possible more than an hour. It is not efficiently implemented at the moment.

### Plot figure 1 (drifter info)
Execute 'drifter_data_info.py'

### Clustering (figures 6,7, C1)
Execute 'clustering_north_atlantic_drifters.py' with the option plot_365days =True (figs 6,7 ) and False (fig. C1)

## Double-gyre flow
1. Create trajectories with 'create_autonomous_doble_gyre_trajectories.py' and 'create_non_autonomous_doble_gyre_trajectories'
2. Execute 'clustering_double_gyre.py', uncommenting the respective options for figs. 3, 4 and 5
3. Execute 'sensitivity_double_gyre_incomplete_data.py' for fig. B3 to see the number of incorrectly assigned labels in the incomplete data case
",2022-08-12
https://github.com/OceanParcels/Galapagos_connectivity,"# Galapagos_connectivity
Public repository for the paper on macroplastic connectivity of the Galapagos Islands
",2022-08-12
https://github.com/OceanParcels/kiel_summer_school_tutorials,# kiel_summer_school_tutorials,2022-08-12
https://github.com/OceanParcels/Mediterranean_inverse_modelling,"# Mediterranean_inverse_modelling
Github repository for ""Closing the Mediterranean Marine Floating Plastic Mass Budget: Inverse Modelling of Sources and Sinks"" by Mikael Kaandorp, Henk A. Dijkstra, and Erik van Sebille, in Environmental Science & Technology:
doi.org/10.1021/acs.est.0c01984
",2022-08-12
https://github.com/OceanParcels/Microbes_EAC,"This repository holds the code for the publication ""Live cell analysis at sea reveals divergent thermal performance between photosynthetic ocean microbial eukaryote populations""
McInnes, AS, OF Laczka, KG Baker, ME Larsson, CM Robinson, JS Clark, L Laiolo, M Alvarez, B Laverock, CT Kremer, E van Sebille and MA Doblin (2019), The ISME Journal, 1751, 7370 (https://doi.org/10.1038/s41396-019-0355-6)
",2022-08-12
https://github.com/OceanParcels/near_surface_microplastic,"# Code for ""Influence of near-surface currents on the global dispersal of marine microplastic""
Authors: David Wichmann, Philippe Delandmeter, Erik van Sebille 

This repository contains all code needed for the simulation and analysis for the paper. For questions, please contact d.wichmann@uu.nl.

# OceanParcels versions
For all but the 3D simulation, we used OceanParcels version 1.11 with slight modifications: 
For loading a sub-set of depths, it was necessary to comment lines 173 and 174 of field.py in the parcels folder.

We used OceanParcels version 2.0 for the 3D simulation. See: Delandmeter, P. & van Sebille, E. The Parcels v2.0 Lagrangian framework: new field interpolation schemes. Geosci. Model Dev. Discuss. 1–24 (2019).

# Particle Initial Coordinates
See InitialCoordinates/CreateGrid.py for creating a uniform 2D initial grid of particles.

# Simulation
For all but 3D simulation: Use Simulations/AdvectParticles.py
For 3D siulation: AdvectParticles_3D.py

## Simulation for fixed depths
The following command will put particles in the 13th depth level of the NEMO data. Initial particle locations are split into different sub-sets. The index posidx refers to this index. Here we use the initial grid no 3:

```python AdvectParticles.py -name FileName -y 2000 -m 1 -d 5 -simdays 3650 -posidx 3 -depth 13```

## Simulation for Uniform mixing
The following command will execute a simulation where particles are uniformly replaced along the vertical at each time step between 0 and 120 m.

```python AdvectParticles.py -name FileName -y 2000 -m 1 -d 5 -simdays 3650 -posidx 3 -uniformmixing True```

## Simulation for Kukulka 2012 mixing
Simulation for exponential mixing according to Kukulka 2012. The rise speed is set to 0.003 m/s.

```python AdvectParticles.py -name FileName -y 2000 -m 1 -d 5 -simdays 3650 -posidx 3 -kukulkamixing True -wrise 0.003```

## Simulation for 3D particles

```python AdvectParticles_3D.py -name FileName -y 2000 -m 1 -d 5 -simdays 3650 -posidx 3```

# Analysis
Execute Analysis/figures_paper.py to create all figures and tables for the main document and the supplementary material (adjust read-in file names according to the simulation output file names).  Analysis/AnaObjects.py contains the objects and region definitions for the data analysis.
",2022-08-12
https://github.com/OceanParcels/NorthSeaBeaching_paper,"# NorthSeaBeaching_paper
Public repository for the paper on the North Sea beaching data analysis
",2022-08-12
https://github.com/OceanParcels/oceanparcels_website,"# Repository for the OceanParcels.org website

This git repository holds the code for the oceanparcels website

## Copyright and License

Content copyright the OceanParcels project, licensed under a CC-BY license.

Layout is based on the [Modern Business](https://blackrockdigital.github.io/startbootstrap-modern-business/) theme, which is copyrighted 2013-2017 by Blackrock Digital LLC and released under the [MIT](https://github.com/BlackrockDigital/startbootstrap-modern-business/blob/gh-pages/LICENSE) license.
",2022-08-12
https://github.com/OceanParcels/parcels,"## Parcels

**Parcels** (**P**robably **A** **R**eally **C**omputationally **E**fficient **L**agrangian **S**imulator) is a set of Python classes and methods to create customisable particle tracking simulations using output from Ocean Circulation models. Parcels can be used to track passive and active particulates such as water, plankton, [plastic](http://www.topios.org/) and [fish](https://github.com/Jacketless/IKAMOANA).

![Arctic-SO-medusaParticles](https://github.com/OceanParcels/oceanparcels_website/blob/master/images/homepage.gif)

*Animation of virtual particles carried by ocean surface flow in the global oceans. The particles are advected with [Parcels](http://oceanparcels.org/) in data from the [NEMO Ocean Model](https://www.nemo-ocean.eu/).*

### Parcels manuscript and code

The manuscript detailing the first release of Parcels, version 0.9, has been published in [Geoscientific Model Development](https://www.geosci-model-dev.net/10/4175/2017/gmd-10-4175-2017.html) and can be cited as 

*Lange, M. and E van Sebille (2017) Parcels v0.9: prototyping a Lagrangian Ocean Analysis framework for the petascale age. Geoscientific Model Development, 10, 4175-4186. https://doi.org/10.5194/gmd-2017-167*

The manuscript detailing version 2.0 of Parcels is available at [Geoscientific Model Development](https://www.geosci-model-dev.net/12/3571/2019/gmd-12-3571-2019-discussion.html) and can be cited as:

*Delandmeter P. and E van Sebille (2019) The Parcels v2.0 Lagrangian framework: new field interpolation schemes. Geoscientific Model Development, 12, 3571-3584. https://doi.org/10.5194/gmd-12-3571-2019*

### Further information

See [oceanparcels.org](http://oceanparcels.org/) for further information about [installing](http://oceanparcels.org/#installing) and [running](http://oceanparcels.org/#tutorials) the Parcels code, as well as extended [documentation](http://oceanparcels.org/gh-pages/html/) of the methods and classes.


### Launch Parcels Tutorials on mybinder.org

[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/OceanParcels/parcels_examples_binder/master?urlpath=lab/tree/parcels_examples/parcels_tutorial.ipynb)
[![Build Status](https://github.com/oceanparcels/parcels/workflows/test-suite/badge.svg?branch=master)](https://github.com/OceanParcels/parcels/actions?query=workflow%3Atest-suite+branch%3Amaster)
[![Anaconda-release](https://anaconda.org/conda-forge/parcels/badges/version.svg)](https://anaconda.org/conda-forge/parcels/)
[![Anaconda-date](https://anaconda.org/conda-forge/parcels/badges/latest_release_date.svg)](https://anaconda.org/conda-forge/parcels/)
[![Zenodo](https://zenodo.org/badge/DOI/10.5281/zenodo.823561.svg)](https://doi.org/10.5281/zenodo.823561)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5353/badge)](https://bestpractices.coreinfrastructure.org/projects/5353)
",2022-08-12
https://github.com/OceanParcels/ParcelsBenchmark_SGEscrips,"# Parcels Benchmark - SGE scripts
The repository holds the Bourne-shell (bash) scripts for starting up the benchmark simulations with Parcels on a Sungrid SGE cluster (e.g. UU-science's GEMINI cluster).

**_IMPORTANT_**: please adapt the shell-scripts (*.sh-files) about their paths and the responding mail address according to you own cluster setup - just cloning the repository and running the job is going to fail for your cluster without adapting those configurations!
",2022-08-12
https://github.com/OceanParcels/ParcelsBenchmark_SLURMscripts,"# Parcels Benchmark - SLURM scripts
The repository holds the Bourne-shell (bash) scripts for starting up the benchmark simulations with Parcels on a SLURM cluster (e.g. SURFsara's CARTESIUS cluster)

**_IMPORTANT_**: please adapt the shell-scripts (*.sh-files) about their paths and the responding mail address according to you own cluster setup - just cloning the repository and running the job is going to fail for your cluster without adapting those configurations!
",2022-08-12
https://github.com/OceanParcels/Parcelsv0.9Paper_Scripts,"This repository holds the code for the publication ""Parcels v0.9: prototyping a Lagrangian Ocean Analysis framework for the petascale age""
Lange, M and E van Sebille (2017), Geoscientific Model Development, 10, 4175-4186 (https://doi.org/10.5194/gmd-10-4175-2017)
",2022-08-12
https://github.com/OceanParcels/Parcelsv2.0PaperNorthSeaScripts,"
This repository contains the files necessary to run and postprocess the simulations studying the distribution of
floating microplastic in the North West European continental shelf and its sensitivity to different physical processes, which is discussed in *The Parcels v2.0 Lagrangian framework: new field interpolation schemes*, by Philippe Delandmeter and Erik van Sebille, submitted to *Geoscientific Model Development*.

The master file is `northsea_mp.py` that calls, using the different parameters, the simulation code `run_northsea_mp.py`. The kernels defining the particle dynamics are implemented in `northsea_mp_kernels.py`.

All postprocessing scripts and useful functions are stored in the `postprocess` directory.
",2022-08-12
https://github.com/OceanParcels/parcels_cgrid_interpolation_schemes,"
This repository contains a minimalist version of C-grid interpolation schemes for:
* a general quadrilateral (2D)
* a hexahedron resulting from a vertical quadrilateral extrusion with planar faces (3D)

Two master files: `2d_uniform_velocity.py` and `3d_uniform_velocity.py` compute the velocity at a point inside the cell, depending on the cell vertices, the point relative location and the velocities at the cell edges (2D) or faces (3D).

In the current version, the velocities at the cell edges/faces correspond to a uniform velocity and the code asserts that this uniform velocity is correctly interpolated.

The code reproduces the interpolation schemes for C-grids in Parcels v2.0.0 (http://oceanparcels.org).
",2022-08-12
https://github.com/OceanParcels/parcels_contributions,"# Parcels Contributions Repository
Repository where users can contribute Kernels and code snippets

## What can you do in this repo.?
* Browse frequent questions, if you have a specific question, to see if an answer exists: https://github.com/OceanParcels/parcels_contributions/wiki/Frequently-Asked-Questions-(FAQ)
* Browse available code snippets from experienced Parcels users (and group members)
* Provide a code snipped (via PR) - either if you have a generally-applicable short code people frequently reuse, or you you want to provide a snippet to a still-open question
* Provide a question (if not already existent) via the Parcels main issue tracker: https://github.com/OceanParcels/parcels/issues

## How to add a kernel or code snippet?
Please do so via a pull request.

## Dependencies

### General

* Numerical libraries: numpy, scipy 
* Libraries for accessing NetCDF and HDF5 data: netcdf4, xarray, h5py 
* Libraries for plotting: matplotlib, cartopy

### Kernels and Examples

Those scripts represent either novel application kernels or more extensive example scripts for Parcels simulations, and thus require [parcels](https://github.com/OceanParcels/parcels) as a package.

* Libraries for simulations: parcels (on conda-forge)


### VTK transform script

For running the 'transform_to_vtk.py' scripts in the _Snippets_ folder, you need to get the VTK library (also available via conda):

* visualisation library: vtk (on anaconda and conda-forge)
",2022-08-12
https://github.com/OceanParcels/parcels_efficiency,"# parcels_efficiency
This repository contains various tests to evaluate Parcels efficiency and scalability
",2022-08-12
https://github.com/OceanParcels/parcels_gallery,"# parcels_gallery
The Gallery repository of Parcels, including scripts and example renderings to visualise Lagrangian ocean flow.

## To see the published plot gallery website, see: [https://oceanparcels.org/#gallery](https://oceanparcels.org/#gallery)

## How to add a picture to the Gallery

 - clone the repo : git clone https://github.com/OceanParcels/parcels_gallery.git
 - git branch <my_plot>
 - git checkout <my_plot>
 - cp yourplot.png images/
 - cp yourplot.ipynb scripts_ipynb/
 (- cp yourplot.py scripts_py/)
 - git add *
 - git commit -m ""<your_short_plot_name> plot for the gallery""
 - git push origin <my_plot>
 - got to https://github.com/OceanParcels/parcels_gallery and click on pull request to add your pull request (appears at top of page)
 - wait for your request to be merged

## 
To have the table in the jupyter-notebook template showing the version of the python packages: <br>
conda install -c conda-forge version_information

# Binder address

https://mybinder.org/v2/gh/OceanParcels/parcels_gallery/HEAD

(https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/OceanParcels/parcels_gallery/HEAD)
",2022-08-12
https://github.com/OceanParcels/parcels_gallery_binder,"# parcels_gallery_binder
Launch Parcels Gallery on mybinder.org
",2022-08-12
https://github.com/OceanParcels/parcels_pubic_animation,"# parcels_pubic_animation
Animations of parcels simulations, collected for public access and visibility
",2022-08-12
https://github.com/OceanParcels/Plastic_CyprusBeaches,"This repository holds the code used in the publication ""The true depth of the Mediterranean plastic problem: Extreme microplastic pollution on marine turtle nesting beaches in Cyprus""
Duncan, EM, J Arrowsmith, C Bain, AC Broderick, J Lee, K Metcalfe, SK Pikesley, RTE Snape, E van Sebille and BJ Godley (2018), Marine Pollution Bulletin, 136, 334-340 (https://doi.org/10.1016/j.marpolbul.2018.09.019)
",2022-08-12
https://github.com/OceanParcels/PumiceEvent,"# PumiceEvent
Repository for the codes of the paper

Jutzeler, M, R Marsh, E van Sebille, T Mittal, RJ Carey, KE Fauria, M Manga, and J McPhie (2020) Ongoing Dispersal of the August 7, 2019 Pumice Raft from the Tonga Arc in the Southwestern Pacific Ocean. _Geophysical Research Letters_, _47_, e1701121. [doi:10.1029/2019GL086768](https://doi.org/10.1029/2019GL086768).
",2022-08-12
https://github.com/OceanParcels/SKIM-garbagepatchlocations,"In this repository, you can find all the code used to generate and analyse the results shown in the paper ""The role of Ekman currents, geostrophy and Stokes drift in the accumulation of floating microplastic"". The layout is as follows:
Data Sets: Code used to download the GlobCurrent & WaveWatch III datasets, compute daily mean fields for the Stokes drift and Wind, along with code to generate the artificial boundary current field used to prevent particle beaching
Figures: Contains code to generate the figures that show results from both the Atlantic and Pacific basins.
Global: Contains code used to carry out the global Lagrangian simulations and analyse the results.
North Atlantic: Contains code used to carry out the North Atlantic Lagrangian simulations and analyse the results.
North Pacific: Contains code used to carry out the North Pacific Lagrangian simulations and analyse the results.

For any questions, please contact Victor Onink at onink@climate.unibe.ch.
",2022-08-12
https://github.com/OceanParcels/SKIM_dFADtracking,"This repository holds the code for the publication ""Regional connectivity and spatial densities of drifting fish aggregating devices, simulated from fishing events in the Western and Central Pacific Ocean"", Scutt Phillips J, L Escalle, G Pilling, A Sen Gupta, E van Sebille (2019), Environmental Research Communications, in press (https://doi.org/10.1088/2515-7620/ab21e9)
",2022-08-12
https://github.com/OceanParcels/SouthAmerica_Epibionts,"# SouthAmerica_Epibionts
Public repository for model - observation comparison along west coast of South America
",2022-08-12
https://github.com/OceanParcels/StructureTutorial,"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/OceanParcels/StructureTutorial/HEAD?labpath=tutorial_parcels_structure.ipynb)
.. image:: https://mybinder.org/badge_logo.svg
 :target: https://mybinder.org/v2/gh/OceanParcels/StructureTutorial/HEAD?labpath=tutorial_parcels_structure.ipynb
",2022-08-12
https://github.com/OceanParcels/surface_mixing,"# Code for simulation and data analysis of ""Mixing of passive tracers at the ocean surface and implications for plastic transport modelling""
Authors: D. Wichmann, P. Delandmeter, H. A. Dijkstra and E. van Sebille

This repository contains all code needed for the simulation and analysis for the paper. For questions, please contact d.wichmann@uu.nl.

# OceanParcels version
We used OceanParcels version 1.10 with slight modifications: 

# Simulation
Use Simulations/AdvectParticles.py. The grid file has to be created beforehand, containing the initial longitudes and latitudes of 0.2 degree (entropy) and 0.1 degree (Markov chain) particles.

## Simulation for fixed depths
The following command is used to advect particles for 60 days for the transition matrix calculation. 'pos' is the index of initial particle grids.

python AdvectParticles.py -name testrun -y 2001 -m 1 -d 1 -simdays 60 -pos 0


# Analysis

## Creation of matrices
Execute the functions in Analysis/create_matrix.py

## Creation of figures
Execute functions in Analysis/figures_paper.py
",2022-08-12
https://github.com/OceanParcels/Tides_GlobalOceanPlastic,"This repository contains the scripts used to generate and analyse the results discussed in the paper ""Influence of tidal currents on transport and accumulation of floating microplastics in the global open ocean"", by Miriam Sterl, Philippe Delandmeter and Erik van Sebille.

The directory *Tracking* contains all the scripts used to carry out the simulations. The other directories contain the scripts used to analyse the results.
",2022-08-12
https://github.com/OceanParcels/TOPIOS,"# Repository for TOPIOS website

This git repository holds the code for the TOPIOS (Tracking of Plastic In Our Seas) website

## Copyright and License

Content copyright the TOPIOS project, licensed under a CC-BY license.

Layout is based on the [Modern Business](https://blackrockdigital.github.io/startbootstrap-modern-business/) theme, which is copyrighted 2013-2017 by Blackrock Digital LLC and released under the [MIT](https://github.com/BlackrockDigital/startbootstrap-modern-business/blob/gh-pages/LICENSE) license.
",2022-08-12
https://github.com/OceanParcels/transition_matrices_plasticadrift,"This repository holds code for creating transition matrices, as described in e.g. [Van Sebille 2014](http://dx.doi.org/10.1016/j.jembe.2014.09.002). See the [PDF of the thesis](https://github.com/OceanParcels/transition_matrices_plasticadrift/blob/master/Simulating%20pathways%20and%20beaching%20effects%20of%20plastic%20originating%20from%20the%20Dutch%20coast.pdf) for more details. 

Createrawmatrix.py is used for creating the crossing matrices, frac_coast_model.py for creating the beaching model used, createtransitmatrix.py for converting the crossing matrices to transit matrices and global_fwd_csv(_Netherlands).py for using these transit matrices for computing dispersion in time.

The methodology and use of these files is explained in the Method section of the accompanying paper. 
Adj_coast_model.py is not used to produce results in this thesis, but is an example of a different beaching model as explained in the Discussion section of the [thesis](https://github.com/OceanParcels/transition_matrices_plasticadrift/blob/master/Simulating%20pathways%20and%20beaching%20effects%20of%20plastic%20originating%20from%20the%20Dutch%20coast.pdf).

All initial values are set in createrawmatrix.py and consequently called in the other files, so e.g. changing the model resolution should be done by setting it in createrawmatrix.py.

The input data used for these files and computed results discussed in the paper can be found here: https://doi.org/10.5281/zenodo.1265457
",2022-08-12
https://github.com/OceanParcels/UtrechtTeam,"This repository holds information specifically useful to the Utrecht team of TOPIOS and OceanParcels.

It has the following sections (most on the Wiki page)
1. [Research Data Management Plan for OceanParcels](https://github.com/OceanParcels/UtrechtTeam/wiki/Research-Data-Management-Plan-for-OceanParcels)
2. [How to run parcels on lorenz, gemini and cartesius](https://github.com/OceanParcels/UtrechtTeam/wiki/How-to-run-parcels-on-lorenz,-gemini-and-cartesius)
3. [How to run long simulations ( 24 hours) with qsub](https://github.com/OceanParcels/UtrechtTeam/wiki/How-to-run-long-simulations-(-24-hours)-with-qsub)
4. [How to archive data on the UU YODA service](https://github.com/IMAU-oceans/data_management/blob/master/yoda.md)
5. Running into speed or memory problems on gemini: using Dask chunking
    - [Simulations](https://github.com/OceanParcels/UtrechtTeam/blob/master/scripts/time_chunking.py)
    - [Performance analysis](https://github.com/OceanParcels/UtrechtTeam/blob/master/scripts/Dask_vs_indices.ipynb)

",2022-08-12
https://github.com/oktaal/museomix-doudoit,"# Museomix Mons, 2018: Dou do it!

The idea was to have an interactive guide allowing people to experience the annual Doudou event themselves at any moment. This is done by retracing the same route as the Doudou event and showing information and videos at the points of interests along the way. The experience also includes a mini-game.

Created by the Team 'Teamporary' at the [Museomix event](http://museomix.be/) in Mons, Belgium (November 9-11, 2018).

This project was generated with [Angular CLI](https://github.com/angular/angular-cli) version 7.0.5.

## Development server

Run `ng serve` for a dev server. Navigate to `http://localhost:4200/`. The app will automatically reload if you change any of the source files.

## Code scaffolding

Run `ng generate component component-name` to generate a new component. You can also use `ng generate directive|pipe|service|class|guard|interface|enum|module`.

## Build

Run `ng build` to build the project. The build artifacts will be stored in the `dist/` directory. Use the `--prod` flag for a production build.

## Running unit tests

Run `ng test` to execute the unit tests via [Karma](https://karma-runner.github.io).

## Running end-to-end tests

Run `ng e2e` to execute the end-to-end tests via [Protractor](http://www.protractortest.org/).

## Further help

To get more help on the Angular CLI use `ng help` or go check out the [Angular CLI README](https://github.com/angular/angular-cli/blob/master/README.md).
",2022-08-12
https://github.com/outfrenk/Dutch_GIC,"# GIC
Calculation of geomagnetically induced currents in the Dutch powergrid. Details about the calculations can be found inside the code. The 'ods' and 'csv' files in the network folder contain the information about the Dutch (+German/Belgian) powergrid.

## Install
1. Clone the git repository

    ```bash
    > git clone https://github.com/outfrenk/Dutch_GIC
    ```
    
2. Go to the (new) Dutch_GIC directory

    ```bash
    > cd Dutch_GIC
    ```

3. Install the package

    ```bash
    > pip install .
    ```

## Data
The raw data used during research can also be found in the data folder

## Example
An example of how to run the code (dutchgic.py) can be found in the 'usage class GIC' jupyter notebook or for a complete jupyter notebook in 'Class_GIC_notebook.ipynb' in the notebooks folder. The logbook.log in the logbook folder contains details about running the code. 

* To import the code, use the command: 

    ```bash
    > from Dutch_GIC.dutchgic import GIC
    ```

## Testing
You can test the code using pytest as well. 

1. However, change the string in test_code.py in the tests folder (in function test_init()) to the location where you store your csv files (spreadsheetcables.csv and spreadsheettrafo.csv) before running.

2. Then run

    ```bash
    > pytest
    ```


## Dependencies external
This class depends on a couple of packages:
- numpy
- re
- os
- pandas
- logging
- multiprocessing
- threading
- scipy
- urllib
- datetime
- matplotlib
- pySECS (instructions and download at https://github.com/greglucas/pySECS)

## Dependencies internal
The functions all have a dependency on each other; some can be run alone, others need to be run with the function who called them. These dependent functions are indicated by @. If the function only push data towards a subfunction &rarr; is used. When a function pushes data towards a subfunction, but this subfunction pushes data back as well &harr; is used.

If we would run the function *runall()*, it would run the following functions:
1. *standard_download()*

   a.&rarr; *download_data()* 
   
   b.&harr; *find_quiet_data()*
   
2. *iteratestation()*

   a.&rarr; *newplotspace()*
   
3. *magnetic_interpolation()*

   a. &rarr;*magnetic_time()*@
   
      aa.&harr; *mag_interpolate()*@ NEEDS pySECS package here!
      
4. *BtoE()*

   a.&harr; *check_sampling()*
   
   b.&harr; *Parzen()*
   
   c.&harr; *filt()*
   
   d.&harr; *transferfunction()*
   
   e.&rarr; *writing_electric()*@
   
5. *calculate_GIC()*

   a.&harr; *check_sampling()*
   
   b.&rarr; *GICfunction()*@
   
      ba.&harr; *ObtainJ()*@ &harr; *calcE()*@
         
6. *plot_GIC()*
 
   a.&harr; *check_sampling()*
    
7. *make_video()*
 

### *GIC_index()*
There is also a GIC_index function (developed by Marshall et al., 2011). This function needs magnetic values from *magnetic_interpolation()* to work. It uses *check_sampling()* in the process.

### Questions about the code?
You can send me a message to my emailadress
",2022-08-12
https://github.com/PABalland/EconGeo,"EconGeo

""EconGeo"" is the first R package to propose user-friendly functions to compute a series of indices commonly used in Economic Geography and Economic Complexity
",2022-08-12
https://github.com/PABalland/ON,# ON,2022-08-12
https://github.com/patrickfinke/mecs,"# mecs

**`mecs` is an implementation of the Entity Component System (ECS) paradigm for Python 3, with a focus on interface minimalism and performance.**

Inspired by [Esper](https://github.com/benmoran56/esper) and Sean Fisk's [ecs](https://github.com/seanfisk/ecs).

### Contents
 - [Changelog](#changelog)
 - [Installation](#installation)
 - [About the ECS paradigm](#ecs-paradigm)
 - [Using `mecs` in your project](#mecs-usage)
    - [Managing entities](#mecs-entities)
    - [Implementing and managing components](#mecs-componentes)
    - [Implementing and running systems](#mecs-systems)
    - [A basic update loop](#mecs-loop)

<a name=""changelog""/>

## Changelog

For a full list of changes see [CHANGELOG.md](CHANGELOG.md).

- v1.2.1 - Improve performance

- **v1.2.0 - Add support for manipulating multiple components at once**

  The methods `scene.new()`, `scene.set()`, `scene.has()`, and `scene.remove()` (where `set()` replaces `add()`) now support multiple components/component types. The appropriate methods have also been modified in the `CommandBuffer`. `scene.get()` now has a conterpart `scene.collect()` which supports multiple component types. Minor changes include better exception messages and `scene.buffer()` being deprecated in favour of `CommandBuffer(scene)`.

- **v1.1.0 - Add command buffer**

  When using `scene.select()`, manipulation of entities can now be recorded using the `CommandBuffer` instance returned by `scene.buffer()`, and played back at a later time. This avoids unexpected behavior that would occur when using the scene instance directly.

- **v1.0.0 - First release**

  The base functionality is implemented. Note that at this stage it is not safe to add or remove components while iterating over their entities. This will be fixed in a future release.

<a name=""installation""/>

## Installation

`mecs` is implemented in a single file with no dependencies. Simply copy `mecs.py` to your project folder and `import mecs`.

<a name=""ecs-paradigm""/>

## About the ECS paradigm

The Entity Component System (ECS) paradigm consists of three different concepts, namely **entities**, **components** and **systems**. These should be understood as follows:

1. **Entities** are unique identifiers, labeling a set of components as belonging to a logical group.
2. **Components** are plain data and implement no logic. They define the behavior of entities.
3. **Systems** are logic that operates on entities and their components. They enforce the appropriate behavior of entities with certain component sets and are also able to change their behavior by adding, removing and mutating components.

For more information about the ECS paradigm, visit the [Wikipedia article](https://en.wikipedia.org/wiki/Entity_component_system) or Sander Mertens' [ecs-faq](https://github.com/SanderMertens/ecs-faq).

<a name=""mecs-usage""/>

## Using `mecs` in your project
For the management of entities, components and systems, `mecs` provides the `Scene` class. Only entities within the same scene may interact with one another. You can create a new scene with

```python
scene = Scene()
```

<a name=""mecs-entities""/>

### Managing entities

Entities are nothing more than unique (integer) identifiers. To get hold of a previously unused **entity id** use

```python
eid = scene.new()
```

<a name=""mecs-componentes""/>

### Implementing and managing components

Components can be instances of any class and `mecs` does not provide a base class for them. For example a `Position` component containing `x` and `y` coordinates could look like this:

```python
class Position():
  def __init__(self, x, y):
    self.x, self.y = x, y
```

Other examples would be a similar `Velocity` component or a `Renderable` component:

```python
class Velocity():
  def __init__(self, vx, vy):
    self.vx, self.vy = vx, vy

class Renderable():
  def __init__(self, textureId):
    self.textureId = textureId
```

Components are distinguished by their **component type**. To get the type of a component use the build-in `type()`:

```python
position = Position(15, 8)
type(position)
# => <class '__main__.Position'>

velocity = Velocity(8, 15)
type(velocity)
# => <class '__main__.Velocity'>
```

\
\
The `Scene` class provides the following methods for interacting with entities and components. Note that the entity id used in these methods must be valid, i.e. must be returned from `scene.new()`. Using an invalid entity id results in a `KeyError`.

#### 1. Setting components while allocating a new entity id using `scene.new(*comps)`.

Returns a valid entity id to be used in other methods. It is also possible to directly set components of the new entity by supplying them to this method.

```python
 # create a new empty entity
eid = scene.new()
# => 0

# create a new entity with a Position, Velocity and Renderable component
anotherEid = scene.new(Position(15, 8), Velocity(8, 15), Renderable(7))
# => 1
```

#### 2. Setting components using `scene.set(eid, *comps)`.

Set components of an entity, which can either result in adding a component or overwriting an existing component. Note that an entity is only allowed to have one component of each type.

```python
# Add a new Position component
scene.set(eid, Position(1, 2))

# Overwrite the Position component
scene.set(eid, Position(3, 4))

# Add a Velocity component and overwrite the Position component again
scene.set(eid, Velocity(0, -5), Position(5, 6))
```

Setting components of the same type in a single call to `set()` is illegal and results in a `ValueError`. Note that the same component instance can be added to multiple entities, making them share the component data.

#### 3. Check if components are part of an entity using `scene.has(eid, *comptypes)`.

This method returns `True` if the entity does have components of all the specified types, `False` otherwise.

```python
# check for Position component (the entity has one)
scene.has(eid, Position)
# => True

# check for Position and Velocity (the entity has both)
scene.has(eid, Position, Velocity)
# => True

# check for Position and Renderable (the entity has a Position but is lacking the Renderable component)
scene.has(eid, Position, Renderable)
# => False
```

#### 4. Getting single components using `scene.get(eid, comptype)`.

Returns the entities component of the specified type, allowing the view or edit the component data.

```python
# move the entity by 10 units on the x-axis
position = scene.get(eid, Position)
position.x += 10

# stop the entity by setting its velocity to zero
velocity = scene.get(eid, Velocity)
velocity.vx, velocity.vy = 0, 0
```

Raises `ValueError` if the entity is missing a component of the specified type.

#### 5. Getting multiple components at once using `scene.collect(eid, *comptypes)`.

Returns a list of the entities components of the specified types.

```python
# repeat the example from above
position, velocity = scene.collect(eid, Position, Velocity)
position.x += 10
velocity.vx, velocity.vy = 0, 0
```

Raises `ValueError` if the entity is missing one or more components of the specified types.

#### 6. Removing components using `scene.remove(eid, *comptype)`.

Removes the components of the entity that are of the specified types.

```python
# remove the Position and the Velocity component
scene.remove(eid, Position, Velocity)
```

Raises `ValueError` if the entity is missing one or more components of the specified types.

#### 7. Removing all components from an entity using `scene.free(eid)`.

Removes all components of the entity.

```python
scene.set(eid, Position(0, 0))
scene.set(eid, Velocity(0, 0))

scene.free(eid)

scene.has(eid, Position) or scene.has(eid, Velocity) or scene.has(eid, Renderable)
# => False
```

Note that this does not make the entity id invalid. In fact, there is no way to invalidate a once valid id. In particular, there is no method to check if an entity is still 'alive'. If you need such behavior, consider attaching an `Alive` component (that has no further data) to every entity that needs it and use `scene.has(eid, Alive)` to determine if the entity is alive.

#### 8. Viewing the archetype of an entity and all of its components using `scene.archetype(eid)` and `scene.components(eid)`.

The **archetype** of an entity is the tuple of all component types that are attached to it.

```python
scene.set(eid, Position(32, 64))
scene.set(eid, Velocity(8, 16))
scene.archetype(eid)
# => (<class '__main__.Position'>, <class '__main__.Velocity'>)
scene.components(eid)
# => (<__main__.Position object at 0x000001EF0358D370>, <__main__.Velocity object at 0x000001EF035B47C0>)
```

The result of `scene.archetype(eid)` is sorted, so comparisons of the form `scene.archetype(eid1) == scene.archetype(eid2)` are safe, but hardly necessary.

#### 9. Iterating over entities and components using `scene.select(*comptypes, exclude=None)`.

The result of this method is a generator object yielding tuples of the form `(eid, (compA, compB, ...))` where `compA`, `compB` belong to the entity with entity id `eid` and have the requested types. Optionally, an iterable (such as a list or tuple) may be passed to the `exclude` argument, in which case all entities having one or more component types listed in `exclude` will not be yielded by the method.

```python
# adjust positions based on velocity
dt = current_deltatime()
for eid, (pos, vel) in scene.select(Position, Velocity):
  pos.x += vel.vx * dt
  pos.y += vel.vy * dt
```

Iterating over entities that have a certain set of components is one of the most important tasks in the ECS paradigm. Usually, this is done by systems to efficiently apply their logic to the appropriate entities. For more examples, see the section about systems.

#### 10. Staying save using the `CommandBuffer`.

Methods such as `scene.new()`, `scene.set()`, `scene.remove()`, or `scene.free()` alter the structure of the underlying database of the scene. This makes them *not save* to use while iterating over the result of `scene.select()`. Using them in this context *will not* raise any exceptions, but will often lead to unexpected behaviour.

To resolve this issue, `mecs` provides the `CommandBuffer` class, which implements `CommandBuffer.new(*comps)`, `CommandBuffer.set(eid, *comps)`, `CommandBuffer.remove(eid, *comptypes)`, and `CommandBuffer.free(eid)`. Any calls to these methods will be recorded, and when it is save to do so, can be played back using `CommandBuffer.flush()`. Alternatively, the command buffer can be used as a context manager, which is strongly recommended.

```python
# remove all entities from the scene that are not withing the screen bounds
with CommandBuffer(scene) as buffer:
  for eid, (pos,) in scene.select(Position):
   if pos.x < 0 or pos.x > screen_width or pos.y < 0 or pos.y > screen_height:
     buffer.free(eid)
```

<a name=""mecs-systems""/>

### Implementing and running systems

As with components, `mecs` does not provide a base class for systems. Instead, a system should implement any of the three callback methods (`onStart()`, `onUpdate()`, and `onStop()`) and must be passed to the corresponding method of the `Scene` class.

#### 1. Initializing a scene using `scene.start(*systems, **kwargs)`.

Any instance of any class that implements a method with the signature `onStart(scene, **kwargs)` may be used as an input to this method.

The scene iterates through all systems in the order they are passed and calls their respective `onStart()` methods, passing itself using the `scene` argument. Additionally, any `kwargs` will also be passed on.

```python
class RenderSystem():
  def onStart(self, scene, resolution=(600, 480), **kwargs):
    self.graphics = init_graphics_devices(resolution)
    self.textures = load_textures(""./resources/textures"")

renderSystem = RenderSystem()
startSystems = [renderSystem, AnotherInitSystem()]
scene.start(*startSystems, resolution=(1280, 720))
```

This method should *not* be called multiple times. Instead, all necessary systems should be instantiated first, followed by a single call to `scene.start()`.

#### 2. Updating a scene using `scene.update(*systems, **kwargs)`.

Any instance of any class that implements a method with the signature `onUpdate(scene, **kwargs)` may be used as an input to this method.

The scene iterates through all systems in the order they are passed and calls their respective `onUpdate()` methods, passing itself using the `scene` argument. Additionally, any `kwargs` will also be passed on.

```python
class RenderSystem():
  def onUpdate(self, scene, **kwargs):
    for eid, (pos, rend) in scene.select(Position, Renderable):
      texture = self.textures[rend.textureId]
      self.graphics.render(pos.x, pos.y, texture))

class MovementSystem():
  def onUpdate(self, scene, dt=1, **kwargs):
    for eid, (pos, vel) in scene.select(Position, Velocity):
      pos.x += vel.vx * dt
      pos.y += vel.vy * dt

updateSystems = [MovementSystem(), renderSystem]
scene.update(*updateSystems, dt=current_deltatime())
```

To avoid any unnecessary overhead, call this method only once per update circle, passing all necessary systems as arguments.

#### 3. Cleaning up a scene using `scene.stop(*systems, **kwargs)`.

Any instance of any class that implements a method with the signature `onStop(scene, **kwargs)` may be used as an input to this method.

The scene iterates through all systems in the order they are passed and calls their respective `onStop()` methods, passing itself using the `scene` argument. Additionally, any `kwargs` will also be passed on.

```python
class RenderSystem():
  def onStop(self, scene, **kwargs):
    stop_graphics_devices(self.graphics)
    unload_textures(self.textures)

stopSystems = [renderSystem, AnotherDestroySystem()]
scene.stop(*stopSystems)
```

As with `scene.start()` this method should *not* be called multiple times, but instead once with all the necessary systems.

<a name=""mecs-loop""/>

### A basic update loop

When trying to write the main loop of your program you may use this pattern.

```python
# Your system instances go here.
systems = []

startSystems = [s for s in systems if hasattr(s, 'onStart')]
updateSystems = [s for s in systems if hasattr(s, 'onUpdate')]
stopSystems = [s for s in systems if hasattr(s, 'onStop')]

print(""[Press Ctrl+C to stop]"")
try:
  scene = Scene()
  scene.start(*startSystems)
  while True:
    deltaTime = current_deltatime()
    scene.update(*updateSystems, dt=deltaTime)
except KeyboardInterrupt:
  pass
finally:
  scene.stop(*stopSystems)
```
",2022-08-12
https://github.com/patrickfinke/simple-mud,"# simple-mud
A simple mud written in Python 3.
",2022-08-12
https://github.com/pcklink/Macaque_Template_Warps,"# Macaque Template Warps
Computation of non-linear warps between 5 commonly used macaque MRI templates.

![image](warp_pairs.png)

For human MRI, the [MNI template](http://www.bic.mni.mcgill.ca/ServicesAtlases/ICBM152NLin2009) serves as the community standard volumetric template. Its integration into most major software packages makes it easy for researchers to register their results to MNI space. This facilitates data-sharing, cross-study comparisons and metanalyses. Most human brain atlases (parcellations) are also provided in MNI space. For surface-based analysis, the [FsAverage](https://surfer.nmr.mgh.harvard.edu/fswiki/FsAverage) (freesurfer average) template serves the same purpose.

For macaque MRI, multiple templates are available (see table below for a non-exhaustive list), with no single one of them adopted as the go-to community standard.

| Template | Species | Resolution (mm<sup>3</sup>) | With atlas | Volume format | Surface format | Links |
| --- | --- | --- | --- | --- | --- | --- |
| NMT | _M. mulatta_ | 0.25 | Saleem Logothetis (D99-SL) | NIFTI | GIFTI | [reference](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5660669/) [download](https://github.com/jms290/NMT) |
| D99 | _M. mulatta_ | 0.25 | D99-SL | NIFTI | GIFTI | [reference](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6075609/) [download](https://afni.nimh.nih.gov/Macaque) |
| INIA19 | _M. mulatta_ | 0.50 | Neuromaps | NIFTI | N/A | [reference](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3515865/) [download](https://www.nitrc.org/projects/inia19/https://www.nitrc.org/projects/inia19/) |
| MNI macaque | _M. fascicularis_ & _M. mulatta_ | 0.25 | Paxinos | MINC & NIFTI | N/A | [reference](https://www.ncbi.nlm.nih.gov/pubmed/21256229) [download](http://www.bic.mni.mcgill.ca/ServicesAtlases/Macaque) |
| Yerkes19 | _M. mulatta_ | 0.50 | F99 | NIFTI & MGZ | GIFTI & MGZ | [reference1](https://www.pnas.org/content/115/22/E5183) [reference2](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3500860/) [download1](https://balsa.wustl.edu/reference/show/976nz) [download2](https://github.com/Washington-University/NHPPipelines) |


I used [ANTs](http://stnava.github.io/ANTs/) to compute transformation warps between the volumetric spaces of the 5 above macaque MRI templates. [See how macaque template warps were computed](macaque_template_warps.ipynb). These warps can be used to transform images (parcellations, statistical maps etc.) between various template spaces. [See how to use macaque template warps](how_to_apply_template_warps.ipynb)

",2022-08-12
https://github.com/pcraster/emis,"# emis
The `emis` service is the API gateway for managing GGHDC requests. It is the single point of contact for clients who wish to use this functionality.

- [Docker Hub repository](https://hub.docker.com/r/pcraster/emis/)
",2022-08-12
https://github.com/pcraster/emis_aggregate_method,"# aggregate_method
The `aggregate_method` service is a REST service managing aggregate method properties.

- [Docker Hub repository](https://hub.docker.com/r/pcraster/emis_aggregate_method/)
",2022-08-12
https://github.com/pcraster/emis_aggregate_query,"# emis_aggregate_query
The `emis_aggregate_query` service is a REST service managing aggregate query requests.

- [Docker Hub repository](https://hub.docker.com/r/pcraster/emis_aggregate_query/)
",2022-08-12
https://github.com/pcraster/emis_base,"# emis_base

Inspired by https://denibertovic.com/posts/handling-permissions-with-docker-volumes/

- [Docker Hub repository](https://hub.docker.com/r/pcraster/emis_base/)
",2022-08-12
https://github.com/pcraster/emis_dataset_manager,"# dataset_manager
The `dataset_manager` service is a RabbitMQ consumer

- [Docker Hub repository](https://hub.docker.com/r/pcraster/emis_dataset_manager/)
",2022-08-12
https://github.com/pcraster/emis_domain,"# emis_domain
The `emis_domain` service is a REST service managing aggregate query domains.

- [Docker Hub repository](https://hub.docker.com/r/pcraster/emis_domain/)
",2022-08-12
https://github.com/pcraster/emis_event_handler,"# event_handler
The `event_handler` service is a RabbitMQ consumer

- [Docker Hub repository](https://hub.docker.com/r/pcraster/emis_event_handler/)
",2022-08-12
https://github.com/pcraster/emis_log,"# log
The `log` service is a REST service managing log records.

- [Docker Hub repository](https://hub.docker.com/r/pcraster/emis_log/)
",2022-08-12
https://github.com/pcraster/emis_monitor,"# monitor
The `monitor` service is a RabbitMQ consumer

- [Docker Hub repository](https://hub.docker.com/r/pcraster/emis_monitor/)
",2022-08-12
https://github.com/pcraster/emis_property,"# property
The `property` service is a REST service managing metadata about the available properties

The service does not know about properties itself, only about medata about properties. A service using the data behind the properties must have access to the dataset(s) containing the properties.

- [Docker Hub repository](https://hub.docker.com/r/pcraster/emis_property/)
",2022-08-12
https://github.com/pcraster/emis_query_executor,"# query_executor
The `query_executor` service is a RabbitMQ consumer

- [Docker Hub repository](https://hub.docker.com/r/pcraster/emis_query_executor/)
",2022-08-12
https://github.com/pcraster/fame,"# fame
Field and Agent-Based Modelling Environment

This repository contains code which is work in progress and not yet intended for widespread usage.
",2022-08-12
https://github.com/pcraster/fwo,"
Repository for the fieldworkonline webapp.

More info to come soon.
",2022-08-12
https://github.com/pcraster/gems,"Readme for Digital Earth project


",2022-08-12
https://github.com/pcraster/gghdc-spatio-temporal-lur-nl,"## Supplementary materials of two related manuscripts:
### LUR tempora coefficients.zip
  Consists of temporal LUR model coefficients, P-values, and adjusted R-squared described in the paper ""Land use regression models revealing spatiotemporal covariation in combustion   related air pollutants in the Netherlands”. Soenario et al. 2019
### gghdc-dev-master.zip/exposure
  Consists of LUR model coefficients described in the paper: ""Activity-based air pollution exposure assessment: differences between homemakers and cycling commuters"". Lu et al. 2019 
",2022-08-12
https://github.com/pcraster/jenkins_base,"# jenkins_base
Docker base image for Jenkins

This image is based on a recent version of the official Jenkins Docker
image. See Dockerfile for details.
",2022-08-12
https://github.com/pcraster/jenkins_docker,"# jenkins_docker
Docker image with support for Docker

This image adds Docker engine and Docker compose to the
pcraster/jenkins_base image. See Dockerfile for details.
",2022-08-12
https://github.com/pcraster/jenkins_pcraster,"# jenkins_pcraster
Docker image containing Jenkins and everything needed to build PCRaster
",2022-08-12
https://github.com/pcraster/maps4society,"# maps4society
Files related to the Maps4Society research project
",2022-08-12
https://github.com/pcraster/paper_2019_physical_data_model,"# 2019_physical_data_model
This repository contains a version of the LUE physical data model as
presented in our 2019 manuscript, as well as example scripts and other
files used in the preparation of that manuscript.

- de Jong, K., Karssenberg, D., A physical data model for spatio-temporal
objects, Environmental Modelling and Software (2019), doi:
https://doi.org/10.1016/j.envsoft.2019.104553.

| directory | contents |
| --------- | -------- |
| `example` | Deer-biomass model referred to from manuscript |
| `lue` | Version of LUE described in manuscript |
| `source` | Scripts used for visualising example-model output |

The most recent LUE source code can be found in LUE's [own
repository](https://github.com/pcraster/lue).

![Deer tracks](deer_tracks.png)


## Build LUE Python package
LUE is currently developed and tested on Linux using GCC-7. All code
should compile and run fine on other platforms too, but this is not
regularly tested.

Here is an example session of building the version of LUE used for our
manuscript and installing the LUE targets in `$HOME/lue_install`:

```bash
cd /tmp
# Recursive is used to also checkout submodules
git clone --recursive https://github.com/pcraster/paper_2019_physical_data_model
cd paper_2019_physical_data_model
mkdir build
cd build
cmake -DCMAKE_INSTALL_PREFIX=$HOME/lue_install ..
cmake --build . --target install
```

The LUE data model source code depends on 3rd party libraries and tools,
that may or may not be installed already on your system. The following
dependencies can usually be installed using your system's package manager.

| package | version used |
| ------- | ------------ |
| libboost-dev | 1.65.1 |
| libgdal-dev | 2.2.3 |
| hdf5-dev | 1.10.0 |

These package names correspond with those used in Debian distributions
and derivatives. Other versions of these packages might also work.

Unless provided by your system's package manager also, these prerequisites
can be installed using [Conan](https://conan.io/):

| package | version used |
| ------- | ------------ |
| fmt | 5.2.1 |
| gsl_microsoft | 2.0.0 |
| jsonformoderncpp | 3.5.0 |
| pybind11 | 2.2.4 |

Other versions of these packages might also work.

To install Conan, some additional Python packages, and the above
prerequisites, [Miniconda](https://docs.conda.io/en/latest/miniconda.html)
(or Conda) can be used:

```bash
conda env create -n test \
    -f ../lue/environment/configuration/conda_environment.yml
conda activate test
conan install ../conanfile.txt
```

Once LUE is installed, some commandline utilities can be found in
`$HOME/lue_install/bin` and the Python package in
`$HOME/lue_install/python`.


# Use LUE Python package
To be able to use the LUE commandline utilities and Python package,
the following environment variables must be set as follows:

```bash
export PATH=$PATH:$HOME/lue_install/bin
export PYTHONPATH=$PYTHONPATH:$HOME/lue_install/python
```

Now these commands should not result in errors:

```bash
lue_validate
python -c ""import lue""
```

Here is an example session of using the LUE Python package. An empty
dataset is created and validated.

Python script:
```python
# create_dataset.py
import lue

dataset = lue.create_dataset(""my_first_dataset.lue"")
```

Shell commands:
```bash
python create_dataset.py
lue_validate my_first_dataset.lue
```


# Run example model
The following commands can be used to run the example model referred to
from the manuscript:

```bash
../example/deer/model/model.py --nr_timesteps=250 --nr_deer=25 deer.lue
```

To visualize the model output, [Blender](https://www.blender.org) can
be used. For information how this has been done when preparing the
manuscript, see `run_model.sh` and `visualize_lue_dataset.sh` in the
example model directory.
",2022-08-12
https://github.com/pcraster/pcraster,"# PCRaster
Environmental modelling software

PCRaster is a collection of tools and software libraries tailored to the construction of spatio-temporal environmental models. Application domains are amongst others hydrology (rainfall-runoff, global water balance, groundwater (with Modflow)), ecology, or land use change. Two scripting languages (Python and PCRcalc) include a rich set of spatial operations for manipulating and analysing raster maps. A Python framework supports Monte Carlo simulations and data assimilation (Ensemble Kalman Filter and Particle Filter). The Aguila tool allows for the interactive visualisation of stochastic spatio-temporal data.

You can find more information about our research and development projects on [our website.](http://computationalgeography.org/) Information on PCRaster is given at the [project website](http://www.pcraster.eu/), and online documentation can be found [here](https://pcraster.geo.uu.nl/pcraster/latest/documentation/index.html). For questions regarding the usage of PCRaster please use our [mailing list](https://lists.geo.uu.nl/mailman/listinfo/pcraster-info), bugs can be reported via our [issue tracker](https://github.com/pcraster/pcraster/issues).


## Installation
[![Anaconda-Server Badge](https://anaconda.org/conda-forge/pcraster/badges/version.svg)](https://anaconda.org/conda-forge/pcraster)
[![Anaconda-Server Badge](https://anaconda.org/conda-forge/pcraster/badges/platforms.svg)](https://anaconda.org/conda-forge/pcraster)
[![Anaconda-Server Badge](https://anaconda.org/conda-forge/pcraster/badges/installer/conda.svg)](https://conda.anaconda.org/conda-forge)

Packages are available for Linux, macOS and Windows via [conda-forge](https://github.com/conda-forge/pcraster-feedstock).
Install PCRaster with:

```bash
conda install -c conda-forge pcraster
```

More information on the installation of PCRaster is given in the [documentation](https://pcraster.geo.uu.nl/pcraster/latest/documentation/pcraster_project/install.html).

## Build status
CI builds of our current development version:

[![Linux build status](https://github.com/pcraster/pcraster/workflows/Linux%20CI/badge.svg)](https://github.com/pcraster/pcraster/actions/workflows/linux.yaml)
[![macOS build status](https://github.com/pcraster/pcraster/workflows/macOS%20CI/badge.svg)](https://github.com/pcraster/pcraster/actions/workflows/macos.yaml)
[![Linux Conda status](https://github.com/pcraster/pcraster/actions/workflows/linux-conda.yaml/badge.svg)](https://github.com/pcraster/pcraster/actions/workflows/linux-conda.yaml)
[![Windows build status](https://github.com/pcraster/pcraster/actions/workflows/windows.yaml/badge.svg)](https://github.com/pcraster/pcraster/actions/workflows/windows.yaml)

",2022-08-12
https://github.com/pcraster/performance_analyst,"# Performance Analyst
The Performance Analyst is a Python package for timing snippets of Python
code. The results can be stored in a database and postprocessed using a
command line tool. This is useful for tracking the performance of Python
software over time. A [tool called pa.py](documentation/pa.md) is supplied
which can be used to query and manage the database contents and to create
plots of the performance results.

The performance is measured by the amount of
[wall clock time](http://en.wikipedia.org/wiki/Wall_clock_time) and
[CPU time](http://en.wikipedia.org/wiki/CPU_time>) in seconds.

The software is modelled after the
[Python unit test library](http://docs.python.org/library/unittest.html#module-unittest),
which makes it easy to use if you are familiar with that. The terminology
is similar to the terms used in the unit test library:

term         | Description
------------ | -----------------------------------------------------------------
timer case   | A method which contains the code that needs to be timed
timer suite  | A class in which related time cases are aggregated
timer loader | Loads timer suites and their associated timer cases
timer runner | Executes all test cases and handles the results, like storing them in a database


Further info:
- [Installation](INSTALL.md)
- [Quick start](documentation/quick_start.md)
- [Generating timer cases](documentation/generating_timer_cases.md)
- [Changes](CHANGES.md)
",2022-08-12
https://github.com/pcraster/rasterformat,"# Rasterformat

[PCRaster](http://www.pcraster.eu)'s native rasterformat
library. This library is also used by the [GDAL PCRaster
driver](https://gdal.org/drivers/raster/pcraster.html).
",2022-08-12
https://github.com/penghaorui/Specfem2D_debug,"# Specfem2D_debug
errors found in Spefem2D
",2022-08-12
https://github.com/penghaorui/test,"# test
 this is a test
",2022-08-12
https://github.com/pimlai/pimlai18,"# The Slate theme

[![Build Status](https://travis-ci.org/pages-themes/slate.svg?branch=master)](https://travis-ci.org/pages-themes/slate) [![Gem Version](https://badge.fury.io/rb/jekyll-theme-slate.svg)](https://badge.fury.io/rb/jekyll-theme-slate)

*Slate is a Jekyll theme for GitHub Pages. You can [preview the theme to see what it looks like](http://pages-themes.github.io/slate), or even [use it today](#usage).*

![Thumbnail of Slate](thumbnail.png)

## Usage

To use the Slate theme:

1. Add the following to your site's `_config.yml`:

    ```yml
    theme: jekyll-theme-slate
    ```

2. Optionally, if you'd like to preview your site on your computer, add the following to your site's `Gemfile`:

    ```ruby
    gem ""github-pages"", group: :jekyll_plugins
    ```

## Customizing

### Configuration variables

Slate will respect the following variables, if set in your site's `_config.yml`:

```yml
title: [The title of your site]
description: [A short description of your site's purpose]
```

Additionally, you may choose to set the following optional variables:

```yml
show_downloads: [""true"" or ""false"" to indicate whether to provide a download URL]
google_analytics: [Your Google Analytics tracking ID]
```

### Stylesheet

If you'd like to add your own custom styles:

1. Create a file called `/assets/css/style.scss` in your site
2. Add the following content to the top of the file, exactly as shown:
    ```scss
    ---
    ---

    @import ""{{ site.theme }}"";
    ```
3. Add any custom CSS (or Sass, including imports) you'd like immediately after the `@import` line

*Note: If you'd like to change the theme's Sass variables, you must set new values before the `@import` line in your stylesheet.*

### Layouts

If you'd like to change the theme's HTML layout:

1. [Copy the original template](https://github.com/pages-themes/slate/blob/master/_layouts/default.html) from the theme's repository<br />(*Pro-tip: click ""raw"" to make copying easier*)
2. Create a file called `/_layouts/default.html` in your site
3. Paste the default layout content copied in the first step
4. Customize the layout as you'd like

### Overriding GitHub-generated URLs

Templates often rely on URLs supplied by GitHub such as links to your repository or links to download your project. If you'd like to override one or more default URLs:

1. Look at [the template source](https://github.com/pages-themes/slate/blob/master/_layouts/default.html) to determine the name of the variable. It will be in the form of `{{ site.github.zip_url }}`.
2. Specify the URL that you'd like the template to use in your site's `_config.yml`. For example, if the variable was `site.github.url`, you'd add the following:
    ```yml
    github:
      zip_url: http://example.com/download.zip
      another_url: another value
    ```
3. When your site is built, Jekyll will use the URL you specified, rather than the default one provided by GitHub.

*Note: You must remove the `site.` prefix, and each variable name (after the `github.`) should be indent with two space below `github:`.*

For more information, see [the Jekyll variables documentation](https://jekyllrb.com/docs/variables/).

## Roadmap

See the [open issues](https://github.com/pages-themes/slate/issues) for a list of proposed features (and known issues).

## Project philosophy

The Slate theme is intended to make it quick and easy for GitHub Pages users to create their first (or 100th) website. The theme should meet the vast majority of users' needs out of the box, erring on the side of simplicity rather than flexibility, and provide users the opportunity to opt-in to additional complexity if they have specific needs or wish to further customize their experience (such as adding custom CSS or modifying the default layout). It should also look great, but that goes without saying.

## Contributing

Interested in contributing to Slate? We'd love your help. Slate is an open source project, built one contribution at a time by users like you. See [the CONTRIBUTING file](docs/CONTRIBUTING.md) for instructions on how to contribute.

### Previewing the theme locally

If you'd like to preview the theme locally (for example, in the process of proposing a change):

1. Clone down the theme's repository (`git clone https://github.com/pages-themes/slate`)
2. `cd` into the theme's directory
3. Run `script/bootstrap` to install the necessary dependencies
4. Run `bundle exec jekyll serve` to start the preview server
5. Visit [`localhost:4000`](http://localhost:4000) in your browser to preview the theme

### Running tests

The theme contains a minimal test suite, to ensure a site with the theme would build successfully. To run the tests, simply run `script/cibuild`. You'll need to run `script/bootstrap` one before the test script will work.
",2022-08-12
https://github.com/plant-plasticity/Evolutionary-flexibility-in-flooding-response-2019,"# Evolutionary-flexibility-in-flooding-response-2019
Code and supplementary files used in analysis for ""Evolutionary flexibility in flooding response circuitry in angiosperms"" (2019)

**Title:** Evolutionary flexibility in flooding response circuitry in angiosperms

**Authors:**  [Mauricio A. Reynoso](https://github.com/reynosoma), [Kaisa Kajala](https://github.com/kaisakajala), [Marko Bajic](https://github.com/DealLab), Donnelly A. West, Germain Pauluzzi, Andrew Yao, Katie Hatch, Kristina  Zumstein, Margaret Woodhouse, [Joel Rodriguez-Medina](https://github.com/rodriguezmDNA), Neelima Sinha, Siobhan M. Brady, [Roger B. Deal](https://github.com/DealLab), [Julia Bailey-Serres](https://github.com/jbserres)

**Journal:** [Science](https://science.sciencemag.org/cgi/doi/10.1126/science.aax8862)

**Abstract:** 

> Flooding due to extreme weather threatens crops and ecosystems. To understand variation in gene regulatory networks activated by submergence,we conducted a high-resolution analysis of chromatin accessibility and gene expression at three scales of transcript control in four angiosperms, ranging from a dryland-adapted wild species to a wetland crop. The data define a cohort of conserved submergence-activated genes with signatures of overlapping cis-regulation by four transcription factor families. Syntenic genes are more highly expressed than nonsyntenic genes, yet both can have the cismotifs and chromatin accessibility associated with submergence up-regulation. Whereas the flexible circuitry spans the eudicot-monocot divide, the frequency of specific cis-motifs, extent of chromatin accessibility, and the degree of submergence activation is more prevalent in thewetland crop andmay have adaptive importance.



## Code used by topic

- [ATAC-seq](https://github.com/plant-plasticity/Evolutionary-flexibility-in-flooding-response-2019/tree/master/ATAC-seq)
- [RNA mapping for rice](https://github.com/plant-plasticity/Evolutionary-flexibility-in-flooding-response-2019/tree/master/Mapping-RNA-seq-Os)
- [Differential Gene Expression by limma-voom](https://github.com/plant-plasticity/Evolutionary-flexibility-in-flooding-response-2019/tree/master/DEG-analysis-limma-voom)
- [Ribotaper for periodicity](https://github.com/plant-plasticity/Evolutionary-flexibility-in-flooding-response-2019/tree/master/Ribotaper-periodicity)
- [Clustering](https://github.com/plant-plasticity/Evolutionary-flexibility-in-flooding-response-2019/blob/master/DEG-analysis-limma-voom/Scripts/Clustering_Heatmap.R)
- [Cross-species comparisons using Phytozome gene families](https://github.com/plant-plasticity/Evolutionary-flexibility-in-flooding-response-2019/tree/master/Gene-families-Phytozome)


## Other resources by topic
- [Genome browser - CoGe viewer of ATAC-seq, nuclear RNA, total RNA, TRAP RNA and Ribo-seq](https://github.com/plant-plasticity/Evolutionary-flexibility-in-flooding-response-2019/tree/master/CoGe)
- [Syntenic relationships for cross-species comparisons](https://github.com/plant-plasticity/Evolutionary-flexibility-in-flooding-response-2019/tree/master/Synteny)
- [Cytoscape files for Figure 4](https://github.com/plant-plasticity/Evolutionary-flexibility-in-flooding-response-2019/tree/master/Cytoscape)

",2022-08-12
https://github.com/plant-plasticity/rice-root-response-atlas,"# Rice-root-response-atlas

Code and supplementary files used in analysis for ""Gene regulatory networks shape developmental plasticity of root cell types under water extremes in rice"" (2022)

**Title:** Gene regulatory networks shape developmental plasticity of root cell types under water extremes in rice

**Authors:**  [Mauricio A. Reynoso](https://github.com/reynosoma),* [Alex Borowsky](https://github.com/alexborowsky),* Germain Pauluzzi*, Elaine Yeung, [Jianhai Zhang](https://github.com/jianhaizhang), Elide Formentin, Joel Velasco, Sean Cabanlit, Christine Duvenjian, Matthew J. Prior, Garo Z. Akmakjian, [Roger B. Deal](https://github.com/DealLab), Neelima Sinha, Siobhan M. Brady, [Thomas Girke](https://github.com/tgirke), [Julia Bailey-Serres](https://github.com/jbserres)
_*These authors contributed equally_

**Journal:** 
Developmental Cell

**Summary:** 

> Understanding how roots modulate development under varied irrigation or rainfall is crucial for development of climate resilient crops. We established a toolbox of tagged rice lines to profile translating mRNAs and chromatin accessibility within specific cell populations. We used these to study roots in a range of environments: plates in the lab, controlled greenhouse stress and recovery conditions, and outdoors in a paddy. Integration of chromatin and mRNA data resolves diverse regulatory networks: cycle genes in proliferating cells that attenuate DNA synthesis under submergence; genes involved in auxin signaling, the circadian clock, and small RNA regulation in ground tissue; and suberin biosynthesis, iron transporters, and nitrogen assimilation in endodermal/exodermal cells modulated with water availability. By applying a systems approach we identify known and candidate driver transcription factors of water deficit responses and xylem development plasticity. Collectively, this resource will facilitate genetic improvements in root systems for optimal climate resilience.



## Code used by topic

- [ATAC-seq mapping THSs definition](https://github.com/plant-plasticity/rice-root-response-atlas/blob/main/Mapping_THS_definition_counts_ATAC.R)
- [ATAC-seq THS differential limma-voom & MDS](https://github.com/plant-plasticity/rice-root-response-atlas/blob/main/THS_diff_volcano_MDS.R)
- [RNA mapping counting](https://github.com/plant-plasticity/rice-root-response-atlas/blob/main/polyA-TRAP-mapping-counting.R)
- [Differential Gene Expression by limma-voom](https://github.com/plant-plasticity/rice-root-response-atlas/blob/main/DEG_analysis_limma_polyA_TRAP.R)
- [Clustering](https://github.com/plant-plasticity/Evolutionary-flexibility-in-flooding-response-2019/blob/master/DEG-analysis-limma-voom/Scripts/Clustering_Heatmap.R)
- [TF enrichment analysis on THS & heatmap](https://github.com/plant-plasticity/rice-root-response-atlas/blob/main/THS_enrichment_analysis.R)
- [FIMO motif](https://github.com/plant-plasticity/rice-root-response-atlas/blob/main/FIMO_annotated.R)
- [Networks TF to gene target](https://github.com/plant-plasticity/rice-root-response-atlas/blob/main/network_data_frame_production.R)

## Other resources by topic
- [Jbrowser - genome viewer of ATAC-seq, total RNA, TRAP RNA](http://ricejbrowse.baileyserreslab.org)
- [Rice root response atlas Spatial heatmap](http://spatialheatmap.baileyserreslab.org)
",2022-08-12
https://github.com/plant-plasticity/tomato-root-atlas-2020,"# Tomato-root-atlas-2020
Code and resources for Tomato Root Atlas

**Title:** Novelty, conservation, and repurposing of gene function in root cell type development

**Authors:**  Kaisa Kajala, Mona Gouran, Lidor Shaar-Moshe, G. Alex Mason, Joel Rodriguez-Medina, Dorota Kawa, Germain Pauluzzi, Mauricio Reynoso, Alex Canto-Pastor, Concepcion Manzano, Vincent Lau, Mariana A. S. Artur, Donnelly A. West, Sharon B. Gray, Alexander T. Borowsky, Bryshal P. Moore, Andrew I. Yao, Kevin W. Morimoto, Marko Bajic, Elide Formentin, Niba Nirmal, Alan Rodriguez, Asher Pasha, Roger B. Deal, Daniel Kliebenstein, Torgeir R. Hvidsten, Nicholas J. Provart, Neelima Sinha, Daniel E. Runcie, Julia Bailey-Serres, Siobhan M. Brady

**Journal:** [Cell](https://authors.elsevier.com/sd/article/S0092-8674(21)00504-3)

**Abstract:** 
> Plant species have evolved myriads of solutions, including complex cell type development and regulation, to adapt to dynamic environments. To understand this cellular diversity, we profiled tomato root cell type translatomes. Using xylem differentiation in tomato, examples of functional innovation, repurposing, and conservation of transcription factors are described, relative to the model plant Arabidopsis. Repurposing and innovation of genes are further observed within an exodermis regulatory network and illustrate its function. Comparative translatome analyses of rice, tomato, and Arabidopsis cell populations suggest increased expression conservation of root meristems compared with other homologous populations. In addition, the functions of constitutively expressed genes are more conserved than those of cell type/tissue-enriched genes. These observations suggest that higher order properties of cell type and pan-cell type regulation are evolutionarily conserved between plants and animals.


## Code used by topic 
- [Obtaining RNA-seq read counts: Trimming, Kallisto and STAR](https://github.com/plant-plasticity/tomato-root-atlas-2020/blob/master/Scripts/Trimming_Kallist_STAR_mapping.sh)
- [Differential expression: limma-voom](https://github.com/plant-plasticity/tomato-root-atlas-2020/tree/master/Scripts/Limma-voom)
- [Relative differential expression: Roku](https://github.com/plant-plasticity/tomato-root-atlas-2020/tree/master/Scripts/Roku)
- [Co-expression network analysis: WGCNA ](https://github.com/plant-plasticity/tomato-root-atlas-2020/blob/master/Scripts/WGCNA-Tomato-ATLAS.rmd)
- [ATAC-aseq analysis](https://github.com/plant-plasticity/tomato-root-atlas-2020/tree/master/Scripts/ATAC)
- [Motif enrichment](https://github.com/plant-plasticity/tomato-root-atlas-2020/tree/master/Scripts/Motif_enrichment)
- [Ontology enrichment](https://github.com/plant-plasticity/tomato-root-atlas-2020/tree/master/Scripts/Ontology_enrichment)
- [Orthology maps](https://github.com/plant-plasticity/tomato-root-atlas-2020/tree/master/Scripts/Orthology_maps)
- [Detecting constitutively expressed genes](https://github.com/plant-plasticity/tomato-root-atlas-2020/tree/master/Scripts/ConstitExpr_homologCT.R)
- [Test for enrichment of the arabidopsis N network genes in the exodermis](https://github.com/plant-plasticity/tomato-root-atlas-2020/blob/master/Scripts/Arabidopsis_YNM_enrichment/NitrogenNetwork_overlap_ExodermisNetwork.R)


## Protocols
These detailed bench protocols describe the methods as carried out in the Tomato Root Atlas paper.
- [TRAP purification of translating ribosomes](https://github.com/plant-plasticity/tomato-root-atlas-2020/blob/master/Protocols/TRAP%20Atlas.pdf)
- [RNA-seq library preparation from TRAP](https://github.com/plant-plasticity/tomato-root-atlas-2020/blob/master/Protocols/Brads%20Rapid%20Ravi%20for%20low%20mRNA%20Atlas.pdf)
- [INTACT isolation of nuclei](https://github.com/plant-plasticity/tomato-root-atlas-2020/blob/master/Protocols/INTACT%20Atlas.pdf)
- [ATAC-seq library preparation from INTACT](https://github.com/plant-plasticity/tomato-root-atlas-2020/blob/master/Protocols/ATAC-seq%20library%20prep%20Atlas.pdf)

## Supplementary data
- [Pairwise comparison of ATAC cut-counts between cell types](https://github.com/plant-plasticity/tomato-root-atlas-2020/blob/master/Figures/Figure_S23_scatter_plot_replicates_repUnion_THSs_ALL_110520_v2_with_legend.pdf)

## Other resources by topic
- [Data on GEO](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE149217)
- [eFP browser](http://bar.utoronto.ca/eplant_tomato/) Click on the “Tissue and Experiment eFP Viewers.”


## Analysis pipeline summaries

- **Flowchart for identification and annotation of ATAC-seq data.** Data analysis overview for methods used for identification of replicate transposase hypersensitive sites. 

   <img src=""https://github.com/plant-plasticity/tomato-root-atlas-2020/blob/master/Figures/Figure_S20_ATACseq_flowchart.jpg"" width=""700"">


- **Unique cell type network construction pipeline.** Data analysis overview for methods used to construct inferred cell type-unique regulatory networks. 

   <img src=""https://github.com/plant-plasticity/tomato-root-atlas-2020/blob/master/Figures/Figure_S26_NetworkFlowchart.jpg"" width=""700"">

- **Pipeline of cross-species analysis.** An overview of the pipeline and methods used for exploring the conservation of homologous cell types and tissues among tomato, Arabidopsis, and rice.

  <img src=""https://github.com/plant-plasticity/tomato-root-atlas-2020/blob/master/Figures/Cross_species_analysis_overview.jpg"" width=""700"">
  
",2022-08-12
https://github.com/plant-plasticity/townsley-fips-2015,"# Townsley, et al. (2015)

**Title:** BrAD-seq: **Br**eath **A**dapter **D**irectional **seq**uencing: a streamlined, ultra-simple and fast library preparation protocol for DNA and strand specific mRNA library construction.

**Authors:** Brad Thomas Townsley, [Michael F. Covington](http://mfcovington.github.io), Yasunori Ichihashi, Kristina Zumstein, and Neelima Roy Sinha

**Journal:** Frontiers in Plant Science 6:366 -- May 8, 2015

**Abstract:** 

> Next Generation Sequencing (NGS) is driving rapid advancement in biological understanding and RNA-sequencing (RNA-seq) has become an indispensable tool for biology and medicine. There is a growing need for access to these technologies although preparation of NGS libraries remains a bottleneck to wider adoption. Here we report a novel method for the production of strand specific RNA-seq libraries utilizing the terminal breathing of double-stranded cDNA to capture and incorporate a sequencing adapter. Breath Adapter Directional sequencing (BrAD-seq) reduces sample handling and requires far fewer enzymatic steps than most available methods to produce high quality strand-specific RNA-seq libraries. The method we present is optimized for 3-prime Digital Gene Expression (DGE) libraries and can easily extend to full transcript coverage shotgun (SHO) type strand-specific libraries and is modularized to accommodate a diversity of RNA and DNA input materials. BrAD-seq offers a highly streamlined and inexpensive option for RNA-seq libraries.

## Code used by topic (*In Progress*)

- [Reference Sequences Containing 3'-UTR]()
- [Flanking Sequence Enrichment]()
- [Coverage Profiles](coverage-profiles.md) (Figures 1A & 1B)
- [Differential Gene Expression](differential-gene-expression.md)
- []()
- []()
",2022-08-12
https://github.com/puregome/annotation,"# annotation
annotation
",2022-08-12
https://github.com/puregome/data,"# Data 

Data related to the Chao et al. paper:

| **File**                     | **Description**                               |
|----------------------------- | --------------------------------------------- |
| 20211207\_municipalities.csv | ids and predicted labels for 88,564 tweets used for municipality counts |
| 20211207\_provinces.csv      | ids and predicted labels for 96,864 tweets used for province counts     |
| 20211207\_readme.txt         | readme file with additional information       |

These files are used by the notebook [social-distancing-aggregate.ipynb](https://github.com/puregome/notebooks/blob/master/social-distancing-aggregate.ipynb).
All tweets used for municipality counts have also been used for province counts.
For an additional 8,300 tweets used for province counts, the municipality was unknown.

| **File**                     | **Description**                               |
|----------------------------- | --------------------------------------------- |
| data-facemasks.csv           | annotated data for wearing facemasks         |
| data-social-distancing.csv   | annotated data for social distancing         |
| data-testing.csv             | annotated data for testing                   |
| data-vaccination.csv         | annotated data for vaccination               |

These files are summaries of the files used by the notebook [ieee.ipynb](https://github.com/puregome/notebooks/blob/master/ieee.ipynb) and others.
The annotated data files contain two columns: tweet id (td\_str) and label. There are 
three labels: EENS (supports the policy), ONEENS (rejects the policy) and ANDERS
(irrelevant). Tweets labelled ANDERS (irrelevant) have not been included in the final analysis.

",2022-08-12
https://github.com/puregome/models,"# models
fastText models
",2022-08-12
https://github.com/puregome/notebooks,"# PuReGoMe

[PuReGoMe](https://research-software.nl/projects/4728) is a research project of [Utrecht University](https://www.uu.nl/en/research/intelligent-software-systems/intelligent-systems) and the [Netherlands eScience Center](https://www.esciencecenter.nl/). We analyze Dutch social media messages to assess the opinion of the public towards the COVID-19 pandemic measures taken by the Dutch government.


## Data

PuReGoMe uses three data sources for social media analysis. Our main data source are Dutch tweets from [Twitter](https://twitter.com/). We use Dutch posts from [Reddit](https://www.reddit.com/) and comments from [Nu.nl](https://www.nu.nl) as sources of verification of the results obtained by the tweet analysis.

After a month has ended, the data from the month are collected. Here are the steps taken for each data source:


### Twitter

1. the tweets are taken from the crawler of [twiqs.nl](http://twiqs.nl). They are stored in json format in hourly files. (this data set is not publicly available)
2. the tweets are extracted from the json files and and stored in csv files (six columns: id\_str, in\_reply\_to\_status\_id\_str, user, verified, text, location). The conversion is performed by the script [query-text.py](https://github.com/puregome/queries/blob/master/query-text.py)
3. duplicate tweets are removed from from the csv files produced in step 2 by the script [text-unique.py](https://github.com/puregome/scripts/blob/master/text-unique.py)

(it would be useful to combine steps 2 and 3 in the future)


### Reddit

1. In a new month directory, run the script [get\_subreddit\_ids.py](https://github.com/puregome/scripts/blob/master/get_subreddit_ids.py) to automatically retrieve the subreddits of the Dutch corona reddits
2. Copy the list of ids of the subreddit [Megathread Coronavirus COVID-19 in Nederland](https://www.reddit.com/r/thenetherlands/search?q=Megathread+Coronavirus+COVID-19+in+Nederland&restrict_sr=on&sort=new&t=all) (submissions_ids_thenetherlands.txt) from a previous month directory and manually add the ids of recent subreddits
3. Run the script [coronamessagesnl.py](https://github.com/puregome/scripts/blob/master/coronamessagesnl.py) on the files `submissions_ids_*` to automatically retrieve the posts in the found subreddits
4. Run the notebook [reddit.ipynb](reddit.ipynb) to get all the posts from the monthly `downloads` directory and store them in the directory `text`


### Nu.nl

1. Run code blocks 1, 3 and 4 of the notebook selenium-test.ipynb, after updating the name of the file in URLFILE in code block 1
2. Restart the notebook and run code blocks 1 and 6, after changing the name of the new downloads directory in DATADIROUT in code block 6. This process takes many hours (even days) to complete
3. The notebook can be copied and several copies can be run in parallel
4. When the notebooks have finished: delete all pairs of files of sizes 1 and 3 in this month's directory (keep the ones with only size 1) and rerun the notebooks
5. Repeat step 4 until no articles with comments are found
6. go to (cd) the directory data/nunl
7. run the script [../../scripts/combineNunlComments.sh](https://github.com/puregome/scripts/blob/master/combineNunlComments.sh) to update the files in the main directory `downloads`
8. run the notebook [nunl-convert-data.ipynb](nunl-convert-data.ipynb) to regenerate the data files in the directory `text`
9. for fetching the article texts: run code blocks 1 and 2 of the notebook [selenium-test.ipynb](selenium-test.ipynb), after updating the variables `URLFILE` and `OUTFILEMETADONELIST`


## Analysis

PuReGoMe performs analysis on three different levels: by counting messages, by determining their polarity (sentiment) and by determining their stance with respect to anti-pandemic government measures.


### Frequency analysis

Frequency analysis of tweets is performed in the notebook [tweet-counts.ipynb](tweet-counts.ipynb). The notebook defines several pandemic queries, for example face mask, lockdown, social distancing and pandemic, where pandemic is a combination of 60+ relevant terms. The notebook produces a graph with the absolute daily frequencies of the tweets matching each of these pandemic queries:

![tweet frequencies](tweet-frequencies.png)

Frequency analysis of the Nu.nl and Reddit data is included in the respective data generation notebooks [nunl-convert-data.ipynb](nunl-convert-data.ipynb) and [reddit.ipynb](reddit.ipynb)


### Polarity analysis

Polarity analysis is the same as sentiment analysis. This analysis is performed by the notebook [sentiment-pattern.ipynb](sentiment-pattern.ipynb) which uses the Python package [Pattern](https://github.com/clips/pattern) for sentiment analysis of Dutch text (De Smedt &amp; Daelemans, 2011). The notebook requires two types of input files: the csv files in the text directories of each of the data sources and sentiment score files which should be generated from these csv files with the script [../../scripts/sentiment-pattern-text.py](https://github.com/puregome/scripts/blob/master/sentiment-pattern-text.py) The polarity analysis of the different topics takes a lot of time and can be run in parallel. It produces time series graphs for all tweets, all pandemic tweets and several individual pandemic topics.

![polarity for face masks, social distancing and lockdown over time](sentiment-all.png)


### Stance analysis

Stance analysis is performed by the notebook [fasttext.ipynb](fasttext.ipynb). The analysis originates from a model trained by [fastText](https://github.com/facebookresearch/fastText) on manually labeled tweets. The notebook contains a section for searching for the best parameters of fastText using grid search but when the training data is unchanged this section can be skipped. The notebook has two main modes related to topics: analysis related to the social distancing policy and analysis related to the former (April 2020) face mask policy. These are the only two topics for which we have manually labeled training data. The graphs combine analysis for all three data sources used in the project: Twitter, Nu.nl and Reddit.

![stance for social distancing](social-distancing-all.png)


### Other analyses

The notebook [topic-analysis.ipynb](topic-analysis.ipynb) is used to find new topics in a day of tweets. It compares the vocabulary of a day of tweets with the vocabulary of the preceding day.

[echo-chambers.ipynb](echo-chambers.ipynb) is used for finding groups of users which collectively retweet similar content. The notebook found a group of a few hundred users retweeting right-wing propaganda. Further study needs to be done to check if this content has any effect on the findings of this project.

[geo-analysis.ipynb](geo-analysis.ipynb) and [geo-classification.ipynb](geo-classification.ipynb) can be used to divide the tweets in groups depending on on the location of the tweeter. This only works for about half of the data set. Next, maps representing tweet data can be created with the notebook [maps.ipynb](maps.ipynb).

[corona-nl-totals.ipynb](corona-nl-totals.ipynb) creates graphs of the number infections, hospitalizations and deaths in The Netherlands based on data provided by the health organization [RIVM](https://www.rivm.nl).


## Publications, talks and media coverage

Erik Tjong Kim Sang, Shihan Wang, Marijn Schraagen and Mehdi Dastani, [**Estimating Stances on Anti-Pandemic Measures By Social Media Analysis**](https://ifarm.nl/erikt/talks/ictopen2022.pdf), ICT.OPEN2022 (poster), Amsterdam, The Netherlands, 2022

Erik Tjong Kim Sang, Shihan Wang, Marijn Schraagen and Mehdi Dastani, [**Extracting Stances on Pandemic Measures from Social Media Data**](https://ifarm.nl/erikt/papers/ieee2021.pdf). 17th IEEE eScience Conference (poster), 2021.

Erik Tjong Kim Sang, Marijn Schraagen, Shihan Wang and  Mehdi Dastani, [**Transfer Learning for Stance Analysis in COVID-19 Tweets**](https://ifarm.nl/erikt/papers/clin-20210419.pdf). CLIN 2021. ([data annotations](http://145.100.59.103/downloads/clin2021-annotations.zip))

Erik Tjong Kim Sang, Marijn Schraagen, Mehdi Dastani and Shihan Wang, [**Discovering Pandemic Topics on Twitter**](https://ifarm.nl/erikt/papers/dhbenelux-20210509.pdf). DHBenelux 2021.

Erik Tjong Kim Sang [**PuReGoMe: Social Media Analysis of the Pandemic**](https://ifarm.nl/erikt/talks/20210211-escience.pdf). Lunch talk, Netherlands eScience Center, Amsterdam, The Netherlands, 11 February 2021.

Shihan Wang, Marijn Schraagen, Erik Tjong Kim Sang and Mehdi Dastani, [**Dutch General Public Reaction on Governmental COVID-19 Measures and Announcements in Twitter Data**](https://arxiv.org/abs/2006.07283). Preprint report on arXiv.org, 21 December 2020.

Shihan Wang, Marijn Schraagen, Erik Tjong Kim Sang and Mehdi Dastani, [**Public Sentiment on Governmental COVID-19 Measures in Dutch Social Media**](https://www.aclweb.org/anthology/2020.nlpcovid19-2.17/). In: Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020
(NLP-COVID19-EMNLP), 20 November 2020.

Erik Tjong Kim Sang [**PuReGoMe: Dutch Public Reaction on Governmental COVID-19 Measures and Announcements**](https://ifarm.nl/erikt/talks/20200626-escience.pdf). Lunch talk, Netherlands eScience Center, Amsterdam, The Netherlands, 26 June 2020.

Shihan Wang, **Public Sentiment during COVID-19 -Data Mining on Twitter data**. Talk at the [CLARIN Café](https://www.clarin.eu/content/clarin-cafe), Utrecht, The Netherlands, 27 May 2020.

Redactie Emerce, [**Onderzoekers leiden publieke opinie coronamaatregelen af uit social media-data**](https://www.emerce.nl/nieuws/onderzoekers-leiden-publieke-opinie-coronamaatregelen-af-uit-social-mediadata). Emerce, 12 May 2020 (in Dutch).

Nienke Vergunst, [**Researchers use social media data to analyse public sentiment about Coronavirus measures**](https://www.uu.nl/en/news/researchers-use-social-media-data-to-analyse-public-sentiment-about-coronavirus-measures). University of Utrecht news message, 11 May 2020.

# Information added by the Python template

## Badges

| fair-software.eu recommendations | |
| :-- | :--  |
| (1/5) code repository              | [![github repo badge](https://img.shields.io/badge/github-repo-000.svg?logo=github&labelColor=gray&color=blue)](https://github.com/puregome/notebooks) |
| (2/5) license                      | [![github license badge](https://img.shields.io/github/license/puregome/notebooks)](https://github.com/puregome/notebooks) |
| (3/5) community registry           | [![Research Software Directory](https://img.shields.io/badge/rsd-Research%20Software%20Directory-00a3e3.svg)](https://www.research-software.nl/software/puregome) |
| (4/5) citation                     | [![DOI](https://zenodo.org/badge/256566641.svg)](https://zenodo.org/badge/latestdoi/256566641) |
| (5/5) checklist                    | [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/4357/badge)](https://bestpractices.coreinfrastructure.org/projects/4357) |
| howfairis                            | [![fair-software badge](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu) |
| **Other best practices**           | &nbsp; |
| Static analysis              | [![workflow scq badge](https://sonarcloud.io/api/project_badges/measure?project=puregome_notebooks&metric=alert_status)](https://sonarcloud.io/dashboard?id=puregome_notebooks) |
| Coverage              | [![workflow scc badge](https://sonarcloud.io/api/project_badges/measure?project=puregome_notebooks&metric=coverage)](https://sonarcloud.io/dashboard?id=puregome_notebooks) |
| **GitHub Actions**                 | &nbsp; |
| Build                              | [![build](https://github.com/puregome/notebooks/actions/workflows/build.yml/badge.svg)](https://github.com/puregome/notebooks/actions/workflows/build.yml) |
|  Metadata consistency              | [![cffconvert](https://github.com/puregome/notebooks/actions/workflows/cffconvert.yml/badge.svg)](https://github.com/puregome/notebooks/actions/workflows/cffconvert.yml) |
| Lint                               | [![lint](https://github.com/puregome/notebooks/actions/workflows/lint.yml/badge.svg)](https://github.com/puregome/notebooks/actions/workflows/lint.yml) |
| Publish                            | [![publish](https://github.com/puregome/notebooks/actions/workflows/publish.yml/badge.svg)](https://github.com/puregome/notebooks/actions/workflows/publish.yml) |
| SonarCloud                         | [![sonarcloud](https://github.com/puregome/notebooks/actions/workflows/sonarcloud.yml/badge.svg)](https://github.com/puregome/notebooks/actions/workflows/sonarcloud.yml) |
| MarkDown link checker              | [![markdown-link-check](https://github.com/puregome/notebooks/actions/workflows/markdown-link-check.yml/badge.svg)](https://github.com/puregome/notebooks/actions/workflows/markdown-link-check.yml) |

## How to use notebooks

The project setup is documented in [project_setup.md](project_setup.md). Feel free to remove this document (and/or the link to this document) if you don't need it.

## Installation

To install notebooks from GitHub repository, do:

```console
git clone https://github.com/puregome/notebooks.git
cd notebooks
python3 -m pip install .
```

## Documentation

Include a link to your project's full documentation here.

## Contributing

If you want to contribute to the development of notebooks,
have a look at the [contribution guidelines](CONTRIBUTING.md).

## Credits

This package was created with [Cookiecutter](https://github.com/audreyr/cookiecutter) and the [NLeSC/python-template](https://github.com/NLeSC/python-template).
",2022-08-12
https://github.com/puregome/queries,"# queries
queries
",2022-08-12
https://github.com/puregome/scripts,"# scripts
scripts
",2022-08-12
https://github.com/qirickyliu/hello-world,"# hello-world
hey, i am liu qi, a phd student at NIOZ! I am working on github and Rstudio.
",2022-08-12
https://github.com/rebeccakuiper/ChiBarSq.DiffTest,"# ChiBarSq.DiffTest
This function calculates the Chi-bar-square difference test of the RI-CLPM versus the CLPM or a general (variance) test. The first is discussed in Hamaker, Kuiper, and Grasman (2015) and the second in Stoel et al. (2006).
",2022-08-12
https://github.com/rebeccakuiper/CTmeta,"# CTmeta
R package that conducts continuous-time meta-analysis (CTmeta) on lagged effects
",2022-08-12
https://github.com/rebeccakuiper/GoricEvSyn,# GoricEvSyn,2022-08-12
https://github.com/rebeccakuiper/Test-ReproCode-course,"# [PROJECT NAME]

## Project organization
- PG = project-generated
- HW = human-writable
- RO = read only
```
.
├── .gitignore
├── CITATION.md
├── LICENSE.md
├── README.md
├── requirements.txt
├── bin                <- Compiled and external code, ignored by git (PG)
│   └── external       <- Any external source code, ignored by git (RO)
├── config             <- Configuration files (HW)
├── data               <- All project data, ignored by git
│   ├── processed      <- The final, canonical data sets for modeling. (PG)
│   ├── raw            <- The original, immutable data dump. (RO)
│   └── temp           <- Intermediate data that has been transformed. (PG)
├── docs               <- Documentation notebook for users (HW)
│   ├── manuscript     <- Manuscript source, e.g., LaTeX, Markdown, etc. (HW)
│   └── reports        <- Other project reports and notebooks (e.g. Jupyter, .Rmd) (HW)
├── results
│   ├── figures        <- Figures for the manuscript or reports (PG)
│   └── output         <- Other output for the manuscript or reports (PG)
└── src                <- Source code for this project (HW)

```


## License

This project is licensed under the terms of the [MIT License](/LICENSE.md)
",2022-08-12
https://github.com/rebeccakuiper/testrepo,"# testrepo
This is a line I’m adding offline to my local copy
This is a line I’m adding from GitHub.com
",2022-08-12
https://github.com/Rel-incode/cop,"# cop

This is the Complaint Design Pattern (CDP), formerly known as Complaint Ontology Pattern (COP)

# more information to come here...",2022-08-12
https://github.com/Rel-incode/CristianaSantosCv,"# al-folio

[![build status](https://travis-ci.org/alshedivat/al-folio.svg?branch=master)](https://travis-ci.org/alshedivat/al-folio)
[![demo](https://img.shields.io/badge/theme-demo-brightgreen.svg)](https://alshedivat.github.io/al-folio/)
[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/alshedivat/al-folio/blob/master/LICENSE)
[![gitter](https://badges.gitter.im/alshedivat/al-folio.svg)](https://gitter.im/alshedivat/al-folio?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)

A simple and clean [Jekyll](https://jekyllrb.com/) theme for academics.

[![Screenshot](assets/img/full-screenshot.png)](https://alshedivat.github.io/al-folio/)

Originally, **al-folio** was based on the [\*folio theme](https://github.com/bogoli/-folio) (published by [Lia Bogoev](http://liabogoev.com) and under the MIT license).
Since then, it got a full re-write of the styles and many additional cool features.
The emphasis is on whitespace, transparency, and academic usage: [theme demo](https://alshedivat.github.io/al-folio/).

## Getting started

For more about how to use Jekyll, check out [this tutorial](https://www.taniarascia.com/make-a-static-website-with-jekyll/).
Why Jekyll? Read this [blog post](https://karpathy.github.io/2014/07/01/switching-to-jekyll/)!

### Installation

Assuming you have [Ruby](https://www.ruby-lang.org/en/downloads/) and [Bundler](https://bundler.io/) installed on your system (*hint: for ease of managing ruby gems, consider using [rbenv](https://github.com/rbenv/rbenv)*), first fork the theme from `github.com:alshedivat/al-folio` to `github.com:<your-username>/<your-repo-name>` and do the following:

```bash
$ git clone git@github.com:<your-username>/<your-repo-name>.git
$ cd <your-repo-name>
$ bundle install
$ bundle exec jekyll serve
```

Now, feel free to customize the theme however you like (don't forget to change the name!).
After you are done, **commit** your final changes.
Now, you can deploy your website to [GitHub Pages](https://pages.github.com/) by running the deploy script:

```bash
$ ./bin/deploy [--user]
```
By default, the script uses the `master` branch for the source code and deploys the webpage to `gh-pages`.
The optional flag `--user` tells it to deploy to `master` and use `source` for the source code instead.
Using `master` for deployment is a convention for [user and organization pages](https://help.github.com/articles/user-organization-and-project-pages/).

**Note:** when deploying your user or organization page, make sure the `_config.yml` has `url` and `baseurl` fields as follows.

```
url: # should be empty
baseurl:  # should be empty
```

### Usage

Note that `_pages/about.md` is built to index.html in the published site. There is therefore no need to have a separate index page for the project. If an index page does exist in the root directory then this will prevent `_pages/about.md` from being added to the built site.

## Features

#### Ergonomic Publications

Your publications page is generated automatically from your BibTex bibliography.
Simply edit `_bibliography/papers.bib`.
You can also add new `*.bib` files and customize the look of your publications however you like by editing `_pages/publications.md`.

Keep meta-information about your co-authors in `_data/coauthors.yml` and Jekyll will insert links to their webpages automatically.

#### Collections
This Jekyll theme implements collections to let you break up your work into categories.
The example is divided into news and projects, but easily revamp this into apps, short stories, courses, or whatever your creative work is.

> To do this, edit the collections in the `_config.yml` file, create a corresponding folder, and create a landing page for your collection, similar to `_pages/projects.md`.

Two different layouts are included: the blog layout, for a list of detailed descriptive list of entries, and the projects layout.
The projects layout overlays a descriptive hoverover on a background image.
If no image is provided, the square is auto-filled with the chosen theme color.
Thumbnail sizing is not necessary, as the grid crops images perfectly.

#### Theming
Six beautiful theme colors have been selected to choose from.
The default is purple, but quickly change it by editing `$theme-color` variable in the `_sass/variables.scss` file (line 72).
Other color variables are listed there, as well.

#### Photos
Photo formatting is made simple using rows of a 3-column system.
Make photos 1/3, 2/3, or full width.
Easily create beautiful grids within your blog posts and projects pages:

<p align=""center"">
  <a href=""https://alshedivat.github.io/al-folio/projects/1_project/"">
    <img src=""assets/img/photos-screenshot.png"" width=""75%"">
  </a>
</p>

#### Code Highlighting
This theme implements Jekyll's built in code syntax highlighting with Pygments.
Just use the liquid tags `{% highlight python %}` and `{% endhighlight %}` to delineate your code:

<p align=""center"">
  <a href=""https://alshedivat.github.io/al-folio/blog/2015/code/"">
    <img src=""assets/img/code-screenshot.png"" width=""75%"">
  </a>
</p>

#### Social media previews
The al-folio theme optionally supports preview images on social media.
To enable this functionality you will need to set `serve_og_meta` to `true` in
your `_config.yml`. Once you have done so, all your site's pages will include
Open Graph data in the HTML head element.

You will then need to configure what image to display in your site's social
media previews. This can be configured on a per-page basis, by setting the
`og_image` page variable. If for an individual page this variable is not set,
then the theme will fall back to a site-wide `og_image` variable, configurable
in your `_config.yml`. In both the page-specific and site-wide cases, the
`og_image` variable needs to hold the URL for the image you wish to display in
social media previews.

## Contributing

Feel free to contribute new features and theme improvements by sending a pull request.
Style improvements and bug fixes are especially welcome.

## License

The theme is available as open source under the terms of the [MIT License](https://opensource.org/licenses/MIT).
",2022-08-12
https://github.com/Rel-incode/rep,"# Minimal Mistakes remote theme starter

Click [**Use this template**](https://github.com/mmistakes/mm-github-pages-starter/generate) button above for the quickest method of getting started with the [Minimal Mistakes Jekyll theme](https://github.com/mmistakes/minimal-mistakes).

Contains basic configuration to get you a site with:

- Sample posts.
- Sample top navigation.
- Sample author sidebar with social links.
- Sample footer links.
- Paginated home page.
- Archive pages for posts grouped by year, category, and tag.
- Sample about page.
- Sample 404 page.
- Site wide search.

Replace sample content with your own and [configure as necessary](https://mmistakes.github.io/minimal-mistakes/docs/configuration/).

---

## Troubleshooting

If you have a question about using Jekyll, start a discussion on the [Jekyll Forum](https://talk.jekyllrb.com/) or [StackOverflow](https://stackoverflow.com/questions/tagged/jekyll). Other resources:

- [Ruby 101](https://jekyllrb.com/docs/ruby-101/)
- [Setting up a Jekyll site with GitHub Pages](https://jekyllrb.com/docs/github-pages/)
- [Configuring GitHub Metadata](https://github.com/jekyll/github-metadata/blob/master/docs/configuration.md#configuration) to work properly when developing locally and avoid `No GitHub API authentication could be found. Some fields may be missing or have incorrect data.` warnings.
",2022-08-12
https://github.com/Rel-incode/ric-ontology,"# ric-ontology
Ontology of legal relevant information in consumer disputes
",2022-08-12
https://github.com/RELabUU/aqusa-core,"# aqusa-core
A command line version of the AQUSA tool<sup>1</sup>, which identifies quality defects in user story requirements. AQUSA-core takes as input a text file that includes a collection of user stories (one user story per line), it executes linguistic processing to identify defects, and it returns a report either in text format (txt) or as an HTML page. This tool is a command-line version of the tool described in a 2016 paper published in the Requirements Engineering journal<sup>2</sup>.

### Installation
  * Tested with Python 3.7
  * Install libraries using `pip install -r requirements.txt`
  
### Usage
`python aqusacore.py -i <inputfile>  [-o <outputfile>] [-f <outputformat>]`
  where
  * -i <inputfile> indicates the input txt file stored in the ./input folder
  * -o <outputfile> indicates the output file, which will be stored in the ./output folder
  * -f <outputformat> can be either txt or html, and indicates the type of output that is desired
 
AQUSA-core comes with a few example files that can be found in the ./input folder
 
If you wish to share the html page that is created by the tool, you also need to copy the styles.css and the sorttable.js files.

### References and links
<sup>1</sup> https://github.com/gglucass/AQUSA

<sup>2</sup> Garm Lucassen, Fabiano Dalpiaz, Jan Martijn E. M. van der Werf, Sjaak Brinkkemper. Improving Agile Requirements: the Quality User Story Framework and Tool, In Requirements Engineering, volume 21, 2016.
",2022-08-12
https://github.com/RELabUU/BakeRE,"# BakeRE
Find BakeRE on Google Play: https://play.google.com/store/apps/details?id=tech.bakere.bakere

Learning requirements notations is a tedious task for most people, because this learning activity is all but engaging. As an alternative to traditional educational methods, this is BakeRE, a serious educational game for requirements engineering. The game focuses on specific learning objectives: the specification and analysis of requirements with user stories. It is an engaging, multilevel puzzle game that can complement courses (traditional or on-line), such as a general software engineering or a specialized RE course. 

This game is created as a part of a Utrecht University Business Informatics master thesis project. BakeRE will be used as a part of the RE course in 2019.


The sourcecode for BakeRE is freely available in the folder ``USGame''. 

To create a new database, fill in the BakeRE database Excel template and turn the file in a Json file by running the Excel Add-in code. 
The code for the leaderboard website is found in the folder ``Website''; couple it to your own SQL database by changing the username and password fields in the files found within this folder.

Kind regards,
Merle
",2022-08-12
https://github.com/RELabUU/concept-suggestor,"# concept-suggestor
Takes concepts from models and suggests relevant and interesting ones to modelers.

## Install
1. Navigate to the root directory of the project. It should contain `requirements.txt`
2. Open an elevated command prompt.
    - If you want to use a virtual environment to install packages in, activate that now.
3. Run `pip install -r requirements1.txt`
4. Run `pip install -r requirements2.txt`
5. Install NumPy+mkl
	- These instructions are for Windows. If you're not on Windows, you'll have to Google around.
	- Navigate to http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy
	- Select the version of Numpy+MLK appropriate for your Python version and OS. cp36-cp36m means Python 3.6.
	- Navigate to the download directory and open an elevated command prompt.
	- Run `pip install <filename>`
6. Install SciPy
	- This depends significantly on your OS and Python version.
	- First, try to run `pip install scipy`. If this works, you're done! If not and you're on Windows, follow the following instructions:
		- Navigate to http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy
		- Select the version of SciPy appropriate to your Python version and OS. cp36-cp36m means Python 3.6.
		- Navigate to the download directory and open an elevated command prompt.
		- Run `pip install <filename>`
7. Run `python -m spacy download en_core_web_md`
8. Run `python -m nltk.downloader`
	- Navigate to the `Corpora` tab.
	- Install the `wordnet_ic` corpus.
9. Launch or open the project.
    - Run the `ConceptSuggestor\ConceptSuggestor.py` file to launch the program.
    - Open `ConceptSuggestor.sln` in Visual Studio to open the solution.

## Usage
Concept Suggestor has several options available for use.
When the program is opened, it shows a list of available options.

A variety of settings is contained in `settings.json`. 
If the `reload` value is set to `true`, you can change the settings in between operations.
The settings will be reloaded after the asks whether you want to try again.
",2022-08-12
https://github.com/RELabUU/icva-experiments,# icva-experiments,2022-08-12
https://github.com/RELabUU/Interactive-Narrator,"# Interactive Narrator
**Developed @ Utrecht University**

**IMPORTANT: This project is not maintained nor functional at the moment. If you are interested in maintaining it, please contact Fabiano Dalpiaz at f.dalpiaz@uu.nl**

Visualize your user stories to improve understanding of the project, inspect system functionality 
and improve communication among stakeholders.

This application uses the Visual Narrator (https://github.com/marcelrobeer/visualnarrator) to generate a conceptual model which is then
visualized using Vis.js and is presented to the user in the browser from where the visualization can be adapted to cater to the 
user's needs.

![alt tag](https://github.com/Gionimo/InteractiveNarrator/blob/master/Screenshot%20Interactive%20Narrator2.png)


**License**

This product is free to use for commercial and non-commercial purposes.

**How To Start Developing**
1. create a ubuntu virtual machine
open a terminal in /Documents and run:
2. apt install python-pip
3. apt-get install python3-dev
4. apt install virtualenv
5. virtualenv -p python3 inarrator (this will create a separate dev. environment with the name inarrator)
6. in the newly created directory open a terminal and activate 
the virtual enironment with `source bin/activate` (do this everytime you need to install/update something)
7. pip install git
8. pip install -r requirements.txt (this will install all the required packages)
9. python -m spacy download en_core_web_md (download the spacy NLP language model)
 

**Telling Python interpreter where to look for packages**
Open a Linux terminal and navigate to your root directory with `~` and then type `sudo nano .profile`
which should open a file with some lines after which you add to the bottom of this file:

`export PYTHONPATH=$PYTHONPATH:/home/path/on/your/computer/yourvirtualenvironment/:/home/path/on/your/computer/yourvirtualenvironment/VisualNarrator`

If after this Python throws import errors it might not be able to find the packages.
A workaround is to add this line after line 22 in app.py and after line 6 in post.py:
   
   `sys.path.append('/path/on/your/computer/yourvirtualenvironment/VisualNarrator')`
   
   **and**
   
Add after line 6 in app.py:
 
   `# sys.path.append('/home/path/on/your/computer/inarrator')`
    
To tell Python where to look for de VisualNarrator package on your computer.

Notes:
- the project has been prepared for using Flask-Security but isn doing so yet. This is why
some commented code is present.
- message flashing is not working, although some code to start working with it is present
- deleting per sprint is ready to be implemented (see method in app.py) but needs looking into as currently 
deleting entities from one sprint also deletes them while they are actually in other sprints too.
solution: only delete entity (concept) when it is in a use story that is in only one sprints
- downloading the PNG of the network now gives a transparent image. This should be reworked to contain a white 
background. Some experimental code exists in visualization.js
- various print statements exist to help logging during development

TO DO LIST:
- improve rendering performance on mobile devices (change settings for algorithm)
- add clustering (method already in place)
- increase security for users (flask_security module already in place but not in use)
- enable delete per sprint (code already present, just needs tweaking. see app.py delete_sprint())
- add progress bar and hide rendering of the visualization until it's done
- add ceiling AND bottom to the weight filter
- add support for themes and epics
- improve support for touch events
- add more succes confirmations
- add a tutorial
- enable drawing on the visualization on touch devices
- enable downloading of the reports VN generates.
- add detection mechanism for redundencies/inconsistencies/dependencies such as color alerts
",2022-08-12
https://github.com/RELabUU/LCDTrace,"# About


This repository provides the code used to produce the results of Van Oosten, Rasiman, Dalpiaz & Hurkmans (2022). 

It takes a set of JIRA issues and a set of SVN commits as input. The program then cleans and preprocesses these. The JIRA identifier (label) is then taken from the SVN commit logs and appended to the commit data using REGEX. The Cartesian product of the JIRA issues and SVN commits is then produced, with each element being a candidate trace. For each candidate trace, a set of features is computed. These features are then utilized as input data for 12 different models (classification algorithm x rebalancing strategy). Finally, these models are evaluated. This is then followed by a similar approach for non-MDD specific features and feature subsets (using the automated feature selection algorithm [mRMR](https://github.com/smazzanti/mrmr)).

**Input:**
* JIRA Issues (.csv or .xlsx): A .csv or .xlsx dump of a JIRA project.
* Mendix SVN Dump (.txt)

**Output:**
* The accuracy, precision, recall, f1-score, f2-score, f0.5-score of the fitted models (on the training set) used to predict te labels of the test set
* the feature importance of the fitted models

# Dependencies
* [Python](https://www.python.org/) (>= 3.9.4)
* [Jupyter Notebook](https://jupyter.org/) (>=  6.2.0)
* [Pandas](https://pandas.pydata.org/) (>=  1.2.4)
* [NumPy](https://numpy.org/) (>=  1.20.1)
* [SciPy](https://scipy.org/) (>=  1.6.2)
* [NLTK](https://www.nltk.org/) (>=  3.5)
* [Scikit-Learn](https://scikit-learn.org) (>=  0.24.1)
* [Scikit-posthocs](https://github.com/maximtrp/scikit-posthocs) (>= 0.7.0)
* [mrmr_selection](https://github.com/smazzanti/mrmr) (>=0.2.5)

# Directory Structure
```
├── data  
│   ├── 01_raw                                      <- Input data
│   │   ├── jira_example.csv                        <- Example of a raw JIRA input
│   │   └── svn_example.txt                         <- Example of an svn input
│   ├── 02_intermediate                             <- Cleaned version of 01_raw, produced by lcd_trace.ipynb
│   └── 03_processed                                <- The data used to train the classifiers (populated features) 
├── notebooks                                       <- Jupyter notebooks
│   ├── lcd_trace.ipynb                             <- Notebook to engineer the features to be used by the models
│   │   lcd_trace_no_lc_features.ipynb              <- Notebook to engineer the non-MDD specific features to be used by the models
│   │   evaluation_notebook.ipynb                   <- Notebook to evaluate trace classification models
│   │   feature_selection.ipynb                     <- Notebook to apply feature selection for three subset sizes
│   └── results processing
│       ├── calculating_feature_importance.ipynb    <- Notebook to calculate feature importance (families) and create boxplots (e.g., Figure 4 in the paper)
│       ├── Friedman-Nemenyi.ipynb                  <- Notebook to perform the statistical tests
│       └── process_results.ipynb                   <- Notebook to process the raw results that returns dataset-grouped results
├── results                                         <- The evaluation results produced by the notebook
│   ├── 01_Trace link Feature Data                  <- Example of populated feature files
│   ├── 02_non_normalised_results                   <- The evaluation for non-normalised feature input data
│   │   ├── light_gbm              
│   │   ├── random_forests
│   │   └── xg_boost  
│   ├── 04_feature_imporance_results                <- The feature importances for the different models
│   │   ├── light_gbm              
│   │   ├── random_forests
│   │   └── xg_boost
│   ├── 05_Feature selection subsets                <- Examples of processed results files of feature subsets data
│   └── Evaluation                                  <- Examples of processed results files of model evaluation
│
├── src                                             <- Source code for use in this project, called by the notebooks.
│   ├── d00_utils                                   <- Methods used across the project
│   │   └── calculateTimeDifference.py              <- Method to calculate time difference in minutes and seconds (string)
│   ├── d01_data                                    <- Scripts to reading and writing data
│   │   └── loadCommits.py                          <- Method to load .txt to a pandas dataframe
│   ├── d02_intermediate                            <- Scripts to transform data from raw to intermediate
│   │   ├── cleanCommitData.py                      <- Methods to clean SVN commit data
│   │   └── cleanJiraData.py                        <- Methods to clean JIRA data
│   ├── d03_processing                              <- Scripts to turn intermediate data into modelling input
│   │   ├── calculateCosineSimilarity.py            <- Methods to calculate cosine similarity between 2 documents
│   │   ├── calculateDocumentStatistics.py          <- Methods to compute document statistics features
│   │   ├── calculateQueryQuality.py                <- Methods to compute query quality features
│   │   ├── calculateTimeDif.py                     <- Method to calculate the time difference between 2 dates in seconds
│   │   ├── checkFullnameEqualsEmail.py             <- Method to check if the full_name is part of the email address
│   │   ├── checkValidityTrace.py                   <- Method to check if a trace is valid or invalid
│   │   ├── createCorpusFromDocumentList.py         <- Method to create a corpus
│   │   ├── createFittedTF_IDF.py                   <- Method to fit a tf_idf on a document
│   │   └── normalize_data.py                       <- Method to normalize data
│   └── d04_model_evaluation                        <- Scripts that analyse model performance and model selection
│   │   └── model_evaluation.py                     <- Methods to produce the evaluation metrics
└── readme.md                 
```

# Running the project
1. Start with the project directory as the current directory. Execute Jupyter Notebook by executing the following code in your CLI:
```
python -m notebook
```
2. Navigate to ```notebooks``` and execetute ```lcd_trace.ipynb```.
3. In the second cell block provide the file path to the JIRA set and the SVN commit set. An example is given underneath using example sets.
	* Import raw JIRA data as a pandas dataframe: ```jira_df_raw = pd.read_csv('../data/01_raw/jira_example.csv')```
	* Import raw svn data as a pandas dataframe:
```svn_df_raw = loadCommits('../data/01_raw/svn_example.txt')```

4. Navigate to ```Cell``` and then ```Run all cells```
5. Navigate to ```notebooks``` and execetute ```evaluation_notebook.ipynb```.
6. In the second cell block it is possible to set a number of evaluation rounds (default=2)
7. Navigate to ```Cell``` and then ```Run all cells```

For non-MDD specific features, the following steps shall be taken:
1. Navigate to ```notebooks``` and execetute ```lcd_trace_no_lc_features.ipynb```.
2. Navigate to ```Cell``` and then ```Run all cells```.

For automated feature selection, the following steps shall be taken:
1. Navigate to ```notebooks``` and execetute ```feature_selection.ipynb```.
2. In the second cell block it is possible to set a number of evaluation rounds (default=2) and a project name that is run.

## Paper (under review)
Van Oosten, W., Rasiman, R.S., Dalpiaz, F., & Hurkmans, T. (2022). On the Effectiveness of Automated Tracing Model Changes to Project Issues. [under review]
",2022-08-12
https://github.com/RELabUU/RE-SWOT,"# RE-SWOT
This is a visualization tool that employs NLP and Visualization to enable the competitive analysis from app reviews.
It relies on R/Shiny and Tableau Desktop.

# Step 1: Download or clone this repository

# Step 2: Install the required R packages

Open RStudio and run:
install.packages(c(""DT"", ""shiny"", ""shinydashboard"", ""shinyjs"", ""stringr"", ""udpipe"", ""quanteda"", ""plyr"", ""dplyr"", ""data.table"", ""rjson"", ""httr"", ""tidytext"", ""gtools"", ""V8""))

Optional: For using Google sheets for storing data, you will also need to install ""googlesheets"" package and uncomment the relevant rows on app.R.

# Step 3: Run a Shiny app and upload review CSV
With app.R open, click on ""Run app"" to run a local version of the RE-SWOT Shiny app.

Within the Shiny app, you can upload a CSV with review data following the format specified in the app.
After the upload, data is processed and the visualization data is ready for download.

# Step 4: Use the downloaded visualization data on Tableau
After making sure you have Tableau Desktop installed, open the workbook ReSWOT.twb. 
It comes with a sample csv of data loaded, which you can replace with the data generated on Shiny.
To replace the csv on Tableau, follow these steps:
- Go to Data > Add new data source > Text file
- Select the csv generated by Shiny
- Replace the sample data by the new csv
- Note: You might need to adjust the text qualifier for """" so that all columns are correctly detected

# Optional: Host app on the cloud
You can optionally host this Shiny app and Tableau visualization on the cloud so it can be accessed from anywhere. For that, the following infrastructure was used:
- Tableau Server: https://www.tableau.com/products/server
- Google Sheets: Create a spreadsheet using a Google account and connect to it using the library googlesheets (https://github.com/jennybc/googlesheets/tree/master/inst/shiny-examples) 
- Shiny Server: https://www.rstudio.com/products/shiny/shiny-server/

In this case, a connection to Google Sheets needs to be established on Shiny using googlesheets package, and a connection to Tableau needs to be established so that a spreadsheet is used as a data connection. Besides that, the Tableau workbook should be uploaded to Tableau Server with the Google credentials embedded.
Finally, the visualization hosted on Tableau server can be embedded on Shiny by replacing the url in row 38 of app.R.
",2022-08-12
https://github.com/RELabUU/Requirements-Synthesizer,"# Requirements Synthesizer
The Requirements Synthesizer is created by Niels Wever for his Master Thesis: Synthesizing Creative Requirements with Natural Language Processing. The files that are created for the thesis are AnalyzeRequirements.py and GenerateRequirements.py

Installing the libraries which are used is described below for a Windows machine (tested with Windows 10 Home). If you are using another operating system, please follow the links to get instructions

## Installing the libraries
Several libraries are needed to use the Requirements Synthesizer. The needed libraries to install are [NLTK](https://pypi.org/project/nltk/), [PractNLPTools](https://pypi.org/project/practnlptools/), sense2vec](https://github.com/explosion/sense2vec), [spacy](https://spacy.io/), [lib3to2](https://pypi.org/project/3to2/) and [grammar-check](https://pypi.org/project/grammar-check/).
1. Make sure you have installed Python 2.7, a newer version will not work with PractNLPTools
2. Install [NLTK](https://pypi.org/project/nltk/)
    * Install with pip is possible `pip install nltk`
  
3. Download the [NLTK data](http://www.nltk.org/data.html)
    * With python in command prompt is possible `python -m nltk.downloader all`
    * Or use the Python command line tool:
      ```python
	  >>> import nltk
	  >>> nltk.download()
	  ```
3. Install [PractNLPTools](https://pypi.org/project/practnlptools/)
    * Download repository and unzip
  Move to the folder with command prompt ```cd path\to\folder```
  Install with Python: `python setup.py install`
4. Install [sense2vec](https://github.com/explosion/sense2vec) with [spacy](https://spacy.io/)
    * Using command prompt with pip and python is possible to install:
      ```
	  pip install sense2vec
	  pip install -U spacy
	  python -m spacy download en
	  ```
5. Install [grammar-check](https://pypi.org/project/grammar-check/) and [lib3to2](https://pypi.org/project/3to2/) (needed for grammar-check to use it in Python 2.7):
    * Install with pip:
      ```
	  pip install 3to2
	  pip install grammar-check
	  ```
 
## Needed resources
Several resources are needed to use the Requirements Synthesizer. The resources have to be downloaded manually and stored in a resources folder. This section describes where they come from and where to place them
1. [SemLink](https://verbs.colorado.edu/semlink/) is used to have a mapping of VerbNet to PropBank. Download 1.2.2c.zip and place the unpacked foler in the resources folder. The file used is `resources\1.2.2c\vn-pb\vnpbMappings`
2. requirements.txt should contain the requirements that are used as input to analyze and synthesize requirements. Every line should contain a seperate User Story in Connextra template: As a [user], I want [goal], so that [reason]. As test the User Stories of the [Recycling 101](https://warm-beach-37724.herokuapp.com/) app are used (retrieved from: Dalpiaz, Fabiano (2018), “Requirements data sets (user stories)”, Mendeley Data, v1 http://dx.doi.org/10.17632/7zbk8zsd8y.1).
3. The latest reddit vectors models are used by [sense2vec](https://github.com/explosion/sense2vec) and are attached to every latest [release](https://github.com/explosion/sense2vec/releases). Download these vectors and unpack them in the resources folder. The folder used is `resources\reddit_vectors-1.1.0`
4. [VerbNet](http://verbs.colorado.edu/~mpalmer/projects/verbnet/downloads.html) files are used to get the right sentence structure. Download the files and unpack them, so that you have them saved in `resources\new-vn`

## Using the Requirements Synthesizer
To use the requirements synthesizer you should have installed the needed libraries and stored the needed resources in the resources folder. Next, an empty output folder needs to be created and the user stories that need to be analyzed should be stored into the requirements.txt file.

If all these steps are taken, first the AnalyzeRequirements.py file should be runned. This file generates a parts.p file in the output folder. With this file GenerateRequirements.py can do its work. Two files are created after running GenerateRequirements.py, an eparts.p file and synthesized_requirements.txt file. This last file contains the synthesized requirements.

If new requirements need to be synthesized, the output folder needs to be emptied.
",2022-08-12
https://github.com/RELabUU/revv,"# REVV (Requirement Engineer Validation & Verification) tool

> Visualizes & Allows to Interact with User Stories

This program extracts concepts and relationships from a User Story HTML report file obtained from the visual-narrator-adapted-for-revv tool and converts it into an interactive visualization.

## Installing
For this tool to run you either need a local or hosted web server that supports PHP and has write permissions. Upload the content of this folder to the root of the web server. Open the tool by visiting the url of the root in the web browser.

## Creating a visualization:
1. Use the visual-narrator-adapted-for-revv tool (https://github.com/RELabUU/visual-narrator-adapted-for-revv ) to create an HTML report from a set of user stories.
2. Open the REVV tool in the browser.
3. Select 'Choose a file' and upload the HTML report.
4. Visit the link that is returned by the REVV tool.",2022-08-12
https://github.com/RELabUU/revv-light,"# REVV-Light tool

> Ambiguity and Incompleteness in User Story Requirements

This program extracts concepts and relationships from a User Story HTML report file obtained from the visual-narrator-adapted-for-revv tool and converts it into an interactive visualization that can be utilized for identifying terminological ambiguity and incompleteness in user story requirements.

## Installing
For this tool to run you either need a local or hosted web server that supports PHP and has write permissions. Upload the content of this folder to the root of the web server. Open the tool by visiting the url of the root in the web browser.

## Creating a visualization:
1. Use the visual-narrator-adapted-for-revv tool (https://github.com/RELabUU/visual-narrator-adapted-for-revv) to create an HTML report from a set of user stories.
2. Open the REVV-Light tool in the browser.
3. Select 'Choose a file' and upload the HTML report. Note it may take a few minutes for converting the HTML file into a REVV-Light visualization.
4. Visit the link that is returned by the REVV tool.

## References
This tool has been used in the context of the following paper: Fabiano Dalpiaz, Ivor van der Schalk, Sjaak Brinkkemper, Fatma Başak Aydemir, Garm Lucassen. Detecting Terminological Ambiguity in User Stories: Tool and Experimentation. Information & Software Technology (110), pp 3--16, 2019 [pdf](https://webspace.science.uu.nl/~dalpi001/papers/dalp-scha-brin-ayde-luca-19-ist.pdf)
",2022-08-12
https://github.com/RELabUU/visual-narrator-adapted-for-revv,"# Visual Narrator

> Tells Your User Story Graphically

This program reads a text file (.txt, .csv, etc.) containing User Stories and outputs a Manchester Ontology file. As of yet, each line may only contain a single User Story.

Optionally, statistics about the User Story set can be output.

## Dependencies
The main dependency for the program is its Natural Language Processor (NLP) [spaCy](http://spacy.io/). To run the program, you need:

* _Python_ >= 3.4
* _spaCy_ >= 0.93 (currently under development using v0.98)
* _NumPy_ >= 1.10.1
* _Pandas_ >= 0.17.1
* _Jinja2_ >= 2.8

## Running the Project
Running the program can only be done from the command line. With the program main directory as current directory, run the program by executing:

```
python run.py <INPUT FILE> [<arguments>]
```

#### Arguments
The most important arguments is `INPUT FILE` to specify the location of the text input file. The table below provides an overview of the currently implemented arguments.

##### Positional arguments
Argument | Required? | Description
--------|-----------|------------
`INPUT FILE` | __Yes__ | Specify the file name of the User Story input file


##### _Optional_ arguments

###### General

Argument | Description
--------|------------
`-h`, `--help` | Show a help message and exit
`-n SYSTEM_NAME`, `--name SYSTEM_NAME` | Specify a name for your system
`-u`, `--print_us` | Print additional information per User Story
`-o`, `--print_ont` | Print the output ontology in the terminal
`--prolog` | Output prolog arguments to a _.pl_ file. Combine with `--link` to reason about user stories
`--version` | Display the program's version number and exit

###### Statistics
Argument | Description
--------|------------
`-s`, `--statistics` | Show statistics for the User Story set and output these in .csv files

###### Ontology generation tuning
Argument | Description | Type | Default
--------|-----------|------------|--------
`-p`, `--per_role` | Create an additional conceptual model per role | _N/A_
`-l`, `--link` | Link all ontology classes to their respective User Story for usage in the set analysis | _N/A_
`-t THRESHOLD` | Set the threshold for the selected classes | _FLOAT_ | 1.0
`-b BASE_WEIGHT` | Set the base weight | _INT_ | 1
`-wfr WEIGHT_FUNC_ROLE` | Weight of functional role | _FLOAT_ | 1.0
`-wmo WEIGHT_MAIN_OBJ` | Weight of main object | _FLOAT_ | 1.0
`-wffm WEIGHT_FF_MEANS` | Weight of noun in free form means | _FLOAT_ | 0.7
`-wffe WEIGHT_FF_ENDS` | Weight of noun in free form ends | _FLOAT_ | 0.5
`-wcompound WEIGHT_COMPOUND` | Weight of nouns in compound compared to head | _FLOAT_ | 0.66

### Example usage

```
python run.py example_stories.txt -n ""TicketSystem"" -u
```

## Conceptual Model
The classes in the program are based on the following conceptual model:

![conceptual_model](https://cloud.githubusercontent.com/assets/1345476/12152551/a6b7dca0-b4b5-11e5-8cee-80f463588df2.png)

The `Reader` starts by reading the input file line by line and generates a list of sentences. These sentences are then enriched using Natural Language Processing, adding Part-of-Speech tags, dependencies, named entity recognition, etc. Subsequently, the `StoryMiner` uses these enriched sentences to create _UserStory_ objects. The User Story objects contain all the information that could be mined from the sentence. These are then used to attach weight to each term in the User Story, creating _Term-by-US Matrix_ in the `Matrix` class. The `Constructor` then constructs patterns out of each user story, using the _Term-by-US Matrix_ to attach a weight to each token. The Constructor forms a model for an ontology, which is then used by the `Generator` to generate a Manchester Ontology file (.omn) and optionally a Prolog file (.pl). Finally, these files are printed to an actual file by the `Writer` in the '/ontologies' and '/prolog' folders respectively.
",2022-08-12
https://github.com/RELabUU/VisualNarrator,"# Visual Narrator

> Tells Your User Story Graphically

This program turns user stories into a conceptual model containing entities and relationships.

#### Input
* __Text file__ (.txt, .csv, etc.) containing _one user story per line_

#### Output
* __Report__ of user story parsing, and conceptual model creation
* __Manchester Ontology__ (.omn) describing the conceptual model
* (Optional) __Prolog__ (.pl) arguments
* (Optional) __JSON__ (.json) of the user stories parts' information
* (Optional) __Statistics__ about the user stories
* Returns the mined user stories, ontology, prolog and matrix of weights, which can be used by other tools

## Publications
Two scientific papers were published on Visual Narrator:
* M. Robeer, G. Lucassen, J. M. E. M. van Der Werf, F. Dalpiaz, and S. Brinkkemper (2016). Automated Extraction of Conceptual Models from User Stories via NLP. In _2016 IEEE 24th International Requirements Engineering (_RE_) Conference_ (pp. 196-205). IEEE. \[[pdf](https://www.staff.science.uu.nl/~dalpi001/papers/robe-luca-werf-dalp-brin-16-re.pdf)\]
* G. Lucassen, M. Robeer, F. Dalpiaz, J. M. E. M. van der Werf, and S. Brinkkemper (2017). Extracting conceptual models from user stories with Visual Narrator. _Requirements Engineering_. \[[url](https://link.springer.com/article/10.1007/s00766-017-0270-1)\]

## Dependencies
The main dependency for the program is its Natural Language Processor (NLP) [spaCy](http://spacy.io/). To run the program, you need:

* _Python_ >= 3.6 (under development using 3.7.3)
* _spaCy_ >= 2.1.2 (under development using 2.1.4; language model 'en_core_web_md')
* _NumPy_ >= 1.16.2
* _Pandas_ >= 0.24.2
* _Jinja2_ >= 2.10

## Running the Project
Running the program can be done in two ways: (1) from the command line, (2) using method `VisualNarrator().run()`.

### (1) Command line
With the program main directory as current directory, run the program by executing:

```bash
python run.py <INPUT FILE> [<arguments>]
```

### (2) Method VisualNarrator().run()
Import the `VisualNarrator` class from `vn.vn` and run `VisualNarrator().run()`:

```python
from vn.vn import VisualNarrator

visualnarrator = VisualNarrator(<ARGUMENTS>)
visualnarrator.run(<INPUT FILE>, <SYSTEM_NAME>)
```

Arguments may be supplied to `VisualNarrator(**args)` to re-use and for a single run to `run(*args, **kwargs)`. Execute `help(visualnarrator)` to see all (optional) arguments.

#### Arguments
For details on arguments, see our [documentation here](vn/documentation.md).

### Example usage

Command line:
```bash
python run.py example_stories.txt -n ""TicketSystem"" -u --json
```

From method:
```python
from vn.vn import VisualNarrator

visualnarrator = VisualNarrator(json = True)
visualnarrator.run(""example_stories.txt"", ""TicketSystem"", print_us = True)
```

## Conceptual Model
The classes in the program are based on the following conceptual model:

![conceptual_model](https://cloud.githubusercontent.com/assets/1345476/12152551/a6b7dca0-b4b5-11e5-8cee-80f463588df2.png)

The `Reader` starts by reading the input file line by line and generates a list of sentences. These sentences are then enriched using Natural Language Processing, adding Part-of-Speech tags, dependencies, named entity recognition, etc. Subsequently, the `StoryMiner` uses these enriched sentences to create _UserStory_ objects. The User Story objects contain all the information that could be mined from the sentence. These are then used to attach weight to each term in the User Story, creating _Term-by-US Matrix_ in the `Matrix` class. The `Constructor` then constructs patterns out of each user story, using the _Term-by-US Matrix_ to attach a weight to each token. The Constructor forms a model for an ontology, which is then used by the `Generator` to generate a Manchester Ontology file (.omn) and optionally a Prolog file (.pl). Finally, these files are printed to an actual file by the `Writer` in the '/ontologies' and '/prolog' folders respectively.

## Visual Narrator is part of the _GRIMM_ method, also see
- AQUSA ([http://aqusa.nl/](http://aqusa.nl/))
- Interactive Narrator ([https://interactivenarrator.science.uu.nl/](https://interactivenarrator.science.uu.nl/))
",2022-08-12
https://github.com/Rensvandeschoot/First-Bayesian-Inference,"# First-Bayesian-Inference
## :star: Purpose
This Shiny App is designed to ease its users first contact with Bayesian statistical inference, investigate the effect of different prior distributions on the posterior result, and understand prior-data conflict. By ""pointing and clicking"", the user can analyze the IQ-example that has been used in the easy-to-go introduction to Bayesian inference of [van de Schoot et al. (2013)](https://doi.org/10.1111/cdev.12169). Different prior distributions can be specified, and data with different characteristics can be simulated on the fly. 

## :gem: How can you profit from it?
First of all, this app might be a useful tool for your teaching if you would like to familiarize your students with the basic logic of Bayesian inference, see also the [exercise](https://github.com/Rensvandeschoot/First-Bayesian-Inference/blob/main/EXERCISE.md) we created. Second, feel free to use this material as a template for your own app. 


## Installation

Download the R-files, open R-studio, install the R-packages and [JAGS](https://mcmc-jags.sourceforge.io/), and run the app.

The Shiny app also runs at a server of [Utrecht University](https://www.rensvandeschoot.com/tutorials/fbi-the-app/). 


[![Overview of FBI Shiny App](fbi_overview.png)](https://utrecht-university.shinyapps.io/bayesian_estimation/)


## Usage

Step 1: Open the Shiny App.

Step 2: Choose a type of distribution (i.e., uniform, truncated Normal) for the prior and fill in values for the hyperparameters.

Step 3: Generate data.

Step 4: Let the software (analytically or via sampling using RJags) generate the posterior distribution.



## Reference

Van de Schoot, R., Kaplan, D., Denissen, J., Asendorpf, J. B., Neyer, F. J., & Aken, M. A. (2014). A gentle introduction to Bayesian analysis: applications to developmental research. Child development, 85(3), 842-860. [DOI: 10.1111/cdev.12169](https://doi.org/10.1111/cdev.12169).

## Contact

For more information about the App, contact [Lion Behrens](https://www.linkedin.com/in/lion-behrens-7173ab102/), [Sonja Winter](https://www.linkedin.com/in/sonjawinter/), or [Rens van de Schoot](https://www.linkedin.com/in/rensvandeschoot/)
",2022-08-12
https://github.com/Rensvandeschoot/Tutorials,"# Tutorials

This repository contains the underlying code behind many different stats training tutorials.

# Usage

The most user-friendly way of working with the tutorials is to navigate to the website https://www.rensvandeschoot.com/tutorials/. 
We are continuously improving the tutorials so let me know if you discover mistakes, or if you have additional resources I can refer to. 


# Citation

Citable versions of the tutorials are available at [Zenodo](https://zenodo.org/communities/stats_training). 


# Contact

If you want to be the first to be informed about updates, follow me on [Twitter](https://twitter.com/RensvdSchoot) or [LinkedIn](https://www.linkedin.com/in/rensvandeschoot/).
",2022-08-12
https://github.com/rhoitink/rhoitink,"### Hi there 👋

- 🥼 Currently: PhD in experimental physics within the [Soft Condensed Matter](https://colloid.nl) group at [UU](https://uu.nl)
- 💻 Experience with Python, C/C++, HTML, CSS
- 🏫 MSc in Nanomaterials Science and BSc in Chemistry
- 🌍 Visit my [UU profile page](https://uu.nl/staff/LDHoitink)
",2022-08-12
https://github.com/rhoitink/slstools,"# SLS python helpers

Opiniated set of helper functions to handle measurments from SCM's self-built static light scattering (SLS) setup and fit models to it.

## Info

Author: Roy Hoitink <L.D.Hoitink@uu.nl>

## Installation

Installation of this package can be done via pip. 
To install the newest version of this package, run the following command:

```bash
pip install git+https://github.com/rhoitink/slstools --upgrade
```

This ensures that the package and its dependencies are installed.

## Documentation

The documentation can be found at [rhoitink.github.io/slstools](https://rhoitink.github.io/slstools).

## Example usage

Several helper classes are available for loading experiments (`Experiment` class), creating models (`Model` class) and fitting a model to an experiment (`Fit` class). 
Below an example is given on how to fit a model to an experiment.

```python
# import the helpers and libaries
from slstools import Experiment, Model, Fit
import matplotlib.pyplot as plt

# load the experiment that is saved in the file 'Sample01.sls'
experiment = Experiment(""Sample01.sls"", K_unit=""nm"")

# correct the experimental data for scattered reflection, optional
experiment.correct_for_reflection()

# Create an initial model of 1000nm (diameter) spheres with a polydispersity of 5%
# Refractive index medium: 1.333 (water) and particle: 1.4345 (n-hexadecane)
# Make sure the given diameter rougly matches the expected diameter, as this will improve the fitting (and its speed)
model = Model(d=1000, pd=5, n_p=1.4345, n_m=1.333)

# Load both model and experiment into the Fit class
# Fitting will occur for scattering angles between 45 and 110 degrees
fit = Fit(experiment, model, fit_theta_bounds=(45.0, 110.0),model_kwargs=dict(K_unit=experiment.K_unit))

# Obtain the optimal model after fitting
optimal_model = fit.fit()

# plot the experimental data and the fit
# scale the optimal_model with the found prefactor such that the lines overlap
plt.plot(optimal_model.K, fit.parameters[""prefactor""]*optimal_model.intensity, label=f""Fit: d={optimal_model.diameter:.0f}nm ({optimal_model.polydispersity:.0f}%)"")

# plot experimental data as comparison
plt.plot(experiment.K, experiment.intensity, 'k.', alpha=.3, label=""Experimental data"")

# setting labels, scales and legend
plt.xlabel(r""K (%s$^{-1}$)"" % optimal_model.K_unit)
plt.ylabel(""Normalised intensity (a.u.)"")
plt.yscale(""log"")
plt.legend()
plt.show()
```
",2022-08-12
https://github.com/RubenSchalk/Apprentices,"# Apprentices
Public apprenticeship data
",2022-08-12
https://github.com/RubenSchalk/grlc-test,"
Repository for SPARQL queries on different LOD endpoints

Execute these queries via <a href=""http://grlc.io"" target=""_blank"">grlc.io</a> using this URL specification: http://grlc.io/api/RubenSchalk/grlc-test  
Make sure to use the endpoint specified in the query.




",2022-08-12
https://github.com/RubenSchalk/marctolod,"# Introduction

Here we document the progress of converting Marc21 files from the Utrecht University Library pamphlet collection (n=50) to Linked Open Data

",2022-08-12
https://github.com/SanliFaez/Commons4Academics,"# Commons for Academics Workshop

*** Consider Academia as a commons, how should we govern like a commoners?***

## Welcome!

Thank you for visiting the Commons4Academics workshop repository. We are organising a toolbox for administering an academic section as a commons and would like to present them in a workshop.

This document (the README file) is a hub to give you some information about the project. Jump straight to one of the sections below, or just scroll down to find out more.

## Vision

We collaborate with experts on the theory of commons to organize a training workshop for fellow academics to enable them to organize their resources and manage them in form of a commons. We work openly because we observe the need for such training and hope to make it widely accessible. 

Our cultural goal is to convince colleagues that commons is a good formation for organization and governance of academic resources. Our ambition is to organise this project as a logical series of tasks and guidelines suitable for adaptation, revision, and reproduction for follow-up events.

* [What are we doing? (And why?)](#what-are-we-doing)
* [Who are we?](#who-are-we)
* [What do we need?](#what-do-we-need)
* [How can you get involved?](#get-involved)
* [Get in touch](#contact-us)
* [Find out more](#find-out-more)
* [Understand the jargon](#glossary)


## What are we doing?

### The problem

Sharing resources is one of the foundations of university education and development. For example, the curriculum, the literature, and the scientific method in the most general sense all form common resources that are transferred from generation to generation. These common resources are so close to us that we sometimes do not see them, and do not realise that they should be properly organised. Meanwhile, some of these resources have been subject to forces of privatisation and marketisation. It is therefore an imperative of modern academia to organise its common resources in such a way that they can stay accessible to ourselves and to future generations. Recent activities on open science could be seen as important steps to achieve this goal.

Most academics are not aware of robust structures that can be employed for organising a sustainable and flourishing commons. (Yes! Commons is singular. See the [definition of commons](#definition-of-commons).)

### The Solution

The main goal of this project is to raise awareness about the notion of the commons and its potential for developing open science, both internationally and inside our university. To reach this goal, our first step is organising a workshop together with academics and experts who have done research into commons and its governance.

The main focus of this workshop is to bring these parties together, and organise a workshop for academics who would like to learn more about organising common academic resources in the form of commons, and start a concrete pilot project based on this workshop. Examples for such projects are:
- How to best plan the library budget of a department on subscriptions and books
- How to organise teaching material to achieve best coherence in a program
- How to share research protocols and other best practices to avoid research waste and promote reproducibility

## Who are we?

Initiators of this project are Annemarie Kalis and Sanli Faez, with the support of Utrecht Young Academy and the Open Science Community Utrecht. We are collaborating with Socrates Schoutens and the Waag Society in organising our first workshop

## What do we need?

- Examples of real-life resources in the university or academia in general that can be governed more efficiently 
- Enthusiastic people who want to think big with us and also take the small steps to make actual change happen

## Definition of commons

The Digital Library of the Commons defines ""commons"" as ""a general term for shared resources in which each stakeholder has an equal interest"". To learn more, you can start by reading the [wikipedia entry on commons](https://en.wikipedia.org/wiki/Commons).
",2022-08-12
https://github.com/SanliFaez/FAIR-Battery,"# The FAIR-Battery project

[![Join the chat at https://gitter.im/FAIR-Battery/community](https://badges.gitter.im/FAIR-Battery/community.svg)](https://gitter.im/FAIR-Battery/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)


Hardware, design, and a learning community for building an open-hardware redox-flow battery from the [Center for Unusual Collaborations](https://www.unusualcollaborations.com/)

![FAIR-Battery logo](https://github.com/SanliFaez/FAIR-Battery/blob/main/docs/_static/fair-battery_logo.png)

In the _FAIR-Battery_ project, we aim to create an open-source electrochemical battery (FAIR = Findable + Accessible + Interoperable + Reproducible).
We seek to present an open-hardware platform for a versatile battery technology and make the platform radically accessible:
1- by deliberately using low cost and locally available materials suitable for local user groups, and
2- by setting up the education communities on top of the open-hardware design.

On this route, we thrive to not only provides the necessary technical details for engineering and production, but also incorporates the local constraints for actually adopting and using the technology.
These constraints relate to language, availability of materials and expertise, maintenance capacity, or other locally varying conditions, which must be identified as part of the project.
Our envisioned FAIR-Battery platform will track and seek to remove these constraints in each stage of the development by direct consultation with the user-groups.


Table of contents:

- [About the project](#about-the-project)
- [The team](#the-team)
- [Contributing](#contributing)
- [Get in touch](#get-in-touch)
- [Project pillars](#project-pillars)


## About the project

## The team

### Founding members

This project is initiated by:

- [Sanli Faez](sanlifaez.github.io/) - Utrecht University
- [Antoni Forner-Cuenca](https://www.fornercuencaresearch.com/) - Technical University of Eindhoven
- [Peter Ngene](https://www.uu.nl/staff/PNgene) - Utrecht University
- [Maarten Voors](https://www.wur.nl/nl/Personen/Maarten-dr.ir.-MJ-Maarten-Voors.htm) - Wageningen University
- [Yali Tang](https://www.tue.nl/en/research/researchers/yali-tang/) - Technical University of Eindhoven
- [Stephanie Hobbis](https://stephaniehobbis.com/) - Wageningen University

### Active students

The following students are currently contributing to the project

- Catherine Doherty - University College Utrecht
- [Nicolas Barker](https://github.com/Cinbarker) - Delft University of Technology
- Emre Burak Boz - Technical University of Eindhoven

### Funding
FAIR-Battery is supported by a SPARK grant from the center for unusual collaborations.

## Contributing

:construction: This repository is always a work in progress and **everyone** is encouraged to help us build something that is useful to the many. :construction:

We are currently setting up the on-boarding instructions for persistent contributors.

The github issues and pull-request functions are currently _not_ actively used for updates.
These will be incorporate into the development procedures at a later stage

This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification.
Contributions of any kind are welcome!

## Get in touch

If you wish to learn more about the project and/or join our learning community, drop an email to [Sanli Faez](mailto:s.faez@uu.nl)

## Project pillars

In our vision, to create a truly accessible FAIR-Battery, we need to form a community, at the same time that we collect and share the technical knowledge necessary for making and maintaining an operational device.
Therefore, we are building our activities on three pillars

### 1- Learning together

In this project, we will bring anthropologists, engineers, chemists,
 economists, and potential local users together to identify the barriers to developing a truly FAIR-battery and envision the first steps to removing some of these barriers in a follow-up project.
In particular, we look for the answers to these questions:
- What range of energy storage capacities are required for the development of typical user-groups and at what cost?
- Which battery technologies can potentially address these demands?
- Are the materials and technologies required for adopting the identifies technologies available in the identifies user-groups? If not, which adjustments are needed?
- What is the missing know-how and expertise for kick-starting the local development of pilot projects?

For investigating these questions, together with an inclusive community, we will develop:
- A. a starters' kit (hardware and software)
- B. a lecture series for self-study


### 2- Battery Hardware

Our aim is to publish the blueprints for an operational open-source battery by July 2022.
We will regularly report on the devices that we are using for intermediate steps, such as testing and material selection, on this repository.

### 3- Testing and Maintenance

A standard procedure for testing FAIR-Batteries is currently being developed. The main purpose of this standardization is to make
results from different contributing groups easy to compare. In order to achieve this, both software and hardware has been developed to fit the testing needs of redox-flow batteries.

_Software for Battery Testing_

We have developed an open-srouce python code that works on any major computer platform.
You can check the installation guide and further documentation on [readthedocs](https://fair-battery.readthedocs.io/en/latest/index.html).

The software is a fork of the Labphew project: https://github.com/SanliFaez/labphew.

### Hardware for Battery Testing

The battery testing software is currently designed around specific hardware, namely a Digilent Analog Discovery 2 (AD2),
a custom charging and discharging circuit, and for impedance spectroscopy, the Analog Discovery Impedance Analyzer
Module, an extension compatible with the AD2. 

",2022-08-12
https://github.com/SanliFaez/labphew,"
# Welcome to labphew!

![labphew logo](https://github.com/SanliFaez/labphew/blob/master/docs/source/_static/labphew_logo.png)

*labphew* is a minimalist code and folder structure for teaching computer-controlled lab instrumentation in Python. The main purpose of Labphew is to provide a basis for those with little coding experience to build their own user-interface(s) for a piece of hardware or to control a measurement and save their date in a properly reproducible manner.

### Python for the Lab

This project is heavily inspired by the instruction exercise written by [Dr. Aquiles Carattino](https://www.aquicarattino.com), the mastermind behind [Python for the Lab](https://www.pythonforthelab.com/). If you want to learn more (serious!) coding for lab automation with Python, check the excellent [PFTL website](https://www.pythonforthelab.com/) or book him for a course.

Python for the Lab (PFTL) is a code architecture and a programming course for computer-controlled instrumentation. PFTL codes are designed following the MVC design pattern, splitting the code into ""controller""s for defining drivers, ""model""s for specifying the logic of the experiment, and ""view""s for parking the GUI.
PFTL was developed to explain to researchers, through simple examples, what can be achieved quickly with little programming experience. The ultimate goal of this project is to serve as a reference place for people interested in instrumentation written in Python.

## The labphew package

Currently, the labphew package contains more wishes than executable pieces. However you can use its framework as a (hopefully convenient) starting point to write your own code. Please note that this package is published under [GNU General Public License v3.0](https://choosealicense.com/licenses/gpl-3.0/). You can read all the available instruction on [labphew documentation](https://labphew.readthedocs.io/en/latest/)

## Hello world with mouse clicks

They are two recommended ways of installing labphew and using it out of the box:

### Installation from the Python package index

You need to have [pip](https://pypi.org/project/pip/) installed.

If affirmative, you can use:

    pip install labphew

### Installation from source

Building the labphew dependencies are tested on Windows and Mac PCs. It should be possible to install also on linux but we have not tested it yet.

    git clone https://github.com/sanlifaez/labphew.git
    cd labphew
    pip install .

If you want to start editing or adding to the code, we recommend that you fork the repository first to your own account and install it from there. This way of installation allows you to stay connected with the  labphew repository and when needed, rebase to future releases.

### Owning it

If you are ready to be more engaged and adapt labphew to control your own favorite setup, please read
[how to labphew](https://labphew.readthedocs.io/en/latest/howtolabphew.html).

## How to contribute

The labphew roadmap is still incomplete and actively discussed between its maintainers, @AronOpheij and @sanlifaez. If your interests are aligned with the main goals of this project and you want to get involved, you can send a few lines to Sanli.

",2022-08-12
https://github.com/SanliFaez/meetingbot,"# meetingbot

The meetingbot is a simple agenda-setter for regular (weekly) group meetings. It collects content from personal folders on a shared repository. Its main purpose is to nudge all group members to come prepared to the group meeting and share their achievements and challenges. The structure of the meeting was designed collectively by the early career researchers at the nanoLINX group at Utrecht University, who inspired Sanli to write the first version of the bot. 

## Folder structure
Each lab member has a personal (to avoid merge conflicts) folder with this structure

..
Admin  
Membername  
|  
|--HiLoPlan  
   |--Membername_hilo_2019_01.md  
|  
|--Publications  
|--Reports  

It is important that the files in the HiLoPlan folder follow to the standard filenames. Inside this folder each file contains: 

* Highlight: _{which achievement of the last week was most satisfying to you, in a couple of sentences}_. 
* Lowlight: _{which issue bothered you most or blocked you from exercising your plan}_. 
* Plan:  _{things in your to do list for the next week?}_. 

Subject to not breaking the bot, members can insert whatever they like in the meeting agenda by editing their hilo files.


## Execution

Set the correct week in the code and run `.\Admin\MeetingBot.py` to create the first draft. The notes will be amended and edited during the meeting. To avoid overwriting the report by accidentally running the bot again, it is recommended to change the filename right before or after the meeting or to move it to another folder for future reference.

",2022-08-12
https://github.com/SanliFaez/MoreIsDifferent,"# MoreIsDifferent

_More Is Different_ is a collaborative project for preparing interactive computer simulations/calculations to demonstrate concepts in Condensed Matter Physics at the undergraduate and graduate level. 

This repository is initiated by Sanli Faez (Utrecht University) for teh Course ""Nature of Matter"" NS-352B. This project is currently not maintained.

",2022-08-12
https://github.com/sara-vanerp/bayesreg,"# bayesreg

[![DOI](https://zenodo.org/badge/154002826.svg)](https://zenodo.org/badge/latestdoi/154002826)

An R package to perform Bayesian regularization using Stan. Models are pre-compiled and run using the probabilistic programming language [Stan](http://mc-stan.org). Currently only supports Bayesian regularization for linear regression models. Multiple shrinkage priors are implemented that will shrink small regression coefficients towards zero. The available options are based on the overview in the preprint [Shrinkage priors for Bayesian penalized regression](https://osf.io/4gr6z/). Note that only the full Bayesian approach is available to estimate the penalty parameter &lambda;.

## Getting Started

These instructions will install the package to your computer.

### Prerequisites

In order to install the package directly from Github, you need to have the **devtools** package:

```
install.packages(""devtools"")
```

Moreover, it might be needed to install a C++ toolchain, such as Rtools, by following these [instructions](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started).

### Installing

To install the package from Github, first load **devtools**:

```
library(devtools)
```

Next, install **bayesreg** and make sure that `local = TRUE` to avoid recompilation of the Stan models. On Mac, the version of the package in the master branch can be installed as follows:

```
devtools::install_github(""sara-vanerp/bayesreg"", local = TRUE)
```
For Windows, the version of the package should be installed that is available in the Windows branch (thanks to Duco Veen for making this happen), as follows:

```
devtools::install_github(""sara-vanerp/bayesreg@windows"", local = TRUE)
```

The package can now be loaded into R and used:

```
library(bayesreg)
```

## Example

To provide an example of the **bayesreg** package, we will first simulate some data from a linear regression model:

```
library(MASS) # call install.packages(""MASS"") first if not yet installed
set.seed(31102018)
sim.lm <- function(nobs, cfs, sd, cov.mat){
  nvar <- length(cfs)  
  
  beta <- as.matrix(cfs)
  X <- mvrnorm(nobs, mu=rep(0, nvar), Sigma=cov.mat)
  y <- X %*% beta + rnorm(nobs,0,sd)
  
  return (list(x=X, y=y, coefficients=cfs, var=sd^2, covX=cov.mat))
}

dat <- sim.lm(nobs = 240, cfs = c(3, 1.5, 0, 0, 2, 0, 0, 0), sd = 3, cov.mat = diag(8))
```

The function `stan_reg_lm` is used to fit the model:

```
fit <- stan_reg_lm(X = dat$x, y = dat$y, N_train = 40, prior = ""lasso"")
```

Note that this function uses the **rstan** MCMC defaults. These can be adapted through the regular **rstan** arguments. For example, the number of iterations can be increased as follows:

```
fit <- stan_reg_lm(X = dat$x, y = dat$y, N_train = 40, prior = ""lasso"", iter = 4000)
```

See the **stan** function in the [**rstan**](https://cran.r-project.org/web/packages/rstan/rstan.pdf) documentation for options and details.

Currently, missing data is not supported. However, it is possible to impute the data before fitting the model, for example using the **mice** package. The model can then be fitted to each of the imputed data sets sepately and the resulting posterior draws can be combined.

The `stan_reg_lm` function returns an object of class `stanfit`, so the results can be extracted and plotted as usual `stanfit` objects. The **bayesreg** package has several post-estimation functions specific to Bayesian penalized regression.
First, we can request a summary of the results, including the 95% credibility intervals:

```
summary_lm(fit, CI = 0.95)
```

The prediction mean squared error (PMSE) on the test set is obtained as follows:

```
pmse_lm(fit, y = dat$y, N_train = 40)
```

We can perform variable selection based on the credibility interval criterion, excluding predictors for which the marginal credibility interval covers 0:

```
select_lm(fit, X = dat$x, prob = 0.90)
```

Finally, a plot function is included to plot point estimates and credibility intervals for various fitobjects obtained with the `stan_reg_lm` function. To use this function, combine different `stanfit` objects, for example using different priors or hyperparameter settings, in a named list:

```
fitlist <- list(fit1, fit2)
names(fitlist) <- c(""fit1"", ""fit2"")
plots <- plot_est(fitlist, est = ""mean"", CI = 0.95)
plots[[1]]
``` 

See also the documentation for the different functions, using e.g., `?select_lm`.

",2022-08-12
https://github.com/sara-vanerp/bayesregsem,# bayesregsem,2022-08-12
https://github.com/sara-vanerp/BSEMworkshop,"# BSEMworkshop
A repository for the workshop ""Bayesian Structural Equation Modeling""
",2022-08-12
https://github.com/sara-vanerp/shinyBSEM,"# shinyBSEM
This Shiny app enables researchers to visualize structural equation models (SEMs) and use this visualization to easily specify prior distributions and subsequently run a Bayesian analysis using blavaan. Currently, this app is still under development, so please send bugs or comments to [Sara van Erp](mailto:sara.vanerp@gmail.com).
",2022-08-12
https://github.com/sara-vanerp/shinyHorseshoe,"# shinyHorseshoe
A shiny app to visualise various regularised horseshoe specifications
",2022-08-12
https://github.com/sara-vanerp/shinySEM,"# shinySEM
A Shiny app to visualize structural equation models based on lavaan syntax and get information about the different types of parameters.

A link to the app can be found [here](https://utrecht-university.shinyapps.io/shinySEM/).
",2022-08-12
https://github.com/sergioespana/openAggre,"# openAggre
An open-source authoring tool for aggregated ethical, social and environmental accounting reports.

This project is steered by University of Utrecht. We welcome collaboration with organisations and individuals.

# Ubuntu 20.04
- git clone https://github.com/sergioespana/openAggre.git
# Mariadb
- apt install mariadb-server
- apt install mariadb-client
- apt install libmariadbclient-dev

- systemctl start mariadb.service
- mysql_secure_installation
- systemctl restart mariadb.service
# Python
- apt install python3
- apt install python3-pip
- pip3 install streamlit
- pip3 install awesome_streamlit
- pip3 install gspread
- pip3 install oauth2client
- pip3 install mariadb
- pip3 install plotly
- pip3 install bokeh
- pip3 install psutil
",2022-08-12
https://github.com/sergioespana/openBest,"# openBest
A model-driven repository of best practices. It can be configured to store best practices for different domains; e.g. organisational sustainability improvement, refugee integration initiatives, information security, research.

This project is steered by University of Utrecht. We welcome collaboration with organisations and individuals.

## Prerequisites

The following is definitely required for running the project. 

Install [Node and npm](https://www.npmjs.com/get-npm) on your machine so that you can use the required packages for the project

Install the Firebase command line tools
```bash
npm install firebase-tools -g
```

## Troubleshooting
We haven't tested if the following installs are required when this project is pulled, but in any case there's no danger in updating these packages frequently. These packages are installed previously and may or may not be required to be installed again for every new project pull:

Install Firebase functions
```bash
npm install firebase-functions
```

Install Firebase admin
```bash
npm install firebase-admin
```


## Running the tool

You can test the tool by spinning up a local web server using

```bash
firebase serve
```

## Deploying
The tool can be deployed using

```bash
firebase deploy
```

after which the tool will be available at a public domain


## Callable functions
Certain functions cannot be executed client-side. We use [Cloud Functions](https://firebase.google.com/docs/hosting/functions#direct_hosting_requests_to_your_function) connected to Firebase hosting for executing scripts server-side. These functions are deployed on Firebase and can be called from our web-app. We use Cloud Functions on Firebase Hosting to overcome any CORS-related problems. Functions can be added to index.js in the functions folder and then called from any JavaScript file. 

It may be required to install and initialize Firebase functions when pulling this project. More info can be found in Google's documentation following the previous link or by looking at the Troubleshooting section above. 
",2022-08-12
https://github.com/sergioespana/openESEA,"openESEA is a model-driven ethical, social and environmental accounting tool. 

This project is steered by University of Utrecht. We welcome collaboration with organisations and individuals.

If you want to cite this tool, please also cite this scientific paper:
S. España, N. Bik and S. Overbeek (2019) Model-driven engineering support for social and environmental accounting. 13th International Conference on Research Challenges in Information Science (RCIS 2019), IEEE, pp. 1-12, https://doi.org/10.1109/RCIS.2019.8877042

The tool has a front end and a back end. Each folder has a readme file that explains the installation procedure.

At this very moment, there are two branches of this tool:
(i) The branch found in https://github.com/sergioespana/openESEA is developed in Javascript and Firestore. It is the original first version of the tool and it includes features to manage networks and organisations, load textual specifications of ESEA methods and interpret them in runtime (e.g. deploying stakeholder surveys asking questions relater to an organisation's non-financial performance, calculating indicators that represent such performance better, generating infographics that summarise the results, checking the compliance of the results with certification rules to determine whether the organisation achieves the certification). The textual models are compliant with the openESEA domain-specific language.
(ii) This branch, found in https://github.com/sergioespana/openESEA is developed in Django/Python. It has been developed by Competa IT. The architecture is more secure and scalable, but it has less features than earlier versions. As an advantage, it allows specifying the ESEA methods with usable user interfaces, and the stakeholder surveys are deployed within the tool, without the need of LimeSurvey. As disadvantages, it does not allow uploading textual models and it has no infographic generation or certification rule checking.

We are currently in the process of re-implementing the features of the earlier versions into this later version, so as to have the best of each branch.


If you want to collaborate in this project, please contact us.

Some of the current and past project members:
Sergio España, Assistant Professor in Utrecht University, is the project leader and manager.
Vijanti Ramautar, doctoral researcher in Utrecht University is the product owner. 
Sietse Overbeek, Assistant Professor in Utrecht University, has supervised the development of some features of the tool.
Niels Bik developed the first version, including features to register organisations and networks, specify basic method components such as topics and indicators, and interpret the method specifications in runtime; his code is kept in a separate github repository https://github.com/nielsrowinbik/open-sea
Henny Kruiper developed a feature to specify and deliver stakeholder surveys; his code is kept in a separate github repository https://github.com/nielsrowinbik/open-sea
Tijmen Derikx developed a feature to specify and automatically generate infographics that present the account results; ; his code is kept in a separate github repository https://github.com/nielsrowinbik/open-sea
Diederik van Rijen has merged branches (i) and (ii) onto a new version of the tool, retaining most of the features from each branch and improving the usability. 
Jelle Verschragen (user interface designer), Tjeerd Verschragen (user interface developer), Tino Trok (backend developer), Matthew Kunkeler (business manager), Michiel Auerbach (project manager), Andy Haxby (Director at Competa), all contributed to the development of the tool while working for Competa IT https://competa.com
Sara Martín, Yulie Anneria Sinaga and Gudrun Thorsteinsdottir contributed theoretical knowledge on ESEA methods that is valuable for the tool development and valorisation.
Ties van Dijk and Artur Moeijes have extended openESEA with features to audit accounts.

If we are forgetting you, please contact us and we will update this list ;-)
",2022-08-12
https://github.com/sergioespana/openMAsses,"# openMAsses
Open source materiality assessment tool.

This project is steered by University of Utrecht. We welcome collaboration with organisations and individuals.
",2022-08-12
https://github.com/ShNadi/news_scrape,"﻿
# News scraper

Project aims to scrape all news articles from the two online only news sources 'Nu.nl' and 'GeenStijl.nl'. Online news will be collected on a daily basis from websites' sitemaps. Using scrapy-deltafetch middelware in python we ignore requests to the pages containing items seen in previous crawls of the same spider which leads to producing a delta crawl containing only new items. 

Sample URLs: https://www.geenstijl.nl/, https://www.nu.nl/

## Researcher & engineers

Data Engineers:
- Shiva Nadi
- Martine de Vos

Researcher:
- Frank van Tubergen

Project Manager:
- Laurence Frank


## Installation

This project requires:
  - Python 3.7 or higher
  - MySQL 8.0.20 or higher
  - Install the dependencies with the code below

  ```sh
  pip install -r requirements.txt
  ```

## Example usage 

``` sh
scrapy crawl geenstijl
scrapy crawl nu

```

## Links 

- https://www.geenstijl.nl/
- https://www.nu.nl/

## Specifications
Scraper is expected to return the following keys:




```python
%%html
<style> 
table td, table th, table tr {text-align:left !important;}
</style>
```


<style> 
table td, table th, table tr {text-align:left !important;}
</style>





| Key | Data type|Description |Example|
| --- | --- |--- | --- |
|id| string | The unique id of articles |a5154933|
|title|string |Title of the article|RIVM UPDATE: Deze week +4013 besmettingen|
|teaser|string|A short paragraph between title and text|Aantal nieuwe besmettingen STABILISEERT|
|text|string| The full text of the document|U mag kiezen:Optie 1:...|
|category|string| News section if any| null|
|created_at|datetime object |Date and time of scraping|2020-08-19 16:39:35|
|image|string | Dictionary of the image urls|{0: ''https://image.gscdn.nl/image/5f8b9b2526_Schermafbeelding... |
|reactions|string |Number of reactions to each article|308 reacties|
|author|string |Author|@Ronaldo|
|publication_time|string | Time of publication|14:20|
|publication_date|string |Date of publication, format: dd-mm-yy|18-08-20|
|doctype	|string | Source of the news| geenstijl.nl|
|url|string |URL to the article|https://www.geenstijl.nl/5154933/rivm-update-deze-week-4013-besmettingen/|
|tags|string |List of tags|corona, rivm|
|sitemap_url|string |Link to the site's sitemap if any|https://www.geenstijl.nl/sitemap.xml|



```python

```
",2022-08-12
https://github.com/ShNadi/okcupid,"# OkCupid Stylometry Analysis Project

Version 0.1.0
# OkCupid classifier

This project aims to examine to what extent educational background can be inferred from the written text, based on the assumption that educational levels are associated with the style of writing. This includes people's signature fashion of using certain vocabulary of words which makes their literature unique and recognizable. Using a large public dataset of almost 60000 dating profiles, we aimed to model author style to be used as a predictor of educational background.

## Researcher & engineers

Researcher:

- Corten, R. (Rense)

Engineer:

- Shiva Nadi


## Installation

This project requires Python 3.5 or higher. Install the dependencies with the
code below.

```sh
pip install -r requirements.txt
```

## Project organization

```
.
├── .gitignore
├── CITATION.md
├── LICENSE.md
├── README.md
├── requirements.txt
├── bin                <- Compiled and external code, ignored by git (PG)
│   └── external       <- Any external source code, ignored by git (RO)
├── config             <- Configuration files (HW)
├── data               <- All project data, ignored by git
│   ├── processed      <- The final, canonical data sets for modeling. (PG)
│   ├── raw            <- The original, immutable data dump. (RO)
│   └── temp           <- Intermediate data that has been transformed. (PG)
├── docs               <- Documentation notebook for users (HW)
│   ├── manuscript     <- Manuscript source, e.g., LaTeX, Markdown, etc. (HW)
│   └── reports        <- Other project reports and notebooks (e.g. Jupyter, .Rmd) (HW)
├── results
│   ├── figures        <- Figures for the manuscript or reports (PG)
│   └── output         <- Other output for the manuscript or reports (PG)
└── src                <- Source code for this project (HW)

```


## License

This project is licensed under the terms of the [MIT License](/LICENSE.md)

## Citation

Please [cite this project as described here](/CITATION.md).
",2022-08-12
https://github.com/ShNadi/OkCupid-project,"# OkCupid Stylometry Analysis Project

Version 0.1.0
# OkCupid classifier

This project aims to examine to what extent educational background can be inferred from the written text, based on the assumption that educational levels are associated with the style of writing. This includes people's signature fashion of using certain vocabulary of words which makes their literature unique and recognizable. Using a large public dataset of almost 60000 dating profiles, we aimed to model author style to be used as a predictor of educational background.
In this project we have used a text analysis program named LIWC(Linguistic Inquiry and Word Count, https://liwc.wpengine.com/) to extract the degree of various categories of words that are used in the userwritten text. With adding these linguistic features to text features we have trained a logistic regression model. The model predicts the user's educational background with accuracy of 83%.

## Researcher & engineers

Researcher:
- Dr. Corten, R. (Rense)

Project Manager:
- Dr. Laurence Frank

Data Engineer:
- Shiva Nadi


## Installation

This project requires Python 3.7.3 or higher. Install the dependencies with the
code below.

```sh
pip install -r requirements.txt
```

## License

This project is licensed under the terms of the [MIT License](/LICENSE.md)

## Citation

Please [cite this project as described here](/CITATION.md).
",2022-08-12
https://github.com/ShNadi/sentinews,"
# Sentinews

Sentinews project aims to investigate the salience of topics like immigration, degree of science-oriented/objectivity 
versus conspiracy/fake and political orientation for online news articles. In this project we use an unsupervised dictionary based sentiment analysis technique (using wordvectors) for calculating the degree of negativity of the articles toward the immigrant outgroups in the Netherlands. The dataset is collected from two online news website, ""nu.nl"" and ""geenstijl.nl"" for a period of nine months. 

## Relevant links

- [Dataset](https://Newsdataset.csv)
- [Scraping project](https://github.com/UtrechtUniversity/news-scraping)
- [NU](https://www.nu.nl/)
- [GeenStijl](https://www.geenstijl.nl/)



## Dataset schema


Keys description in the dataset:

<style> 
table td, table th, table tr {text-align:left !important;}
</style>


| Key | Data type|Description |Example|
| --- | --- |--- | --- |
|id| string | The unique id of articles |a5154933|
|title|string |Title of the article|RIVM UPDATE: Deze week +4013 besmettingen|
|teaser|string|A short paragraph between title and text|Aantal nieuwe besmettingen STABILISEERT|
|text|string| The full text of the document|U mag kiezen:Optie 1:...|
|category|string| News section if any| null|
|created_at|datetime object |Date and time of scraping|2020-08-19 16:39:35|
|image|string | Dictionary of the image urls|{0: ''https://image.gscdn.nl/image/5f8b9b2526_Schermafbeelding... |
|reactions|string |Number of reactions to each article|308 reacties|
|author|string |Author|@Ronaldo|
|publication_time|string | Time of publication|14:20|
|publication_date|string |Date of publication, format: dd-mm-yy|18-08-20|
|doctype	|string | Source of the news| geenstijl.nl|
|url|string |URL to the article|https://www.geenstijl.nl/5154933/rivm-update-deze-week-4013-besmettingen/|
|tags|string |List of tags|corona, rivm|
|sitemap_url|string |Link to the site's sitemap if any|https://www.geenstijl.nl/sitemap.xml|


In this project we only make use of ""text"" field in the dataset.


## Usage


Outgroup's salience is calculated using:

    Salience outgroups = (N articles on [outgroup] / N articles full)*100
    
![alt text](image/outtable1.JPG ""Salience"")

![alt text](image/outtable2.JPG ""Salience"")


## Installation

This project requires:
  - Python 3.7 or higher
  -  Install the dependencies with the code below
  ```sh
    pip install -r requirements.txt
  ```

## License and citation

The sentinews project is licensed under the terms of the [MIT License](/LICENSE.md). When using sentinews for academic work, please cite:
-	Tubergen, F., Nadi, S., Bagheri, A. (2021).
sentinews - version 0.1.0. url: github.com/ShNadi/sentinews

## Conribution


To contribute code to sentinews, please follow these steps:

- Create a branch and make your changes
- Push the branch to GitHub and issue a Pull Request (PR)
- Discuss with us the pull request and iterate of eventual changes
- Read more about pull requests using [GitHub's official documentation](https://docs.github.com/en/github/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests).


## Contact

Do you have any questions, suggestions, or remarks? Feel free to contact ""s.nadi@uu.nl""
",2022-08-12
https://github.com/ShNadi/study_motivation,"# study_motivation

Version 0.1.0

A short description of your project


## Project organization

```
.
├── .gitignore
├── CITATION.md
├── LICENSE.md
├── README.md
├── requirements.txt
├── bin                <- Compiled and external code, ignored by git (PG)
│   └── external       <- Any external source code, ignored by git (RO)
├── config             <- Configuration files (HW)
├── data               <- All project data, ignored by git
│   ├── processed      <- The final, canonical data sets for modeling. (PG)
│   ├── raw            <- The original, immutable data dump. (RO)
│   └── temp           <- Intermediate data that has been transformed. (PG)
├── docs               <- Documentation notebook for users (HW)
│   ├── manuscript     <- Manuscript source, e.g., LaTeX, Markdown, etc. (HW)
│   └── reports        <- Other project reports and notebooks (e.g. Jupyter, .Rmd) (HW)
├── results
│   ├── figures        <- Figures for the manuscript or reports (PG)
│   └── output         <- Other output for the manuscript or reports (PG)
└── src                <- Source code for this project (HW)

```


## License

This project is licensed under the terms of the [MIT License](/LICENSE.md)

## Citation

Please [cite this project as described here](/CITATION.md).
",2022-08-12
https://github.com/sodascience/awesome-citizen-science-nl,"# Awesome Citizen Science NL

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4724569.svg)](https://doi.org/10.5281/zenodo.4724569)

A list of awesome Citizen Science projects from the Netherlands with additional information such as duration, organizations, and links to resources. For a full overview of all variables, see the [dataset specifications](https://github.com/sodascience/awesome-citizen-science-nl/tree/main/data) ([CSV](https://github.com/sodascience/awesome-citizen-science-nl/blob/main/data/citizen-science-projects-nl.csv), [Excel](https://github.com/sodascience/awesome-citizen-science-nl/blob/main/data/citizen-science-projects-nl.xlsx)).

We aim to make this list as complete as possible; therefore, we need your help! You can easily contribute a new project to the *Awesome Citizen Science Projects in the Netherlands* dataset by following the [contributing guide](#contribute-or-update-project).

:x: Link to project might be broken

<!---->

- [Awesome Citizen Science Projects](#awesome-citizen-science-projects)
  - [Archaeology](#Archaeology)
  - [Cities](#Cities)
  - [Ecology](#Ecology)
  - [Environment](#Environment)
  - [Health](#Health)
  - [History](#History)
  - [Science](#Science)
  - [Society](#Society)
  - [Uncategorized](#Uncategorized)
- [Contribute or update project](#contribute-or-update-project)
- [Citation](#citation)
- [Contact](#contact)

<!---->

## Projects

### Archaeology

- [Erfgoed Gezocht 2](https://www.universiteitleiden.nl/citizensciencelab/projecten/erfgoed-gezocht) - The project consists of two stages. In the first stage, citizen examine elevation maps of the designated area where they identify possible locations for excavations. In the second stage, citizens can actually join and assist archeologists on their excavations.  (`2020` - `ongoing`)
- [Erfgoed Gezocht 1](https://www.archeologieleeft.nl/archeosuccessen/erfgoed-gezocht/) - The project consists of two stages. In the first stage, citizen examine elevation maps of the designated area where they identify possible locations for excavations. In the second stage, citizens can actually join and assist archeologists on their excavations.  (`2019` - `2019`)
- [Erfgoed Gezocht Junior](https://www.universiteitleiden.nl/nieuws/2020/04/erfgoed-gezocht-junior-kinderen-speuren-vanuit-huis-mee-met-archeologen) - Children between the ages of 8 and 12 examine elevation maps of the designated area where they identify possible locations for excavations.  (`2020` - `ongoing`)

### Cities

- [Nationale PlasticWatch](https://www.tudelft.nl/scd/waterlab/doe-mee-aan-onderzoek/project-6-nationale-plasticwatch) - Citizens report litter to help Rijkswaterstaat gain insights on the origin, transport and distribution of litter. (`2020` - `ongoing`)

### Ecology

- [Fishing in the Past](https://www.zooniverse.org/projects/anneoverduin/fishing-in-the-past/about/research) - Citizens identify fish species on paintings to gain information on biodiversity and commercial use of fish species throughout history.  (`2020` - `2020`)
- [Kilometerhokken Inventariseren](https://www.floron.nl/meedoen/kilometerhokken-inventariseren ) - Citizens register the locations plant species by crossing them off on a list in a predetermined area. (`1988` - `ongoing`)
- [Bodemdierendagen](https://bodemdierendagen.nl/) - Citizens count the benthic animals in their backyards, planters and on their roof if they happen to have a so-called green roof. They register to what animal group (out of ten) the benthic animals belong.  (`2015` - `recurring`)
- [Van Kroeg tot Kraamkamer](https://www.tuintelling.nl/kraamkamer) - Citizens observe the animals in their backyards for a predetermined period of ten days and note what kind of behavior they display (kroeg, hotel or kraamkamer).  (`2017` - `recurring`)
- [Wadertrack ](https://www.naturetoday.com/intl/nl/nature-reports/message/?msg=25068) - Citizens register their observations of color ringed oystercatchers. (`2012` - `2020`)
- [Wildspotter](https://www.naturetoday.com/intl/nl/nature-reports/message/?msg=22655) - Citizens check whether animals can find their way at fauna passages by watching videos.  (`2016` - `unknown`)
- [RAS: het ""Retrapping Adults for Survival"" project](https://vogeltrekstation.nl/nl/onderzoek/monitoringsprojecten/ras-het-retrapping-adults-survival-project) - Citizens focus on one bird species they decide on themselves and then try the trap birds in nets to ring them. When a bird was also ringed last year, this is noted. Together, this gives insights in the survival of birds.  (`1998` - `ongoing`)
- [Muggenradar](https://www.naturetoday.com/intl/nl/observations/mosquito-radar) - Citizens report how much trouble they experience from mosquitoes (none, a little, much, very much).  (`2016` - `ongoing`)
- [Witte Gebieden](https://www.floron.nl/meedoen/witte-gebieden ) - Citizens (mostly florists) reserve a ""Wit Gebied"" beforehand. These are areas where florists are not as active, so there are less observations compared to other areas. Florists then visit such an area for a maximum of one day and fill in lists of what they found in the area that covers at least three ""kilometerhokken"".  (`2016` - `ongoing`)
- [Zomerganzentelling](https://vogeltrekstation.nl/nl/onderzoek/monitoringsprojecten/zo%C3%B6nosenproject) - Citizens count summer geese and report on their number and species.  (`2005` - `recurring`)
- [Tuinvlindertelling](https://www.vlinderstichting.nl/vlindermee/) - Citizens count on one of the predetermined days for 15 minutes the number of butterflies in their backyards and also report on their species.  (`2009` - `recurring`)
- [Essentaksterfte](https://www.naturetoday.com/intl/nl/observations/essentaksterfte) - Citizens report whether ash trees are affected by a fungus. (`2016` - `ongoing`)
- [Natuurkalender](https://www.naturetoday.com/intl/nl/observations/natuurkalender?utm_source=natuurkalender.nl&utm_medium=redirect&utm_campaign=olddomain) - Citizens observe a certain animal species on a location they have chosen themselves and note the first day when they enter a new phenological phase. (`1868` - `recurring`)
- [Koolmezen](https://www.universiteitleiden.nl/citizensciencelab/projecten/evoscope---koolmezen) - Citizens hang two fat balls outside during the winter, one with a weird object underneath and the other without one. Then they sit down for half an hour and count how many great tits (koolmezen) come to the ball with the weird object and how many come to the ball without it.  (`2019` - `2019`)
- [Bioblitz](https://www.tuintelling.nl/data/result/evenement/EBB) - A contest where citizens try to discover as many animals and plants in their backyard within a 24-hour time period.  (`2020` - `recurring`)
- [Ring - MUS](https://vogeltrekstation.nl/nl/onderzoek/monitoringsprojecten/ring-mus%20) - Citizens report on the color-rings they see on birds. (`2011` - `ongoing`)
- [Signalering (Invasieve) Exoten](https://www.floron.nl/meedoen/exoten-melden) - Citizens report the coordinates and abundance (surface they span) of invasive exotics classified as such on a list. Moreover, when a rare exotic is found, material can be send in for examination.   (`2012` - `ongoing`)
- [Avondtelling Amfibie√´n](https://www.tuintelling.nl/evenementen/avondtelling) - Citizens report how many and what kind of amphibians they have seen and heard on a predetermined evening.  (`2016` - `2016`)
- [SnailSnap](https://www.universiteitleiden.nl/citizensciencelab/projecten/naturalis) - Citizens download an app with which they make pictures of snails they encountered from April up until October. The app sends the photo, along with the coordinates of the place it was taken at to waarneming.nl. (`2017` - `recurring`)
- [Herbariummateriaal Verzamelen](https://www.floron.nl/meedoen/herbariummateriaal) - Citizens collect samples of plants that are either rare, believed to have gone extinct in the Netherlands, plants with flowers or fruits that they are unable to identify or they want to argue a species can better be split into multiple species. After collecting the sample, it is send to a predetermined address.  (`2013` - `ongoing`)
- [GrowApp](https://www.naturetoday.com/intl/nl/observations/growapp) - Citizens make pictures of a certain spot in nature, which the app transforms into a time-lapse to capture the change of seasons. The app sheds light on the impact of climate change on nature.  (`2017` - `ongoing`)
- [Snapshot Hoge Veluwe](https://www.iedereenwetenschapper.be/projects/bestudeer-wilde-dieren-de-hoge-veluwe) - Citizens count the number of animals and species present on a photo.  (`2018` - `ongoing`)
- [Animal Tracker](https://www.iedereenwetenschapper.be/projects/volg-de-weg-van-trekvogels) - Citizens localize spoonbills and report on them.  (`2019` - `ongoing`)
- [CR-Birding Submit](https://submit.cr-birding.org/page/who_are_we/) - Citizens register their observations of color marked birds. (`2016` - `ongoing`)
- [Special Project Muggenradar](https://www.naturetoday.com/intl/nl/nature-reports/message/?msg=27308) - Citizens send researchers the mosquitos they have killed. (`2021` - `ongoing`)
- [Kikkerdriltelling](https://www.tuintelling.nl/evenementen/kikkerdriltelling) - Citizens count the number of clumps of spawn in the ponds in their backyards during a predetermined period of time.  (`2018` - `recurring`)
- [Mijn Berm Bloeit!](https://www.floron.nl/bermen) - Citizens choose between a predetermined period a road verge. Then, they register one hundred meter of road verge and stand still every ten meters to note the nectar plants that are within one meter of themselves. (`2017` - `recurring`)
- [Pullen](https://vogeltrekstation.nl/nl/onderzoek/monitoringsprojecten/pullen#quicktabs-pullen=0) - Citizens ring young birds to gain insights in the first year of life of their peers.  (`2007` - `ongoing`)
- [Plonzenweekend](https://www.tuintelling.nl/data/result/evenement/EPLONS) - Citizens count the number of splashes they hear when passing the pond in their backyard or another pond on a walk. If they are able to get close enough to see a frog, they can also count the frog species.  (`2008` - `recurring`)
- [Kruidenrijk Grasland App](https://www.naturetoday.com/intl/nl/nature-reports/message/?msg=26207) - Citizens make pictures with an app of different kinds of grasslands.  (`2019` - `ongoing`)
- [Leuke Vliegen](https://www.eis-nederland.nl/actueel/projecten/leuke-vliegen) - Citizens count the number and species of flies they see in their backyard by making photos of them. The goal is to collect information on five families of flies, to see how many there are and where. This information is used to determine which species are well and which are not.  (`2012` - `2014`)
- [Wantsen](https://www.eis-nederland.nl/wantsenproject) - Citizens make photos of the bugs they encounter and load them unto waarneming.nl.  (`2017` - `2018`)
- [Kauwtjes](https://www.universiteitleiden.nl/citizensciencelab/projecten/evoscope---kauwtjes) - Citizens report how close they can venture near a jackdaw.  (`2019` - `2019`)
- [Eindejaars Plantenjacht](https://www.floron.nl/plantenjacht) - Citizens count between predetermined days which plants bloom in their backyard. (`2014` - `recurring`)
- [Paardenbloemen](https://www.universiteitleiden.nl/citizensciencelab/projecten/evoscope---paardenbloemen) - Citizens collect blowballs of dandelions, and report how long and how wide the tails are. Then they go inside, select five achenes which they let go five times each and report how long it takes them to land on the ground.  (`2019` - `2019`)
- [Paddenoverzetacties](https://www.ravon.nl/Portals/2/Bestanden/Publicaties/Rapporten/2007.003k.pdf) - Citizens (mostly in groups) report how many and what kind of toads they transferred.  (`2008` - `recurring`)
- [Nationale NajaarsVleermuistuintelling](https://www.vleermuis.net/) - Citizens count on one of the predetermined days the number of bats in their backyards and also report on their species from sunset until they have not seen a bat in over 10 minutes. They also give information on where in their backyard they saw the bats and where they did not see any. (`2019` - `recurring`)
- [Constant Effort Site Project](https://vogeltrekstation.nl/nl/onderzoek/monitoringsprojecten/ces-het-constant-effort-site-project) - Citizens help ring birds. What is different for this project is that it is all very standardized: citizens go every ten days, for the same amount of time and place the nets on the same places.  (`1994` - `ongoing`)
- [Tweede Boerenzwaluw - nestentelling](https://www.tuintelling.nl/evenementen ) - Citizens count the number of new nests of the barn swallow in their own backyards.  (`2011` - `2019`)
- [Stoepplantjes](https://www.hortusleiden.nl/onderwijs/stoepplantjes-1) - Citizens report where and what plants/weeds they found on their sidewalks.  (`2020` - `ongoing`)
- [Oeverplanten](https://www.universiteitleiden.nl/citizensciencelab/projecten/oeverplanten) - Citizens choose a strip of fourty meters alongside a shore. Every ten meters, they report all indicator plants they see and cross them out on a card. In the middle, they report the coordinates and afterwards they make a photo of the strip of land they examined.  (`2018` - `2018`)
- [Nationale Bijentelling](https://www.nationalebijentelling.nl/) - Citizens count on one of the predetermined days for 30 minutes  the number of bees in their backyards and also report on their species. (`2017` - `recurring`)
- [Meetnet Urbane Soorten](https://www.sovon.nl/nl/MUS) - Citizens count birds and species three times per season, on top of the yearly separate counts.  (`2007` - `ongoing`)
- [Nationale Waterdiertjes Telling](https://www.waterdiertjes.nl/#/info) - Citizens report which and how many animals live in the water in their nearby ditches. They also report the type and quality of the water. (`2018` - `recurring`)
- [Nationale Nachtvlindernachten](https://www.vlinderstichting.nl/nachtvlindernacht/) - Citizens count on one of the predetermined nights the number and species of moths they see in their backyard.  (`2005` - `recurring`)
- [TreeWiFi](https://www.samenmetenaanluchtkwaliteit.nl/projecten/treewifi) - A project that was started by a citizen where bird houses measure the air quality. When the quality increases, you get free wifi in return.  (`2016` - `ongoing`)
- [Boerenzwaluw - nestentelling](https://www.tuintelling.nl/evenementen ) - Citizens count the number of nests of the barn swallow in their own backyards.  (`2011` - `2019`)
- [Kuikenteller](https://www.iedereenwetenschapper.be/projects/hoe-gaat-het-met-de-wilde-eend) - Citizens count the number of ducklings in a place near them and then follow them and report of their survival for the rest of the season.  (`2020` - `2020`)
- [Nationale Tuinvogeltelling](https://www.vogelbescherming.nl/tuinvogeltelling) - Citizens count on one of the predetermined days for 30 minutes the number of birds in their backyards and also report on their species.  (`2001` - `recurring`)
- [Egelweekend](https://www.tuintelling.nl/data/result/evenement/EE) - Citizens register when and how many hedgehogs they have encountered. Dead hedgehogs are also reported.  (`2009` - `recurring`)
- [Boerenzwaluwproject](https://vogeltrekstation.nl/nl/onderzoek/monitoringsprojecten/boerenzwaluw) - Citizens put in extra effort to ring barn swallows and thereby gain insights in the survival and reproduction of the species.  (`1997` - `ongoing`)
- [Spinnentelling](https://www.tuintelling.nl/index.php/data/result/evenement/ES/) - Citizens register on two predetermined days the number and kind of spiders they see in their backyards.  (`2015` - `recurring`)
- [Lijsters- en Bessentelling](https://www.sovon.nl/nl/actueel/nieuws/lijsters-en-bessen-tellen) - Citizens register which fructuous trees are in their backyard and which birds make use of them. (`2020` - `recurring`)
- [Libellentelling](https://www.tuintelling.nl/evenementen/libellentelling) - Citizens register how many and what kinds of dragonflies they have seen on two predetermined days. They also register whether they saw any dead dragonflies. (`1998` - `recurring`)
- [Mollentelling](https://www.zoogdiervereniging.nl/wat-we-doen/monitoring/jaarrond-tuintelling/mollentelling) - Citizens count on predetermined days the number of moles or molehills they see in their surroundings. (`2019` - `recurring`)
- [Tuinvijver-opschoontelling](https://tuintelling.nl/evenementen/tuinvijvertelling) - Citizens cleaning the ponds in their backyards report on what species they encountered and how many of each species they saw.  (`2015` - `2015`)
- [Staat Deze Plant Er Nog?](https://www.floron.nl/meedoen/actualisering-groeiplaatsen-bijzondere-soorten) - Citizens search for rare plant species, guided by a map of where they were previously found.  (`2012` - `ongoing`)

### Environment

- [Vuurwerkexperiment 2017/2018](https://www.samenmetenaanluchtkwaliteit.nl/vuurwerkexperiment-20172018) - Citizens help measure fine dust in the air during New Year's Eve. (`2017` - `2018`)
- :x:  [Boeren Meten Water](https://boerenmetenwater.nl/aanleiding/) - Farmers and the water board work together to gain insights into the current situation of salinization and subsidence. (`2019` - `ongoing`)
- [Geluid-​ en luchtkwaliteit](https://www.samenmetenaanluchtkwaliteit.nl/burgermetingen-schiedam) - Citizens in Schiedam place sensors that measure the amount of noise and the air quality.  (`2017` - `ongoing`)
- [Explane](https://explane.org/#:~:text=ExPlane%20is%20the%20app%20to%20register%20aviation%20noise&text=The%20app%20gives%20you%20the,by%20residents%20living%20near%20airports) - Citizens meausre the noise made by planes with their smartphones. (`2018` - `ongoing`)
- [Boeren en Buren](https://www.rivm.nl/boeren-en-buren) - Citizens place sensors that measure the local air quality and fill in an app regarding the smell.  (`2019` - `2021`)
- [Wadden Plastic](https://survey123.arcgis.com/share/9c12036c270940c58de97252e1676ea8) - Citizens choose a flood mark on either the Wadden or North Sea beaches, follow that mark and report every ten meters their estimate for the number of plastic granules for an area of 40x40 cm.  (`2018` - `ongoing`)
- [OeverWatch](https://www.tudelft.nl/scd/waterlab/doe-mee-aan-onderzoek/project-4-oeverwatch) - Citizens report on biodiversity and biological water quality. They also measure the chemical water quality.  (`2019` - `ongoing`)
- [Samen stikstofdioxide meten met Palmes buisjes](https://www.samenmetenaanluchtkwaliteit.nl/projecten/samen-stikstofdioxide-meten-met-Palmes-buisjes) - Citizens measure nitrogen dioxide using Palmes tubes. (`2019` - `ongoing`)
- [Jubileumproject DCMR - NO2 burgermetingen in Rijnmondgebied](https://www.samenmetenaanluchtkwaliteit.nl/jubileumproject-dcmr-no2-burgermetingen-in-rijnmondgebied) - In honor of their anniversary, the DCMR provided 230 citizens with sensors to measure nitrogen dioxide. (`2019` - `2019`)
- [Meet je Stad](https://www.meetjestad.net/nl/MeetjeStad) - Citizens measure temperature, humidity, air quity and soil moisture.  (`2015` - `ongoing`)
- [EyeOnWater](https://www.eyeonwater.org/about-us) - Citizens help classify rivers, lakes, coastal waters, seas and oceans based on their color with the help of an app. (`2016` - `ongoing`)
- [Smart Emission Nijmegen Project](https://data.smartemission.nl/home) - Citizens in Nijmegen place sensors that measure the air quality, the amount of noise, vibrations en meteorological indicators.  (`2015` - `ongoing`)
- [Weer & Klimaat](https://globenederland.nl/onderzoeksproject/weer/#info) - High school students measure the air pressure, temperature and humidity.  (`2012` - `ongoing`)
- [Check de Stadvergroening!](https://www.tudelft.nl/scd/waterlab/doe-mee-aan-onderzoek/project-2-check-de-stadsvergroening) - Citizens first register, alone or in a group, and then follow a step-by-step plan of what they ought to do. They inspect a place they choose themselves on what the soil consists of, and check the draining of rainwater and whether flooding occurs after heavy rain. (`2018` - `ongoing`)
- [Urban AirQ (Amsterdam)](https://www.samenmetenaanluchtkwaliteit.nl/projecten/urban-airq-amsterdam) - Citizens help measure fine dust and nitrogen dioxide in the air in their street. (`2016` - `2016`)
- [AiREAS Eindhoven](https://www.samenmetenaanluchtkwaliteit.nl/aireas-eindhoven) - Citizens set up a project to measure air quality in the city of Eindhoven. (`2013` - `ongoing`)
- [Papillottenproject](https://www.naturalis.nl/collectie/verborgen-vlinders-het-gestroomlijnd-zichtbaar-maken-van-papillotten) - Citizens photograph, register and repackage papillots in new acid-free bags and store them in high drawers in a standard manner again. (`2016` - `ongoing`)
- [Maaspoort Meet!](https://www.samenmetenaanluchtkwaliteit.nl/maaspoort-meet) - Citizens living close to Maaspoort place sensors that measure the local air quality. (`2017` - `ongoing`)
- [Samen Meten aan Luchtkwaliteit](https://www.samenmetenaanluchtkwaliteit.nl/) - Citizens measure airquality by putting sensors in their backyards.  (`2016` - `ongoing`)
- [Versheid van Water](https://www.kwrwater.nl/projecten/citizen-science-praktijkpilot-versheid-water/) - Citizens help researchers use the new technique Next Generation Sequencing to determine whether water is a fresh product.  (`2015` - `2015`)
- [Watermetingen door Mijn Omgeving](https://www.samenmetenaanluchtkwaliteit.nl/projecten/watermetingen-door-mijn-omgeving) - Citizens buy cheap sensors that measure the electrical conductivity of the water all day long.  (`2019` - `ongoing`)
- [WaterLab Circulair Water Flevoland](https://www.flevoland.nl/dossiers/waterlab-circulair-water) - Citizens test circular water solutions in seven situations in practice.  (`2021` - `ongoing`)
- [Samen Luchtkwaliteit Meten in Zuid-Holland](https://www.samenmetenaanluchtkwaliteit.nl/projecten/samen-luchtkwaliteit-meten-in-zuid-holland) - Citizens place sensors that measure the local air quality. This is mostly done in so-called ""meetclubs"", where multiple sensors are placed close to each other in cluster.  (`2017` - `ongoing`)
- [Waterkwaliteit in Europa](https://www.tudelft.nl/scd/waterlab/doe-mee-aan-onderzoek/project-1-waterkwaliteit-in-europa) - Citizens first request research packages with which they collect a sample of water they send back.  (`2017` - `2017`)
- [SCOREwater](https://www.scorewater.eu/#3cases) - Citizens help investigate how the city of Amersfoort can become more adaptive to the changing climate, especially with regards to flooding as this is a problem in the city.  (`2019` - `ongoing`)
- [Luftdaten op Sensor.community](https://www.samenmetenaanluchtkwaliteit.nl/luftdaten) - Citizens place sensors that measure the local air quality.  (`2019` - `ongoing`)
- [Vuurwerkexperiment 2018 - 2019](https://www.samenmetenaanluchtkwaliteit.nl/vuurwerkexperiment-2018-2019) - Citizens help measure fine dust in the air during New Year's Eve. (`2018` - `2019`)
- [Onze Lucht – zelf fijnstof meten in Noord-Nederland](https://www.samenmetenaanluchtkwaliteit.nl/projecten/onze-lucht-zelf-fijnstof-meten-in-noord-nederland) - Citizens build gaugers that measure the amount of fine dust in the air. For every area with the same postal code in Groningen, Friesland or Drenthe, one person can partake in the project. (`2020` - `ongoing`)
- [Een neus voor luchtkwaliteit](https://www.samenmetenaanluchtkwaliteit.nl/maastricht) - Sensors are placed in places in Maastricht that are chosen by citizens. The sensors measure the local air quality.  (`2016` - `ongoing`)
- [Visibilis](https://www.samenmetenaanluchtkwaliteit.nl/visibilis) - On the initiative of two citizens, a suitcase containing multiple measuring instruments was attached to a measuring location already in use. The goal was to compare different tools. The citizens have written  report detailing the results.  (`2018` - `2019`)
- [Waterlab Flevoland](https://www.waterforum.net/officiele-start-waterlab-flevoland/) - Citizens test innovative purification systems for two years.  (`2019` - `2021`)
- [De Levende Bodem](https://globenederland.nl/onderzoeksproject/bodem/#metingen) - High school students take samples of the soil and measure their quality.  (`2011` - `ongoing`)
- [MySense](http://behouddeparel.nl/samen-meten) - Citizens place sensors that measure the local air quality. This project is special considering it entails measuring fine dust and gas in an agricultural context.  (`2019` - `ongoing`)
- [Nationale Lichtmeting](https://globenederland.nl/onderzoeksprojecten/lichtmeting/) - Elementary school children build their own spectroscope and see what lamps are used in their surroundings.  (`2015` - `2020`)
- [Meetcampagne NO2 (Milieudefensie)](https://www.samenmetenaanluchtkwaliteit.nl/projecten/meetcampagne-no2-milieudefensie) - Citizens help measure nitrogen dioxide in the air by placing sensors around place that are important to them. (`2015` - `2015`)
- [Amsterdam Sounds](https://amsterdamsounds.waag.org/) - Citizens in Amsterdam place sensors that measure the amount and kind of noise.  (`2019` - `ongoing`)
- [Plastic Spotter ](https://www.plasticspotter.nl/) - Citizens spot plastic in the canals of Leiden and upload photos of it.  (`2019` - `ongoing`)
- [AirSensEUR](https://www.samenmetenaanluchtkwaliteit.nl/airsenseur) - Citizens help measure the quality of the air by placing sensors. (`2015` - `ongoing`)
- [Luchtmeetnet Hillegersberg Schiebroek](https://inhillegersberg.nl/geluid-lucht/424-luchtmeetnet-hillegersberg-schiebroek) - Citizens place sensors in their backyards that measure the air quality.  (`2017` - `ongoing`)
- [Grip Op Water - Altena](http://altena.gripopwater.nl/over-ons/) - Citizens report abnormal water circumstances. For instance, flooding or a place where the water does not drain.  (`2018` - `ongoing`)
- [Metingen 's Gravendijkwal (Rotterdam)](https://www.samenmetenaanluchtkwaliteit.nl/metingen-s-gravendijkwal-rotterdam) - Citizens investigate how the emissions of hazardous substances on the 's Gravendijkwasstraat can be limited with the use of a hoze that spreads very small particles of water.  (`2015` - `ongoing`)
- [Vang de Watermonsters](https://www.natuurenmilieu.nl/themas/voedsel/projecten-voedsel/waterkwaliteit-biodiversiteit/watermonsters/) - Citizens take samples of water in lakes, canals etc. to measure the quality of the water.  (`2019` - `ongoing`)
- [iSPEX](https://www.samenmetenaanluchtkwaliteit.nl/ispex) - Citizens help measure fine dust in the air. (`2012` - `2013`)
- [Hoe gezond is onze lucht?](https://milieudefensie.nl/actueel/eindrapport-hoe-gezond-is-onze-lucht.pdf) - Citizens help measure nitrogen dioxide in the air by placing sensors around place that are important to them, like the local school or the street through which they cycle every day. (`2012` - `2014`)
- [Hollandse Luchten](https://hollandseluchten.waag.org/) - Citizens in Noord-Holland place sensors that measure the local air quality.  (`2019` - `ongoing`)
- [Vuurwerkexperiment 2019 - 2020](https://www.samenmetenaanluchtkwaliteit.nl/projecten/vuurwerkexperiment-2019-2020) - Citizens help measure fine dust in the air during New Year's Eve. (`2019` - `2019`)
- [Oud & Nieuw 2020 - 2021](https://www.samenmetenaanluchtkwaliteit.nl/oud-nieuw-2020-2021) - Citizens help measure fine dust in the air during New Year's Eve. (`2020` - `2020`)
- [Schone Rivieren ](https://www.iedereenwetenschapper.be/projects/word-afvalonderzoeker) - Citizens help researchers by cleaning up rivers and investigating what they find.  (`2015` - `ongoing`)
- [Vuurwerk 2016 - 2017](https://www.samenmetenaanluchtkwaliteit.nl/vuurwerk-2016-2017) - Citizens help measure fine dust in the air during New Year's Eve. (`2016` - `2017`)
- [Snuffelfiets](https://www.samenmetenaanluchtkwaliteit.nl/snuffelfiets) - Citizens measure air quality through sensors attached to their bicycles by cycling different routes.  (`2019` - `ongoing`)
- [Samen Geluid Meten](https://www.samenmetenaanluchtkwaliteit.nl/projecten/samen-geluid-meten) - Citizens place sensors that measure the amount of noise.  (`2021` - `ongoing`)
- [CHARRED](https://www.samenmetenaanluchtkwaliteit.nl/projecten/samen-houtrook-meten ) - Citizens measure the amount of fine dust by putting sensors in their backyards.  (`2019` - `ongoing`)
- [Delft Meet Regen](https://www.tudelft.nl/scd/waterlab/doe-mee-aan-onderzoek/project-7-delft-meet-regen) - Citizens get a kit from the WaterLab which assists them in making their own rain gauge. Then, from the 18th of July until half September, they report every day how much rain has fallen. (`2020` - `ongoing`)
- [Almere meet water](https://almeremeetwater.nl/) - Citizens get one of the measuring toolboxes and measure the quality of the water in their neighborhood.  (`2019` - `2019`)
- [Drinkable Rivers](https://drinkablerivers.org/what-we-do/citizen-science/) - Citizens sign in and pick up one of the test kits from one of the locations located all over Europe. The kits are used to collect samples of water and measure the water quality. Results are entered into a database.  (`2019` - `ongoing`)
- [Zet 'm op 70!](https://www.tudelft.nl/scd/waterlab/doe-mee-aan-onderzoek/project-5-zet-m-op-70) - Citizens decrease the temperature of the water with which their house is warmed, from 80-90 degrees Celsius to 70 degrees Celsius for two months. They also place automatic sensors that measure the temperatures inside and outside. Lastly, they fill out a questionnaire regarding their experiences.   (`2018` - `ongoing`)

### Health

- [Grote Griep & Corona Meting](https://www.griepencorona.nl/het-project/) - Continuation of the Grote Griepmeting, now also including corona (`2021` - `recurring`)
- [Grote Griep Meting](https://www.nemokennislink.nl/publicaties/doe-mee-aan-de-grote-griepmeting/) - Citizens report when they or someone they know gets a cold or the flu.  (`2003` - `2017`)
- [Ik heb last-app](https://www.rivm.nl/rivm/kennis-en-kunde/strategisch-programma-rivm/spr-2015-2018/integrale-risicobeoordeling/ik-heb-nu-last-app) - Citizens indicate how much their airways bother them and what symptoms they experience to provide further insights into the causes (and aggrevation) of respiratory diseases. (`2015` - `ongoing`)
- [Tekenradar](https://www.tekenradar.nl/) - Citizens report whether they or their child have a tick bite, tick bite fever, a red circular rash (Erythema Migrans) and/or Lyme disease, alongside their age. (`2012` - `ongoing`)
- [Zoönosenproject](https://www.jagersvereniging.nl/downloads/bestuurdersdocumenten/telprotocol-zomerganzentelling/) - Citizens take blood samples, throat and cloacal swabs of bird species in order for researchers to be able to investigate how the spread of viruses can be prohibited.  (`2017` - `ongoing`)
- [Hooikoortsradar ](https://hooikoortsradar.nl/) - Citizens report how much they suffer from hayfever at the moment of reporting on a ten-point scale.  (`2009` - `ongoing`)
- [CoronaWatchNL](https://github.com/J535D165/CoronaWatchNL) - Citizens help collect numbers on COVID-19 infections and deaths in The Netherlands. With these data, a dataset is compiled that meets the FAIR (Findable, Accessible, Interoperable and Reusable) guidelines. (`2020` - `2021`)
- [MyCardio](https://www.mycardio.nl/) - Citizens with cardiovascular diseases or citizens that are at risk of hart problems, report on their health to get more insights into the cause.  (`2019` - `ongoing`)

### History

- [Geboeid door boeven! Inschrijfboeken van Brabantse Huizen van Bewaring 19e eeuw](https://velehanden.nl/projecten/bekijk/details/page/2/status/closed/project/bhic_strafgevangenissen_2) - Citizens help digitalize the registration files from prisons in Brabant in the 19th century.  (`2015` - `2017`)
- [Ja -  ik wil!](https://velehanden.nl/projecten/bekijk/details/project/ja_ik_wil) - Citizens help digitalize registered banns from Amsterdam between 1602 and 1811.  (`2014` - `2016`)
- [Notaris van toen zoekt transcribent van nu](https://velehanden.nl/projecten/bekijk/details/page/2/project/tlb_index) - Citizens transcribe notary deeds from 19th and 20th century notaries from Tilburg.  (`2017` - `ongoing`)
- [HISGIS Nederlandse steden 1832](https://velehanden.nl/projecten/bekijk/details/page/2/project/fak_noat) - Citizens help index scans of the old parcel registers of Limburg, Noord-Brabant, Noord-Holland and Zuid-Holland.  (`2015` - `ongoing`)
- [Tag de tekst!](https://velehanden.nl/projecten/bekijk/details/project/sibr_annotate) - Citizens tag names, locations and dates in already transcribed texts to train an AI to recognize those that were missed by people. (`2020` - `ongoing`)
- [Bredase notarissen 1843 - 1905](https://velehanden.nl/projecten/bekijk/details/project/brd_index_repertoria) - Citizens help index archives of notaries located in Breda from 1843 to 1905.  (`2020` - `ongoing`)
- [Adressen Bevolkingsregisters Stad Utrecht (1890 - 1899)](https://utrecht.nieuws.nl/onderwijswetenschap/74887/utrechts-archief-vraagt-hulp-bij-het-ontsluiten-van-utrechtse-bevolkingsregisters/) - Citizens help index scans of population registers from Utrecht between 1890 and 1899. (`2017` - `2019`)
- [Hagenaars en Hagenezen - Bevolkingsregister Den Haag 1913 – 1939](https://velehanden.nl/projecten/bekijk/details/project/hga_index_gezinskaarten) - Citizens describe cards with photos of families that lived in Den Haag between 1913 and 1939.  (`2020` - `ongoing`)
- [Verlos Gelderse Geboorteakten](https://velehanden.nl/projecten/bekijk/details/status/closed/page/2/project/gda) - Citizens help index birth certificates from Gelderland between 1819 and 1912.  (`2015` - `2016`)
- [Het Vrije Volk](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/amsterdam_vrije_volk) - Citizens describe photos that have appeared in the paper Het Vrije Volk. (`2017` - `2019`)
- [Bevolkingsregisters stad Utrecht (1850 - 1889)](https://velehanden.nl/projecten/bekijk/details/project/utr_index_bvr_1850_1889) - Citizens help index scans of population registers from Utrecht between 1850 and 1889. (`2020` - `ongoing`)
- [Tot uw Dienst!](https://velehanden.nl/projecten/bekijk/details/status/closed/page/2/project/amsterdam_pensioen) - Citizens help index retirement cards from people that have worked for the municipality of Amsterdam. (`2015` - `2016`)
- [Bevolkingsregisters stad Utrecht (1900 - 1912)](https://velehanden.nl/projecten/bekijk/details/page/2/status/closed/project/utr_bvr) - Citizens help index scans of population registers from Utrecht between 1900 and 1912. (`2015` - `2017`)
- [HISGIS Drenthe kadastrale registers 1832](https://velehanden.nl/projecten/bekijk/details/status/closed/page/2/project/fak_oat) - Citizens help index scans of the oldest parcel registers of Drenthe.  (`2014` - `2015`)
- [Historisch Amstelveen in beeld](https://velehanden.nl/projecten/bekijk/details/page/2/project/gea_tagging) - Citizens describe photos in the archives of Amstelveen. (`2016` - `ongoing`)
- [Het beeld van Groningen: Persfotobureau D. van der Veen](https://velehanden.nl/projecten/bekijk/details/project/gra_tagging_vanderveen) - Citizens describe photos of Persfotobureau D. van der Veen from the sixties and seventies of the 20th century.  (`2018` - `ongoing`)
- [Goetgevonden! Besluiten van de Staten-Generaal (1576 - 1796)](https://velehanden.nl/projecten/bekijk/details/project/huy_correct_resoluties_transkribus) - Citizens help transcribe the resolutions adopted by the Staten-Generaal between 1576 and 1796. (`2020` - `ongoing`)
- [Toen-en-nu: Zet Noordwest-Holland op de kaart](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/raa) - Citizens do georeferencing by placing old maps of Noordwest-Holland over existing ones to determine their placement.  (`2018` - `2018`)
- [Bevolkingsregisters gemeentearchief Steenwijkerland 1826 - 1939](https://velehanden.nl/projecten/bekijk/details/status/closed/page/2/project/swl_bvr) - Citizens help index scans of population registers from Steenwijkerland between 1826 and 1939. (`2015` - `2016`)
- [Compareerden voor mij](https://velehanden.nl/projecten/bekijk/details/project/wfg_transcription_notarieel) - Citizens transcribe notary deeds from notaries from Westfriesland between 1843 and 1915.  (`2019` - `ongoing`)
- [Bevolkingsregisters Amsterdam 1853 - 1863](https://velehanden.nl/projecten/bekijk/details/status/closed/page/2/project/amsterdam_13442) - Citizens help index scans of population registers from Amsterdam between 1853 and 1863. (`2015` - `2017`)
- [Duik in historisch Amsterdam](https://www.iedereenwetenschapper.be/projects/duik-historisch-amsterdam) - Citizens describe photos of monuments in Amsterdam so they can be digitalized.  (`2014` - `2017`)
- [Bevolkingsregisters Noord-Hollands Archief](https://velehanden.nl/projecten/bekijk/details/page/2/project/ranh_index) - Citizens help index scans of population registers from Kennemerland, and Amstel- and Meerlanden from 19th and 20th century. (`2017` - `ongoing`)
- [Nieuws! Lokale kronieken - 1500 - 1850](https://velehanden.nl/projecten/bekijk/details/project/vua_train_kronieken_transkribus) - Citizens transcribe local chronicles made available by the Koninklijke Bibliotheek. (`2019` - `ongoing`)
- [Holland Amerika Lijn Passagierslijsten](https://velehanden.nl/projecten/bekijk/details/project/cbg_index_hal) - Citizens help index scans of repatriation lists of the people that were emigrated using the Holland Amerika Lijn between 1900 and 1969. (`2018` - `ongoing`)
- [Extra serie Amersfoortse bevolkingsregisters online](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/gam_gezinskaarten) - Citizens help index scans of population registers from Amersfoort between 1860 and 1915. (`2017` - `2019`)
- [Gezocht! Inkloppers van Brabantse gevangenisregisters 1821 - 1940](https://velehanden.nl/projecten/bekijk/details/status/closed/page/3/project/bhic_strafgevangenissen) - Citizens help digitalize the registration files from prisons in Brabant from the period from 1821 up until 1940. (`2014` - `2015`)
- [80.000 minuten](https://velehanden.nl/projecten/bekijk/details/page/2/project/leiden_na1) - Citizens transcribe notary deeds from notaries from Leiden between 1564 and 1811.  (`2014` - `ongoing`)
- [Geef de Belgische vluchtelingen in Gouda een naam](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/sahm_indexing_persons) - Citizens help index scans of registers containing the information of refugees from Belgium that came to Gouda during WWI. (`2018` - `2018`)
- [Vele Ajacieden 2.0](https://velehanden.nl/projecten/bekijk/details/project/afc_tagging_2) - Citizens describe photos in the archive of Ajax. (`2019` - `ongoing`)
- [Thuis in Terneuzen](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/picvh_zar_index_terneuzenaxel) - Citizens help index scans of population registers from Terneuzen, Axel and Sas van Gent between 1812 and 1900. (`2018` - `2020`)
- [Amsterdamse Doodsoorzaken 1854 – 1940](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/picvh_run) - Citizens register the information on scans of cards regarding cause of death in Amsterdam betweeen 1854 and 1940. (`2016` - `2020`)
- [Oudste bevolkingsregisters van Nijmegen (1820 - 1850)](https://velehanden.nl/projecten/bekijk/details/status/closed/page/2/project/picvh_han_nijmegen) - Citizens help index scans of population registers from Nijmegen between 1820 and 1850. (`2015` - `2016`)
- [WieWasWie - Bevolkingsregisters](https://velehanden.nl/projecten/bekijk/details/page/2/project/wiewaswie_bvr) - Citizens help index scans of population registers from Breda, Rotterdam and Maastricht between 1850 and 1938. (`2014` - `ongoing`)
- [Met Oprechte Deelneming](https://velehanden.nl/projecten/bekijk/details/page/2/status/closed/project/gda_2) - Citizens help index scans of death certificates from Gelderland between 1951 and 1960.  (`2017` - `2017`)
- [Oude Krantenproject](https://www.iedereenwetenschapper.be/projects/reis-terug-de-tijd) - Citizens register the information on scans of the newspapers from the 17th century. (`NA` - `unknown`)
- [Utrechtse notariële akten (1780 - 1811)](https://velehanden.nl/projecten/bekijk/details/project/utrecht_transcription_notarieel) - Citizens transcribe notary deeds from notaries from Utrecht between 1780 and 1811.  (`2019` - `ongoing`)
- [Zaanse briefhoofden](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/zaa_index_bron) - Citizens describe the information in letterheads of old letters from the 19th and 20th century from companies in the Zaanstreek.  (`2019` - `2020`)
- [Westfriezen Geregistreerd](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/wfg_index) - Citizens help index scans of population registers from West Friesland betweeen 1840 and 1930. (`2017` - `2018`)
- [Vreemdelingenkaarten](https://velehanden.nl/projecten/bekijk/details/project/amsterdam_index_vreemdelingenkaarten) - Citizens register the information on scans of cards from Amsterdam between 1920 and 1970.  (`2021` - `ongoing`)
- [30 dagen op zee](https://velehanden.nl/projecten/bekijk/details/project/moei_index_30dagen) - Citizens help index scans of repatriation lists of the people that were passengers on one of the ships sailing from the Dutch East Indies to the Netherlands between 1945 and 1967.  (`2019` - `ongoing`)
- [Maak geschiedenis: Zet Groningen op de kaart](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/gra_kaarten) - Citizens do georeferencing by placing old maps of Groningen over existing ones to determine their placement.  (`2017` - `2019`)
- [Buitenlui: Geannexeerde gemeenten Amsterdam](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/amsterdam_annex_gem) - Citizens help index scans of population registers from municapalities that were annexed to Amsterdam between 1830 and 1921. (`2017` - `2019`)
- [Binnen- en buitengesticht van Zutphen](https://velehanden.nl/projecten/bekijk/details/status/closed/page/2/project/szu_bevolkingsregister) - Citizens help index bedlam registers from the period between 1842 and 1938.   (`2016` - `2016`)
- [Glashelder!](https://velehanden.nl/projecten/bekijk/details/status/closed/page/3/project/nat_nbc) - Citizens help index the old labels on microscopic glass preparations containing mites, springtails and other small organisms in the collection of Naturalis.  (`2013` - `2014`)
- [Wonen op Walcheren vervolg Bevolkingsregisters 1812 - 1900](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/picvh_zar) - Citizens help index scans of population registers from Zeeland between 1812 and 1900. (`2015` - `2018`)
- [AlleFriezen - Bevolkingsregisters](https://velehanden.nl/projecten/bekijk/details/page/2/project/frl) - Citizens help index scans of population registers from Friesland between 1850 and 1939. (`2013` - `ongoing`)
- [Kijk je Rijk!](https://velehanden.nl/projecten/bekijk/details/status/closed/page/3/project/gam) - Citizens help describe digitalize film fragments of Omroep Amersfoort between 1985 and 2011.  (`2013` - `2013`)
- [Vele Voeten](https://velehanden.nl/projecten/bekijk/details/status/closed/page/2/project/picvh_han) - Citizens help digitalize participant registers of the Nijmegen Four Days Marches in a public project (regarding the participants between 1921 and 1939) and a closed project (regarding the participants between 1941 and 1987).  (`2015` - `2016`)
- [Bevolkingsregisters Amsterdam 1874 - 1893 ](https://velehanden.nl/projecten/bekijk/details/status/closed/page/2/project/amsterdam_bvr) - Citizens help index scans of population registers from Amsterdam between 1874 and 1893. (`2012` - `2016`)
- [Dijken van Polders](https://velehanden.nl/projecten/bekijk/details/project/hfl_tagging_rijpcollectie) - Citizens add unto the already existing description of photos with events and people depicted on them.  (`2020` - `ongoing`)
- [Dagboeken van schrijvers](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/lkm_index) - Citizens transcribe diaries of Lodewijk van Deyssel, Henri van Booven and J. Greshoff. (`2017` - `2019`)
- [Familie van je?](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/dre_index_inschrijvingsregisters) - Citizens help index scans with data of the people that have lived between 1822 and 1859 in Veenhuizen, on the Ommerschans and in the colonies Frederiksoord, Wilhelminaoord and Willemsoord. (`2018` - `2019`)
- [Er zit muziek in de crowd](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/som_index) - Citizens help digitalize music written for radio or television from 1930 to 1980.  (`2019` - `2019`)
- [Bevolkingsregisters Regionaal Archief Rivierenland](https://velehanden.nl/projecten/bekijk/details/page/2/status/closed/project/rivierenland_bvr) - Citizens help index scans of population registers from Regionaal Archief Rivierenland between 1820 and 1940. (`2014` - `2017`)
- [Overgenomen Delen 1892-1920](https://velehanden.nl/projecten/bekijk/details/status/closed/page/3/project/amsterdam_ogd) - Citizens help index the data from scans on people that have died or left Amsterdam from 1892 up until 1920.  (`2012` - `2015`)
- [Opgemaakt samengevat en ingeklopt: notari√´le akten in Brabant 1743 - 1935](https://velehanden.nl/projecten/bekijk/details/page/2/project/bhic_repnot) - Citizens transcribe notary deeds from notaries from Brabant between 1743 and 1935.  (`2015` - `ongoing`)
- [Dagboeken Caspar Reuvens](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/glv_reuvens_transkribus) - Citizens transcribe the diaries of Caspar Reuvens. (`2020` - `2020`)
- [Kijk Je Rijk(er)!](https://velehanden.nl/projecten/bekijk/details/page/2/status/closed/project/gam2) - Citizens help describe digitalize film fragments of Omroep Amersfoort between 1985 and 2011.  (`2015` - `2017`)
- [Nieuws 2.0! Lokale kronieken -  1500 - 1850](https://velehanden.nl/projecten/bekijk/details/project/vua_annoteer_kronieken_transkribus) - Citizens help annotate transcribed Dutch chronicles.  (`2020` - `ongoing`)
- [Post van Weldadigheid](https://velehanden.nl/projecten/bekijk/details/status/closed/page/2/project/dre_cmw) - Citizens digitalize scans of letters between 1818 and 1847. (`2015` - `2017`)
- [Crowd leert computer lezen](https://velehanden.nl/projecten/bekijk/details/project/amsterdam_correct_notarieel_transkribus) - Citizens help train an AI to read old notary deeds by correcting the transcription made by the computer.  (`2019` - `ongoing`)
- [Wittebroodsweken](https://velehanden.nl/projecten/bekijk/details/project/gda_index_huwelijk) - Citizens help index scans of marriage certificates from Gelderland between 1923 and 1942.  (`2019` - `ongoing`)
- [Bevolkingsregisters Amsterdam 1864 - 1874](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/amsterdam_index_bvr3) - Citizens help index scans of population registers from Amsterdam between 1864 and 1874. (`2018` - `2020`)
- [Hallo Midden-Holland! De bevolkingsregisters van Midden-Holland ](https://velehanden.nl/projecten/bekijk/details/project/sahm_index_bevolkingsregisters) - Citizens help index scans of population registers from Midden-Holland between 1825 and 1930. (`2019` - `ongoing`)
- [Alumni KABK](https://velehanden.nl/projecten/bekijk/details/status/closed/page/3/project/kabk) - Citizens help digitalize scans of cards containing the alumni and teachers of the KABK.  (`2012` - `2013`)
- [Bevolkingsregisters Deventer 1811 - 1940](https://velehanden.nl/projecten/bekijk/details/status/closed/page/3/project/sad) - Citizens help index scans of population registers of the population of Deventer between 1829 and 1940, of Diepenveen between 1840 and 1939, and of Bathmen between 1811 and 1937. (`2013` - `2013`)
- [Diamantbewerkers Amsterdam](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/mbi) - Citizens help index membership registers from the union for diamond workers from 1894 on up. (`2018` - `2019`)
- [Vele Ajacieden](https://velehanden.nl/projecten/bekijk/details/status/closed/page/2/project/afc_tagging) - Citizens describe photos in the archives of Ajax. (`2014` - `2016`)
- [Burgerlijke stand Breda: Wie nog meer en wanneer!](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/brd) - Citizens help complete the population registers of the municipality of Breda and some municipalities near it.  (`2018` - `2018`)
- [Van de kaart: georefereren van historische kaarten van Stadsarchief Amsterdam](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/amsterdam_kaarten) - Citizens do georeferencing by placing old maps of Amsterdam over existing ones to determine their placement.  (`2016` - `2019`)
- [Westfriese Lidmaten](https://velehanden.nl/projecten/bekijk/details/status/closed/page/2/project/wfg) - Citizens help index membership registers from churches between 1572 and 1913. (`2013` - `2017`)
- [Fotografisch Geheugen: de reportages van Fotopersbureau De Boer](https://velehanden.nl/projecten/bekijk/details/project/ranh_tagselection_deboer) - Citizens link the photos on a camera roll sorted by subject to the descriptions provided by the photographers that made the photos.  (`2020` - `ongoing`)
- [Missing Links](https://velehanden.nl/projecten/bekijk/details/status/closed/page/1/project/leiden) - Citizens link data related to photos to data already present in population registers of Hillegom, Leiden, Leiderdorp, Noordwijk, Rijnsburg, Sassenheim, Voorhout, Voorschoten, Warmond and Zoeterwoude. (`2012` - `2019`)
- [De bevolkingsregisters Voorne-Putten](https://velehanden.nl/projecten/bekijk/details/page/2/status/closed/project/picvh_vpr) - Citizens help index scans of population registers of the population of Voorne-Putten between 1826 and 1939.  (`2015` - `2017`)
- [Officiersboekjes Nationaal Militair Museum 1811 - 1940](https://velehanden.nl/projecten/bekijk/details/page/2/project/legermuseum) - Citizens help transcribe books of officers in service between 1811 and 1940.  (`2013` - `ongoing`)
- [Meertens-vragenlijsten Taal en Cultuur (1931 - 2005)](https://velehanden.nl/projecten/bekijk/details/page/2/project/mei) - Citizens retype written answers to surveys.  (`2018` - `ongoing`)
- [Pilotproject transcripties Statenresoluties Overijssel 1799](https://velehanden.nl/projecten/bekijk/details/project/hco_resoluties_transkribus) - Citizens transcribe two books containing resolutions adopted in Overijssel in 1799. (`2020` - `ongoing`)
- [Regionaal Archief Nijmegen - Bevolkingsregisters 1850 - 1890](https://velehanden.nl/projecten/bekijk/details/status/closed/page/3/project/nijmegen_bvr) - Citizens help index scans of population registers from Nijmegen between 1850 and 1890. (`2013` - `2014`)
- [Waarvan Akte!](https://velehanden.nl/projecten/bekijk/details/project/rar_index_repertoria) - Citizens transcribe notary deeds from notaries from the Rivierengebied between 1811 and 1935.  (`2020` - `ongoing`)
- [Goed geregeld! Brabanders in het Bosch' Protocol 1501 - 1793](https://velehanden.nl/projecten/bekijk/details/page/2/project/picvh_bhic_bevolkingsregister) - Citizens help transcribe scans of the Bosch' Protocol by name, location and event.  (`2016` - `ongoing`)
- [Militieregisters 1814 - 1941](https://velehanden.nl/projecten/bekijk/details/status/closed/page/3/project/militieregisters) - Citizens help index scans of the people that once were registered with the military from the period from 1814 up until 1941. (`2011` - `2014`)
- [Alle Amsterdamse Akten](https://velehanden.nl/projecten/bekijk/details/page/2/project/amsterdam_notarieel_2) - Citizens transcribe notary deeds from notaries from Amsterdam between 1578 and 1915.  (`2016` - `ongoing`)
- [Zaanse Verpondingsregisters e.a.](https://velehanden.nl/projecten/bekijk/details/project/zaa_index_gaarder) - Citizens enter in the names of landowners in Zaanstad from scans.  (`2019` - `ongoing`)
- [Captions for Cas](https://velehanden.nl/projecten/bekijk/details/page/2/project/nfm_tagging_photos) - Citizens caption photos made by Cas Oorthuys between 1935 and 1975.  (`2018` - `ongoing`)
- [Volgende Patiënt!](https://velehanden.nl/projecten/bekijk/details/status/closed/page/2/project/amsterdam_patient) - Citizens help index patient registers from hospitals in Amsterdam between 1818 and 1899.  (`2014` - `2016`)
- [Surinaamse slavenregisters 1830 - 1863](https://velehanden.nl/projecten/bekijk/details/page/2/project/run_slavenregisters) - Citizens index scans of slave registers of plantations in Suriname between 1830 and 1863. (`2017` - `ongoing`)
- [In dienst van het Hof (1814 - 1940)](https://velehanden.nl/projecten/bekijk/details/status/closed/page/2/project/kha) - Citizens digitalize genealogical registers of the staff from King Willem I up until Queen Wilhelmina. (`2017` - `2017`)
- [Tags en uitleg](https://velehanden.nl/projecten/bekijk/details/page/2/project/mai_tagging) - Citizens describe photos in the archives of the Maria Austria Instituut. (`2013` - `ongoing`)

### Science

- [SMAP: bodemvochtigheid](https://globenederland.nl/onderzoeksproject/smap/#info) - High school students measure the soil moisture to help NASA calibrate its satellite that attempts to estimate this from a distance.  (`2011` - `ongoing`)
- [CREDO](https://www.iedereenwetenschapper.be/projects/download-je-eigen-deeltjesdetector) - Citizens help detect glowing pixels with the cameras of their phones to test one of the theories of what dark matter could be.  (`2019` - `ongoing`)
- [Verdwaald in de Nacht](https://www.iedereenwetenschapper.be/projects/bekijk-de-aarde-vanuit-de-ruimte) - Citizens match photos made of places on the Earth in the night from space with the correct location.  (`2014` - `ongoing`)
- [Aerosolen](https://globenederland.nl/onderzoeksproject/aerosolen/#info) - High school students help calibrate the measures of a satellite by measuring the amount of aerosols in an area of the air. (`2004` - `ongoing`)

### Society

- [Zie Ik Spoken?](https://www.nemokennislink.nl/publicaties/zie-ik-spoken/) - Citizens first complete an extensive questionnaire about who they are, what their sleeping habits are and whether they sometimes feel like seeing, hearing, feeling or smelling things that are not there. Then they play a game where they have to press a certain key when they see a specific item of clothing or hear a specific word in a movie. Finally, they receive a few questions about substance use (drugs and alcohol) and the project concludes with a hearing test. (`2016` - `2016`)
- [WeSense](http://www.wesense.info/en/index.html) - Citizens fill out short surveys about their perception of their current surroundings and wellbeing, who they are with, where they are, and what they are doing.  (`2019` - `ongoing`)
- [Shift-Diets](https://www.rivm.nl/burgerwetenschap/shift-diets) - Citizens take part in the study of the RIVM to investigate what measures motivate young adults to change their diet.  (`2020` - `ongoing`)
- [Stimmen](http://stimmen.nl/) - Citizens record themselves saying different words and take a quiz to determine what dialect they speak.  (`2017` - `ongoing`)
- [CurioUs?](https://www.rug.nl/sciencelinx/maatschappij/curious) - Citizens can go up to  the CurioUs lab to borrow measuring equipment and receive tips on how to use it. Moreover, the lab encourages the citizens to partake in different citizen science projects. For instance, the first project they took on was Onze Lucht (also on this list).  (`2020` - `ongoing`)
- [Kijk! Een gezonde wijk](http://www.kijkeengezondewijk.nl/#/info) - Citizens install an app to report on opportunities to sport and play in their neighborhood (`2017` - `2017`)
- [Hooked!](https://www.iedereenwetenschapper.be/projects/hoe-snel-herken-jij-een-liedje) - Citizens listen to fragments of songs from the Top 2000 and click a button when they remember what song is played.  (`2017` - `2017`)
- [Personagebank](http://personagebank.nl/) - Citizens answer questions on the characters in the book they last read.  (`2016` - `2019`)
- [Straatpoëzie](https://straatpoezie.nl/) - Citizens report streetpoetry, poetry that is placed out in the open and can be read by anyone.  (`2017` - `ongoing`)
- [Zicht op Licht](https://ronaldbierings.com/over-zicht-op-licht/) - Citizens report with the help of an app how dark it is in their surroundings and how much trouble they experience in their sight.  (`2015` - `2018`)
- [Stemmen – Gronings (Nedersaksisch)](https://woordwaark.nl/stemmen/) - Citizens record themselves saying different words and take a quiz to determine what dialect they speak.  (`2019` - `ongoing`)
- [Alle Scholen Verzamelen](https://www.wetenschapsknooppunten.nl/alle-scholen-verzamelen/citizen-science/) - Elementary school children partake in science projects and help gather data for research. (`2019` - `ongoing`)

### Uncategorized

- [Het Nieuwe Strepen](https://www.floron.nl/meedoen/het-nieuwe-strepen ) - Special form of ""Kilometerhokken Inventariseren"". It works similar to that project, but now the area is searched by two citizens, independent from each other.  (`2012` - `ongoing`)

<!---->

## Contribute or update project

Everyone is welcome to add a new citizen science project (from the Netherlands) to this
list. To keep the contribution process simple and fast, we ask you to follow
the guide below:

1. Search [previous Pull Requests](https://github.com/sodascience/awesome-citizen-science-nl/pulls) before making a new one, as your project may be a duplicate.

2. Browse to the [`data/categories`](data/categories) folder of this repository.

3. Select the category that belongs to the project you want to add and browse to that folder. If you cannot find the category that fits your project, go directly to step **4**.

4. Above the list of files, using the **Add file** drop-down, click **Create new file**.

5. In the file name field, type the name of the file and use as extension `yml`. For example: `NAME_OF_NEW_PROJECT.yml`. If you did not find the cateogry that fits your project, you need to add the new category name before the name of your project, for example: `NEW_CATEGORY/NAME_OF_NEW_PROJECT.yml`.

6. On the **Edit new file** tab, copy and paste the content below and fill it in with the information concerning your project. Next to each attribute, we provided a small description of the content we ask you to fill in. Please do follow these indications as it will speed up the process of review. If one or more attributes are unknown to you, feel free to leave the attribute empty (e.g. `data_accessibility: `). Finally, before proposing a new project remember to delete the comments (everything from ""#"" onwards) leaving only your newly added content.

```yaml
---
name: # Official name of the project. | *Compulsory
description: # Description of the project. Please try to summarise the main aim of the project. | *Compulsory
main_category: # Main category used to classify the project, choose only one! | *Compulsory
extra_categories: # Additional categories under which the project may be classified
organization: # Organizer(s) of the project
country: The Netherlands
location: # Regions/ Provinces included in the project
notes_location: # Additional notes on the location of the project (e.g. if specific regions are included/ excluded)
start_date: # Fill in the starting year
end_date: # If the project is ended, fill in the ending year
status: # Choose from: ongoing, recurring or ended
notes_duration: # Additional notes on the duration of the project
data_accessibility: # Choose from: Viewable, Probably viewable, Probably viewable once finished, No direct download
accessibility_for_research: # Choose from: yes, no or unsure
data_url: # Url to the main source of the project's data, if not available please use '-'
project_information_url: # Url to the main page of the project | *Compulsory
```

7. At the bottom of the page, in the **Propose new file** box add a title describing the new file you added. It might be also very simple as: `Add PROJECT NAME, DATE START - DATE END`. You might also specify the new addition further in the *optional extended description field*.

<p align=""center"">
  <img src=""docs/img/readme-propose-message.png"">
</p>

8. Below the commit message click **Propose new file**.

<!---->

## Citation

To cite this dataset in academic publications, you can cite the following Zenodo publication [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4724569.svg)](https://doi.org/10.5281/zenodo.4724569). 

```
Timmers, Annemarie, & Lugtig, Peter. (2021). List of Citizen Science Projects 
in the Netherlands (Version v2021.4.29) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.4724570
```

<!---->

## Contacts

This list of open science projects is curated by [Annemarie Timmers](https://nl.linkedin.com/in/annemarie-timmers-35381613a) (Utrecht University) and maintained by the [ODISSEI Social Data Science
(SoDa)](https://odissei-data.nl/nl/soda/) team.

<img src=""docs/img/readme-soda-logo.png"" width=""250px""></img>

Inspired by [Awesome Python](https://github.com/vinta/awesome-python).
",2022-08-12
https://github.com/sodascience/cbs_microdata_computing,"# Beyond the limits of the CBS RA environment: efficient programming and the ODISSEI secure supercomputer

Presentation and code for CBS microdata meeting on May 16th, 2022.

What to do when your CBS microdata analysis takes too many computational resources to run on the remote access environment? In this meeting, Erik-Jan van Kesteren (Utrecht University) will talk about solutions to this problem. It will be an accessible introduction to a variety of ways in which you can programme more efficiently when using microdata in your research. Furthermore, it will discuss when you should and should not move your project to the ODISSEI Secure Supercomputer. 

The introduction will include some live coding, exploring different options for project organisation, speeding up code, benchmarking, profiling, and reducing memory requirements. During his talk, Van Kesteren will also touch upon topics such as ""embarassingly parallel"", scientific programming, data pipelines, open source, and open science. Although the presentation will center around data analysis with R, these principles also hold for other languages, such as Python or Julia.


## Contact

This project is developed and maintained by the [ODISSEI Social Data
Science (SoDa)](https://odissei-data.nl/nl/soda/) team.

<img src=""word_colour-l.png"" alt=""SoDa logo"" width=""250px""/>

Do you have questions, suggestions, or remarks? File an issue in the
issue tracker or feel free to contact [Erik-Jan van
Kesteren](https://github.com/vankesteren)
([@ejvankesteren](https://twitter.com/ejvankesteren))
",2022-08-12
https://github.com/sodascience/ddi-synth,"# Dataverse synthesizer

This app lets you __synthesize__, __inspect__, and __download__ data from a [dataverse](https://dataverse.org/) data page (such as [this one](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/QAPATF)). You enter the data page URL (e.g., `https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/QAPATF`) and the app will present you with a fake, randomly generated synthetic dataset which you can save on disk:

![screenshot](img/screenshot.png)

## How it works
The app grabs the [DDI](https://ddialliance.org/)-formatted xml metadata of the dataset from the dataverse page, finds the variable-level information and generates columns accordingly. This also means that if your data page does not contain variable-level information, the app will not generate any data.

## How to run locally
This is a small prototype [Shiny](https://shiny.rstudio.com/) app. First, open `R` and make sure the dependencies are installed:
```r
install.packages(""tidyverse"", ""DT"", ""httr"", ""xml2"", ""truncnorm"")
```

Then run the app from `R` by entering the following code:

```r
shiny::runGitHub(""sodascience/ddi-synth"", ref = ""main"")
```

## Contact
This package is developed by the [ODISSEI Social Data Science (SoDa) team](https://odissei-data.nl/soda).

Do you have questions, suggestions, or remarks? File an issue in the issue tracker or feel free to contact Erik-Jan van Kesteren [@ejvankesteren](https://twitter.com/ejvankesteren).

<img src=""img/word_colour-l.png"" alt=""SoDa logo"" width=""250px""/>",2022-08-12
https://github.com/sodascience/empathy-viz,"# Dynamiek in beeld 

<!-- Include Github badges here (optional) -->
<!-- e.g. Github Actions workflow status -->

Application to be used in a clinical setting to understand empathy. The application allows clinician to conduct a survey and immediately visualize the results. No data is stored in the RShiny server. 

## Usage

<!-- We should add here -->
The app runs online at https://utrecht-university.shinyapps.io/empathy-viz/.

The app has three parts:

### Input
Background information of the participant including gender, age and survey code are provided in this section.


<img src=""man/resources/Screenshot_input.png"" alt=""Participant background information""/>



### Questionnaire
Questionnaire starts with a basic description of the survey, followed by six question, each with three subquestions. 
![Questions](man/resources/screenshot_question.png)

At the end of the questionnaire, the results of the survey can be downloaded.


### Visualization

Start with selecting the datasource. There is possibility to visualize the current survey or upload existing results saved in .csv format.

Visualization is available in three styles:




<img src=""man/resources/Screenshot_emot.png"" alt=""Dynamics in Emotions""/>



<img src=""man/resources/Screenshot_rel.png"" alt=""Dynamics in relationships"" />



<img src=""man/resources/Screenshot_rel_emot.png"" alt=""Dynamics in relationships * emotions""/>

## Adapting the survey with new questions

If you would like to modify the app and run locally, please follow the following steps:


### Set up survey
- `data/introduction.csv` - Text in the introduction page, explaining the survey to the participant
- `data/vignettes.csv` - ID of the questions, title, description, and attached figure (to be displayed in the left)
- `data/relationships.csv - Each vignette is described for two more types of relationships, i.e. stranger and foe.
- `data/RadioMatrixFrame.csv` - Scales associated (see image above)
- `data/ending.csv` - Text in the last page
- `data/guideline_vis.csv` - A short explanation of how to use visualization


### Run code
- Clone repository OR download repository (Code --> Download as ZIP)
- Start RStudio and navigate to the folder
- Click on run app

### Built with

- [shiny1.7.1](https://shiny.rstudio.com)

## Contributing

Contributions are what make the open source community an amazing place
to learn, inspire, and create. Any contributions you make are **greatly
appreciated**.

Please refer to the
[CONTRIBUTING](https://github.com/sodascience/osmenrich/blob/main/CONTRIBUTING.md)
file for more information on issues and pull requests.


<!-- Do not forget to also include the license in a separate file(LICENSE[.txt/.md]) and link it properly. -->
### License

The code in this project is released under [MIT license](LICENSE.md).

<!-- CONTACT -->

## Contact

**Dynamiek in Beeld** is project by [Dr. Minet de Wied](https://www.uu.nl/medewerkers/mdewied).
The technical implementation is provided by the [ODISSEI Social Data
Science (SoDa)](https://odissei-data.nl/nl/soda/) team.

Do you have questions, suggestions, or remarks on the technical implementation? File an issue in the
issue tracker or feel free to contact [Javier Garcia-Bernardo](https://github.com/jgarciab)
(:bird: [@jgarciab](<https://twitter.com/javiergb_com>)) or [Parisa 
Zahedi](https://github.com/parisa-zahedi)

<img src=""man/resources/word_colour-l.png"" alt=""SoDa logo"" width=""250px""/> 

Project Link: [https://github.com/sodascience/empathy-viz](https://github.com/sodascience/empathy-viz)
",2022-08-12
https://github.com/sodascience/night_globe,"# Correcting inferences for volunteer-collected data with geospatial sampling bias


[![DOI](https://zenodo.org/badge/379621562.svg)](https://zenodo.org/badge/latestdoi/379621562)


Repository containing reproducible code belonging to the manuscript _""Correcting inferences for volunteer-collected data with geospatial sampling bias""_ by Peter Lugtig, Annemarie Timmers, and Erik-Jan van Kesteren

## Reproducing the analysis
This is an R project with packages managed by `renv`, on R version `4.1.2`. Clone or download this repository, enter the folder and open the project file (`night_globe.Rproj`) in RStudio. Then, if you have installed the `renv` package, run `renv::restore()` to obtain the right versions of all packages used.

The starting point is the file [`01_data_loading.R`](./01_data_loading.R)

## Extended description
The Globe at Night project contains volunteer-collected data about the brightness of the sky. For example, in Pennsylvania in 2020, the following observations were made:
![](/img/raw_gan.png)

If researchers want to make inferences about how bright the night sky is in Pennsylvania, it would be optimal to observe the night sky at random locations in the state. The volunteer data is not randomly distributed: there is sampling bias. In this repository, we correct for such sampling bias by using geospatial covariates and geospatial models to predict sky brightness throughout Pennsylvania. 

As covariates, we use moon illumination, cloud cover as well as land use data from [https://www.mrlc.gov/](https://www.mrlc.gov/). 

![](/img/raw_landuse.png)

In addition, we have information about where the main roads lie in Pennsylvania. This is data from OpenStreetMaps:

![](/img/raw_highway.png)

Using different models with increasing levels of complexity we obtain the following predicted sky brightness values:

![](/img/model_predictions.png)

We validate these internally, using leave-one-out cross-validation. There, we conclude that the most complex model (model 8, with all covariates and kriging) leads to the best out-of-sample prediction performance.

We can additionally externally validate these models by comparing them against (log-)skyglow measurements derived from satellite imagery:

![](/img/skyglow.png)

![](/img/external_validation.png)

## Contact

This project is developed and maintained by the [ODISSEI Social Data
Science (SoDa)](https://odissei-data.nl/nl/soda/) team.

<img src=""word_colour-l.png"" alt=""SoDa logo"" width=""250px""/>

Do you have questions, suggestions, or remarks? File an issue in the
issue tracker or feel free to contact [Erik-Jan van
Kesteren](https://github.com/vankesteren)
([@ejvankesteren](https://twitter.com/ejvankesteren))

",2022-08-12
https://github.com/sodascience/osmenrich,"
<!-- README.md is generated from README.Rmd-->

<p align=""center"">

<a href=""https://github.com/sodascience/osmenrich"">
<img width=""30%"" height=""30%"" src=""man/figures/logo.png""> </a>

</p>

# osmenrich: enrich geocoded data using OpenStreetMap

<!-- badges: start -->

[![Github Action
test](https://github.com/sodascience/osmenrich/workflows/R-CMD-check/badge.svg)](https://github.com/sodascience/osmenrich/actions)
[![DOI](https://zenodo.org/badge/337555188.svg)](https://zenodo.org/badge/latestdoi/337555188)
[![Project Status: Active – The project has reached a stable, usable
state and is being actively
developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)
<!-- badges: end -->

The goal of `osmenrich` is to easily enrich geocoded data
(`latitude`/`longitude`) with geographic features from OpenStreetMap
(OSM). The main language of the package is `R` and this package is
designed to work with the [`sf`](https://r-spatial.github.io/sf/) and
[`osmdata`](https://cran.r-project.org/web/packages/osmdata/vignettes/osmdata.html)
packages for collecting and manipulating geodata.

## Installation

To install the package, you first need to have the `remotes` package
installed. If you do not have this package yet, please install it first
with:

``` r
install.packages(""remotes"")
```

If you do have this package, due to recent changes in GitHub’s naming of
branches, please make sure you have the latest version of `remotes` or
at least version `2.2`.

Once you did this, to continue the installation of the `osmenrich`
package, run:

``` r
remotes::install_github(""sodascience/osmenrich@main"")
```

or, for the development version, run:

``` r
remotes::install_github(""sodascience/osmenrich@develop"")
```

This will use the default public APIs for OSM data and routing (for
computing driving/walking distances and durations). **Do not use
`osmenrich` with default APIs for large datasets\!** If you want to
learn how to use `osmenrich` for large queries follow the instructions
in section [Local API Setup](#local-api-setup) below.

## Usage

### Simple enrichment example

Let’s enrich a spatial (`sf`) dataset (`sf_example`) with the number of
waste baskets in a radius of 500 meters from each of the point specified
in a dataset:

``` r
# Import libraries
library(tidyverse)
library(sf)
library(osmenrich)

# Create an example dataset to enrich
sf_example <-
  tribble(
    ~person,  ~lat,   ~lon,
    ""Alice"",  52.12,  5.09,
    ""Bob"",    52.13,  5.08,
  ) %>%
  sf::st_as_sf(
    coords = c(""lon"", ""lat""),
    crs = 4326
  )

# Print it
sf_example
#> Simple feature collection with 2 features and 1 field
#> geometry type:  POINT
#> dimension:      XY
#> bbox:           xmin: 5.08 ymin: 52.12 xmax: 5.09 ymax: 52.13
#> CRS:            EPSG:4326
#> # A tibble: 2 x 2
#>   person     geometry
#> * <chr>   <POINT [°]>
#> 1 Alice  (5.09 52.12)
#> 2 Bob    (5.08 52.13)
```

To enrich the `sf_example` dataset with “waste baskets” in a 500m
radius, you can create a query using the `enrich_osm()` function. This
function uses the bounding box created by the points present in the
example dataset and searches for the specified `key = ""amenity""` and
`value = ""waste_basket`. You can also add a custom `name` for the newly
created column and specify the radius (`r`) used in the search. See [Map
Features on the website of
OSM](https://wiki.openstreetmap.org/wiki/Map_features) for a complete
list of `key` and `value` combinations.

``` r
# Simple OSMEnrich query
sf_example_enriched <- sf_example %>%
  enrich_osm(
    name = ""n_waste_baskets"",
    key = ""amenity"",
    value = ""waste_basket"",
    r = 500
  )
#> Downloading data for waste_baskets... Done.
#> Downloaded 147 points, 0 lines, 0 polygons, 0 mlines, 0 mpolygons.
#> Computing distance matrix for n_waste_baskets...Done.
```

The resulting enriched dataset `sf_example_enriched` is a `sf` object
and can be printed as usual to inspect the newly added column
`n_waste_baskets`.

``` r
sf_example_enriched
#> Simple feature collection with 2 features and 2 fields
#> geometry type:  POINT
#> dimension:      XY
#> bbox:           xmin: 5.08 ymin: 52.12 xmax: 5.09 ymax: 52.13
#> geographic CRS: WGS 84
#> # A tibble: 2 x 3
#>   person     geometry n_waste_baskets
#> * <chr>   <POINT [°]>         <int>
#> 1 Alice  (5.09 52.12)            75
#> 2 Bob    (5.08 52.13)             1
```

The waste baskets column is now the result of summing all the
wastebaskets in a 500 meter radius for Alice and Bob:

![](man/figures/example_wastebaskets_r500.png)

## Local API setup

OSM enrichment can ask for a lot of data, which can overload public
APIs. If you intend to enrich large amounts of data or compute routing
distances (e.g., driving duration) between many points, you should set
up a local API endpoint.

Multiple `docker-compose` workflows for doing this are avaialble in the
separate [osmenrich\_docker
repository](https://github.com/sodascience/osmenrich_docker). Use the
`README` in the repository to select the workflow that fits your desired
outcome.

<img src=""man/figures/docker.png"" alt=""Docker logo"" width=""250px""/>

<!-- CONTRIBUTING -->

## Contributing

Contributions are what make the open source community an amazing place
to learn, inspire, and create. Any contributions you make are **greatly
appreciated**.

Please refer to the
[CONTRIBUTING](https://github.com/sodascience/osmenrich/blob/main/CONTRIBUTING.md)
file for more information on issues and pull requests.

## License and citation

The `osmenrich` package is published under the MIT license. When using
`osmenrich` for academic work, please cite:

    van Kesteren, Erik-Jan, Vida, Leonardo, de Bruin, Jonathan, & Oberski, Daniel. (2021, February 11).
    Enrich sf Data with Geographic Features from OpenStreetMaps (Version v1.0). Zenodo. http://doi.org/10.5281/zenodo.4534188

<!-- CONTACT -->

## Contact

This package is developed and maintained by the [ODISSEI Social Data
Science (SoDa)](https://odissei-data.nl/nl/soda/) team.

Do you have questions, suggestions, or remarks? File an issue in the
issue tracker or feel free to contact [Erik-Jan van
Kesteren](https://github.com/vankesteren)
(\[@ejvankesteren\](<https://twitter.com/ejvankesteren>)) or [Leonardo
Vida](https://github.com/leonardovida)
(\[@leonardojvida\](<https://twitter.com/leonardojvida>))

<img src=""man/figures/word_colour-l.png"" alt=""SoDa logo"" width=""250px""/>
",2022-08-12
https://github.com/sodascience/osmenrich_docker,"<img src=""img/docker.png"" width=""200px""></img>

# Docker-compose workflow for `osmenrich`

We provide a docker-compose workflow to install and run local instances of the Overpass API, used to serve data from openstreetmap, and OSRM, used to compute routing distances and durations.

This repository is maintained by the [ODISSEI Social Data Science (SoDa)](https://odissei-data.nl/nl/soda/) team.

<img src=""img/word_colour-l.png"" width=""250px""></img>

<!-- INSTALLATION -->
## Installation

We provide one supported way to install the Overpass API and OSRM servers used in `osmenrich`, which makes use of `docker-compose`.

We run instances of the Overpass API and of OSRM via `Docker`.

- The Overpass API uses the docker image from [wiktorn](https://github.com/wiktorn/Overpass-API).
- The OSRM instances use the official docker image of the OSRM project: [https://hub.docker.com/r/osrm/osrm-backend/](https://hub.docker.com/r/osrm/osrm-backend/).

**Note**: Docker needs to have **at least** 4GB of RAM and 2GB of Swap memory _available_ at the time of installation to complete the installation of the overpass API instance. In case your machine does not have enough memory, we suggest you to close the instance(s) of OSRM created using docker-compose to free up memory before trying again.

### Installation via Docker

We assume that `docker` is installed and running on your machine. If that is not the case, please head to the [Docker website](https://www.docker.com/products/docker-desktop) to download Docker Desktop.

Before setting up the server instances using the default settings set by the SoDa science team you need to the setup that most resembles the objective of your project. The table below offers an overview of the choices that we currently provide.


| Version     | Overpass server/ <br />OSM server | OSRM server for<br />distances/durations | Name to use |
| :---------- | :-------------------------------: | :--------------------------------------: | :---------: |
| Base        |                Yes                |                    No                    |   `base`    |
| Normal car  |                Yes                |               Only by car                |    `car`    |
| Normal foot |                Yes                |               Only by foot               |   `foot`    |
| Normal bike |                Yes                |               Only by bike               |   `bike`    |
| Advanced    |                Yes                |            Car, foot and bike            | `advanced`  |


#### On MacOS or Linux

Once you chose which setup is the most fitting with what required by your project, you need to carry out two steps:

**1.** Go to the docker folder for the use case you want to setup and modify what comes after the `=` for the following variables in the `.env` file:
   - To select a specific region to use with the OpenStreetMap server, go to <https://download.geofabrik.de/> and find the region of interest. Once found, add the name of the country and of the region (or subregion) after the `=` in the following two variable in the `.env` file:
     - `COUNTRY_MAP=` _add name of the country_
     - `REGION=` _add name of the region or subregion_
   - Within the same webpage, find the link to the _map file_ that ends with `.osm.bz2` (usually under the section `Other Formats and Auxiliary Files`) for the country or region you previoulsy selected and copy this link after the `=` in the following variable in the `.env` file:
     - `OVERPASS_PLANET_URL=` _add the link to the `.osm.bz2` file (usually under the section `Other Formats and Auxiliary Files`)_
   - To select the replication server, go to <http://download.openstreetmap.fr/replication/> and find the same country or region you selected above. When found, navigate to the `minute` folder and copy the URL after the `=` of the following variable:
     - `OVERPASS_DIFF_URL=` _add the URL of the replication server (needs to end with `/minute/`)_
     
**2.** Then, just use the following commands to download the map and start the containers:

```bash
  # You need to be in the **root** folder of the repository
  cd docker
  bash ./build.sh <name-of-the-chose-docker-setup>
```

#### On Windows

**1.** Follow **step 1** of the _MacOS or Linux guide_ with the only difference that you also need to download the `<your-location>-latest.osm.pbf` data for your location of interest to `docker/osrm/data` from <https://download.geofabrik.de/>

**2.** `cd` to the folder of your preferred choice of setup, for example `cd docker base`

**3.** Then, with Docker Desktop open, start docker compose with `docker-compose up`

The complete install procedure will take at least one hour. If you run into any problem, please look at the [troubleshooting section](#troubleshooting) below or open an [issue](#issue).

### Manual Installation

However, we also provided a small guide to manually setup the instances, available [here](MANUAL.md). This guide is not actively supported by the SoDa science team and we recommend to use it only if you know what you are doing and want to have precise control the installation settings for each instance.


<!-- TROUBLESHOOTING -->
## Troubleshooting

- If Docker complains about not being able to connect to its daemon, make sure you are in the `docker` group:
  
  ```cmd
  sudo usermod -aG docker $USER
  ```

<!-- CONTRIBUTING -->
## Contributing

Contributions are what make the open source community an amazing place to
learn, inspire, and create. Any contributions you make are **greatly
appreciated**.

In this project we use the
[Gitflow workflow](https://nvie.com/posts/a-successful-git-branching-model/)
to help us with continious development. Instead of having a single
`master`/`main` branch we use two branches to record the history of the
project: `develop` and `master`. The `master` branch is used only for the
official releases of the project, while the `develop` branch is used to
integrate the new features developed. Finally, `feature` branches are used to
develop new features or additions to the project that will be `rebased and
squash` in the `develop` branch.

The workflow to contribute with Gitflow becomes:

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/<AmazingFeature>`)
3. Commit your Changes (`git commit -m 'Add some <AmazingFeature>'`)
4. Push to the Branch (`git push origin feature/<AmazingFeature>`)
5. Open a Pull Request

## License

The `osmenrich_docker` repository is published under the MIT license.
",2022-08-12
https://github.com/sodascience/ossc_workshop,"# Supercomputing with R for Social Scientists

This repository contains the code and presentation of the workshop on parallel computing & cluster computing using R in collaboration with SURF. More information & sign-up can be found on the website [here](https://www.surf.nl/en/agenda/supercomputing-for-social-scientists-with-r).

![image](img/abm.png)

## Prerequisites
- Some `R` knowledge & experience :)
- `R`, minimum version `4.2.0` (download [here](https://cran.r-project.org/))
- If you are on Windows: `RTools` (download [here](https://cran.r-project.org/bin/windows/Rtools/))
- Recommended IDE: `RStudio` (download [here](https://www.rstudio.com/products/rstudio/download/#download))
- Install the following packages:
  ```r
  install.packages(c(""Rcpp"", ""tidyverse"", ""pbapply"", ""sf"", ""mgcv""))
  ```
- Download this repository ([link](https://github.com/sodascience/ossc_workshop/archive/refs/heads/main.zip)) and unzip it to a nice location
- Open the project by double-clicking on `ossc_workshop.Rproj` in the folder.

## Schedule

| Time  | Title                                           | Required resource                   |
| :---- | :---------------------------------------------- | :---------------------------------- |
| 13:00 | Lecture: computational limits in social science | [`slides`](./slides/intro.pdf)      |
| 13:45 | Hands-on: a parallel agent-based model in R     | [`assignment`](./hands_on/intro.md) |
| 14:30 | Break                                           | `your own coffee :)`                |
| 14:45 | Lecture: supercomputing with R                  | [`slides`](./slides/supercomp.pdf)  |
| 15:30 | Hands-on: submitting an R array job             | [`assignment`](./hands_on/super.md) |
| 16:15 | Break                                           | `your own coffee :)`                |
| 16:30 | Lecture: combining & analysing the results      | [`slides`](./slides/results.pdf)    |
| 16:45 | Conclusion & Q&A                                | `your remaining attention!`         |

## End result: a map of the Netherlands
The below map would take around 6 months to produce using the naïve implementation on a normal computer. You will learn how to compute it in a few hours on a supercomputer.

![map](img/segr_map.png)

## Contact

This project is developed and maintained by the [ODISSEI Social Data
Science (SoDa)](https://odissei-data.nl/nl/soda/) team.

<img src=""img/soda_logo.png"" alt=""SoDa logo"" width=""250px""/>

Do you have questions, suggestions, or remarks? File an issue in the
issue tracker or feel free to contact [Erik-Jan van
Kesteren](https://github.com/vankesteren)
([@ejvankesteren](https://twitter.com/ejvankesteren))
",2022-08-12
https://github.com/sodascience/presentation-osmenrich-sig,"# osmenrich sig presentation
Presentation & R code for presentation about `sf` and `osmenrich` for the applied data science special interest group open statistical software.

- `presentation.odp` contains the slides.
- `sf_osmenrich_analysis.R` contains R code for the sf and osmenrich parts of the presentation.
- `gierzwaluw_analysis.R` contains the R code for the example analysis discussed in the presentation.

For more information about the `osmenrich` package, go to [github.com/sodascience/osmenrich](https://github.com/sodascience/osmenrich).",2022-08-12
https://github.com/Southparkfan/cms,"cms
===

Simple CMS build in PHP with MySQL as database
",2022-08-12
https://github.com/Southparkfan/dn42-peering,"# AS 4242421964 / SPF-NET
I operate [AS 4242421964](https://explorer.burble.com/?#/4242421964) in DN42. The ranges `172.23.24.32/27` and `fd4f:b934:7802::/48` have been reserved for this purpose. At the moment, a full mesh is in use to avoid split-horizon issues in iBGP.

## Nodes
Quick overview of the available nodes:
| Hostname                  | Location       | Bandwidth    | OS              | Supported protocols |
|---------------------------|----------------|--------------|-----------------|---------------------|
| nl1.dn42.southparkfan.org | Netherlands 🇳🇱 | >= 1000 Mbit | Debian Buster   | WireGuard           |
| nl2.dn42.southparkfan.org | Netherlands 🇳🇱 | >= 1000 Mbit | Debian Bullseye | WireGuard           |
| fr1.dn42.southparkfan.org | France 🇫🇷      | >= 100 Mbit  | VyOS 1.4        | OpenVPN, WireGuard  |

### nl1.dn42.southparkfan.org
- Status: being decommissioned 🚧
- In use since: January 2022
- Physical location: Alblasserdam, Netherlands
- ISP: RamNode
- Clearnet IPv4: 176.56.237.46/32
- Clearnet IPv6: 2a00:d880:11::396/128
- DN42 IPv4: 172.23.24.33/32
- DN42 IPv6: fd4f:b934:7802::1/128
- Link-local IPv6: fe80::ade0
- Port: 4 + \<last four digits of your AS number\> (only if your AS number starts with `424242`)
- Multi-protocol BGP: supported ✔
- Supported protocols: WireGuard
- WireGuard public key: `OBpyD/rruK4pOCgRVrWVoexBNHZadtn4qwPmrEjt0gY=`
- Routing daemon: Bird 2.0

### nl2.dn42.southparkfan.org
- Status: online ✔
- In use since: July 2022
- Physical location: Haarlem, Netherlands
- ISP: Vultr
- Clearnet IPv4: 95.179.143.233/32
- Clearnet IPv6: 2001:19f0:5001:1f9a:5400:4ff:fe0d:d8f1/64
- DN42 IPv4: 172.23.24.35/32
- DN42 IPv6: fd4f:b934:7802::3/128
- Link-local IPv6: fe80::ade0
- Port: 4 + \<last four digits of your AS number\> (only if your AS number starts with `424242`)
- Multi-protocol BGP: supported ✔
- Supported protocols: WireGuard
- WireGuard public key: `GsmcrWbFTcje5e+hmc0D6qonrkTpHksGZuk0cLBjDmY=`
- Routing daemon: Bird 2.0

### fr1.dn42.southparkfan.org
- Status: being decommissioned 🚧
- In use since: May 2022
- Physical location: Paris, France
- ISP: Vultr
- Clearnet IPv4: 95.179.219.72/32
- Clearnet IPv6: 2a05:f480:1c00:f7b:5400:3ff:fefd:4756/128
- DN42 IPv4: 172.23.24.34/32
- DN42 IPv6: fd4f:b934:7802::2/128
- Link-local IPv6: fe80::ade0
- Port: 4 + \<last four digits of your AS number\> (only if your AS number starts with `424242`)
- Multi-protocol BGP: supported ✔
- Supported protocols: OpenVPN, WireGuard
- WireGuard public key: `DHKTSvfaIhXdT07udajvVGtIt0NGMAqTjztrdVKGQzQ=`
- Routing daemon: FRRouting

## Peering with me
Welcome! I have set the following standards:
- Only export routes with valid ROAs (and no clearnet routes, of course). I'll drop routes that are not valid.
- WireGuard is preferred for connecting with my nodes, but OpenVPN may be used if my node supports it.
- Multi-protocol BGP is preferred, but not required. Let me know if you need separate sessions for IPv4 and IPv6.
- I prefer to peer over IPv6 link-local, although it's not mandatory.
- Even though transit traffic is fine, please be considerate. The nodes do not have unlimited traffic. 

You can contact me via [Libera Chat](ircs://irc.libera.chat:6697) or [hackint](ircs://irc.hackint.org:6697) (my name is `southparkfan`). The template below may be used for reference:
```
* Name and email address:
* DN42 AS number:
* Your clearnet endpoint (<IPv4/IPv6/FQDN>:<port>):
* Protocol (usually WireGuard):
* WireGuard public key (if applicable):
* Session type (IPv4/IPv6 separate or multi-protocol BGP):
* NIC IP addresses (usually DN42 IPv4 + link-local IPv6): 
* BGP peering IP(s) (usually link-local IPv6):
```
",2022-08-12
https://github.com/Southparkfan/WMT-bots,"# WMT-bots
https://phabricator.wikimedia.org/T91941#1139353
",2022-08-12
https://github.com/stan-dev/.github,"## Stan-Dev organization repo

This repo holds pages displayed on the stan-dev Github organization's profile.
",2022-08-12
https://github.com/stan-dev/atom-language-stan,"[![APM license](https://img.shields.io/apm/l/language-stan.svg)](https://atom.io/packages/language-stan)
[![CI](https://github.com/stan-dev/atom-language-stan/actions/workflows/main.yml/badge.svg)](https://github.com/stan-dev/atom-language-stan/actions/workflows/main.yml)

# Stan language support in Atom

Adds syntax highlighting and snippets for [Stan](http://mc-stan.org/) files in [Atom](https://atom.io/).

![Example Stan file with syntax highlighting](https://github.com/stan-dev/atom-language-stan/blob/master/screenshot.png?raw=true)

Originally the grammar was converted from the [sublime-stan](https://github.com/dougalsutherland/sublime-stan) Stan package,
though many changes have been made since then.

This package was originally developed by [jrnold](https://github.com/jrnold).

### Installation

The recommended installation is using

```
apm install stan-dev/atom-language-stan
```

Older (outdated) versions can be installed using
```
apm install language-stan
```
or find and install it from the Packages tab under settings.

### Features

* Syntax highlighting
* Snippets
* Indentation and folding patterns
* Sets configuration so `stan-dev` preferences: no tabs, two spaces
",2022-08-12
https://github.com/stan-dev/bayesplot,"# bayesplot <img src=""man/figures/stanlogo.png"" align=""right"" width=""120"" />

<!-- badges: start -->
[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/bayesplot?color=blue)](https://cran.r-project.org/web/packages/bayesplot)
[![Downloads](https://cranlogs.r-pkg.org/badges/bayesplot?color=blue)](https://cran.rstudio.com/package=bayesplot)
[![R-CMD-check](https://github.com/stan-dev/bayesplot/workflows/R-CMD-check/badge.svg)](https://github.com/stan-dev/bayesplot/actions)
[![codecov](https://codecov.io/gh/stan-dev/bayesplot/branch/master/graph/badge.svg)](https://codecov.io/gh/stan-dev/bayesplot)
<!-- badges: end -->

### Overview

**bayesplot** is an R package providing an extensive library of plotting
functions for use after fitting Bayesian models (typically with MCMC). 
The plots created by **bayesplot** are ggplot objects, which means that after 
a plot is created it can be further customized using various functions from
the **ggplot2** package. 

Currently **bayesplot** offers a variety of plots of posterior draws, 
visual MCMC diagnostics, graphical posterior (or prior) predictive checking, 
and general plots of posterior (or prior) predictive distributions.

The idea behind **bayesplot** is not only to provide convenient functionality
for users, but also a common set of functions that can be easily used by
developers working on a variety of packages for Bayesian modeling, particularly
(but not necessarily) those powered by [**RStan**](https://mc-stan.org/rstan).

### Getting started 

If you are just getting started with **bayesplot** we recommend starting with
the tutorial [vignettes](https://mc-stan.org/bayesplot/articles/index.html), 
the examples throughout the package [documentation](https://mc-stan.org/bayesplot/reference/index.html), 
and the paper _Visualization in Bayesian workflow_:

* Gabry J, Simpson D, Vehtari A, Betancourt M, Gelman A (2019). Visualization in Bayesian workflow. 
_J. R. Stat. Soc. A_, 182: 389-402. doi:10.1111/rssa.12378. 
([journal version](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssa.12378),
[arXiv preprint](https://arxiv.org/abs/1709.01449),
[code on GitHub](https://github.com/jgabry/bayes-vis-paper))

### Resources

* [mc-stan.org/bayesplot](https://mc-stan.org/bayesplot) (online documentation, vignettes)
* [Ask a question](https://discourse.mc-stan.org) (Stan Forums on Discourse)
* [Open an issue](https://github.com/stan-dev/bayesplot/issues) (GitHub issues for bug reports, feature requests)

### Installation

* Install from CRAN:

```r
install.packages(""bayesplot"")
```

* Install latest development version from GitHub (requires [devtools](https://github.com/hadley/devtools) package):

```r
if (!require(""devtools"")) {
  install.packages(""devtools"")
}
devtools::install_github(""stan-dev/bayesplot"", dependencies = TRUE, build_vignettes = FALSE)
```

This installation won't include the vignettes (they take some time to build), but all of the vignettes are 
available online at [mc-stan.org/bayesplot/articles](https://mc-stan.org/bayesplot/articles/).


### Examples

Some quick examples using MCMC draws obtained from the [__rstanarm__](https://mc-stan.org/rstanarm) 
and [__rstan__](https://mc-stan.org/rstann) packages.

```r
library(""bayesplot"")
library(""rstanarm"")
library(""ggplot2"")

fit <- stan_glm(mpg ~ ., data = mtcars)
posterior <- as.matrix(fit)

plot_title <- ggtitle(""Posterior distributions"",
                      ""with medians and 80% intervals"")
mcmc_areas(posterior, 
           pars = c(""cyl"", ""drat"", ""am"", ""wt""), 
           prob = 0.8) + plot_title
```

<img src=https://github.com/stan-dev/bayesplot/blob/master/images/mcmc_areas-rstanarm.png width=50%/>

```r
color_scheme_set(""red"")
ppc_dens_overlay(y = fit$y, 
                 yrep = posterior_predict(fit, draws = 50))
```

<img src=https://github.com/stan-dev/bayesplot/blob/master/images/ppc_dens_overlay-rstanarm.png width=50%/>

```r
# also works nicely with piping
library(""dplyr"")
color_scheme_set(""brightblue"")
fit %>% 
  posterior_predict(draws = 500) %>%
  ppc_stat_grouped(y = mtcars$mpg, 
                   group = mtcars$carb, 
                   stat = ""median"")

```

<img src=https://github.com/stan-dev/bayesplot/blob/master/images/ppc_stat_grouped-rstanarm.png width=50%/>

```r
# with rstan demo model
library(""rstan"")
fit2 <- stan_demo(""eight_schools"", warmup = 300, iter = 700)
posterior2 <- extract(fit2, inc_warmup = TRUE, permuted = FALSE)

color_scheme_set(""mix-blue-pink"")
p <- mcmc_trace(posterior2,  pars = c(""mu"", ""tau""), n_warmup = 300,
                facet_args = list(nrow = 2, labeller = label_parsed))
p + facet_text(size = 15)
```

<img src=https://github.com/stan-dev/bayesplot/blob/master/images/mcmc_trace-rstan.png width=50% />

```r
# scatter plot also showing divergences
color_scheme_set(""darkgray"")
mcmc_scatter(
  as.matrix(fit2),
  pars = c(""tau"", ""theta[1]""), 
  np = nuts_params(fit2), 
  np_style = scatter_style_np(div_color = ""green"", div_alpha = 0.8)
)
```

<img src=https://github.com/stan-dev/bayesplot/blob/master/images/mcmc_scatter-rstan.png width=50% />

```r
color_scheme_set(""red"")
np <- nuts_params(fit2)
mcmc_nuts_energy(np) + ggtitle(""NUTS Energy Diagnostic"")
```

<img src=https://github.com/stan-dev/bayesplot/blob/master/images/mcmc_nuts_energy-rstan.png width=50% />

```r
# another example with rstanarm
color_scheme_set(""purple"")

fit <- stan_glmer(mpg ~ wt + (1|cyl), data = mtcars)
ppc_intervals(
  y = mtcars$mpg,
  yrep = posterior_predict(fit),
  x = mtcars$wt,
  prob = 0.5
) +
  labs(
    x = ""Weight (1000 lbs)"",
    y = ""MPG"",
    title = ""50% posterior predictive intervals \nvs observed miles per gallon"",
    subtitle = ""by vehicle weight""
  ) +
  panel_bg(fill = ""gray95"", color = NA) +
  grid_lines(color = ""white"")
```

<img src=https://github.com/stan-dev/bayesplot/blob/master/images/ppc_intervals-rstanarm.png width=55% />
",2022-08-12
https://github.com/stan-dev/ci-scripts,"stan-scripts
============
",2022-08-12
https://github.com/stan-dev/cmdstan,"<a href=""http://mc-stan.org"">
<img src=""https://raw.githubusercontent.com/stan-dev/logos/master/logo.png"" width=200 alt=""Stan Logo""/>
</a>

# CmdStan

<b>CmdStan</b> is the command line interface to Stan, a package providing

* full Bayesian inference using the No-U-Turn sampler (NUTS), a variant of Hamiltonian Monte Carlo (HMC),
* penalized maximum likelihood estimation (MLE) using optimization, either Newton or quasi-Newton algorithms BFGS and L-BFGS,
* approximate Bayesian inference using automatic differentiation variational inference (ADVI),
* a full first- and higher-order automatic differentiation library based on C++ template overloads, and
* a supporting fully-templated matrix, linear algebra, and probability special function library.

[![DOI](https://zenodo.org/badge/16967338.svg)](https://zenodo.org/badge/latestdoi/16967338)

### Home Page
Stan's home page, with links to everything you'll need to use Stan is:

[http://mc-stan.org/](http://mc-stan.org/)

### Interfaces
There are separate repositories here on GitHub for interfaces:
* [CmdStan](https://github.com/stan-dev/cmdstan) (command-line/shell interface)
* [CmdStanPy](https://github.com/stan-dev/cmdstanpy) (a lightweight interface to CmdStan for Python users)
* [CmdStanR](https://github.com/stan-dev/cmdstanr) (a lightweight interface to CmdStan for R users)
* [PyStan](https://github.com/stan-dev/pystan) (Python interface)
* [RStan](https://github.com/stan-dev/rstan) (R interface)

### Source Repository
CmdStan's source-code repository is hosted here on GitHub.

### Licensing
The Stan-to-C++ compiler written in OCaml, core Stan C++ code, and CmdStan are licensed under new BSD.

Note that the Stan math library depends on the Intel TBB library which is licensed under the Apache 2.0 license. This dependency implies an additional restriction as compared to the new BSD lincense alone. The Apache 2.0 license is incompatible with GPL-2 licensed code if distributed as a unitary binary. You may refer to the Apache 2.0 evaluation page on the [Stan Math wiki](https://github.com/stan-dev/math/wiki/Apache-2.0-License-Evaluation).

## Installation
1. Download the latest release tarball (use the ""green"" link) from: [CmdStan releases](https://github.com/stan-dev/cmdstan/releases)
2. Unpack the tarball.
3. From the folder, type `make` for a quick tutorial on how to build models.

## Installation using git
See [Getting Started with
CmdStan](https://github.com/stan-dev/cmdstan/wiki/Getting-Started-with-CmdStan) for instructions how to clone both CmdStan and Stan submodule.

## Troubleshooting

As of version 2.22, CmdStan has switched to the new Stan-to-C++ compiler, called [stanc3](https://github.com/stan-dev/stanc3).  This compiler is intended to be backwards compatible with the existing Stan language and should accept all models that compile under the release 2.21 compiler, (see this [list of bug fixes](https://github.com/stan-dev/stanc3/wiki/changes-from-stanc2)). 

If a model that compiled for versions prior to 2.22, please report a bug on the [stanc3](https://github.com/stan-dev/stanc3) repository. Otherwise, report the issue to the [CmdStan](https://github.com/stan-dev/cmdstan) repository.
",2022-08-12
https://github.com/stan-dev/cmdstanpy,"# CmdStanPy

[![codecov](https://codecov.io/gh/stan-dev/cmdstanpy/branch/master/graph/badge.svg)](https://codecov.io/gh/stan-dev/cmdstanpy)


CmdStanPy is a lightweight pure-Python interface to CmdStan which provides access to the Stan compiler and all inference algorithms.  It supports both development and production workflows. Because model development and testing may require many iterations, the defaults favor development mode and therefore output files are stored on a temporary filesystem. Non-default options allow all aspects of a run to be specified so that scripts can be used to distributed analysis jobs across nodes and machines.

CmdStanPy is distributed via PyPi: https://pypi.org/project/cmdstanpy/

or Conda Forge: https://anaconda.org/conda-forge/cmdstanpy

### Goals

- Clean interface to Stan services so that CmdStanPy can keep up with Stan releases.

- Provide access to all CmdStan inference methods.

- Easy to install,
  + minimal Python library dependencies: numpy, pandas
  + Python code doesn't interface directly with c++, only calls compiled executables

- Modular - CmdStanPy produces a MCMC sample (or point estimate) from the posterior; other packages do analysis and visualization.

- Low memory overhead - by default, minimal memory used above that required by CmdStanPy; objects run CmdStan programs and track CmdStan input and output files.


### Source Repository

CmdStanPy and CmdStan are available from GitHub: https://github.com/stan-dev/cmdstanpy and https://github.com/stan-dev/cmdstan


### Docs

The latest release documentation is hosted on  https://mc-stan.org/cmdstanpy, older release versions are available from readthedocs:  https://cmdstanpy.readthedocs.io

### Licensing

The CmdStanPy, CmdStan, and the core Stan C++ code are licensed under new BSD.

### Example

```python
import os
from cmdstanpy import cmdstan_path, CmdStanModel

# specify locations of Stan program file and data
stan_file = os.path.join(cmdstan_path(), 'examples', 'bernoulli', 'bernoulli.stan')
data_file = os.path.join(cmdstan_path(), 'examples', 'bernoulli', 'bernoulli.data.json')

# instantiate a model; compiles the Stan program by default
model = CmdStanModel(stan_file=stan_file)

# obtain a posterior sample from the model conditioned on the data
fit = model.sample(chains=4, data=data_file)

# summarize the results (wraps CmdStan `bin/stansummary`):
fit.summary()
```
",2022-08-12
https://github.com/stan-dev/cmdstanr,"# CmdStanR <img src=""man/figures/logo.png"" align=""right"" width=""120"" />

<!-- badges: start -->
[![CRAN status](https://www.r-pkg.org/badges/version/cmdstanr)](https://CRAN.R-project.org/package=cmdstanr)
[![Unit tests](https://github.com/stan-dev/cmdstanr/workflows/Unit%20tests/badge.svg)](https://github.com/stan-dev/cmdstanr/actions?workflow=Unit-tests)
[![Codecov test coverage](https://codecov.io/gh/stan-dev/cmdstanr/branch/master/graph/badge.svg)](https://app.codecov.io/gh/stan-dev/cmdstanr?branch=master)
<!-- badges: end -->

### Overview

CmdStanR is a lightweight interface to [Stan](https://mc-stan.org) for R users
(see [CmdStanPy](https://github.com/stan-dev/cmdstanpy) for Python).

If you are new to CmdStanR we recommend starting with these vignettes:

* [_Getting started with CmdStanR_](https://mc-stan.org/cmdstanr/articles/cmdstanr.html)

* [_How does CmdStanR work?_](https://mc-stan.org/cmdstanr/articles/cmdstanr-internals.html)

### Goals

* A clean interface to Stan services so that CmdStanR can keep up with Stan
releases.

* R code that doesn't interface directly with C++, only calls compiled executables.

* Modularity: CmdStanR runs Stan's algorithms and lets downstream modules do the
analysis.

* Flexible [BSD-3 license](https://opensource.org/licenses/BSD-3-Clause).


### Installation

#### Installing the R package

You can install the latest beta release of the **cmdstanr** R package with

```r
# we recommend running this is a fresh R session or restarting your current session
install.packages(""cmdstanr"", repos = c(""https://mc-stan.org/r-packages/"", getOption(""repos"")))
```
This does not install the vignettes, which take a long time to build, but they are always available
online at https://mc-stan.org/cmdstanr/articles/.

To instead install the latest development version of the package from GitHub use

```r
# install.packages(""remotes"")
remotes::install_github(""stan-dev/cmdstanr"")
```

#### Installing CmdStan

If you don't already have CmdStan installed then, in addition to installing the
R package, it is also necessary to install CmdStan using CmdStanR's
`install_cmdstan()` function. A suitable C++ toolchain is also required.
Instructions are provided in the [_Getting started with
CmdStanR_](https://mc-stan.org/cmdstanr/articles/cmdstanr.html) vignette.


### Contributing

There is a lot of work still to be done and contributions are very welcome!
If you are interested in contributing please comment on an open issue
or open a new one if none are applicable.

### License

CmdStanR, like CmdStan and the core Stan C++ code, is licensed under the
following licenses:

- Code: BSD 3-clause (https://opensource.org/licenses/BSD-3-Clause)
- Documentation: CC-BY 4.0 (https://creativecommons.org/licenses/by/4.0/)
",2022-08-12
https://github.com/stan-dev/design-docs,"# Stan-dev design documents repo
This is where we will start putting design documents and functional specs that attempt to describe major changes to our code base. We're taking heavy inspiration from the [Rust project's version](https://github.com/rust-lang/rfcs)

# Design Review

## When is design necessary?

Most changes can be implemented and reviewed via the [normal pull request workflow](https://github.com/stan-dev/math/wiki/Developer-Doc#pull-requests). Some changes are substantial enough that we ask that these be put through a design review for approval, similar to how an implementation is approved in a pull request.

## What is a design?

A major change to Stan will include at least two designs. The first is a functional specification that defines user-facing changes; an example here might be new proposed syntax for the Stan language, or an alternative architecture around how the interfaces communicate with a Stan model. The functional specification should be specific enough that a user/client would understand the impact of the change and start out summarizing the proposed change at a very high level in a sentence or two, identifying all of the clients, laying out an ontology of objects the change traffics in from the perspective of these clients, and then highlighting any additional concerns.

Once a functional spec has been agreed upon, we next need a design codified as a technical specification that defines how the functional spec will be implemented. The technical specification needs to be specific enough that it can be approved as a plan for implementation. These documents should not include every last detail of an implementation---for example, rarely will code other than function or class signatures be appropriate in either a functional or technical specification. They should include text to support the design and component interaction diagrams such as UML where appropriate.

[This design](https://github.com/stan-dev/design-docs/blob/master/designs/0001-logger-io.md) has a functional spec in the first 5 or 6 pages and then a fairly detailed technical spec following, and should be used as an example.

## Goals for design review

For user-facing designs, we want to give them a lot of exposure to the world before we implement something, as we rarely take away features and want to maintain backwards compatibility for as long as possible (possibly forever). Once it's in, it's in, and this puts a high bar on new additions and we want to make sure everyone has a chance to look them over.

Even if a change is not user-facing, good software design and architecture goes a long way towards long-term sustainability of a project and is especially important for open-source projects. Software engineering is primarily about how to break up your ideas into easily digestible chunks for communication to others, including yourself in the future.  Computers are not the audience for design documents, people are.

Design review ensures that the high level technical choices are simple, communicable, and robust enough to warrant a lower level implementation.  These design criteria must be applied to the (concepts and models)[https://medium.com/all-things-product-management/conceptual-debt-is-worse-than-technical-debt-5b65a910fd46] used to represent the design components and their interconnections.  Time spent hashing out the design before coding takes place is worth a decent multiple of the time spent after the design has been committed to code. Getting broad community involvement in brainstorming and designs helps remove the ego from the process and allows us to make decisions quickly and reliably.

## The life cycle of a major change

Designs and issues vary in a multitude of details concerning how they are staged.  A successful life cycle for a major issue might include:

* a brainstorming session on Discourse where the underlying problem is raised and broad, preliminary ideas are proposed and discussed,
* one or more developers turning these ideas into a design proposal, typically on a shared Wiki page,
* one proposal by the relevant managing stakeholder,
* the accepted design and accompanying discussion being summarized and specified at greater detail a GitHub issue, which may host its own lower-level discussion of technical issues,
* the design being implemented on a branch from the development branch of the relevant repositories with sufficient unit test coverage and documentation to release the feature,
* a pull request being made for the issue and goes through normal code review on GitHub, returning to the previous step until it is approved through code review
* the issue being merged into the development branches of the relevant repositories, which will include the feature in the next release.

Once a design is accepted, it should not be substantially changed. Minor changes may be submitted as amendments.  This is one reason why it may not be productive to go into fine-grained implementation details in the technical specification.  An approved design does not assign anyone in particular to work on the design, but one should create an issue linking or incorporating the design document.

## What requires a design review?

What exactly constitutes a truly major change is evolving based on community norms and varies depending on what part of the ecosystem you are proposing to change, but may include:
semantic or syntactic changes to the language that is not a bugfix,
adding or removing language features,
changing the interfaces among modules impacting multiple managing stakeholders, such as math, algorithms, services, and interfaces.

In general, thinking “oh, this might include a large refactor” or “this is really exciting” or “this might not work everywhere” are good indicators you should seek design approval.

Some changes that definitely do not require a design review:
new functionality that closely follows existing patterns, such as adding new functions to the math library and language,
additions that strictly improve objective quality criteria, such as false-positive warning removal, performance improvements, better test coverage, more informative error handling,
bug fixes that touch only a few files.

## Before submitting a design for review

It is generally a good idea to pursue feedback from other project developers beforehand.  The easiest way to do this is in a Discourse post that brings up the problem or issue and perhaps a rough outline of proposed solution at a high level and solicits further ideas and brainstorming. As a rule of thumb, receiving encouraging feedback from longstanding project developers on such a thread is a good indication that the design or issue is worth pursuing.

## The process

Write a design document roughly following [the template](0000-template.md). Designs that do not present convincing motivation, demonstrate understanding of the impact of the design, or are disingenuous about the drawbacks or alternatives tend to be poorly received.
Share it with the Stan core developers, advertising it on discourse and ideally the weekly meeting. Especially share it with one or more developers with design approval privileges.
Designs rarely go through this process unchanged, especially as alternatives and drawbacks are illuminated. Edit the existing Google doc and allow it to preserve the history.
Submit the design to a reviewer and work with them until it is accepted or postponed.

## Conducting design reviews

Anyone involved may schedule meetings to discuss the design with interested parties over a video chat. It’s generally a good idea to get an approving stakeholder involved for at least one of these. A design may be postponed if we don’t want to evaluate the proposal immediately for reasons such as not being urgent or depending on other designs to be completed.  Postponing a design indicates that at least someone thinks we might reasonably consider making the change in the future---otherwise it would just be closed. To close a design request for comments, a reviewer should describe the reasoning in the relevant discourse post and lock the thread.

If the motivation and the design are reasonably sound, a reviewer should interact with the proposer and community to give constructive feedback if the reviewer thinks the design could be accepted in the near term.

## Implementing an accepted design

No one in particular is obligated to implement an accepted design.  There will be an associated issue in the relevant repository that keeps track of anyone working on it. If you see an accepted design and want to work on it, post in that issue and tag the relevant parties.
",2022-08-12
https://github.com/stan-dev/docs,"# Repository `docs`

Repository for the sources and published documentation set, versioned for each Stan minor release.

* The Stan User's Guide - example models and techniques for coding statistical models in Stan and using them to do inference and prediction.
* The Stan Reference Manual - specification for Stan language and core inference algorithms.
* The Stan Functions Reference - functions and distributions built into the Stan language.
* The CmdStan Guide - guide to the reference command-line interface.


## Repository directory structure

* `src` : directory of source files for Stan and CmdStan guides and reference manuals, each in its own named subdirectory:
    + `src/cmdstan-guide` - CmdStan Guide
    + `src/functions-reference` - Stan Functions Reference
	+ `src/reference-manual` - Stan Reference Manual
	+ `src/stan-users-guide` - Stan Users Guide

* `docs`: the directory `docs` on branch `master` is the [publishing source](https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site) for the project pages site.  Whenever a verified member of the Stan organization pushes to `docs` on branch `master`,
GitHub (re)builds and (re)deploys the website.

## Documentation toolset

The documentation source files are written in [Rmarkdown](https://rmarkdown.rstudio.com)
and the RStudio's [bookdown package](https://github.com/rstudio/bookdown) converts these to HTML and pdf.
**Required bookdown version:** 0.23 or higher, cf [the bookdown changelog](https://github.com/rstudio/bookdown/blob/main/NEWS.md#changes-in-bookdown-version-023).
Reported via [issue #380](https://github.com/stan-dev/docs/issues/380).


The conversion engine is [Pandoc](https://pandoc.org).  It is bundled with RStudio.
To use the build scripts to build the docset,
you might need to [install Pandoc](https://pandoc.org/installing.html) separately.

To build the pdf version of the docs, you will need to [install LaTeX](https://www.latex-project.org/get/) as well.
The Stan documentation uses the [Lucida fonts](https://www.pctex.com/Lucida_Fonts.html),
which must be [installed manually](https://tex.stackexchange.com/questions/88423/manual-font-installation).


## Scripts to build and maintain the docset

**`build.py`**

The program `build.py` convert the markdown files under `src` to html and pdf and populates the `docs` dir with the generated documentation.
Requires Python 3.7 or higher, due to call to `subprocess.run`, kwarg `capture_output`.
  + 2 required argments:  <Major> <minor> Stan version, expecting 2 positive integer arguments, e.g. `2 28`
  + 2 optional arguments:  <document> <format>.  The document name corresponds to the name of the `src` subdirectory or `all`.  The output format is either `html` or `pdf`.


**Build script examples**

* `python build.py 2 28` - creates directory `docs/2_28` as needed; populates it will all generated documentation.
* `python build.py 2 28 functions-reference` - builds both HTML and pdf versions of the Stan functions reference, resulting documents are under `docs/2_28`
* `python build.py 2 28 functions-reference pdf` - builds only the pdf version of the Stan functions reference,  resulting document is `docs/2_28/functions-reference_2_28.pdf`
* `python build.py 2 28 all pdf` - builds all pdfs from the Stan documentation set, resulting pdfs are in `docs/2_28`.
 

**Additional scripts**

The release process generates a new documentation set and adds links and redirects across the docset.

* `add_redirects.py` manages the redirects from unversioned links to the latest version.
* `link_to_latest.py` adds the ""latest version"" link into a docset.

The Stan Functions Reference contains HTML comments which describe the function signature for all functions.  The script `extract_function_sigs.py` is used to scrape these signatures into a plain text file.

## Build a single docset in R:  `bookdown::render_book`

To build a single documet, you must have R or RStudio installed.
To build a document from the command line, first `cd` to the correct `src` dir,
then use the `Rscript` utility.

```
# build html
> Rscript -e ""bookdown::render_book('index.Rmd', output_format='bookdown::gitbook')""

# build pdf
> Rscript -e ""bookdown::render_book('index.Rmd', output_format='bookdown::pdf_book')""
```

The output will be written to subdirectory `_build`.

## GitHub Pages

This repository uses
[GitHub Pages](https://docs.github.com/en/pages/getting-started-with-github-pages)
to serve the
[project pages](https://docs.github.com/en/pages/getting-started-with-github-pages/about-github-pages#project-pages-sites) site
with URL https://mc-stan.org/docs.
The publishing strategy is to serve the contents of the directory `docs` on branch `master`.
The `docs` directory contains an empty file named `.nojekyll` so that GitHub will treat the contents
as pre-generated HTML instead of trying to run [jekyll](https://jekyllrb.com).

",2022-08-12
https://github.com/stan-dev/example-models,"## Example Models

This repository holds open source Stan models, data simulators, and real data.  There are models translating those found in books, most of the BUGS examples, and some basic examples used in the manual.

#### Books

* [Applied Regression Modeling](https://github.com/stan-dev/example-models/wiki/ARM-Models) (Gelman and Hill 2007)
* [Bayesian Cognitive Modeling](https://github.com/stan-dev/example-models/tree/master/Bayesian_Cognitive_Modeling) (Lee and Wagenmakers 2014)
* [Bayesian Population Analysis using WinBUGS: A Hierarchical Perspective](https://github.com/stan-dev/example-models/tree/master/BPA) (Kéry and Schaub 2012)

#### BUGS examples

- [BUGS Models (volumes 1--3)](https://github.com/stan-dev/example-models/wiki/BUGS-Examples)

#### Basics

- Basic Distributions
- Basic Estimators

## Organization

Each example model should be organized to include the following

1.  Stan program(s) implementing the model (and variants),
2.  program to simulate data (in R or Python),
3.  simulated data file itself (for now in .data.R dump format). For each Stan file(s), such as foo.stan and bar.stan, there must be a foo.data.R file and a bar.data.R file in the same subdirectory as the .stan file (even if foo.data.R is the same as bar.data.R).
4.  summary of output fit for simulated data file (text),
    (a) check diagnostics for post-warmup iterations: n_divergent is zero for all post-warmup iterations, tree_depth < max_depth for all post-warmup iterations,
    (b) check for all parameters (and unnormalized density lp__) that R_hat < 1.1 and n_eff > 20, and
    (c) if diagnostics fail improve model with a note explaining the fix.
5.  any real data sets with summary of fit (in original and Stan-readable format, with munging program from original to Stan-readable),
    (a) check diagnostics (as above),
    (b) check convergence (as above), and
    (c) if fit is poor then improve model with a note explaining the fix.
6.  citations to source for model and/or data,
7.  documentation needed to understand the model (LaTeX, text, or HTML), including discussion of parameterizations and/or optimizations and their relations to performance (and to each other if there are multiple models),
8.  keywords or other tags to help organize by category (e.g., from manual, from BUGS volume, from book, involving logistic regression, application area such as population model, model type such as IRT or mark-recapture, etc.)
9.  author/copyright-holder info and open-source license info if not new BSD.

The idea is to encourage people to go through *all* of these steps for their models, particularly 3 and 4, which often get overlooked. And if the example cannot be executed via rstan::stan_demo(), then something is wrong.

### Licensing

All of the example models are copyrighted by their author(s) or assignees under the new BSD license unless another open-source license is explicitly stipulated in the directory containing the model.
",2022-08-12
https://github.com/stan-dev/gmo,"# Gradient-based marginal optimization

__gmo__ is an R package for fast optimization of marginal posterior
distributions. Using a stochastic gradient-based algorithm, gmo
estimates a set of parameters from a model while marginalizing out the
rest. This provides uncertainty over any nuisance parameters, and
generalizes parameter estimation using marginal densities. It acts as
a middleground between full Bayesian inference over all parameters and
point estimation over all parameters.

Here is an example for a mixture model from
[Rubin's 8 schools analysis (1981)](http://jeb.sagepub.com/content/6/4/377.short).
Each data point belongs to one of 8 groups, and gmo estimates the mean
and variance parameters of the prior on each group.
```R
library(gmo)
library(rstan)

data <- list(J = 8,
             K = 2,
             y = c(28,  8, -3,  7, -1,  1, 18, 12),
             sigma = c(15, 10, 16, 11,  9, 11, 10, 18))

fit.gmo <- gmo(""models/8schools.stan"", ""models/8schools_local.stan"", data=data)
```
The two Stan programs used above are found [here](demo/models/8schools.stan) and
[here](demo/models/8schools_local.stan). More examples are found in [`demo/`](demo/).

The gmo package uses a modified Stan program in order to distinguish
between parameters to estimate and parameters to marginalize out.
<!--[A tutorial for writing this Stan program is available-->
<!--here](https://github.com/gelman/gmo/wiki/Tutorial).   -->

## Features

The core feature of gmo is a fast way to maximize marginal posterior
densities, which includes, for example, maximum marginal likelihood, empirical
Bayes, and type II maximum likelihood. It is done in an iterative
scheme that is closely inspired by the EM algorithm. Here are
additional features it supports:

+ Support for all models written in [Stan](http://mc-stan.org)
<!--+ Specialized algorithms for mixed-effects models in-->
<!--  [lme4](https://github.com/lme4/lme4)              -->
+ Uncertainty using covariance estimates
+ Approximate maximum marginal likelihood
+ Penalized maximum marginal likelihood
+ Fully maximum marginal likelihood
+ Bayesian inference with data-dependent priors

## Installation
<!--(TODO not submitted to CRAN yet)-->
<!--To install the latest version from CRAN:                           -->
<!--```R                                                               -->
<!--install.packages(""gmo"")                                            -->
<!--```                                                                -->
GMO is experimental software and undergoing development. We plan to
submit to CRAN once it is ready.

To install the latest development version from Github:
```R
# install.packages(""devtools"")
devtools::install_github(""stan-dev/gmo"")
```

## Citation

We appreciate citations for GMO if you apply or build off it in your work.

+ Dustin Tran, Andrew Gelman, and Aki Vehtari. 2016. Gradient-based marginal optimization. In preparation.

```
@article{tran2016gmo,
  title = {Gradient-based marginal optimization},
  author = {Dustin Tran and Andrew Gelman and Aki Vehtari},
  journal = {In preparation},
  year = {2016}
}
```
",2022-08-12
https://github.com/stan-dev/httpstan,"========
httpstan
========

.. image:: https://raw.githubusercontent.com/stan-dev/logos/master/logo.png
    :alt: Stan logo
    :height: 333px
    :width: 333px
    :scale: 40 %

|pypi|

HTTP-based REST interface to Stan, a package for Bayesian inference.

An HTTP 1.1 interface to the Stan_ C++ package, **httpstan** is a shim_ that
allows users to interact with the Stan C++ library using a REST API. The
package is intended for use as a universal backend for frontends which know how
to make HTTP requests. The primary audience for this package is developers.

In addition to providing the essential functionality of the command-line interface
to Stan (CmdStan_) over HTTP, **httpstan** provides the following features:

* Automatic caching of compiled Stan models
* Automatic caching of samples from Stan models
* Parallel sampling

Documentation: `https://httpstan.readthedocs.org <https://httpstan.readthedocs.org>`_.

Requirements
============

- macOS or Linux.
- C++ compiler: gcc ≥9.0 or clang ≥10.0.

Background
==========

**httpstan** is a shim_ allowing clients able to make HTTP-based requests to
call functions in the Stan C++ library's ``stan::services`` namespace.
**httpstan** was originally developed as a ""backend"" for a Stan interface
written in Python, PyStan_.

Stability and maintainability are two overriding goals of this software package.

Install
=======

.. These instructions appear in both README.rst and installation.rst

::

    $ python3 -m pip install httpstan


Usage
=====

After installing ``httpstan``, running the module will begin listening on
localhost, port 8080::

    python3 -m httpstan

In a different terminal, make a POST request to
``http://localhost:8080/v1/models`` with Stan program code to compile the
program::

    curl -H ""Content-Type: application/json"" \
        --data '{""program_code"":""parameters {real y;} model {y ~ normal(0,1);}""}' \
        http://localhost:8080/v1/models

This request will return a model name along with all the compiler output::

    {""compiler_output"": ""In file included from …"", ""stanc_warnings"": """", ""name"": ""models/xc2pdjb4""}

(The model ``name`` depends on the platform and the version of Stan.)

Drawing samples from this model using default settings requires two steps: (1)
launching the sampling operation and (2) retrieving the output of the operation
(once it has finished).

First we make a request to launch the sampling operation::

    curl -H ""Content-Type: application/json"" \
        --data '{""function"":""stan::services::sample::hmc_nuts_diag_e_adapt""}' \
        http://localhost:8080/v1/models/xc2pdjb4/fits

This request instructs ``httpstan`` to draw samples from the normal
distribution described in the model. The function name picks out a specific
function in the ``stan::services`` namespace found in the Stan C++ library (see
the Stan C++ documentation for details).  This request will return immediately
with a reference to a long-running fit operation::

    {""name"": ""operations/gkf54axb"", ""done"": false, ""metadata"": {""fit"": {""name"": ""models/xc2pdjb4/fits/gkf54axb""}}}

Once the operation is complete, the ""fit"" can be retrieved. The name of the fit,
``models/xc2pdjb4/fits/gkf54axb``, is included in the ``metadata`` field of the operation.
The fit is saved as sequence of JSON-encoded messages. These messages are strung together
with newlines. To retrieve these messages, saving them locally in the file
``myfit.jsonlines``, make the following request::

    curl http://localhost:8080/v1/models/xc2pdjb4/fits/gkf54axb > myfit.jsonlines

The Stan ""fit"", saved in ``myfit.jsonlines``, aggregates all messages. By reading
them one by one you can recover all messages sent by the Stan C++ library.

Citation
========

We appreciate citations as they let us discover what people have been doing
with the software. Citations also provide evidence of use which can help in
obtaining grant funding.

To cite httpstan in publications use:

Riddell, A., Hartikainen, A., & Carter, M. (2021). httpstan (4.4.0). https://pypi.org/project/httpstan

Or use the following BibTeX entry::

    @misc{httpstan,
      title = {httpstan (4.4.0)},
      author = {Riddell, Allen and Hartikainen, Ari and Carter, Matthew},
      year = {2021},
      month = mar,
      howpublished = {PyPI}
    }

Please also cite Stan.

License
=======

ISC License.

.. _shim: https://en.wikipedia.org/wiki/Shim_%28computing%29
.. _CmdStan: http://mc-stan.org/interfaces/cmdstan.html
.. _PyStan: http://mc-stan.org/interfaces/pystan.html
.. _Stan: http://mc-stan.org/
.. _`OpenAPI documentation for httpstan`: api.html

.. |pypi| image:: https://img.shields.io/pypi/v/httpstan.svg
    :target: https://pypi.org/project/httpstan/
    :alt: pypi version
",2022-08-12
https://github.com/stan-dev/httpstan-wheels,"httpstan-wheels
===============

We use [multibuild](https://github.com/matthew-brett/multibuild) to build wheels.

No manual action is required to make a release to PyPI.
GitHub Actions tries to release previously-unreleased versions of `httpstan` twice a week.

## Notes

### Differences from the standard ``multibuild`` instructions

- `BUILD_COMMIT` is automatically set to the most recent tagged version of `httpstan`. See `.github/workflows/wheels.yml`.
- ``MACOSX_DEPLOYMENT_TARGET`` is currently set to ``10.9``. Multibuild gives the variable
  a different default value. This environment variable is set in `.github/workflows/wheels.yml`.
",2022-08-12
https://github.com/stan-dev/jenkins-shared-libraries,"# jenkins-shared-libraries
Libraries for our Jenkinsfiles
",2022-08-12
https://github.com/stan-dev/logos,"# logos

Repository for Stan-related logos.

The Stan name and logo are trademarks of NumFOCUS under the direction of the Stan Leadership Body.
",2022-08-12
https://github.com/stan-dev/loo,"# loo <img src=""man/figures/stanlogo.png"" align=""right"" width=""120"" />

<!-- badges: start -->
[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/loo?color=blue)](https://cran.r-project.org/web/packages/loo)
[![RStudio_CRAN_mirror_downloads_badge](https://cranlogs.r-pkg.org/badges/loo?color=blue)](https://cran.r-project.org/web/packages/loo)
[![codecov](https://codecov.io/gh/stan-dev/loo/branch/master/graph/badge.svg)](https://codecov.io/github/stan-dev/loo?branch=master)
[![R-CMD-check](https://github.com/stan-dev/loo/workflows/R-CMD-check/badge.svg)](https://github.com/stan-dev/loo/actions)
<!-- badges: end -->

### Efficient approximate leave-one-out cross-validation for fitted Bayesian models

__loo__ is an R package that allows users to compute efficient approximate
leave-one-out cross-validation for fitted Bayesian models, as well as model
weights that can be used to average predictive distributions. 
The __loo__ package package implements the fast and stable computations for 
approximate LOO-CV and WAIC from

* Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model 
evaluation using leave-one-out cross-validation and WAIC. 
_Statistics and Computing_. 27(5), 1413--1432. 
doi:10.1007/s11222-016-9696-4. [Online](https://link.springer.com/article/10.1007/s11222-016-9696-4), 
[arXiv preprint arXiv:1507.04544](https://arxiv.org/abs/1507.04544).

and computes model weights as described in

* Yao, Y., Vehtari, A., Simpson, D., and Gelman, A. (2018). Using
stacking to average Bayesian predictive distributions. In Bayesian
Analysis, doi:10.1214/17-BA1091. 
[Online](https://projecteuclid.org/euclid.ba/1516093227),
[arXiv preprint arXiv:1704.02030](https://arxiv.org/abs/1704.02030).

From existing posterior simulation draws, we compute approximate LOO-CV using
Pareto smoothed importance sampling (PSIS), a new procedure for regularizing
importance weights. As a byproduct of our calculations, we also obtain
approximate standard errors for estimated predictive errors and for comparing
predictive errors between two models. We recommend PSIS-LOO-CV instead of WAIC, 
because PSIS provides useful diagnostics and effective sample size and Monte 
Carlo standard error estimates.


### Resources

* [mc-stan.org/loo](https://mc-stan.org/loo) (online documentation, vignettes)
* [Ask a question](https://discourse.mc-stan.org) (Stan Forums on Discourse)
* [Open an issue](https://github.com/stan-dev/loo/issues) (GitHub issues for bug reports, feature requests)


### Installation

* Install the latest release from CRAN:

```r
install.packages(""loo"")
```

* Install the latest development version from GitHub:

```r
# install.packages(""remotes"")
remotes::install_github(""stan-dev/loo"")
```

We do _not_ recommend setting `build_vignettes=TRUE` when installing from GitHub
because some of the vignettes take a long time to build and are always available
online at [mc-stan.org/loo/articles/](https://mc-stan.org/loo/articles/).

### Python and Matlab/Octave Code

Corresponding Python and Matlab/Octave code can be found at the
[avehtari/PSIS](https://github.com/avehtari/PSIS) repository.

",2022-08-12
https://github.com/stan-dev/math,"<div><a href=""https://zenodo.org/badge/latestdoi/38388440""><img src=""https://zenodo.org/badge/38388440.svg""/></a></div>

The <b>Stan Math Library</b> is a C++, reverse-mode automatic
differentiation library designed to be usable, extensive and
extensible, efficient, scalable, stable, portable, and redistributable
in order to facilitate the construction and utilization of algorithms
that utilize derivatives.


Licensing
---------
The Stan Math Library is licensed under the [new BSD
license](https://github.com/stan-dev/math/blob/develop/LICENSE%2Emd).

The Stan Math Library depends on the Intel TBB library which is
licensed under the Apache 2.0 license. This dependency implies an
additional restriction as compared to the new BSD license alone. The
Apache 2.0 license is incompatible with GPL-2 licensed code if
distributed as a unitary binary. You may refer to the Licensing page on the [Stan wiki](https://github.com/stan-dev/stan/wiki/Stan-Licensing).

Required Libraries
------------------
Stan Math depends on four libraries:

- Boost (version 1.78.0): [Boost Home Page](https://www.boost.org)
- Eigen (version 3.3.9: [Eigen Home Page](https://eigen.tuxfamily.org/index.php?title=Main_Page)
- SUNDIALS (version 6.1.1): [Sundials Home Page](https://computing.llnl.gov/projects/sundials)
- Intel TBB (version 2020.3): [Intel TBB Home Page](https://www.threadingbuildingblocks.org)

These are distributed under the `lib/` subdirectory. Only these
versions of the dependent libraries have been tested with Stan Math.

Documentation
------------

Documentation for Stan math is available at [mc-stan.org/math](https://mc-stan.org/math/)

Contributing
------------

We love contributions from everyone in the form of good discussion, issues, and pull requests.
If you are interested in contributing to Stan math please check the Contributor Guide at [mc-stan.org/math](https://mc-stan.org/math/).

Installation
------------
The Stan Math Library is a C++ library which depends on the Intel TBB
library and requires for some functionality (ordinary differential
equations and root solving) the Sundials library. The build system is
the make facility, which is used to manage all dependencies.

A simple hello world program using Stan Math is as follows:

```cpp
#include <stan/math.hpp>
#include <iostream>

int main() {
  std::cout << ""log normal(1 | 2, 3)=""
            << stan::math::normal_log(1, 2, 3)
            << std::endl;
}
```

If this is in the file `/path/to/foo/foo.cpp`, then you can compile
and run this with something like this, with the `/path/to` business
replaced with actual paths:

```bash
> cd /path/to/foo
> make -j4 -f /path/to/stan-math/make/standalone math-libs
> make -f /path/to/stan-math/make/standalone foo
> ./foo
log normal(1 | 2, 3)=-2.07311
```

The first make command with the `math-libs` target ensures that all
binary dependencies of Stan Math are built and ready to use. The `-j4`
instructs `make` to use 4 cores concurrently which should be adapted
to your needs. The second make command ensures that the Stan Math
sources and all of the dependencies are available to the compiler when
building `foo`.

An example of a real instantiation whenever the path to Stan Math is
`~/stan-dev/math/`:

```bash
> make -j4 -f ~/stan-dev/math/make/standalone math-libs
> make -f ~/stan-dev/math/make/standalone foo
```
The `math-libs` target has to be called only once, and can be omitted
for subsequent compilations.

The standalone makefile ensures that all the required `-I` include
statements are given to the compiler and the necessary libraries are
linked: `~/stan-dev/math` and `~/stan-dev/math/lib/eigen_3.3.9` and
`~/stan-dev/math/lib/boost_1.78.0` and
`~/stan-dev/math/lib/sundials_6.1.1/include` and
`~/stan-dev/math/lib/tbb_2020.3/include`. The
`~/stan-dev/math/lib/tbb` directory is created by the `math-libs`
makefile target automatically and contains the dynamically loaded
Intel TBB library. The flags `-Wl,-rpath,...` instruct the linker to
hard-code the path to the dynamically loaded Intel TBB library inside
the stan-math directory into the final binary. This way the Intel TBB
is found when executing the program.

Note for Windows users: On Windows the `-rpath` feature as used by
Stan Math to hardcode an absolute path to a dynamically loaded library
does not work. On Windows the Intel TBB dynamic library `tbb.dll` is
located in the `math/lib/tbb` directory. The user can choose to copy
this file to the same directory of the executable or to add the
directory `/path/to/math/lib/tbb` as absolute path to the system-wide
`PATH` variable.

Intel TBB
---------

`math` supports the new interface of Intel TBB, can be configured to use an external copy of TBB (e.g., with [`oneTBB`](https://github.com/oneapi-src/oneTBB) or the system TBB library), using the `TBB_LIB` and `TBB_INC` environment variables.

To build the development version of `math` with [`oneTBB`](https://github.com/oneapi-src/oneTBB):

- Install [`oneTBB`](https://github.com/oneapi-src/oneTBB).

For example, installing [`oneTBB`](https://github.com/oneapi-src/oneTBB) on Linux 64-bit (`x86_64`) to `$HOME` directory (change if needed!):
```bash
TBB_RELEASE=""https://api.github.com/repos/oneapi-src/oneTBB/releases/latest""
TBB_TAG=$(curl --silent $TBB_RELEASE | grep -Po '""tag_name"": ""\K.*?(?="")')
TBB_VERSION=${TBB_TAG#?}

wget https://github.com/oneapi-src/oneTBB/releases/download/v${TBB_VERSION}/oneapi-tbb-${TBB_VERSION}-lin.tgz
tar zxvf oneapi-tbb-$TBB_VERSION-lin.tgz -C $HOME

export TBB=""$HOME/oneapi-tbb-$TBB_VERSION""
```
Note that you may replace `TBB_VERSION=${TBB_TAG#?}` with a custom version number if needed ( check available releases [here](https://github.com/oneapi-src/oneTBB/releases) ).

- Set the TBB environment variables (specifically: `TBB` for the installation prefix, `TBB_INC` for the directory that includes the header files, and `TBB_LIB` for the libraries directory).

For example, installing [`oneTBB`](https://github.com/oneapi-src/oneTBB) on Linux 64-bit (`x86_64`) to `$HOME` directory (change if needed!):
```bash
source $TBB/env/vars.sh intel64

export TBB_INC=""$TBB/include""
export TBB_LIB=""$TBB/lib/intel64/gcc4.8""
```

- Set `Stan` local compiler flags to use the new TBB interface:
```bash
mkdir -p ~/.config/stan
echo TBB_INTERFACE_NEW=true>> ~/.config/stan/make.local
```

Compilers
---------

The above example will use the default compiler of the system as
determined by `make`. On Linux this is usually `g++`, on MacOS
`clang++`, and for Windows this is `g++` if the RTools for Windows are
used. There's nothing special about any of these and they can be
changed through the `CXX` variable of `make`. The recommended way to
set this variable for the Stan Math library is by creating a
`make/local` file within the Stan Math library directory. Defining
`CXX=g++` in this file will ensure that the GNU C++ compiler is always
used, for example. The compiler must be able to fully support C++11
and partially the C++14 standard. The `g++` 4.9.3 version part of
RTools for Windows currently defines the minimal C++ feature set
required by the Stan Math library.

Note that whenever the compiler is changed, the user usually must
clean and rebuild all binary dependencies with the commands:
```bash
> make -f path/to/stan-math/make/standalone math-clean
> make -j4 -f path/to/stan-math/make/standalone math-libs
```
This ensures that the binary dependencies are created with the new
compiler.
",2022-08-12
https://github.com/stan-dev/MathematicaStan,"#+OPTIONS: toc:nil todo:nil pri:nil tags:nil ^:nil tex:t
#+TITLE: MathematicaStan v2.1
#+SUBTITLE: A Mathematica (v11+) package to interact with CmdStan
#+AUTHOR: Picaud Vincent

# +TOC: headlines 3

* Table of contents                                            :TOC_3:noexport:
- [[#introduction][Introduction]]
  - [[#news][News]]
    - [[#2020-12-21][2020-12-21]]
    - [[#2019-06-28][2019-06-28]]
- [[#installation][Installation]]
  - [[#the-stan-cmdstan-shell-interface][The Stan CmdStan shell interface]]
  - [[#the-mathematica-cmdstan-package][The Mathematica CmdStan package]]
  - [[#first-run][First run]]
- [[#tutorial-1-linear-regression][Tutorial 1, linear regression]]
  - [[#introduction-1][Introduction]]
  - [[#stan-code][Stan code]]
  - [[#code-compilation][Code compilation]]
  - [[#simulated-data][Simulated data]]
  - [[#create-the-datar-data-file][Create the =data.R= data file]]
  - [[#run-stan-likelihood-maximization][Run Stan, likelihood maximization]]
  - [[#load-the-csv-result-file][Load the CSV result file]]
  - [[#run-stan-variational-bayes][Run Stan, Variational Bayes]]
  - [[#more-about-option-management][More about Option management]]
    - [[#overwriting-default-values][Overwriting default values]]
    - [[#reading-customized-values][Reading customized values]]
    - [[#erasing-customized-option-values][Erasing customized option values]]
- [[#tutorial-2-linear-regression-with-more-than-one-predictor][Tutorial 2, linear regression with more than one predictor]]
  - [[#parameter-arrays][Parameter arrays]]
  - [[#simulated-data-1][Simulated data]]
  - [[#exporting-data][Exporting data]]
  - [[#run-stan-hmc-sampling][Run Stan, HMC sampling]]
  - [[#load-the-csv-result-file-1][Load the CSV result file]]

* Introduction

*MathematicaStan* is a package to interact with [[http://mc-stan.org/interfaces/cmdstan][CmdStan]] from
Mathematica. 

It is developed under *Linux* and is compatible with *Mathematica v11+*

It should work under *MacOS* and also under *Windows*.

*Author & contact:* picaud.vincent at gmail.com

** News
   
*** 2020-12-21
    
*New MathematicaStan version 2.1!*

This version has been fixed and should now run under Windows.

I would like to thank *Ali Ghaderi* who had the patience to help me to
debug the Windows version (I do not have access to this OS). Nothing
would have been possible without him. All possibly remaining bugs are
mine.
 
As a remainder also note that one should not use path/filename with
spaces (=Make= really does not like that). This consign is also true
under Linux or MacOS. See [[https://stackoverflow.com/questions/9838384/can-gnu-make-handle-filenames-with-spaces][SO:can-gnu-make-handle-filenames-with-spaces]]
by example.

*** 2019-06-28 

*New MathematicaStan version 2.0!*

This version uses Mathematica v11 and has been completely refactored

*Caveat:* breaking changes!

*Note*: the ""old"" MathematicaStan version based on Mathematica v8.0 is now archived in
the [[https://github.com/stan-dev/MathematicaStan/tree/v1][v1 git branch]]. 

* Installation

** The Stan CmdStan shell interface

First you must install [[http://mc-stan.org/interfaces/cmdstan][CmdStan]]. Once this is done you get a directory containing stuff like:

#+BEGIN_EXAMPLE
bin  doc  examples  Jenkinsfile  LICENSE  make  makefile  README.md  runCmdStanTests.py  src  stan  test-all.sh
#+END_EXAMPLE

With my configuration *CmdStan* is installed in:
#+BEGIN_EXAMPLE
~/ExternalSoftware/cmdstan-2.19.1
#+END_EXAMPLE

For Windows users it is possibly something like:
#+BEGIN_EXAMPLE
C:\\Users\\USER_NAME\\Documents\\R\\cmdstan-?.??.?
#+END_EXAMPLE

** The Mathematica CmdStan package

To install the Mathematica CmdStan package:
- open the =CmdStan.m= file with Mathematica.
- install it using the Mathematica Notebook *File->Install* menu.

** First run

The first time the package is imported
#+BEGIN_SRC mathematica :eval never
<<CmdStan`
#+END_SRC
you will get an error message:
#+BEGIN_EXAMPLE
CmdStan::cmdStanDirectoryNotDefined: CmdStan directory does not exist, use SetCmdStanDirectory[dir] to define it (with something like SetCmdStanDirectory[""~/ExternalSoftware/cmdstan-2.19.1""])
#+END_EXAMPLE
This is normal as we must define the Stan StanCmd shell interface root directory. 

With my configuration this is:
#+BEGIN_SRC matheematica :eval never
SetCmdStanDirectory[""~/ExternalSoftware/cmdstan-2.19.1""]
#+END_SRC

For Windows user this is certainly something like:
#+BEGIN_SRC matheematica :eval never
SetCmdStanDirectory[""C:\\Users\\USER_NAME\\Documents\\R\\cmdstan-?.??.?""]
#+END_SRC

*Note:* this location is recorded in the =$CmdStanConfigurationFile= file
 and you will not have to redefine it every time you import the
 CmdStan package.


* Tutorial 1, linear regression

** Introduction

You can use the file =tutorial.wls= or manually follow the instruction
below.

Import the package as usual

#+BEGIN_SRC mathematica :eval never
<<CmdStan`
#+END_SRC

This package defines these functions (and symbols):

#+BEGIN_SRC mathematica :eval never
?CmdStan`*
#+END_SRC

| CmdStan             | GetStanOption          | RemoveStanOption     | StanOptionExistsQ  | StanResultReducedKeys     |
| CompileStanCode     | GetStanResult          | RunStan              | StanOptions        | StanResultReducedMetaKeys |
| ExportStanCode      | GetStanResultMeta      | SampleDefaultOptions | StanResult         | StanVerbose               |
| ExportStanData      | ImportStanResult       | SetCmdStanDirectory  | StanResultKeys     | VariationalDefaultOptions |
| GetCmdStanDirectory | OptimizeDefaultOptions | SetStanOption        | StanResultMetaKeys | $CmdStanConfigurationFile |

For this tutorial we use a simple [[https://mc-stan.org/docs/2_19/stan-users-guide/linear-regression.html][linear regression]] example and we will work in a temporary location:

#+BEGIN_SRC mathematica :eval never
SetDirectory[$TemporaryDirectory]
#+END_SRC
#+BEGIN_EXAMPLE
/tmp
#+END_EXAMPLE

** Stan code 

Define the Stan code
#+BEGIN_SRC mathematica :eval never
stanCode = ""data
  {
    int<lower = 0> N;
    vector[N] x;
    vector[N] y;
  }
  parameters
  {
    real alpha;
    real beta;
    real<lower = 0> sigma;
  }
  model {
    y ~normal(alpha + beta * x, sigma);
  }"";
#+END_SRC

and export it

#+BEGIN_SRC mathematica :eval never
stanCodeFile = ExportStanCode[""linear_regression.stan"", stanCode]
#+END_SRC
#+BEGIN_EXAMPLE
/tmp/linear_regression.stan
#+END_EXAMPLE

** Code compilation

Stan code compilation is performed by 
 #+BEGIN_SRC mathematica :eval never
stanExeFile = CompileStanCode[stanCodeFile] (* Attention: this takes some time *)
 #+END_SRC

With my configuration I get
 #+BEGIN_EXAMPLE
make: Entering directory '/home/picaud/ExternalSoftware/cmdstan-2.19.1'

--- Translating Stan model to C++ code ---
bin/stanc  --o=/tmp/linear_regression.hpp /tmp/linear_regression.stan
Model name=linear_regression_model
Input file=/tmp/linear_regression.stan
Output file=/tmp/linear_regression.hpp
g++ -std=c++1y -pthread -Wno-sign-compare     -O3 -I src -I stan/src -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.3.3 -I stan/lib/stan_math/lib/boost_1.69.0 -I stan/lib/stan_math/lib/sundials_4.1.0/include    -DBOOST_RESULT_OF_USE_TR1 -DBOOST_NO_DECLTYPE -DBOOST_DISABLE_ASSERTS -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION     -c -MT /tmp/linear_regression.o -MT /tmp/linear_regression -include /tmp/linear_regression.hpp -include src/cmdstan/main.cpp -MM -E -MG -MP -MF /tmp/linear_regression.d /tmp/linear_regression.hpp

--- Linking C++ model ---
g++ -std=c++1y -pthread -Wno-sign-compare     -O3 -I src -I stan/src -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.3.3 -I stan/lib/stan_math/lib/boost_1.69.0 -I stan/lib/stan_math/lib/sundials_4.1.0/include    -DBOOST_RESULT_OF_USE_TR1 -DBOOST_NO_DECLTYPE -DBOOST_DISABLE_ASSERTS -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION             -include /tmp/linear_regression.hpp src/cmdstan/main.cpp        stan/lib/stan_math/lib/sundials_4.1.0/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_4.1.0/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_4.1.0/lib/libsundials_idas.a  -o /tmp/linear_regression
make: Leaving directory '/home/picaud/ExternalSoftware/cmdstan-2.19.1'
 #+END_EXAMPLE

*Note:* if you do not want to have information printed you can use the =StanVerbose= option:

 #+BEGIN_SRC mathematica :eval never
stanExeFile = CompileStanCode[stanCodeFile, StanVerbose -> False]
 #+END_SRC

** Simulated data

Let's simulate some data:
 #+BEGIN_SRC mathematica :eval never
σ = 3; α = 1; β = 2;
n = 20;
X = Range[n];
Y = α + β*X + RandomVariate[NormalDistribution[0, σ], n];
Show[Plot[α + β*x, {x, Min[X], Max[X]}], 
     ListPlot[Transpose@{X, Y}, PlotStyle -> Red]]
 #+END_SRC

[[file:figures/linRegData.png][file:./figures/linRegData.png]]

** Create the =data.R= data file 

The data are stored in a =Association= and then exported thanks to the
=ExportStanData= function.

#+BEGIN_SRC mathematica :eval never
stanData = <|""N"" -> n, ""x"" -> X, ""y"" -> Y|>;
stanDataFile = ExportStanData[stanExeFile, stanData]
#+END_SRC

#+BEGIN_EXAMPLE
/tmp/linear_regression.data.R
#+END_EXAMPLE

*Note:* this function returns the created file
name =/tmp/linear_regression.data.R=. Its first argument, =stanExeFile=
is simply the Stan executable file name with its path. The
=ExportStanData[]= function modifies the file name extension and
replace it with "".data.R"", but you can use it with
any file name:
#+BEGIN_SRC mathematica :eval never
ExportStanData[""~/tmp/my_custom_filename.data.R"",stanData]
#+END_SRC

** Run Stan, likelihood maximization

We are now able to run the =stanExeFile= executable. 

Let's start by maximizing the likelihood
#+BEGIN_SRC mathematica :eval never
stanResultFile = RunStan[stanExeFile, OptimizeDefaultOptions]
#+END_SRC

#+BEGIN_EXAMPLE
Running: /tmp/linear_regression method=optimize data file=/tmp/linear_regression.data.R output file=/tmp/linear_regression.csv

method = optimize
  optimize
    algorithm = lbfgs (Default)
      lbfgs
        init_alpha = 0.001 (Default)
        tol_obj = 9.9999999999999998e-13 (Default)
        tol_rel_obj = 10000 (Default)
        tol_grad = 1e-08 (Default)
        tol_rel_grad = 10000000 (Default)
        tol_param = 1e-08 (Default)
        history_size = 5 (Default)
    iter = 2000 (Default)
    save_iterations = 0 (Default)
id = 0 (Default)
data
  file = /tmp/linear_regression.data.R
init = 2 (Default)
random
  seed = 2775739062
output
  file = /tmp/linear_regression.csv
  diagnostic_file =  (Default)
  refresh = 100 (Default)

Initial log joint probability = -8459.75
    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes 
      19      -32.5116    0.00318011    0.00121546      0.9563      0.9563       52   
Optimization terminated normally: 
  Convergence detected: relative gradient magnitude is below tolerance
#+END_EXAMPLE

The =stanResultFile= variable contains now the csv result file:
#+BEGIN_EXAMPLE
/tmp/linear_regression.csv
#+END_EXAMPLE

*Note:* again, if you do not want to have printed output, use the =StanVerbose->False= option.

#+BEGIN_SRC mathematica :eval never
stanResultFile = RunStan[stanExeFile, OptimizeDefaultOptions,StanVerbose->False]
#+END_SRC

*Note:* the method we use is defined by the second argument
=OptimizeDefaultOptions.= If you want to use Variational Bayes or HMC
sampling you must use

#+BEGIN_SRC mathematica :eval never
RunStan[stanExeFile, VariationalDefaultOptions]
#+END_SRC
or
#+BEGIN_SRC mathematica :eval never
RunStan[stanExeFile, SampleDefaultOptions]
#+END_SRC

*Note*: option management will be detailed later in this tutorial.

** Load the CSV result file

To load CSV result file, do

#+BEGIN_SRC mathematica :eval never
stanResult = ImportStanResult[stanResultFile]
#+END_SRC

which prints
#+BEGIN_EXAMPLE
     file: /tmp/linear_regression.csv
     meta: lp__ 
parameter: alpha , beta , sigma 
#+END_EXAMPLE

To access estimated variable α, β and σ, simply do:
#+BEGIN_SRC mathematica :eval never

GetStanResultMeta[stanResult, ""lp__""]
αe=GetStanResult[stanResult, ""alpha""]
βe=GetStanResult[stanResult, ""beta""]
σe=GetStanResult[stanResult, ""sigma""]
#+END_SRC

which prints:

#+BEGIN_EXAMPLE
{-32.5116}
{2.51749}
{1.83654}
{3.08191}
#+END_EXAMPLE

*Note*: as with likelihood maximization we only have a point estimation,
the returned values are lists of *one* number.

You can plot the estimated line:

#+BEGIN_SRC mathematica :eval never
Show[Plot[{αe + βe*x, α + β*x}, {x, Min[X],Max[X]}, PlotLegends -> ""Expressions""], 
     ListPlot[Transpose@{X, Y}, PlotStyle -> Red]]
#+END_SRC

[[file:./figures/linRegEstimate.png]]

** Run Stan, Variational Bayes

We want to solve the same problem but using variational inference. 

As explained before we must use 
#+BEGIN_SRC mathematica :eval never
stanResultFile = RunStan[stanExeFile, VariationalDefaultOptions]
#+END_SRC
instead of 
#+BEGIN_SRC mathematica :eval never
stanResultFile = RunStan[stanExeFile, OptimizeDefaultOptions]
#+END_SRC

However, please note that running this command will erase
=stanResultFile= which is the file where result are exported. To avoid
this we can modify the output file name by modifying option values.

The default option values are stored in the write-protected
=VariationalDefaultOptions= variable.

To modify them we must first copy this protected symbol:

#+BEGIN_SRC mathematica :eval never
myOpt=VariationalDefaultOptions
#+END_SRC
which prints
#+BEGIN_EXAMPLE
method=variational
#+END_EXAMPLE

The option values are printed when you run the =RunStan= command:

#+BEGIN_EXAMPLE
method = variational
  variational
    algorithm = meanfield (Default)
      meanfield
    iter = 10000 (Default)
    grad_samples = 1 (Default)
    elbo_samples = 100 (Default)
    eta = 1 (Default)
    adapt
      engaged = 1 (Default)
      iter = 50 (Default)
    tol_rel_obj = 0.01 (Default)
    eval_elbo = 100 (Default)
    output_samples = 1000 (Default)
id = 0 (Default)
data
  file =  (Default)
init = 2 (Default)
random
  seed = 2784129612
output
  file = output.csv (Default)
  diagnostic_file =  (Default)
  refresh = 100 (Default)
#+END_EXAMPLE

We have to modify the =output file= option value. This can be done by:
#+BEGIN_SRC mathematica :eval never
myOpt = SetStanOption[myOpt, ""output.file"", FileNameJoin[{Directory[], ""myOutputFile.csv""}]]
#+END_SRC
which prints:
#+BEGIN_EXAMPLE
method=variational output file=/tmp/myOutputFile.csv
#+END_EXAMPLE

Now we can run Stan:

#+BEGIN_SRC mathematica :eval never
myOutputFile=RunStan[stanExeFile, myOpt, StanVerbose -> False]
#+END_SRC
which must print:
#+BEGIN_EXAMPLE
/tmp/myOutputFile.csv
#+END_EXAMPLE

Now import this CSV file:
#+BEGIN_SRC mathematica :eval never
myResult = ImportStanResult[myOutputFile]
#+END_SRC
which prints:
#+BEGIN_EXAMPLE
     file: /tmp/myOutputFile.csv
     meta: lp__ , log_p__ , log_g__ 
parameter: alpha , beta , sigma 
#+END_EXAMPLE

As before you can use:
#+BEGIN_SRC mathematica :eval never
GetStanResult[myResult,""alpha""]
#+END_SRC

to get =alpha= parameter value, but now you will get a list of 1000 sample:
#+BEGIN_EXAMPLE
{2.03816, 0.90637, ..., ..., 1.22068, 1.66392}
#+END_EXAMPLE

Instead of the full sample list we are often interested by sample
mean, variance... You can get these quantities as follows:

#+BEGIN_SRC mathematica :eval never
GetStanResult[Mean, myResult, ""alpha""]
GetStanResult[Variance, myResult, ""alpha""]
#+END_SRC

which prints:

#+BEGIN_EXAMPLE
2.0353
0.317084
#+END_EXAMPLE

You can also get the sample hstogram as simply as:

#+BEGIN_SRC mathematica :eval never
GetStanResult[Histogram, myResult, ""alpha""]
#+END_SRC

[[file:figures/linRegHisto.png][file:./figures/linRegHisto.png]]

** More about Option management

*** Overwriting default values

We provide further details concerning option related functions.

To recap the first step is to perform a copy of the write-protected
default option values. By example to modify default MCMC option values
the first step is:

#+BEGIN_SRC mathematica :eval never
  myOpt = SampleDefaultOptions
#+END_SRC

The available option are:
#+begin_example
method = sample (Default)
  sample
    num_samples = 1000 (Default)
    num_warmup = 1000 (Default)
    save_warmup = 0 (Default)
    thin = 1 (Default)
    adapt
      engaged = 1 (Default)
      gamma = 0.050000000000000003 (Default)
      delta = 0.80000000000000004 (Default)
      kappa = 0.75 (Default)
      t0 = 10 (Default)
      init_buffer = 75 (Default)
      term_buffer = 50 (Default)
      window = 25 (Default)
    algorithm = hmc (Default)
      hmc
        engine = nuts (Default)
          nuts
            max_depth = 10 (Default)
        metric = diag_e (Default)
        metric_file =  (Default)
        stepsize = 1 (Default)
        stepsize_jitter = 0 (Default)
id = 0 (Default)
data
  file = /tmp/linear_regression.data.R
init = 2 (Default)
random
  seed = 3714706817 (Default)
output
  file = /tmp/linear_regression.csv
  diagnostic_file =  (Default)
  refresh = 100 (Default)
  sig_figs = -1 (Default)
#+end_example

If we want to modify:
#+begin_example
method = sample (Default)
  sample
    num_samples = 1000 (Default)
    num_warmup = 1000 (Default)
#+end_example
and
#+begin_example
method = sample (Default)
  sample
    algorithm = hmc (Default)
      hmc
        engine = nuts (Default)
          nuts
            max_depth = 10 (Default)
#+end_example
you must proceed as follows. For each hierarchy level use a ""."" as
separator and do not forget to rewrite ""="" with the associated
value. With our example this gives:

#+BEGIN_SRC mathematica :eval never
myOpt = SetStanOption[myOpt, ""adapt.num_samples"", 2000]
myOpt = SetStanOption[myOpt, ""adapt.num_warmup"", 1500]
myOpt = SetStanOption[myOpt, ""algorithm=hmc.engine=nuts.max_depth"", 5]
#+END_SRC

Now you can run the sampler with these new option values:
#+BEGIN_SRC mathematica :eval never
stanResultFile = RunStan[stanExeFile, myOpt]
#+END_SRC
which should print:
#+begin_example
method = sample (Default)
  sample
    num_samples = 2000
    num_warmup = 1500
    save_warmup = 0 (Default)
    thin = 1 (Default)
    adapt
      engaged = 1 (Default)
      gamma = 0.050000000000000003 (Default)
      delta = 0.80000000000000004 (Default)
      kappa = 0.75 (Default)
      t0 = 10 (Default)
      init_buffer = 75 (Default)
      term_buffer = 50 (Default)
      window = 25 (Default)
    algorithm = hmc (Default)
      hmc
        engine = nuts (Default)
          nuts
            max_depth = 5
        metric = diag_e (Default)
        metric_file =  (Default)
        stepsize = 1 (Default)
        stepsize_jitter = 0 (Default)
id = 0 (Default)
data
  file = /tmp/linear_regression.data.R
init = 2 (Default)
random
  seed = 3720771451 (Default)
output
  file = /tmp/linear_regression.csv
  diagnostic_file =  (Default)
  refresh = 100 (Default)
  sig_figs = -1 (Default)
stanc_version = stanc3 b25c0b64
stancflags = 


Gradient evaluation took 1.3e-05 seconds
1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.
Adjust your expectations accordingly!


Iteration:    1 / 3500 [  0%]  (Warmup)
Iteration:  100 / 3500 [  2%]  (Warmup)
Iteration:  200 / 3500 [  5%]  (Warmup)
Iteration:  300 / 3500 [  8%]  (Warmup)
Iteration:  400 / 3500 [ 11%]  (Warmup)
Iteration:  500 / 3500 [ 14%]  (Warmup)
Iteration:  600 / 3500 [ 17%]  (Warmup)
Iteration:  700 / 3500 [ 20%]  (Warmup)
Iteration:  800 / 3500 [ 22%]  (Warmup)
Iteration:  900 / 3500 [ 25%]  (Warmup)
Iteration: 1000 / 3500 [ 28%]  (Warmup)
Iteration: 1100 / 3500 [ 31%]  (Warmup)
Iteration: 1200 / 3500 [ 34%]  (Warmup)
Iteration: 1300 / 3500 [ 37%]  (Warmup)
Iteration: 1400 / 3500 [ 40%]  (Warmup)
Iteration: 1500 / 3500 [ 42%]  (Warmup)
Iteration: 1501 / 3500 [ 42%]  (Sampling)
Iteration: 1600 / 3500 [ 45%]  (Sampling)
Iteration: 1700 / 3500 [ 48%]  (Sampling)
Iteration: 1800 / 3500 [ 51%]  (Sampling)
Iteration: 1900 / 3500 [ 54%]  (Sampling)
Iteration: 2000 / 3500 [ 57%]  (Sampling)
Iteration: 2100 / 3500 [ 60%]  (Sampling)
Iteration: 2200 / 3500 [ 62%]  (Sampling)
Iteration: 2300 / 3500 [ 65%]  (Sampling)
Iteration: 2400 / 3500 [ 68%]  (Sampling)
Iteration: 2500 / 3500 [ 71%]  (Sampling)
Iteration: 2600 / 3500 [ 74%]  (Sampling)
Iteration: 2700 / 3500 [ 77%]  (Sampling)
Iteration: 2800 / 3500 [ 80%]  (Sampling)
Iteration: 2900 / 3500 [ 82%]  (Sampling)
Iteration: 3000 / 3500 [ 85%]  (Sampling)
Iteration: 3100 / 3500 [ 88%]  (Sampling)
Iteration: 3200 / 3500 [ 91%]  (Sampling)
Iteration: 3300 / 3500 [ 94%]  (Sampling)
Iteration: 3400 / 3500 [ 97%]  (Sampling)
Iteration: 3500 / 3500 [100%]  (Sampling)

 Elapsed Time: 0.053 seconds (Warm-up)
               0.094 seconds (Sampling)
               0.147 seconds (Total)
#+end_example

You can check than the new option values have been taken into account:
#+begin_example
    num_samples = 2000
    num_warmup = 1500

    algorithm = hmc (Default)
      hmc
        engine = nuts (Default)
          nuts
            max_depth = 5
#+end_example

*** Reading customized values

You can get back the modified values as follows:

  #+BEGIN_SRC mathematica :eval never
GetStanOption[myOpt, ""adapt.num_warmup""]
GetStanOption[myOpt, ""algorithm=hmc.engine=nuts.max_depth""]
  #+END_SRC
  which prints
  #+BEGIN_EXAMPLE
  1500
  5
  #+END_EXAMPLE
  *Caveat*: if the option was not defined (by =SetStanOption=) the function
  returns =$Failed=.

*** Erasing customized option values

To erase an option value (and use its default value) use:
  #+BEGIN_SRC mathematica :eval never
  myOpt = RemoveStanOption[myOpt, ""algorithm=hmc.engine=nuts.max_depth""]
  #+END_SRC
  which prints
  #+BEGIN_EXAMPLE
  method=sample adapt num_samples=2000 num_warmup=1500 
  #+END_EXAMPLE

* Tutorial 2, linear regression with more than one predictor

** Parameter arrays

By now the parameters alpha, beta, sigma, were *scalars*. We will see
how to handle parameters that are vectors or matrices. 

We use second section of the [[https://mc-stan.org/docs/2_19/stan-users-guide/linear-regression.html][linear regression]] example, entitled
""Matrix notation and Vectorization"".

The β parameter is now a vector of size K. 

#+BEGIN_SRC mathematica :eval never 
stanCode = ""data {
    int<lower=0> N;   // number of data items
    int<lower=0> K;   // number of predictors
    matrix[N, K] x;   // predictor matrix
    vector[N] y;      // outcome vector
  }
  parameters {
    real alpha;           // intercept
    vector[K] beta;       // coefficients for predictors
    real<lower=0> sigma;  // error scale
  }
  model {
    y ~ normal(x * beta + alpha, sigma);  // likelihood
  }"";

stanCodeFile = ExportStanCode[""linear_regression_vect.stan"", stanCode];
stanExeFile = CompileStanCode[stanCodeFile];
#+END_SRC

** Simulated data

Here we use {x,x²,x³} as predictors, with their coefficients
β = {2,0.1,0.01} so that the model is 

y = α + β1 x + β2 x² + β3 x³ + ε

where ε follows a normal distribution.

#+BEGIN_SRC mathematica :eval never 
σ = 3; α = 1; β1 = 2; β2 = 0.1; β3 = 0.01;
n = 20;
X = Range[n];
Y = α + β1*X + β2*X^2 + β3*X^3 + RandomVariate[NormalDistribution[0, σ], n];
Show[Plot[α + β1*x + β2*x^2 + β3*x^3, {x, Min[X], Max[X]}],
     ListPlot[Transpose@{X, Y}, PlotStyle -> Red]]
#+END_SRC

[[file:figures/linReg2Data.png][file:./figures/linReg2Data.png]]

** Exporting data

The expression 

y = α + β1 x + β2 x² + β3 x³ + ε

is convenient for random variable manipulations. However in practical
computations where we have to evaluate:

y[i] = α + β1 x[i] + β2 (x[i])² + β3 (x[i])³ + ε[i], for i = 1..N

it is more convenient to rewrite this in a ""vectorized form"":

*y* = *α* + *X.β* + *ε*

where *X* is a KxN matrix of columns X[:,j] = j th-predictor = (x[:])^j
and *α* a vector of size N with constant components = α.

Thus data is exported as follows:

#+BEGIN_SRC mathematica :eval never 
stanData = <|""N"" -> n, ""K"" -> 3, ""x"" -> Transpose[{X,X^2,X^3}], ""y"" -> Y|>;
stanDataFile = ExportStanData[stanExeFile, stanData]
#+END_SRC

*Note:* as Mathematica stores its matrices rows by rows (the C
 language convention) we have to transpose ={X,X^2,X^3}= to get the
 right matrix X.

** Run Stan, HMC sampling

We can now run Stan using the Hamiltonian Monte Carlo (HMC) method:

#+BEGIN_SRC mathematica :eval never 
stanResultFile = RunStan[stanExeFile, SampleDefaultOptions]
#+END_SRC

which prints:

#+BEGIN_EXAMPLE
Running: /tmp/linear_regression_vect method=sample data file=/tmp/linear_regression_vect.data.R output file=/tmp/linear_regression_vect.csv

method = sample (Default)
  sample
    num_samples = 1000 (Default)
    num_warmup = 1000 (Default)
    save_warmup = 0 (Default)
    thin = 1 (Default)
    adapt
      engaged = 1 (Default)
      gamma = 0.050000000000000003 (Default)
      delta = 0.80000000000000004 (Default)
      kappa = 0.75 (Default)
      t0 = 10 (Default)
      init_buffer = 75 (Default)
      term_buffer = 50 (Default)
      window = 25 (Default)
    algorithm = hmc (Default)
      hmc
        engine = nuts (Default)
          nuts
            max_depth = 10 (Default)
        metric = diag_e (Default)
        metric_file =  (Default)
        stepsize = 1 (Default)
        stepsize_jitter = 0 (Default)
id = 0 (Default)
data
  file = /tmp/linear_regression_vect.data.R
init = 2 (Default)
random
  seed = 3043713420
output
  file = /tmp/linear_regression_vect.csv
  diagnostic_file =  (Default)
  refresh = 100 (Default)


Gradient evaluation took 4e-05 seconds
1000 transitions using 10 leapfrog steps per transition would take 0.4 seconds.
Adjust your expectations accordingly!


Iteration:    1 / 2000 [  0%]  (Warmup)
Iteration:  100 / 2000 [  5%]  (Warmup)
Iteration:  200 / 2000 [ 10%]  (Warmup)
Iteration:  300 / 2000 [ 15%]  (Warmup)
Iteration:  400 / 2000 [ 20%]  (Warmup)
Iteration:  500 / 2000 [ 25%]  (Warmup)
Iteration:  600 / 2000 [ 30%]  (Warmup)
Iteration:  700 / 2000 [ 35%]  (Warmup)
Iteration:  800 / 2000 [ 40%]  (Warmup)
Iteration:  900 / 2000 [ 45%]  (Warmup)
Iteration: 1000 / 2000 [ 50%]  (Warmup)
Iteration: 1001 / 2000 [ 50%]  (Sampling)
Iteration: 1100 / 2000 [ 55%]  (Sampling)
Iteration: 1200 / 2000 [ 60%]  (Sampling)
Iteration: 1300 / 2000 [ 65%]  (Sampling)
Iteration: 1400 / 2000 [ 70%]  (Sampling)
Iteration: 1500 / 2000 [ 75%]  (Sampling)
Iteration: 1600 / 2000 [ 80%]  (Sampling)
Iteration: 1700 / 2000 [ 85%]  (Sampling)
Iteration: 1800 / 2000 [ 90%]  (Sampling)
Iteration: 1900 / 2000 [ 95%]  (Sampling)
Iteration: 2000 / 2000 [100%]  (Sampling)

 Elapsed Time: 0.740037 seconds (Warm-up)
               0.60785 seconds (Sampling)
               1.34789 seconds (Total)
#+END_EXAMPLE
** Load the CSV result file

As before, 

#+BEGIN_SRC mathematica :eval never
stanResult = ImportStanResult[stanResultFile]
#+END_SRC

load the generated CSV file and prints:

#+BEGIN_EXAMPLE
     file: /tmp/linear_regression_vect.csv
     meta: lp__ , accept_stat__ , stepsize__ , treedepth__ , n_leapfrog__ , divergent__ , energy__ 
parameter: alpha , beta 3, sigma 
#+END_EXAMPLE

Compared to the scalar case, the important thing to notice is the =beta 3=. That means that β is not a scalar anymore but a vector of size 3

*Note*: here β is a vector, but if it had been a 3x5 matrix we would
 have had =β 3x5= printed instead.

A call to 
#+BEGIN_SRC mathematica :eval never
GetStanResult[stanResult, ""beta""]
#+END_SRC
returns a vector of size 3 but where each component is a list of 1000
sample (for β1, β2 and β3).

As before it generally useful to summarize this sample with function like mean or histogram:

#+BEGIN_SRC mathematica :eval never
GetStanResult[Mean, stanResult, ""beta""]
GetStanResult[Histogram, stanResult, ""beta""]
#+END_SRC

prints:
#+BEGIN_EXAMPLE
{3.30321, -0.010088, 0.0126913}
#+END_EXAMPLE
and plots:

[[file:figures/linReg2Histo.png][file:./figures/linReg2Histo.png]]


This is the moment to digress about Keys. If you try:
#+BEGIN_SRC mathematica :eval never
StanResultKeys[stanResult]
StanResultMetaKeys[stanResult]
#+END_SRC

this will print:
#+BEGIN_EXAMPLE
{""alpha"", ""beta.1"", ""beta.2"", ""beta.3"", ""sigma""}
{""lp__"", ""accept_stat__"", ""stepsize__"", ""treedepth__"", ""n_leapfrog__"", ""divergent__"", ""energy__""}
#+END_EXAMPLE

These functions are useful to get the complete list of keys. Note
that, as β is an 1D-array of size 1 we have =beta.1, beta.2, beta.3=. If
β was a NxM matrix, the list of keys would have been: =beta.1.1,
beta.1.2,... beta.N.M=.

There is also *reduced keys* functions:

#+BEGIN_SRC mathematica :eval never
StanResultReducedKeys[stanResult]
StanResultReducedMetaKeys[stanResult]
#+END_SRC

which print

#+BEGIN_EXAMPLE
{""alpha"", ""beta"", ""sigma""}
{""lp__"", ""accept_stat__"", ""stepsize__"", ""treedepth__"", ""n_leapfrog__"", ""divergent__"", ""energy__""}
#+END_EXAMPLE

As you can see the *reduced keys* functions collect and discard indices
to keys associated to arrays.

When accessing a parameter you can work at the component level or globally:
#+BEGIN_SRC mathematica :eval never
GetStanResult[Mean, stanResult, ""beta.2""]
GetStanResult[Mean, stanResult, ""beta""]
#+END_SRC

which prints

#+BEGIN_EXAMPLE
-0.010088
{3.30321, -0.010088, 0.0126913}
#+END_EXAMPLE

",2022-08-12
https://github.com/stan-dev/nomad,"<img src=""https://github.com/stan-dev/nomad/blob/master/logo/logo.png?raw=true"" alt=""Nomad Logo""/>

Nomad is a high-performance automatic differentiation library particularly focused
on the efficient computation of first, second, and third-order derivatives.
",2022-08-12
https://github.com/stan-dev/perf-math,"## Getting started
```
git clone --recursive https://github.com/seantalts/perf-math.git
```

### First run 

(unless you have the Google benchmark library installed)
```
cd benchmark
git clone https://github.com/google/googletest
mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=RELEASE
make
```

## Writing a benchmark
There is an example, this matstuff.cpp. You can make your own or reuse this
file. See https://github.com/google/benchmark for details.

Essential structure:
```cpp
static void BM_Name(benchmark::State& state) {
  setup();
  for (auto _ : state) {
    thing_to_time();
  }
}
BENCHMARK(BM_Name);

BENCHMARK_MAIN();
```

## Making a benchmark
```
$ make matstuff && ./matstuff 
c++ -Imath/lib/eigen_3.3.3/ -Ibenchmark/include -std=c++1y -Imath/
-Imath/lib/boost_1.66.0  -Lbenchmark/build/src  matstuff.cpp  -lbenchmark -o
matstuff
2018-08-29 04:52:47
Running ./matstuff
Run on (4 X 2200 MHz CPU s)
CPU Caches:
  L1 Data 32K (x2)
    L1 Instruction 32K (x2)
      L2 Unified 262K (x2)
        L3 Unified 4194K (x1)
        ------------------------------------------------------
        Benchmark               Time           CPU Iterations
        ------------------------------------------------------
        BM_LogOrig     2688940713 ns 2669717000 ns          1
        BM_LogProposed 2711084835 ns 2696761000 ns          1
```
",2022-08-12
https://github.com/stan-dev/performance-tests-cmdstan,"# performance-tests-cmdstan
Performance testing tools for use with CmdStan

## ""Install""

```
git clone --recursive https://github.com/stan-dev/performance-tests-cmdstan.git
```

## Test performance in current working directory

To test the performance of the current cmdstan et al working directory on, for example, @betanalpha's stat_comp_benchmarks model repo, you can run the following:
```
./runPerformanceTests.py -j8 stat_comp_benchmarks
```

`./runPerformanceTests.py --help` for more options.

## Cleaning
`make clean` will recursively remove all non-checked-in files from all submodules. `make revert` will bring cmdstan and its submodules back to the commit specified by the current commit of the top-level `performance-tests-cmdstan` repo.

## Testing one git commit against another
to test e.g. develop against a branch you've made on cmdstan,
```
./compare-git-hashes.sh ""stat_comp_benchmarks -j8 --runs 10 <other options to runPerformanceTests.py>"" <Baseline CmdStan hash/branch/PR-???> <CmdStan hash for comparison run> <Stan hash for comparison run> <Math hash for comparison run>
```

All of these take pull request numbers, so to test stan-dev/math#1244 against develop (for example) you can run:
```
./compare-git-hashes.sh example-models/bugs_examples/vol2/schools/ develop develop false d013e55
```
Here the false could be replaced with `develop` - just says to use the Stan hash associated with the CmdStan hash, in this case `develop`.

The script will then check out and pull all of these commits, branches, or PRs from stan-dev. It should print out which commit hashes it ends up on; please check that these are correct as the script is new. For PRs, you will see an unfamiliar hash that GitHub creates to store the result of the merge of the PR into the base branch the PR is against.
",2022-08-12
https://github.com/stan-dev/posterior,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# posterior <img src=""man/figures/stanlogo.png"" align=""right"" width=""120"" />

<!-- badges: start -->

[![CRAN
status](https://www.r-pkg.org/badges/version/posterior)](https://CRAN.R-project.org/package=posterior)
[![R-CMD-check](https://github.com/stan-dev/posterior/workflows/R-CMD-check/badge.svg)](https://github.com/stan-dev/posterior/actions?workflow=R-CMD-check)
[![Coverage
Status](https://codecov.io/gh/stan-dev/posterior/branch/master/graph/badge.svg)](https://app.codecov.io/gh/stan-dev/posterior)
<!-- badges: end -->

The **posterior** R package is intended to provide useful tools for both
users and developers of packages for fitting Bayesian models or working
with output from Bayesian models. The primary goals of the package are
to:

-   Efficiently convert between many different useful formats of draws
    (samples) from posterior or prior distributions.
-   Provide consistent methods for operations commonly performed on
    draws, for example, subsetting, binding, or mutating draws.
-   Provide various summaries of draws in convenient formats.
-   Provide lightweight implementations of state of the art posterior
    inference diagnostics.

If you are new to **posterior** we recommend starting with these
vignettes:

-   [*The posterior R
    package*](https://mc-stan.org/posterior/articles/posterior.html): an
    introduction to the package and its main functionality
-   [*rvar: The Random Variable
    Datatype*](https://mc-stan.org/posterior/articles/rvar.html): an
    overview of the new random variable datatype

### Installation

You can install the latest official release version via

``` r
install.packages(""posterior"")
```

or build the developmental version directly from GitHub via

``` r
# install.packages(""remotes"")
remotes::install_github(""stan-dev/posterior"")
```

### Examples

Here we offer a few examples of using the package. For a more detailed
overview see the vignette [*The posterior R
package*](https://mc-stan.org/posterior/articles/posterior.html).

``` r
library(""posterior"")
#> This is posterior version 1.2.0
#> 
#> Attaching package: 'posterior'
#> The following objects are masked from 'package:stats':
#> 
#>     mad, sd, var
```

To demonstrate how to work with the **posterior** package, we will use
example posterior draws obtained from the eight schools hierarchical
meta-analysis model described in Gelman et al. (2013). Essentially, we
have an estimate per school (`theta[1]` through `theta[8]`) as well as
an overall mean (`mu`) and standard deviation across schools (`tau`).

#### Draws formats

``` r
eight_schools_array <- example_draws(""eight_schools"")
print(eight_schools_array, max_variables = 3)
#> # A draws_array: 100 iterations, 4 chains, and 10 variables
#> , , variable = mu
#> 
#>          chain
#> iteration   1    2     3   4
#>         1 2.0  3.0  1.79 6.5
#>         2 1.5  8.2  5.99 9.1
#>         3 5.8 -1.2  2.56 0.2
#>         4 6.8 10.9  2.79 3.7
#>         5 1.8  9.8 -0.03 5.5
#> 
#> , , variable = tau
#> 
#>          chain
#> iteration   1    2    3   4
#>         1 2.8 2.80  8.7 3.8
#>         2 7.0 2.76  2.9 6.8
#>         3 9.7 0.57  8.4 5.3
#>         4 4.8 2.45  4.4 1.6
#>         5 2.8 2.80 11.0 3.0
#> 
#> , , variable = theta[1]
#> 
#>          chain
#> iteration     1     2    3     4
#>         1  3.96  6.26 13.3  5.78
#>         2  0.12  9.32  6.3  2.09
#>         3 21.25 -0.97 10.6 15.72
#>         4 14.70 12.45  5.4  2.69
#>         5  5.96  9.75  8.2 -0.91
#> 
#> # ... with 95 more iterations, and 7 more variables
```

The draws for this example come as a `draws_array` object, that is, an
array with dimensions iterations x chains x variables. We can easily
transform it to another format, for instance, a data frame with
additional meta information.

``` r
eight_schools_df <- as_draws_df(eight_schools_array)
print(eight_schools_df)
#> # A draws_df: 100 iterations, 4 chains, and 10 variables
#>      mu tau theta[1] theta[2] theta[3] theta[4] theta[5] theta[6]
#> 1  2.01 2.8     3.96    0.271    -0.74      2.1    0.923      1.7
#> 2  1.46 7.0     0.12   -0.069     0.95      7.3   -0.062     11.3
#> 3  5.81 9.7    21.25   14.931     1.83      1.4    0.531      7.2
#> 4  6.85 4.8    14.70    8.586     2.67      4.4    4.758      8.1
#> 5  1.81 2.8     5.96    1.156     3.11      2.0    0.769      4.7
#> 6  3.84 4.1     5.76    9.909    -1.00      5.3    5.889     -1.7
#> 7  5.47 4.0     4.03    4.151    10.15      6.6    3.741     -2.2
#> 8  1.20 1.5    -0.28    1.846     0.47      4.3    1.467      3.3
#> 9  0.15 3.9     1.81    0.661     0.86      4.5   -1.025      1.1
#> 10 7.17 1.8     6.08    8.102     7.68      5.6    7.106      8.5
#> # ... with 390 more draws, and 2 more variables
#> # ... hidden reserved variables {'.chain', '.iteration', '.draw'}
```

Different formats are preferable in different situations and hence
posterior supports multiple formats and easy conversion between them.
For more details on the available formats see `help(""draws"")`. All of
the formats are essentially base R object classes and can be used as
such. For example, a `draws_matrix` object is just a `matrix` with a
little more consistency and additional methods.

#### Summarizing draws

Computing summaries of posterior or prior draws and convergence
diagnostics for posterior draws is one of the most common tasks when
working with Bayesian models fit using Markov Chain Monte Carlo (MCMC)
methods. The **posterior** package provides a flexible interface for
this purpose via `summarise_draws()`:

``` r
# summarise_draws or summarize_draws
summarise_draws(eight_schools_df)
#> # A tibble: 10 × 10
#>    variable  mean median    sd   mad      q5   q95  rhat ess_bulk ess_tail
#>    <chr>    <dbl>  <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>    <dbl>    <dbl>
#>  1 mu        4.18   4.16  3.40  3.57  -0.854  9.39  1.02     558.     322.
#>  2 tau       4.16   3.07  3.58  2.89   0.309 11.0   1.01     246.     202.
#>  3 theta[1]  6.75   5.97  6.30  4.87  -1.23  18.9   1.01     400.     254.
#>  4 theta[2]  5.25   5.13  4.63  4.25  -1.97  12.5   1.02     564.     372.
#>  5 theta[3]  3.04   3.99  6.80  4.94 -10.3   11.9   1.01     312.     205.
#>  6 theta[4]  4.86   4.99  4.92  4.51  -3.57  12.2   1.02     695.     252.
#>  7 theta[5]  3.22   3.72  5.08  4.38  -5.93  10.8   1.01     523.     306.
#>  8 theta[6]  3.99   4.14  5.16  4.81  -4.32  11.5   1.02     548.     205.
#>  9 theta[7]  6.50   5.90  5.26  4.54  -1.19  15.4   1.00     434.     308.
#> 10 theta[8]  4.57   4.64  5.25  4.89  -3.79  12.2   1.02     355.     146.
```

Basically, we get a data frame with one row per variable and one column
per summary statistic or convergence diagnostic. The summaries `rhat`,
`ess_bulk`, and `ess_tail` are described in Vehtari et al. (2020). We
can choose which summaries to compute by passing additional arguments,
either functions or names of functions. For instance, if we only wanted
the mean and its corresponding Monte Carlo Standard Error (MCSE) we
would use:

``` r
summarise_draws(eight_schools_df, ""mean"", ""mcse_mean"")
#> # A tibble: 10 × 3
#>    variable  mean mcse_mean
#>    <chr>    <dbl>     <dbl>
#>  1 mu        4.18     0.150
#>  2 tau       4.16     0.213
#>  3 theta[1]  6.75     0.319
#>  4 theta[2]  5.25     0.202
#>  5 theta[3]  3.04     0.447
#>  6 theta[4]  4.86     0.189
#>  7 theta[5]  3.22     0.232
#>  8 theta[6]  3.99     0.222
#>  9 theta[7]  6.50     0.250
#> 10 theta[8]  4.57     0.273
```

For a function to work with `summarise_draws`, it needs to take a vector
or matrix of numeric values and returns a single numeric value or a
named vector of numeric values.

#### Subsetting draws

Another common task when working with posterior (or prior) draws, is
subsetting according to various aspects of the draws (iterations,
chains, or variables). **posterior** provides a convenient interface for
this purpose via the `subset_draws()` method. For example, here is the
code to extract the first five iterations of the first two chains of the
variable `mu`:

``` r
subset_draws(eight_schools_df, variable = ""mu"", chain = 1:2, iteration = 1:5)
#> # A draws_df: 5 iterations, 2 chains, and 1 variables
#>      mu
#> 1   2.0
#> 2   1.5
#> 3   5.8
#> 4   6.8
#> 5   1.8
#> 6   3.0
#> 7   8.2
#> 8  -1.2
#> 9  10.9
#> 10  9.8
#> # ... hidden reserved variables {'.chain', '.iteration', '.draw'}
```

The same call to `subset_draws()` can be used regardless of whether the
object is a `draws_df`, `draws_array`, `draws_list`, etc.

#### Mutating and renaming draws

The magic of having obtained draws from the joint posterior (or prior)
distribution of a set of variables is that these draws can also be used
to obtain draws from any other variable that is a function of the
original variables. That is, if are interested in the posterior
distribution of, say, `phi = (mu + tau)^2` all we have to do is to
perform the transformation for each of the individual draws to obtain
draws from the posterior distribution of the transformed variable. This
procedure is automated in the `mutate_variables` method:

``` r
x <- mutate_variables(eight_schools_df, phi = (mu + tau)^2)
x <- subset_draws(x, c(""mu"", ""tau"", ""phi""))
print(x)
#> # A draws_df: 100 iterations, 4 chains, and 3 variables
#>      mu tau   phi
#> 1  2.01 2.8  22.8
#> 2  1.46 7.0  71.2
#> 3  5.81 9.7 240.0
#> 4  6.85 4.8 135.4
#> 5  1.81 2.8  21.7
#> 6  3.84 4.1  62.8
#> 7  5.47 4.0  88.8
#> 8  1.20 1.5   7.1
#> 9  0.15 3.9  16.6
#> 10 7.17 1.8  79.9
#> # ... with 390 more draws
#> # ... hidden reserved variables {'.chain', '.iteration', '.draw'}
```

When we do the math ourselves, we see that indeed for each draw, `phi`
is equal to `(mu + tau)^2` (up to rounding two 2 digits for the purpose
of printing).

We may also easily rename variables, or even entire vectors of variables
via `rename_variables`, for example:

``` r
x <- rename_variables(eight_schools_df, mean = mu, alpha = theta)
variables(x)
#>  [1] ""mean""     ""tau""      ""alpha[1]"" ""alpha[2]"" ""alpha[3]"" ""alpha[4]"" ""alpha[5]""
#>  [8] ""alpha[6]"" ""alpha[7]"" ""alpha[8]""
```

As with all **posterior** methods, `mutate_variables` and
`rename_variables` can be used with all draws formats.

#### Binding draws together

Suppose we have multiple draws objects that we want to bind together:

``` r
x1 <- draws_matrix(alpha = rnorm(5), beta = 1)
x2 <- draws_matrix(alpha = rnorm(5), beta = 2)
x3 <- draws_matrix(theta = rexp(5))
```

Then, we can use the `bind_draws` method to bind them along different
dimensions. For example, we can bind `x1` and `x3` together along the
`'variable'` dimension:

``` r
x4 <- bind_draws(x1, x3, along = ""variable"")
print(x4)
#> # A draws_matrix: 5 iterations, 1 chains, and 3 variables
#>     variable
#> draw alpha beta theta
#>    1  0.39    1  2.27
#>    2 -0.80    1  0.86
#>    3  0.95    1  1.93
#>    4  0.38    1  0.67
#>    5  0.18    1  2.04
```

Or, we can bind `x1` and `x2` together along the `'draw'` dimension:

``` r
x5 <- bind_draws(x1, x2, along = ""draw"")
print(x5)
#> # A draws_matrix: 10 iterations, 1 chains, and 2 variables
#>     variable
#> draw alpha beta
#>   1   0.39    1
#>   2  -0.80    1
#>   3   0.95    1
#>   4   0.38    1
#>   5   0.18    1
#>   6   0.13    2
#>   7   0.10    2
#>   8  -0.61    2
#>   9   0.12    2
#>   10  1.48    2
```

As with all **posterior** methods, `bind_draws` can be used with all
draws formats.

#### Converting from regular R objects to draws formats

The `eight_schools` example already comes in a format natively supported
by posterior but we could of course also import the draws from other
sources, for example, from common base R objects:

``` r
x <- matrix(rnorm(50), nrow = 10, ncol = 5)
colnames(x) <- paste0(""V"", 1:5)
x <- as_draws_matrix(x)
print(x)
#> # A draws_matrix: 10 iterations, 1 chains, and 5 variables
#>     variable
#> draw    V1    V2    V3    V4    V5
#>   1  -0.89  0.37 -0.25 -0.57 -2.85
#>   2   1.84  0.19  0.39 -0.52  1.26
#>   3   0.79 -0.74 -1.61  0.99 -0.11
#>   4  -2.25  0.28 -0.19 -0.33  0.92
#>   5   0.58  0.35 -0.92  0.56  0.82
#>   6  -1.38 -0.12 -0.40 -1.23 -0.60
#>   7  -0.18  1.18 -1.27  0.51  0.78
#>   8   0.17  1.50 -2.12 -0.45 -0.73
#>   9  -0.60  0.69 -0.43 -1.40  1.14
#>   10  0.18  0.96 -1.37 -0.58 -0.63

summarise_draws(x, ""mean"", ""sd"", ""median"", ""mad"")
#> # A tibble: 5 × 5
#>   variable      mean    sd   median   mad
#>   <chr>        <dbl> <dbl>    <dbl> <dbl>
#> 1 V1       -0.174    1.16  -0.00490 1.03 
#> 2 V2        0.467    0.651  0.358   0.596
#> 3 V3       -0.817    0.770 -0.671   0.798
#> 4 V4       -0.301    0.773 -0.486   0.672
#> 5 V5       -0.000826 1.27   0.338   1.27
```

Instead of `as_draws_matrix()` we also could have just used
`as_draws()`, which attempts to find the closest available format to the
input object. In this case this would result in a `draws_matrix` object
either way.

### Contributing to posterior

We welcome contributions! The **posterior** package is under active
development. If you find bugs or have ideas for new features (for us or
yourself to implement) please open an issue on GitHub
(<https://github.com/stan-dev/posterior/issues>).

### Citing posterior

Developing and maintaining open source software is an important yet
often underappreciated contribution to scientific progress. Thus,
whenever you are using open source software (or software in general),
please make sure to cite it appropriately so that developers get credit
for their work.

When using **posterior**, please cite it as follows:

-   Bürkner P. C., Gabry J., Kay M., & Vehtari A. (2020). “posterior:
    Tools for Working with Posterior Distributions.” R package version
    XXX, \<URL: <https://mc-stan.org/posterior/>\>.

When using the MCMC convergence diagnostics `rhat`, `ess_bulk`, or
`ess_tail`, please also cite

-   Vehtari A., Gelman A., Simpson D., Carpenter B., & Bürkner P. C.
    (2021). Rank-normalization, folding, and localization: An improved
    Rhat for assessing convergence of MCMC (with discussion). *Bayesian
    Analysis*. 16(2), 667–718. doi.org/10.1214/20-BA1221

The same information can be obtained by running `citation(""posterior"")`.

### References

Gelman A., Carlin J. B., Stern H. S., David B. Dunson D. B., Aki Vehtari
A., & Rubin D. B. (2013). *Bayesian Data Analysis, Third Edition*.
Chapman and Hall/CRC.

Vehtari A., Gelman A., Simpson D., Carpenter B., & Bürkner P. C. (2021).
Rank-normalization, folding, and localization: An improved Rhat for
assessing convergence of MCMC (with discussion). *Bayesian Analysis*.
16(2), 667–718. doi.org/10.1214/20-BA1221

### Licensing

The **posterior** package is licensed under the following licenses:

-   Code: BSD 3-clause (<https://opensource.org/licenses/BSD-3-Clause>)
-   Documentation: CC-BY 4.0
    (<https://creativecommons.org/licenses/by/4.0/>)
",2022-08-12
https://github.com/stan-dev/posteriordb,"<!-- README.md is generated from README.Rmd. Please edit that file -->

[![posteriordb
Content](https://github.com/stan-dev/posteriordb/actions/workflows/posteriordb_content.yml/badge.svg)](https://github.com/stan-dev/posteriordb/actions/workflows/posteriordb_content.yml)
[![R-CMD-check](https://github.com/stan-dev/posteriordb-r/actions/workflows/check-release.yaml/badge.svg)](https://github.com/stan-dev/posteriordb-r/actions/workflows/check-release.yaml)
[![Codecov test
coverage](https://codecov.io/gh/stan-dev/posteriordb-r/branch/main/graph/badge.svg)](https://codecov.io/gh/stan-dev/posteriordb-r?branch=main)
[![Python](https://github.com/stan-dev/posteriordb-python/actions/workflows/push.yml/badge.svg)](https://github.com/stan-dev/posteriordb-python/actions/workflows/push.yml)

`posteriordb`: a database of Bayesian posterior inference
=========================================================

What is `posteriordb`?
----------------------

`posteriordb` is a set of posteriors, i.e. Bayesian statistical models
and data sets, reference implementations in probabilistic programming
languages, and reference posterior inferences in the form of posterior
samples.

Why use `posteriordb`?
----------------------

`posteriordb` is designed to test inference algorithms across a wide
range of models and data sets. Applications include testing for
accuracy, speed, and scalability. `posteriordb` can be used to test new
algorithms being developed or deployed as part of continuous integration
for ongoing regression testing algorithms in probabilistic programming
frameworks.

`posteriordb` also makes it easy for students and instructors to access
various pedagogical and real-world examples with precise model
definitions, well-curated data sets, and reference posteriors.

`posteriordb` is framework agnostic and easily accessible from R and
Python.

For more details regarding the use cases of `posteriordb`, see
[doc/use\_cases.md](https://github.com/stan-dev/posteriordb/blob/master/doc/use_cases.md).

Content
-------

See
[DATABASE\_CONTENT.md](https://github.com/stan-dev/posteriordb/blob/master/doc/DATABASE_CONTENT.md)
for the details content of the posterior database.

Contributing
------------

We are happy with any help in adding posteriors, data, and models to the
database! See
[CONTRIBUTING.md](https://github.com/stan-dev/posteriordb/blob/master/doc/CONTRIBUTING.md)
for the details on how to contribute.

Using `posteriordb`
-------------------

To simplify the use of `posteriordb`, there are convenience functions
both in Python and in R. To use R, see the
[posteriordb-r](https://github.com/stan-dev/posteriordb-r) repository,
and to use Python, see the
[posteriordb-python](https://github.com/stan-dev/posteriordb-python)
repository.

Citing `posteriordb`
--------------------

Developing and maintaining open-source software is an important yet
often underappreciated contribution to scientific progress. Thus, please
make sure to cite it appropriately so that developers get credit for
their work. Information on how to cite `posteriordb` can be found in the
[CITATION.cff](https://github.com/stan-dev/posteriordb/blob/master/CITATION.cff)
file. Use the “cite this repository” button under “About” to get a
simple BibTeX or APA snippet.

As `posteriordb` rely heavily on Stan, so please consider also to cite
Stan:

Carpenter B., Gelman A., Hoffman M. D., Lee D., Goodrich B., Betancourt
M., Brubaker M., Guo J., Li P., and Riddell A. (2017). Stan: A
probabilistic programming language. Journal of Statistical Software.
76(1). 10.18637/jss.v076.i01

Design choices (so far)
-----------------------

The main focus of the database is simplicity, both in understanding and
in use.

The following are the current design choices in designing the posterior
database.

1.  Priors are hardcoded in model files as changing the prior changes
    the posterior. Create a new model to test different priors.
2.  Data transformations are stored as different datasets. Create new
    data to test different data transformations, subsets, and variable
    settings. This design choice makes the database larger/less memory
    efficient but simplifies the analysis of individual posteriors.
3.  Models and data has (model/data).info.json files with model and data
    specific information.
4.  Templates for different JSONs can be found in content/templates and
    schemas in schemas (Note: these don’t exist right now and will be
    added later)
5.  Prefix ‘syn\_’ stands for synthetic data where the generative
    process is known and found in content/data-raw.
6.  All data preprocessing is included in content/data-raw.
7.  Specific information for different PPL representations of models is
    included in the PPL syntax files as comments, not in the
    model.info.json files.

Versioning of models
--------------------

We might update models included in posteriordb over time. However, the
models will only have the same name in posteriordb if the log density is
the same (up to a normalizing constant). Otherwise, we will include a
new model in the database.
",2022-08-12
https://github.com/stan-dev/posteriordb-python,"[![Python](https://github.com/stan-dev/posteriordb-python/actions/workflows/push.yml/badge.svg)](https://github.com/stan-dev/posteriordb-python/actions/workflows/push.yml)

# `posteriordb-python`: a python library to work with `posteriordb`

This repository contain the python library to easily work with the [posteriordb](https://github.com/stan-dev/posteriordb) repository. The library included database contain convenience functions to access data, model code and information for individual posteriors, models, data and reference draws.

## Python versions

Currently only python 3.6+ is supported. Python 3.5+ support can be added if needed. We don't plan to support python 2.

## Installation

Installation from PyPI is recommended.

```bash
pip install posteriordb
```

Installing from the local clone.

```bash
git clone https://github.com/stan-dev/posteriordb-python
cd posteriordb-python
python setup.py bdist_wheel
pip install .
```

## Using the posterior database from python

The included database contains convenience functions to access data, model code and information for individual posteriors.

First we create the posterior database to use, here the cloned posterior database.

```python
>>> from posteriordb import PosteriorDatabase
>>> import os
>>> pdb_path = os.path.join(os.getcwd(), ""path/to/posterior_database"")
>>> my_pdb = PosteriorDatabase(pdb_path)
```
The above code requires you to specify the path to posterior_database folder in the posteriordb repository. If you clone it in the same folder as this repository, the path will be ../posteriordb/posterior_database.

Online database can be used with the `PosteriorDatabaseGithub` class. Remember to create and set `GITHUB_PAT` environmental variable.
It's recommended that users create a read-only (no extra permissions) [GitHub Personal Access Token (PAT)](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/creating-a-personal-access-token) for `posteriordb` use. It's also recommended that the
`GITHUB_PAT` environmental variable is added to user environmental variables and it is not shown in Python script as in the example below.


If not explicitly defined, `PosteriorDatabase` and `PosteriorDatabaseGithub` will create a new (or use old database) located at `POSTERIOR_DB_PATH` if it's
defined. `PosteriorDatabaseGithub` will finally use `$HOME/.posteriordb/posterior_database` as a fallback location if no environmental variables have been set.
Each model and data is only downloaded and cached when needed.

```python
>>> from posteriordb import PosteriorDatabaseGithub
>>> import os
>>> # It is recommended that GITHUB_PAT is added to the user environmental variables
>>> # outside python and not in a python script as shown in this example code
>>> os.environ[""GITHUB_PAT""] = ""token-string-here""
>>> my_pdb = PosteriorDatabaseGithub()
```

To list the posteriors available, use `posterior_names`.

```python
>>> pos = my_pdb.posterior_names()
>>> pos[:5]

['roaches-roaches_negbin',
 'syn_gmK2D1n200-gmm_diagonal_nonordered',
 'radon_mn-radon_variable_intercept_centered',
 'syn_gmK3D2n300-gmm_nonordered',
 'radon-radon_hierarchical_intercept_centered']

```

In the same fashion, we can list data and models included in the database as

```python
>>> mn = my_pdb.model_names()
>>> mn[:5]

['gmm_diagonal_nonordered',
 'radon_pool',
 'radon_partial_pool_noncentered',
 'blr',
 'radon_hierarchical_intercept_noncentered']


>>> dn = my_pdb.dataset_names()
>>> dn[:5]

['radon_mn',
 'wells_centered',
 'radon',
 'wells_centered_educ4_interact',
 'wells_centered_educ4']


```

The posterior's name is made up of the data and model fitted
to the data. Together, these two uniquely define a posterior distribution.
To access a posterior object we can use the posterior name.

```python
>>> posterior = my_pdb.posterior(""eight_schools-eight_schools_centered"")
```

From the posterior we can access the dataset and the model

```python
>>> model = posterior.model
>>> data = posterior.data
```

We can also access the names of posteriors, models and datasets.

```python
>>> posterior.name
""eight_schools-eight_schools_centered""

>>> model.name
""eight_schools_centered""

>>> data.name
""eight_schools""

```

We can access the same model and dataset also directly from the posterior database
```python
>>> model = my_pdb.model(""eight_schools_centered"")
>>> data = my_pdb.data(""eight_schools"")
```

From the model we can access model code and information about the model

```python
>>> model.code(""stan"")
data {
  int <lower=0> J; // number of schools
  real y[J]; // estimated treatment
  real<lower=0> sigma[J]; // std of estimated effect
}
parameters {
  real theta[J]; // treatment effect in school j
  real mu; // hyper-parameter of mean
  real<lower=0> tau; // hyper-parameter of sdv
}
model {
  tau ~ cauchy(0, 5); // a non-informative prior
  theta ~ normal(mu, tau);
  y ~ normal(theta , sigma);
  mu ~ normal(0, 5);
}

>>> model.code_file_path(""stan"")
'/home/eero/posterior_database/content/models/stan/eight_schools_centered.stan'

>>> model.information
{'keywords': ['bda3_example', 'hiearchical'],
 'description': 'A centered hiearchical model for the 8 schools example of Rubin (1981)',
 'urls': ['http://www.stat.columbia.edu/~gelman/arm/examples/schools'],
 'title': 'A centered hiearchical model for 8 schools',
 'references': ['rubin1981estimation', 'gelman2013bayesian'],
 'added_by': 'Mans Magnusson',
 'added_date': '2019-08-12'}
```
Note that the references are referencing to BibTeX items that can be found in `content/references/references.bib`.

From the dataset we can access the data values and information about it

```python
>>> data.values()
{'J': 8,
 'y': [28, 8, -3, 7, -1, 1, 18, 12],
 'sigma': [15, 10, 16, 11, 9, 11, 10, 18]}

>>> data.file_path()
'/tmp/tmpx16edu0w'

>>> data.information
{'keywords': ['bda3_example'],
 'description': 'A study for the Educational Testing Service to analyze the effects of\nspecial coaching programs on test scores. See Gelman et. al. (2014), Section 5.5 for details.',
 'urls': ['http://www.stat.columbia.edu/~gelman/arm/examples/schools'],
 'title': 'The 8 schools dataset of Rubin (1981)',
 'references': ['rubin1981estimation', 'gelman2013bayesian'],
 'added_by': 'Mans Magnusson',
 'added_date': '2019-08-12'}
```

To access gold standard posterior draws we can use `reference_draws` as follows.

```python
>>> posterior.reference_draws_info()
{'name': 'eight_schools-eight_schools_noncentered',
 'inference': {'method': 'stan_sampling',
  'method_arguments': {'chains': 10,
   'iter': 20000,
   'warmup': 10000,
   'thin': 10,
   'seed': 4711,
   'control': {'adapt_delta': 0.95}}},
 'diagnostics': {'diagnostic_information': {'names': ['mu',
    'tau',
    'theta[1]',
    ...

>>> gs = posterior.reference_draws()
>>> import pandas as pd
>>> pd.DataFrame(gs)

	theta[1]	                                        theta[2]
0	[10.6802773011458, 6.45383910854259, -2.241629...	[9.71770681295263, 4.41030824418493, 0.7617047...
1	[5.70891361633589, 10.3012059848039, 4.2439533...	[-2.32310565394337, 14.8121789773659, 6.517256...
2	[7.23747096507585, -0.427831558524343, 9.14782...	[7.35425759420389, 8.69579738064637, 8.9058764...
3	[4.44915522912766, 2.34711393762556, 17.680378...	[2.4368039319606, 5.89809320808632, 8.63031558...
...
```
",2022-08-12
https://github.com/stan-dev/posteriordb-r,"<!-- README.md is generated from README.Rmd. Please edit that file -->
<!-- badges: start -->

[![R-CMD-check](https://github.com/stan-dev/posteriordb-r/actions/workflows/check-release.yaml/badge.svg)](https://github.com/stan-dev/posteriordb-r/actions/workflows/check-release.yaml)
[![Codecov test
coverage](https://codecov.io/gh/stan-dev/posteriordb-r/branch/main/graph/badge.svg)](https://codecov.io/gh/stan-dev/posteriordb-r?branch=main)
<!-- badges: end -->

# `posteriordb`: an R package to work with `posteriordb`

This repository contains the R package to efficiently work with the
[posteriordb](https://github.com/stan-dev/posteriordb) repository. The R
package includes convenience functions to access data, model code and
information for individual posteriors, models, data and draws.

## Installation

To install only the R package and then access the posteriors remotely,
install the package from GitHub using the `remotes` package.

``` r
remotes::install_github(""stan-dev/posteriordb-r"")
```

To load the package, just run.

``` r
library(posteriordb)
```

## Connect to the posterior database

First, we create the posterior database connection to use. Here we want
to use the database locally. We assume the `posteriordb` repo has been
cloned and is accessible locally.

``` r
my_pdb <- pdb_local()
```

The above code requires that your working directory be the cloned
repository’s main folder. Otherwise, we can use the `path` argument in
`pdb_local()` to point to the local posterior database. We can also set
the environment variable `PBD_PATH` to handle the connection. For more
details, see `?pdb`.

The most straightforward approach is to use the GitHub repository
directly to access the database.

``` r
my_pdb <- pdb_github()
```

When you have a connection to the posterior database of choice, you can
access the data, models etc., using the same functionality.

## Contributing content using R

If you want to contribute to a posteriordb, see the vignette
[vignettes/contributing](https://htmlpreview.github.io/?https://github.com/stan-dev/posteriordb-r/blob/main/vignettes/contributing.html).

## Access content

To list the posteriors in the database, use `posterior_names()`.

``` r
pos <- posterior_names(my_pdb)
head(pos)
```

    ## [1] ""arK-arK""                         ""arma-arma11""                    
    ## [3] ""bball_drive_event_0-hmm_drive_0"" ""bball_drive_event_1-hmm_drive_1""
    ## [5] ""bones_data-bones_model""          ""butterfly-multi_occupancy""

In the same fashion, we can list data and models included in the
database as

``` r
mn <- model_names(my_pdb)
head(mn)
```

    ## [1] ""2pl_latent_reg_irt"" ""accel_gp""           ""accel_splines""     
    ## [4] ""arK""                ""arma11""             ""blr""

``` r
dn <- data_names(my_pdb)
head(dn)
```

    ## [1] ""arK""                 ""arma""                ""bball_drive_event_0""
    ## [4] ""bball_drive_event_1"" ""bones_data""          ""butterfly""

We can also get all information on each posterior as a table with

``` r
pos <- posteriors_tbl_df(my_pdb)
head(pos)
```

    ## # A tibble: 6 × 7
    ##   name        model_name reference_poste… data_name added_by added_date keywords
    ##   <chr>       <chr>      <chr>            <chr>     <chr>    <date>     <chr>   
    ## 1 arK-arK     arK        arK-arK          arK       Mans Ma… 2019-11-19 stan_be…
    ## 2 arma-arma11 arma11     arma-arma11      arma      Mans Ma… 2020-01-08 stan_be…
    ## 3 bball_driv… hmm_drive… bball_drive_eve… bball_dr… Oliver … 2020-05-10 stan_ex…
    ## 4 bball_driv… hmm_drive… bball_drive_eve… bball_dr… Oliver … 2020-05-10 stan_be…
    ## 5 bball_driv… hmm_drive… bball_drive_eve… bball_dr… Oliver … 2020-05-10 stan_ex…
    ## 6 bball_driv… hmm_drive… bball_drive_eve… bball_dr… Oliver … 2020-05-10 stan_be…

The posterior’s name is made up of the data and model fitted to the
data. Together, these two uniquely define a posterior distribution. To
access a posterior object, we can use the posterior name.

``` r
po <- posterior(""eight_schools-eight_schools_centered"", my_pdb)
```

From the posterior object, we can access data, model code (i.e., Stan
code in this case) and other useful information.

``` r
dat <- pdb_data(po)
dat
```

    ## $J
    ## [1] 8
    ## 
    ## $y
    ## [1] 28  8 -3  7 -1  1 18 12
    ## 
    ## $sigma
    ## [1] 15 10 16 11  9 11 10 18

``` r
code <- stan_code(po)
code
```

    ## data {
    ##   int <lower=0> J; // number of schools
    ##   real y[J]; // estimated treatment
    ##   real<lower=0> sigma[J]; // std of estimated effect
    ## }
    ## parameters {
    ##   real theta[J]; // treatment effect in school j
    ##   real mu; // hyper-parameter of mean
    ##   real<lower=0> tau; // hyper-parameter of sdv
    ## }
    ## model {
    ##   tau ~ cauchy(0, 5); // a non-informative prior
    ##   theta ~ normal(mu, tau);
    ##   y ~ normal(theta, sigma);
    ##   mu ~ normal(0, 5);
    ## }

We can also access the paths to data after they have been unzipped and
copied to the cache directory set in `pdb` (the R temp directory by
default).

``` r
dfp <- data_file_path(po)
dfp
```

    ## [1] ""/var/folders/8x/bgssdq5n6dx1_ydrhq1zgrym0000gn/T//Rtmpwafi9o/posteriordb_cache/data/data/eight_schools.json""

``` r
scfp <- stan_code_file_path(po)
scfp
```

    ## [1] ""/var/folders/8x/bgssdq5n6dx1_ydrhq1zgrym0000gn/T//Rtmpwafi9o/posteriordb_cache/models/stan/eight_schools_centered.stan""

We can also access information regarding the model and the data used to
compute the posterior.

``` r
data_info(po)
```

    ## Data: eight_schools
    ## The 8 schools dataset of Rubin (1981)

``` r
model_info(po)
```

    ## Model: eight_schools_centered
    ## A centered hiearchical model for 8 schools
    ## Frameworks: 'stan', 'pymc3'

Note that the references reference BibTeX items found in
`content/references/references.bib`.

We can access most of the posterior information as a `tbl_df` using

``` r
tbl <- posteriors_tbl_df(my_pdb)
head(tbl)
```

    ## # A tibble: 6 × 7
    ##   name        model_name reference_poste… data_name added_by added_date keywords
    ##   <chr>       <chr>      <chr>            <chr>     <chr>    <date>     <chr>   
    ## 1 arK-arK     arK        arK-arK          arK       Mans Ma… 2019-11-19 stan_be…
    ## 2 arma-arma11 arma11     arma-arma11      arma      Mans Ma… 2020-01-08 stan_be…
    ## 3 bball_driv… hmm_drive… bball_drive_eve… bball_dr… Oliver … 2020-05-10 stan_ex…
    ## 4 bball_driv… hmm_drive… bball_drive_eve… bball_dr… Oliver … 2020-05-10 stan_be…
    ## 5 bball_driv… hmm_drive… bball_drive_eve… bball_dr… Oliver … 2020-05-10 stan_ex…
    ## 6 bball_driv… hmm_drive… bball_drive_eve… bball_dr… Oliver … 2020-05-10 stan_be…

In addition, we can also access a list of posteriors with
`filter_posteriors()`. The filtering function follows dplyr filter
semantics.

``` r
pos <- filter_posteriors(pdb = my_pdb, data_name == ""eight_schools"")
pos
```

    ## [[1]]
    ## Posterior (eight_schools-eight_schools_centered)
    ## 
    ## Data: eight_schools
    ## The 8 schools dataset of Rubin (1981)
    ## 
    ## Model: eight_schools_centered
    ## A centered hiearchical model for 8 schools
    ## Frameworks: 'stan', 'pymc3'
    ## 
    ## [[2]]
    ## Posterior (eight_schools-eight_schools_noncentered)
    ## 
    ## Data: eight_schools
    ## The 8 schools dataset of Rubin (1981)
    ## 
    ## Model: eight_schools_noncentered
    ## A non-centered hiearchical model for 8 schools
    ## Frameworks: 'stan'

To access reference posterior draws, we use
`reference_posterior_draws()`.

``` r
rpd <- reference_posterior_draws(po)
```

The function `reference_posterior_draws()` returns a posterior
`draws_list` object that can be summarized and transformed using the
`posterior` package.

``` r
posterior::summarize_draws(rpd)
```

    ## # A tibble: 10 × 10
    ##    variable  mean median    sd   mad     q5   q95  rhat ess_bulk ess_tail
    ##    <chr>    <dbl>  <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>    <dbl>
    ##  1 theta[1]  6.15   5.59  5.62  4.56 -1.68  16.3   1.00   10095.    9732.
    ##  2 theta[2]  4.94   4.77  4.65  4.14 -2.22  12.8   1.00   10049.   10139.
    ##  3 theta[3]  3.91   4.11  5.28  4.48 -4.91  11.8   1.00    9533.    9339.
    ##  4 theta[4]  4.80   4.70  4.77  4.22 -2.67  12.6   1.00   10026.    9666.
    ##  5 theta[5]  3.61   3.82  4.61  4.15 -4.26  10.6   1.00    9922.   10207.
    ##  6 theta[6]  4.05   4.16  4.80  4.32 -3.87  11.5   1.00    9783.   10039.
    ##  7 theta[7]  6.32   5.80  5.00  4.39 -0.855 15.3   1.00   10039.    9690.
    ##  8 theta[8]  4.88   4.79  5.32  4.47 -3.32  13.5   1.00    9605.    9871.
    ##  9 mu        4.41   4.36  3.31  3.30 -0.936  9.83  1.00   10041.    9973.
    ## 10 tau       3.60   2.75  3.20  2.55  0.257  9.73  1.00    9989.    9992.

To access information on the reference posterior we can use
`reference_posterior_draws_info()` or use `info()` on the reference
posterior. The posterior reference draws return information on how the
reference posterior was computed.

``` r
rpi <- reference_posterior_draws_info(po)
rpi
```

    ## Posterior: eight_schools-eight_schools_noncentered
    ## Method: stan_sampling (rstan 2.21.1)
    ## Arguments:
    ##   chains: 10
    ##   iter: 20000
    ##   warmup: 10000
    ##   thin: 10
    ##   seed: 4711
    ##     adapt_delta: 0.95

``` r
info(rpd)
```

    ## Posterior: eight_schools-eight_schools_noncentered
    ## Method: stan_sampling (rstan 2.21.1)
    ## Arguments:
    ##   chains: 10
    ##   iter: 20000
    ##   warmup: 10000
    ##   thin: 10
    ##   seed: 4711
    ##     adapt_delta: 0.95
",2022-08-12
https://github.com/stan-dev/projpred,"<!-- badges: start -->
<!-- [![codecov](https://codecov.io/gh/stan-dev/projpred/branch/master/graph/badge.svg)](https://app.codecov.io/gh/stan-dev/projpred) -->
[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/projpred?color=blue)](https://CRAN.R-project.org/package=projpred)
<!-- badges: end -->

# projpred [<img src=""man/figures/logo.svg"" align=""right"" height=""139"" alt=""Stan Logo""/>](https://mc-stan.org)

The **projpred** R package performs the projection predictive variable selection
for generalized linear and additive models as well as for generalized linear and
additive multilevel models (with the support for additive models being still
experimental). The package is compatible with the
[**rstanarm**](https://mc-stan.org/rstanarm/) and
[**brms**](https://paul-buerkner.github.io/brms/) packages, but custom reference
models can also be used.

The projection predictive variable selection is based on the ideas of Goutis and
Robert (1998) and Dupuis and Robert (2003). The methods implemented in
**projpred** are described in detail in Piironen et al. (2020) and Catalina et
al. (2020). They are evaluated in comparison to many other methods in Piironen
and Vehtari (2017). Type `citation(""projpred"")` in R (or see the `CITATION`
file) for details on how to cite **projpred**.

Currently, the supported response distributions (objects of class `family` in R)
are `gaussian()`, `binomial()` (via the **brms** package, `brms::bernoulli()` is
also supported), and `poisson()`.

The [vignettes](https://mc-stan.org/projpred/articles/) (currently, there is
only a single one) illustrate how to use the **projpred** functions in
conjunction. Details on the **projpred** functions as well as some shorter
examples may be found in the documentation.

## Installation

There are two ways for installing **projpred**: from
[CRAN](https://CRAN.R-project.org/package=projpred) or from
[GitHub](https://github.com/stan-dev/projpred). The GitHub version might be more
recent than the CRAN version, but the CRAN version might be more stable.

### From CRAN

```r
install.packages(""projpred"")
```

### From GitHub

This requires the [**devtools**](https://devtools.r-lib.org/) package, so if
necessary, the following code will also install **devtools** (from
[CRAN](https://CRAN.R-project.org/package=devtools)):
```r
if (!requireNamespace(""devtools"", quietly = TRUE)) {
  install.packages(""devtools"")
}
devtools::install_github(""stan-dev/projpred"", build_vignettes = TRUE)
```
To save time, you may omit `build_vignettes = TRUE`.

## References

Catalina, A., Bürkner, P.-C., and Vehtari, A. (2020). Projection predictive
inference for generalized linear and additive multilevel models.
*arXiv:2010.06994*. URL: <https://arxiv.org/abs/2010.06994>.

Dupuis, J. A. and Robert, C. P. (2003). Variable selection in qualitative models
via an entropic explanatory power. *Journal of Statistical Planning and
Inference*, **111**(1-2):77–94. DOI:
[10.1016/S0378-3758(02)00286-0](https://doi.org/10.1016/S0378-3758(02)00286-0).

Goutis, C. and Robert, C. P. (1998). Model choice in generalised linear models:
A Bayesian approach via Kullback–Leibler projections. *Biometrika*,
**85**(1):29–37.

Piironen, J. and Vehtari, A. (2017). Comparison of Bayesian predictive methods
for model selection. *Statistics and Computing*, **27**(3):711-735. DOI:
[10.1007/s11222-016-9649-y](https://doi.org/10.1007/s11222-016-9649-y).

Piironen, J., Paasiniemi, M., and Vehtari, A. (2020). Projective inference in
high-dimensional problems: Prediction and feature selection. *Electronic Journal
of Statistics*, **14**(1):2155-2197. DOI:
[10.1214/20-EJS1711](https://doi.org/10.1214/20-EJS1711).
",2022-08-12
https://github.com/stan-dev/pystan,"******
PyStan
******

**PyStan** is a Python interface to Stan, a package for Bayesian inference.

Stan® is a state-of-the-art platform for statistical modeling and
high-performance statistical computation. Thousands of users rely on Stan for
statistical modeling, data analysis, and prediction in the social, biological,
and physical sciences, engineering, and business.

Notable features of PyStan include:

* Automatic caching of compiled Stan models
* Automatic caching of samples from Stan models
* An interface similar to that of RStan
* Open source software: ISC License

Getting started
===============

Install PyStan with ``pip install pystan``. PyStan runs on Linux and macOS. You will also need a C++ compiler such as gcc ≥9.0 or clang ≥10.0.

The following block of code shows how to use PyStan with a model which studied coaching effects across eight schools (see Section 5.5 of Gelman et al (2003)). This hierarchical model is often called the ""eight schools"" model.

.. code-block:: python

    import stan

    schools_code = """"""
    data {
      int<lower=0> J;         // number of schools
      real y[J];              // estimated treatment effects
      real<lower=0> sigma[J]; // standard error of effect estimates
    }
    parameters {
      real mu;                // population treatment effect
      real<lower=0> tau;      // standard deviation in treatment effects
      vector[J] eta;          // unscaled deviation from mu by school
    }
    transformed parameters {
      vector[J] theta = mu + tau * eta;        // school treatment effects
    }
    model {
      target += normal_lpdf(eta | 0, 1);       // prior log-density
      target += normal_lpdf(y | theta, sigma); // log-likelihood
    }
    """"""

    schools_data = {""J"": 8,
                    ""y"": [28,  8, -3,  7, -1,  1, 18, 12],
                    ""sigma"": [15, 10, 16, 11,  9, 11, 10, 18]}

    posterior = stan.build(schools_code, data=schools_data)
    fit = posterior.sample(num_chains=4, num_samples=1000)
    eta = fit[""eta""]  # array with shape (8, 4000)
    df = fit.to_frame()  # pandas `DataFrame`


Citation
========

We appreciate citations as they let us discover what people have been doing
with the software. Citations also provide evidence of use which can help in
obtaining grant funding.

To cite PyStan in publications use:

Riddell, A., Hartikainen, A., & Carter, M. (2021). PyStan (3.0.0). https://pypi.org/project/pystan

Or use the following BibTeX entry::

    @misc{pystan,
      title = {pystan (3.0.0)},
      author = {Riddell, Allen and Hartikainen, Ari and Carter, Matthew},
      year = {2021},
      month = mar,
      howpublished = {PyPI}
    }

Please also cite Stan.
",2022-08-12
https://github.com/stan-dev/pystan-wheels,"####################################
Building and uploading pystan wheels
####################################

NOTE: This repository is no longer used. It was used for building PyStan 2 wheels.

See https://travis-ci.org/MacPython/matplotlib-wheels for details on how this
all works.


## Notes

*Differences from the standard ``multibuild`` instructions are noted in this section.**

Stan version 2.19 and higher requires C++14. In ``env_vars.sh`` we ``export MACOSX_DEPLOYMENT_TARGET=10.9``
so that ``clang`` will use ``libc++`` for the C++ standard library.

We also use a custom `xenial` image because to even test pystan we need a
version of gcc more recent than the one that ships with trusty.
",2022-08-12
https://github.com/stan-dev/pystan2,"PyStan: The Python Interface to Stan
====================================

.. image:: https://raw.githubusercontent.com/stan-dev/logos/master/logo.png
    :alt: Stan logo
    :scale: 50 %

|pypi| |travis| |appveyor| |zenodo|

.. tip:: PyStan 3 is available for Linux and macOS users. Visit the `PyStan 3 documentation <https://pystan.readthedocs.io/en/latest/>`_ for details. PyStan 2 is not maintained.

**PyStan** provides a Python interface to Stan, a package for Bayesian inference
using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo.

For more information on `Stan <http://mc-stan.org>`_ and its modeling language,
see the Stan User's Guide and Reference Manual at `http://mc-stan.org/
<http://mc-stan.org/>`_.


Important links
---------------

- HTML documentation: https://pystan2.readthedocs.org
- Issue tracker: https://github.com/stan-dev/pystan/issues
- Source code repository: https://github.com/stan-dev/pystan
- Stan: http://mc-stan.org/
- Stan User's Guide and Reference Manual (pdf) available at http://mc-stan.org

Related projects
----------------

- ArviZ: `Exploratory analysis of Bayesian models with Python <https://github.com/arviz-devs/arviz>`_ by @arviz-devs
- Jupyter tool: `StanMagic <https://github.com/Arvinds-ds/stanmagic>`_ by @Arvinds-ds
- Jupyter tool: `JupyterStan <https://github.com/janfreyberg/jupyterstan>`_ by @janfreyberg
- Scikit-learn integration: `pystan-sklearn <https://github.com/rgerkin/pystan-sklearn>`_ by @rgerkin.

Projects using PyStan
---------------------
- BAMBI: `BAyesian Model-Building Interface <https://github.com/bambinos/bambi>`_ by @bambinos
- hBayesDM: `hierarchical Bayesian modeling of Decision-Making tasks <https://hbayesdm.readthedocs.io>`_ by @CCS-Lab
- Orbit: `Object-oRiented BayesIan Timeseries models <https://github.com/uber/orbit>`_ by @uber
- Prophet: `Timeseries forecasting <https://facebook.github.io/prophet/>`_ by @facebook

Similar projects
----------------

- PyMC3: https://docs.pymc.io/
- emcee: https://emcee.readthedocs.io/en/stable/

PyStan3 / Stan3
---------------
The development of PyStan3 with updated API can be found under `stan-dev/pystan-next <https://github.com/stan-dev/pystan-next>`_

Detailed Installation Instructions
----------------------------------
Detailed installation instructions can be found in the
`doc/installation_beginner.md <doc/installation_beginner.rst/>`_ file.

Windows Installation Instructions
---------------------------------
Detailed installation instructions for Windows can be found in docs under `PyStan on Windows <https://pystan2.readthedocs.io/en/latest/windows.html>`_

Quick Installation (Linux and macOS)
------------------------------------

`NumPy  <http://www.numpy.org/>`_ and `Cython <http://www.cython.org/>`_
(version 0.22 or greater) are required. `matplotlib <http://matplotlib.org/>`_
is optional. ArviZ is recommended for visualization and analysis.

PyStan and the required packages may be installed from the `Python Package Index
<https://pypi.python.org/pypi>`_ using ``pip``.

::

   pip install pystan

Alternatively, if Cython (version 0.22 or greater) and NumPy are already
available, PyStan may be installed from source with the following commands

::

   git clone --recursive https://github.com/stan-dev/pystan.git
   cd pystan
   python setup.py install

To install latest development version user can also use ``pip``

::

    pip install git+https://github.com/stan-dev/pystan

If you encounter an ``ImportError`` after compiling from source, try changing
out of the source directory before attempting ``import pystan``. On Linux and
OS X ``cd /tmp`` will work.

``make`` (``mingw32-make`` on Windows) is a requirement for building from source.

Example
-------

.. code-block:: python

    import pystan
    import numpy as np
    import matplotlib.pyplot as plt

    schools_code = """"""
    data {
        int<lower=0> J; // number of schools
        real y[J]; // estimated treatment effects
        real<lower=0> sigma[J]; // s.e. of effect estimates
    }
    parameters {
        real mu;
        real<lower=0> tau;
        real eta[J];
    }
    transformed parameters {
        real theta[J];
        for (j in 1:J)
            theta[j] = mu + tau * eta[j];
    }
    model {
        eta ~ normal(0, 1);
        y ~ normal(theta, sigma);
    }
    """"""

    schools_dat = {'J': 8,
                   'y': [28,  8, -3,  7, -1,  1, 18, 12],
                   'sigma': [15, 10, 16, 11,  9, 11, 10, 18]}

    sm = pystan.StanModel(model_code=schools_code)
    fit = sm.sampling(data=schools_dat, iter=1000, chains=4)

    print(fit)

    eta = fit.extract(permuted=True)['eta']
    np.mean(eta, axis=0)

    # if matplotlib is installed (optional, not required), a visual summary and
    # traceplot are available
    fit.plot()
    plt.show()

    # updated traceplot can be plotted with
    import arviz as az
    az.plot_trace(fit)

.. |pypi| image:: https://badge.fury.io/py/pystan.png
    :target: https://badge.fury.io/py/pystan
    :alt: pypi version

.. |travis| image:: https://travis-ci.org/stan-dev/pystan.png?branch=master
    :target: https://travis-ci.org/stan-dev/pystan
    :alt: travis-ci build status

.. |appveyor| image:: https://ci.appveyor.com/api/projects/status/49e69yl5ngxkpmab?svg=true
    :target: https://ci.appveyor.com/project/pystan/pystan
    :alt: appveyor-ci build status

.. |zenodo| image:: https://zenodo.org/badge/10256919.svg
    :target: https://zenodo.org/badge/latestdoi/10256919
    :alt: zenodo citation DOI
",2022-08-12
https://github.com/stan-dev/r-packages,"# Repository for distributing (some) stan-dev R packages

A place for publishing new versions of (some) `stan-dev` R packages before they reach CRAN and for `stan-dev` R packages and versions where releasing on CRAN is not a (current) goal. As of 2021-03-16 this is most relevant for `rstan`, where the CRAN version is unfortunately several releases behind and pushing a new version to CRAN has been difficult.

The packages currently hosted in this repository are:
 - [cmdstanr](https://github.com/stan-dev/cmdstanr)
 - [rstan](https://github.com/stan-dev/rstan)
 - [StanHeaders](https://github.com/stan-dev/stan) (required for rstan)
 - [rstanarm](https://github.com/stan-dev/rstanarm) (Survival branch - https://arxiv.org/pdf/2002.09633.pdf)

To install latest `rstan` or other packages from the repo, add `https://mc-stan.org/r-packages/` to your repository list, e.g.:

```r
install.packages(""rstan"", repos = c(""https://mc-stan.org/r-packages/"", getOption(""repos"")))
```

## How to publish/update packages on this repository

**For stan-dev/r-packages maintainers**

1. Install the `drat` package
```r
install.packages(""drat"")
```

2. Clone this repository and make sure you are on the `gh-pages` branch (the default branch).
```
git clone https://github.com/stan-dev/r-packages
git status
```

3. Create the package tarball (tar.gz) file
```r
devtools::build()
```
or

```
R CMD build packageFolder
```

4. Run `drat` to update the repository
```r
drat::insertPackage(""path/to/package_tarball.tar.gz"", ""path/to/r-packages/"")
```

For example:
```r
drat::insertPackage(""cmdstanr_0.4.0.tar.gz"", ""r-packages/"")
```

5. Commit and push changes to the `stan-dev/r-packages` repository.


**For others that wish to publish their package on https://mc-stan.org/r-packages**


1. Install the `drat` package
```r
install.packages(""drat"")
```

2. Fork this repository, clone your fork and make sure you are on the `gh-pages` branch (the default branch).
```
git clone https://github.com/yourusername/r-packages
git status
```

3. Create the package tarball (tar.gz) file
```r
devtools::build()
```
or

```
R CMD build packageFolder
```

4. Run `drat` to update the repository
```r
drat::insertPackage(""path/to/package_tarball.tar.gz"", ""path/to/r-packages/"")
```

For example:
```r
drat::insertPackage(""cmdstanr_0.4.0.tar.gz"", ""r-packages/"")
```

5. Commit and push changes to your repository `yourusername/r-packages`.

6. Open a pull request on this repository to add your package.
",2022-08-12
https://github.com/stan-dev/rstan,"# RStan <img src=""rstan/rstan/man/figures/stanlogo.png"" align=""right"" width=""120"" />

<!-- badges: start -->
[![CRAN\_Status\_Badge](http://www.r-pkg.org/badges/version/rstan?color=blue)](http://cran.r-project.org/package=rstan)
[![Downloads](http://cranlogs.r-pkg.org/badges/rstan?color=blue)](http://cran.rstudio.com/package=rstan)
[![Research software impact](http://depsy.org/api/package/cran/rstan/badge.svg)](http://depsy.org/package/r/rstan)
[![R-CMD-check](https://github.com/stan-dev/rstan/workflows/R-CMD-check/badge.svg)](https://github.com/stan-dev/rstan/actions)
<!-- badges: end -->

**RStan** is the R interface to [Stan](http://mc-stan.org). 

### Quick links

* [mc-stan.org/rstan](http://mc-stan.org/rstan) (online RStan documentation, vignettes)
* [Stan documentation](http://mc-stan.org/users/documentation/) (language manual, case studies, and more)
* [Ask a question](http://discourse.mc-stan.org) (Stan Forums on Discourse)
* [Open an issue](https://github.com/stan-dev/rstan/issues) (GitHub issues for bug reports, feature requests)


### Getting Started

For installation instructions and other tips on getting started with RStan see

* [RStan Getting Started](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started)

Several Stan users have also contributed translations of the _RStan Getting Started_ page:

* [RStan Getting Started (French)](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(Français))
* [RStan Getting Started (Japanese)](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(Japanese))
* [RStan Getting Started (繁體中文)](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(%E7%B9%81%E9%AB%94%E4%B8%AD%E6%96%87))
* [RStan Getting Started (Portuguese)](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started-(Portugu%C3%AAs))

### Source Repository

RStan's source code repository is hosted here on GitHub. Stan's source repository is defined as a submodule. 
See [how to work with stan submodule in rstan repo](https://github.com/stan-dev/rstan/wiki/How-to-work-with-the-stan-submodule-in-rstan-repo%3F).

### Licensing

RStan is licensed under GPLv3.  The Stan code packaged in RStan is licensed under new BSD.   
",2022-08-12
https://github.com/stan-dev/rstanarm,"# rstanarm <img src=""man/figures/stanlogo.png"" align=""right"" width=""120"" />

<!-- badges: start -->
[![CRAN\_Status\_Badge](https://www.r-pkg.org/badges/version/rstanarm?color=blue)](https://cran.r-project.org/package=rstanarm)
[![Downloads](https://cranlogs.r-pkg.org/badges/rstanarm?color=blue)](https://cran.rstudio.com/package=rstanarm)
[![R-CMD-check](https://github.com/stan-dev/rstanarm/workflows/R-CMD-check/badge.svg)](https://github.com/stan-dev/rstanarm/actions)
<!-- badges: end -->

### Bayesian applied regression modeling (arm) via Stan

This is an R package that emulates other R model-fitting functions but uses
[Stan](https://mc-stan.org) (via the **rstan** package) for the back-end
estimation. The primary target audience is people who would be open to Bayesian
inference if using Bayesian software were easier but would use frequentist
software otherwise. 

Fitting models with **rstanarm** is also useful for experienced Bayesian
software users who want to take advantage the pre-compiled Stan programs that
are written by Stan developers and carefully implemented to prioritize numerical
stability and the avoidance of sampling problems.

Click the arrows for more details:
<details><summary>More detail</summary>

The **rstanarm** package is an appendage to the **rstan** package, the R
interface to [Stan](https://mc-stan.org/). **rstanarm** enables many of the most
common applied regression models to be estimated using Markov Chain Monte Carlo,
variational approximations to the posterior distribution, or optimization. The
package allows these models to be specified using the customary R modeling
syntax (e.g., like that of `glm` with a `formula` and `data.frame`).
Additional arguments are provided for specifying prior distributions.

The set of models supported by **rstanarm** is large (and will continue to
grow), but also limited enough so that it is possible to integrate them
tightly with the [`pp_check`](https://mc-stan.org/rstanarm/reference/pp_check.stanreg.html) function for graphical posterior predictive checks using [**bayesplot**](https://mc-stan.org/bayesplot) and the
[`posterior_predict`](https://mc-stan.org/rstanarm/reference/posterior_predict.stanreg.html)
function to easily estimate the effect of specific manipulations of predictor
variables or to predict the outcome in a training set.

The fitted model objects returned by the **rstanarm** modeling functions are
called _stanreg_ objects. In addition to all of the traditional
[methods](https://mc-stan.org/rstanarm/reference/stanreg-methods.html)
defined for fitted model objects, stanreg objects can also be used with the
[**loo**](https://mc-stan.org/rstanarm/reference/loo.stanreg.html) package for
leave-one-out cross-validation, model comparison, and model weighting/averaging
and the [**shinystan**](https://mc-stan.org/rstanarm/reference/shinystan.html) 
package for exploring the posterior distribution and model diagnostics
with a graphical user interface. 

Check out the **rstanarm** [vignettes](https://mc-stan.org/rstanarm/articles/)
for examples and more details about the entire process.
</details>

<details><summary>Modeling functions</summary>

The model estimating functions are described in greater detail in their
individual help pages and vignettes. Here we provide a very brief overview:

* [__`stan_lm`__, __`stan_aov`__,__`stan_biglm`__](https://mc-stan.org/rstanarm/reference/stan_lm.html)

  Similar to  `lm` and `aov` but with novel regularizing priors on the model
  parameters that are driven by prior beliefs about R-squared, the proportion of
  variance in the outcome attributable to the predictors in a linear model.

* [__`stan_glm`__, __`stan_glm.nb`__](https://mc-stan.org/rstanarm/reference/stan_glm.html)

  Similar to `glm` but with various possible prior distributions for the
  coefficients and, if applicable, a prior distribution for any auxiliary
  parameter in a Generalized Linear Model (GLM) that is characterized by a
  `family` object (e.g. the shape parameter in Gamma models). It is also possible
  to estimate a negative binomial model similar to the `glm.nb` function
  in the `MASS` package.

* [__`stan_glmer`__, __`stan_glmer.nb`__, __`stan_lmer`__](https://mc-stan.org/rstanarm/reference/stan_glmer.html)

  Similar to the `glmer`, `glmer.nb`, and `lmer` functions (__lme4__ package) in
  that GLMs are augmented to have group-specific terms that deviate from the
  common coefficients according to a mean-zero multivariate normal distribution
  with a highly-structured but unknown covariance matrix (for which **rstanarm**
  introduces an innovative prior distribution). MCMC provides more appropriate
  estimates of uncertainty for models that consist of a mix of common and
  group-specific parameters.
  
* [__`stan_nlmer`__](https://mc-stan.org/rstanarm/reference/stan_nlmer.html)

  Similar to `nlmer` (__lme4__ package) package for nonlinear ""mixed-effects""
  models, but flexible priors can be specified for all parameters in the model, 
  including the unknown covariance matrices for the varying 
  (group-specific) coefficients.

* [__`stan_gamm4`__](https://mc-stan.org/rstanarm/reference/stan_gamm4.html)

  Similar to `gamm4` (__gamm4__ package), which augments a GLM (possibly with
  group-specific terms) with nonlinear smooth functions of the predictors to
  form a Generalized Additive Mixed Model (GAMM). Rather than calling
  `lme4::glmer` like `gamm4` does, `stan_gamm4` essentially calls `stan_glmer`,
  which avoids the optimization issues that often crop up with GAMMs and
  provides better estimates for the uncertainty of the parameter estimates.
 
* [__`stan_polr`__](https://mc-stan.org/rstanarm/reference/stan_polr.html)

  Similar to `polr` (__MASS__ package) in that it models an ordinal response,
  but the Bayesian model also implies a prior distribution on the unknown
  cutpoints. Can also be used to model binary outcomes, possibly while
  estimating an unknown exponent governing the probability of success.
 
* [__`stan_betareg`__](https://mc-stan.org/rstanarm/reference/stan_betareg.html)

  Similar to `betareg` (__betareg__ package) in that it models an outcome that
  is a rate (proportion) but, rather than performing maximum likelihood
  estimation, full Bayesian estimation is performed by default, with
  customizable prior distributions for all parameters.

* [__`stan_clogit`__](https://mc-stan.org/rstanarm/reference/stan_clogit.html)

   Similar to `clogit` (__survival__ package) in that it models an binary outcome
   where the number of successes and failures is fixed within each stratum by
   the research design. There are some minor syntactical differences relative
   to `survival::clogit` that allow `stan_clogit` to accept
   group-specific terms as in `stan_glmer`.

* [__`stan_mvmer`__](https://mc-stan.org/rstanarm/reference/stan_mvmer.html)

   A multivariate form of `stan_glmer`, whereby the user can specify
   one or more submodels each consisting of a GLM with group-specific terms. If
   more than one submodel is specified (i.e. there is more than one outcome
   variable) then a dependence is induced by assuming that the group-specific
   terms for each grouping factor are correlated across submodels.

* [__`stan_jm`__](https://mc-stan.org/rstanarm/reference/stan_jm.html)

   Estimates shared parameter joint models for longitudinal and time-to-event
   (i.e. survival) data. The joint model can be univariate (i.e. one longitudinal
   outcome) or multivariate (i.e. more than one longitudinal outcome). A variety
   of parameterisations are available for linking the longitudinal and event
   processes (i.e. a variety of association structures).

</details>

<details><summary>Estimation algorithms</summary>

The modeling functions in the **rstanarm** package take an `algorithm`
argument that can be one of the following:

* __Sampling__ (`algorithm=""sampling""`):
 
 Uses Markov Chain Monte Carlo (MCMC) --- in particular, Stan's implementation
 of Hamiltonian Monte Carlo (HMC) with a tuned but diagonal mass matrix --- 
 to draw from the posterior distribution of the parameters. This is the slowest
 but most reliable of the available estimation algorithms and it is __the
 default and recommended algorithm for statistical inference__.

* __Mean-field__ (`algorithm=""meanfield""`):

 Uses mean-field variational inference to draw from an approximation to the
 posterior distribution. In particular, this algorithm finds the set of
 independent normal distributions in the unconstrained space that --- when
 transformed into the constrained space --- most closely approximate the
 posterior distribution. Then it draws repeatedly from these independent
 normal distributions and transforms them into the constrained space. The
 entire process is much faster than HMC and yields independent draws but
 __is not recommended for final statistical inference__. It can be useful to
 narrow the set of candidate models in large problems, particularly when
 specifying `QR=TRUE` in `stan_glm`, `stan_glmer`, and `stan_gamm4`, but is
 __only an approximation to the posterior distribution__.

* __Full-rank__ (`algorithm=""fullrank""`):

 Uses full-rank variational inference to draw from an approximation to the
 posterior distribution by finding the multivariate normal distribution in
 the unconstrained space that --- when transformed into the constrained space
 --- most closely approximates the posterior distribution. Then it draws
 repeatedly from this multivariate normal distribution and transforms the
 draws into the constrained space. This process is slower than meanfield
 variational inference but is faster than HMC. Although still an
 approximation to the posterior distribution and thus __not recommended
 for final statistical inference__, the approximation is more realistic than
 that of mean-field variational inference because the parameters are not
 assumed to be independent in the unconstrained space. Nevertheless, fullrank
 variational inference is a more difficult optimization problem and the
 algorithm is more prone to non-convergence or convergence to a local
 optimum.

* __Optimizing__ (`algorithm=""optimizing""`):

 Finds the posterior mode using a C++ implementation of the LBGFS algorithm. If
 there is no prior information, then this is equivalent to maximum likelihood,
 in which case there is no great reason to use the functions in the **rstanarm**
 package over the emulated functions in other packages. However, if priors are
 specified, then the estimates are penalized maximum likelihood estimates, which
 may have some redeeming value. Currently, optimization is only supported for
 `stan_glm`.

</details>

---

### Resources

* [mc-stan.org/rstanarm](https://mc-stan.org/rstanarm) (online documentation, vignettes)
* [Ask a question](https://discourse.mc-stan.org) (Stan Forums on Discourse)
* [Open an issue](https://github.com/stan-dev/rstanarm/issues) (GitHub issues for bug reports, feature requests)

### Installation

#### Latest Release

The most recent **rstanarm** release can be installed from CRAN via

```r
install.packages(""rstanarm"")
```

#### Development Version

To install from GitHub, first make sure that you can install the **rstan**
package and C++ toolchain by following these
[instructions](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started).
Once **rstan** is successfully installed, you can install **rstanarm** from
GitHub using the **remotes** package by executing the following in R:

```r
# Change 2 to however many cores you can/want to use to parallelize install
# If you experience crashes or run out RAM during installation, try changing this to 1
Sys.setenv(MAKEFLAGS = ""-j2"")
Sys.setenv(""R_REMOTES_NO_ERRORS_FROM_WARNINGS"" = ""true"")
remotes::install_github(""stan-dev/rstanarm"", INSTALL_opts = ""--no-multiarch"", force = TRUE)
```

You can switch `build_vignettes` to `TRUE` but it takes a lot longer to install and the 
vignettes are already separately available from the 
[Stan website](https://mc-stan.org/rstanarm/articles/index.html) 
and 
[CRAN](https://cran.r-project.org/package=rstanarm/vignettes). 
If installation fails, please let us know by [filing an issue](https://github.com/stan-dev/rstanarm/issues).

#### Survival Analysis Version

The `feature/survival` branch on GitHub contains a development version of **rstanarm** that includes survival analysis functionality (via the `stan_surv` modelling function). Until this functionality is available in the CRAN release of **rstanarm**, users who wish to use the survival analysis functionality can install a binary version of the survival branch of **rstanarm** from the Stan R packages repository with:

```
install.packages(""rstanarm"", repos = c(""https://mc-stan.org/r-packages/"", getOption(""repos"")))
```

Note that this binary is static (i.e. it is not automatically updated) and is only hosted so that users can access the (experimental) survival analysis functionality without needing to go through the time consuming (and sometimes painful) task of installing the development version of **rstanarm** from source.

### Contributing 

If you are interested in contributing to the development of **rstanarm** please 
see the [developer notes](https://mc-stan.org/rstanarm/dev-notes/index.html) page.
",2022-08-12
https://github.com/stan-dev/rstantools,"# rstantools <img src=""man/figures/stanlogo.png"" align=""right"" width=""120"" />

<!-- badges: start -->
[![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/rstantools?color=blue)](https://cran.r-project.org/web/packages/rstantools)
[![RStudio_CRAN_mirror_downloads_badge](https://cranlogs.r-pkg.org/badges/rstantools?color=blue)](https://cran.r-project.org/web/packages/rstantools)
[![R-CMD-check](https://github.com/stan-dev/rstantools/workflows/R-CMD-check/badge.svg)](https://github.com/stan-dev/rstantools/actions)
<!-- badges: end -->

### Overview 

The __rstantools__ package provides tools for developing R packages interfacing
with [Stan](https://mc-stan.org/). The package vignettes provide guidelines and
recommendations for developers as well as a demonstration of creating a working
R package with a pre-compiled Stan program.
 
* [Guidelines for developers of R Packages interfacing with Stan](https://mc-stan.org/rstantools/articles/developer-guidelines.html)

* [Step by step guide for creating a package that depends on RStan](https://mc-stan.org/rstantools/articles/minimal-rstan-package.html)

### Resources

* [mc-stan.org/rstantools](https://mc-stan.org/rstantools) (online documentation, vignettes)
* [Ask a question](https://discourse.mc-stan.org) (Stan Forums on Discourse)
* [Open an issue](https://github.com/stan-dev/rstantools/issues) (GitHub issues for bug reports, feature requests)

### Installation


* Install from CRAN:

```r
install.packages(""rstantools"")
```

* Install latest development version from GitHub (requires [remotes](https://github.com/r-lib/remotes) package):

```r
if (!require(""remotes"")) {
  install.packages(""remotes"")
}
  
remotes::install_github(""stan-dev/rstantools"")
```

This installation from GitHub will not build the vignettes, but we recommend 
viewing the them online at [mc-stan.org/rstantools/articles](https://mc-stan.org/rstantools/articles/).
",2022-08-12
https://github.com/stan-dev/sgb,"# Stan Governing Body (SGB)

The Stan Governing Body provides project governance structure
in accord with the NumFOCUS requirements and is the liaison between
the project and NumFOCUS for all fiscally sponsored initiatives.

The SGB is intended to be a transparent body in service of the Stan community.
In that spirit, this repository is used to track SGB initiatives.


",2022-08-12
https://github.com/stan-dev/shinystan,"# ShinyStan <img src=""man/figures/stanlogo.png"" align=""right"" width=""120"" />

<!-- badges: start -->
[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/shinystan?color=blue)](http://cran.r-project.org/web/packages/shinystan)
[![RStudio CRAN Mirror Downloads](http://cranlogs.r-pkg.org/badges/grand-total/shinystan?color=blue)](http://cran.rstudio.com/package=shinystan)
[![Codecov](http://codecov.io/gh/stan-dev/shinystan/branch/master/graph/badge.svg)](https://codecov.io/gh/stan-dev/shinystan)
<!-- badges: end -->

ShinyStan provides immediate, informative, customizable visual and
numerical summaries of model parameters and convergence diagnostics for
MCMC simulations. The ShinyStan interface is coded primarily in R using
the [Shiny](http://shiny.rstudio.com) web application framework and is
available via the **shinystan** R package.

### Resources

* [mc-stan.org/shinystan](http://mc-stan.org/shinystan) (Website with online documentation)
* [Ask a question](http://discourse.mc-stan.org) (Stan Forums on Discourse)
* [Open an issue](https://github.com/stan-dev/shinystan/issues) (GitHub issues for bug reports, feature requests)

### Installation

* Install the latest release from CRAN:

```r
install.packages(""shinystan"")
```

* Install the development version from GitHub (requires [devtools](https://github.com/r-lib/devtools) package):

```r
if (!require(""devtools"")) {
  install.packages(""devtools"")
}
devtools::install_github(""stan-dev/shinystan"", build_vignettes = TRUE)
```

### Demo

After installing run

```r
library(""shinystan"")
launch_shinystan_demo()
```

### Screenshots

<img src=https://github.com/stan-dev/shinystan/blob/master/images/home.png width=19% /><img src=https://github.com/stan-dev/shinystan/blob/master/images/explore.png width=24.5% /><img src=https://github.com/stan-dev/shinystan/blob/master/images/diagnose.png width=24.5% />

### About ShinyStan

Applied Bayesian data analysis is primarily implemented through the MCMC
algorithms offered by various software packages. When analyzing a posterior sample
obtained by one of these algorithms the first step is to check for signs that
the chains have converged to the target distribution and also for signs that
the algorithm might require tuning or might be ill-suited for the given model.
There may also be theoretical problems or practical inefficiencies with the
specification of the model.

ShinyStan provides interactive plots and tables helpful for analyzing a
posterior sample, with particular attention to identifying potential problems
with the performance of the MCMC algorithm or the specification of the model.
ShinyStan is powered by RStudio's Shiny web application framework and works with
the output of MCMC programs written in any programming language (and has extended
functionality for models fit using [RStan](http://mc-stan.org/interfaces/rstan.html)
and the No-U-Turn sampler).

#### Saving and deploying (sharing)

The **shinystan** package allows you to store the basic components of an entire
project (code, posterior samples, graphs, tables, notes) in a single object.
Users can save many of the plots as ggplot2 objects for further customization
and easy integration in reports or post-processing for publication.

**shinystan** also provides the `deploy_shinystan` function,
which lets you easily deploy your own ShinyStan apps online using RStudio's
[ShinyApps](https://www.shinyapps.io) service for any of
your models. Each of your apps (each of your models) will have a unique url
and is compatible with Safari, Firefox, Chrome, and most other browsers.


### Licensing

The **shinystan** R package and ShinyStan interface are open source licensed under
the GNU Public License, version 3 (GPLv3).
",2022-08-12
https://github.com/stan-dev/stan,"<a href=""http://mc-stan.org"">
<img src=""https://raw.githubusercontent.com/stan-dev/logos/master/logo.png"" width=200 alt=""Stan Logo""/>
</a>

<b>Stan</b> is a C++ package providing

* full Bayesian inference using the No-U-Turn sampler (NUTS), a variant of Hamiltonian Monte Carlo (HMC),
* approximate Bayesian inference using automatic differentiation variational inference (ADVI), and
* penalized maximum likelihood estimation (MLE) using L-BFGS optimization.

It is built on top of the [Stan Math library](https://github.com/stan-dev/math), which provides

* a full first- and higher-order automatic differentiation library based on C++ template overloads, and
* a supporting fully-templated matrix, linear algebra, and probability special function library.

There are interfaces available in R, Python, MATLAB, Julia, Stata, Mathematica, and for the command line.

[![DOI](https://zenodo.org/badge/19868/stan-dev/stan.svg)](https://zenodo.org/badge/latestdoi/19868/stan-dev/stan)

Home Page
---------
Stan's home page, with links to everything you'll need to use Stan is:

[http://mc-stan.org/](http://mc-stan.org/)

Interfaces
----------
There are separate repositories in the stan-dev GitHub organization for the interfaces, higher-level libraries and lower-level libraries.  

Source Repository
-----------------
Stan's source-code repository is hosted here on GitHub.

Licensing
---------
The Stan math library, core Stan code, and CmdStan are licensed under new BSD. RStan and PyStan are licensed under GPLv3, with other interfaces having other open-source licenses.

Note that the Stan math library depends on the Intel TBB library which is licensed under the Apache 2.0 license. This dependency implies an additional restriction as compared to the new BSD lincense alone. The Apache 2.0 license is incompatible with GPL-2 licensed code if distributed as a unitary binary. You may refer to the Licensing page on the [Stan wiki](https://github.com/stan-dev/stan/wiki/Stan-Licensing).
",2022-08-12
https://github.com/stan-dev/stan-discourse-theme-component,"# Custom Theme Component For Stan Discourse

Extensions to Discourse for https://discourse.mc-stan.org

Currently includes:

- A ""Needs love"" menu item leading to search for unanswered topics
- A banner directing attention of users with trust level 3 or more 
to topics that are unanswered if there are too many of them
- A custom message showing that private messages can (in theory) be read by
admins.",2022-08-12
https://github.com/stan-dev/stan-mode,"# Emacs support for Stan

[![Build Status](https://github.com/stan-dev/stan-mode/workflows/test/badge.svg?branch=master)](https://github.com/stan-dev/stan-mode/actions)
[![License GPL 3](https://img.shields.io/badge/license-GPL_3-blue.svg)](COPYING)

<!-- https://engineering.giphy.com/how-to-make-gifs-with-ffmpeg/ -->
![Example](example.gif)

## NEWS

- 2021-01-30 Version 10.2.1 bugfix for https://github.com/stan-dev/stan-mode/issues/64
- 2020-08-30 Version 10.2.0 update for Stan version 2.24


## Packages and tools included

This repository contains several Emacs packages and tools to make editing [Stan](https://mc-stan.org) files easier. For information on Stan itself, see its [documentation](https://mc-stan.org/users/documentation/) and [example models](https://github.com/stan-dev/example-models).

- [`stan-mode`](https://github.com/stan-dev/stan-mode/tree/master/stan-mode) is a major mode for editing Stan files. Its current features include:
  - syntax highlighting
  - indentation
  - [imenu](http://www.emacswiki.org/emacs/ImenuMode) support for blocks, variables, and user-defined functions.

- [`company-stan`](https://github.com/stan-dev/stan-mode/tree/master/company-stan): Adds a [`company-mode`](https://company-mode.github.io) completion backend.

- [`eldoc-stan`](https://github.com/stan-dev/stan-mode/tree/master/eldoc-stan): Adds Stan support for the [`eldoc`](https://www.emacswiki.org/emacs/ElDoc) minor mode, which show the argument list of the function being written.

- [`flycheck-stan`](https://github.com/stan-dev/stan-mode/tree/master/flycheck-stan) adds a [`flycheck`](https://www.flycheck.org/en/latest/) on-the-fly syntax checker.

- [`stan-snippets`](https://github.com/stan-dev/stan-mode/tree/master/stan-snippets): Adds Stan support for [yasnippet](https://github.com/capitaomorte/yasnippet). Yasnippet is a template system for Emacs. Snippets are defined for blocks, control structures, and *all* the built-in functions and distributions.

- [`ac-stan`](https://github.com/stan-dev/stan-mode/tree/master/ac-stan): Adds a [`auto-complete`](https://github.com/auto-complete/auto-complete) stan dictionary. This is not on MELPA because `auto-complete` itself is semi-deprecated.

- [`indent-stan-files`](https://github.com/stan-dev/stan-mode/tree/master/indent-stan-files): A shell script that uses `stan-mode` to indent a file.

- [`stan-language-definitions`](https://github.com/jrnold/stan-language-definitions): The file [`stan_lang.json`](https://github.com/jrnold/stan-language-definitions/blob/master/stan_lang.json) contains all keywords, functions (with their signatures and documentation) in the Stan modeling language. This is used to generate the keyword lists and snippets used by the modes. It could also be useful for developers designing tools for Stan, e.g. other editor modes.


## Installing

Example configurations are available in the package-specific README.md file under each subfolder (linked above).

### Via package.el

The recommended way to install these packages is using the built-in package manager: `package.el`.
These packages are available from [MELPA](http://melpa.org).
If you're not already using MELPA, follow its installation [instructions](http://melpa.org/#/getting-started).

You can then install the packages using the following commands:

<kbd>M-x package-install [RET] stan-mode [RET]</kbd><br>
<kbd>M-x package-install [RET] company-stan [RET]</kbd><br>
<kbd>M-x package-install [RET] eldoc-stan [RET]</kbd><br>
<kbd>M-x package-install [RET] flycheck-stan [RET]</kbd><br>
<kbd>M-x package-install [RET] stan-snippets [RET]</kbd><br>

If the installation does not work, try refreshing the package list:

<kbd>M-x package-refresh-contents [RET]</kbd>

Or add the following to your `init.el` to ensure installation of these packages:
```lisp
(package-refresh-contents)
(mapc
 (lambda (p)
   (unless (package-installed-p p)
     (package-install p)))
 '(stan-mode
   company-stan
   eldoc-stan
   flycheck-stan
   stan-snippets))
```

### Via Cask

Another way to manage dependencies is to to use [Cask](https://github.com/cask/cask).
See its [docs](http://cask.readthedocs.org/en/latest/guide/introduction.html#emacs-configuration) for an argument as to why to use Cask to manage your configuration.

Simply add the following to your configuration Cask file:
```lisp
(source melpa)
(depends-on ""stan-mode"")
(depends-on ""company-stan"")
(depends-on ""eldoc-stan"")
(depends-on ""flycheck-stan"")
(depends-on ""stan-snippets"")
```
and from the command line in the same directory as the Cask file use `cask` to install the packages,
```console
$ cask install
```
See the Cask [documentation](http://cask.readthedocs.org/en/latest/index.html) for more information.


### Via Github
The package maintainer's current development version is available on Github. This can be cloned as follows. You can also use `--recurse-submodules`, but this will recurse more than one level, which is not necessary for our purpose.

```console
# Clone the develop branch. Change as appropriate.
$ git clone --branch=develop https://github.com/stan-dev/stan-mode.git
$ cd stan-mode
# Clone submodules
$ git submodule update --init -- stan-language-definitions
$ git submodule update --init -- local-melpa
$ git submodule update --init -- rstanarm
# To run automated tests, do the following.
make clean; make all
# If just installing, do the following.
make local-melpa
```

Add the following to your `init.el`. The local development versions will be installed provided their version numbers are greater than the ones in MELPA.

```lisp
;; To make the local package archive visible.
(let ((local-melpa-stan
       ""your-path/stan-mode/local-melpa/packages""))
  (when (file-exists-p local-melpa-stan)
    (add-to-list 'package-archives
                 `(""local-melpa-stan"" . ,local-melpa-stan) t)))
;; Installation
(package-refresh-contents)
(mapc
 (lambda (p)
   (unless (package-installed-p p)
     (package-install p)))
 '(stan-mode
   company-stan
   eldoc-stan
   flycheck-stan
   stan-snippets
   ;; If you use auto-complete, uncomment the line below.
   ;; ac-stan
   ))
```


## Configuration
The recommended mode of configuration is via the `use-package`. One example of configuration is included below. Please the package-specific README files for configuration without use-package.

```lisp
;; Uncomment the line below if not required elsewhere.
;; (require 'use-package)

;;; stan-mode.el
(use-package stan-mode
  :mode (""\\.stan\\'"" . stan-mode)
  :hook (stan-mode . stan-mode-setup)
  ;;
  :config
  ;; The officially recommended offset is 2.
  (setq stan-indentation-offset 2))

;;; company-stan.el
(use-package company-stan
  :hook (stan-mode . company-stan-setup)
  ;;
  :config
  ;; Whether to use fuzzy matching in `company-stan'
  (setq company-stan-fuzzy nil))

;;; eldoc-stan.el
(use-package eldoc-stan
  :hook (stan-mode . eldoc-stan-setup)
  ;;
  :config
  ;; No configuration options as of now.
  )

;;; flycheck-stan.el
(use-package flycheck-stan
  ;; Add a hook to setup `flycheck-stan' upon `stan-mode' entry
  :hook ((stan-mode . flycheck-stan-stanc2-setup)
         (stan-mode . flycheck-stan-stanc3-setup))
  :config
  ;; A string containing the name or the path of the stanc2 executable
  ;; If nil, defaults to `stanc2'
  (setq flycheck-stanc-executable nil)
  ;; A string containing the name or the path of the stanc2 executable
  ;; If nil, defaults to `stanc3'
  (setq flycheck-stanc3-executable nil))

;;; stan-snippets.el
(use-package stan-snippets
  :hook (stan-mode . stan-snippets-initialize)
  ;;
  :config
  ;; No configuration options as of now.
  )

;;; ac-stan.el (Not on MELPA; Need manual installation)
(use-package ac-stan
  :load-path ""path-to-your-directory/ac-stan/""
  ;; Delete the line below if using.
  :disabled t
  :hook (stan-mode . stan-ac-mode-setup)
  ;;
  :config
  ;; No configuration options as of now.
  )
```


## For developers
### Updating packages for a new Stan version

To update stan-mode when a new version of the Stan language comes out:

1. Replace `stan-lang/stan-functions-MAJORVERSION_MINORVERSION.txt` with the newest version generated by `extract_function_sigs.py` in the [stan-dev/docs repo](https://github.com/stan-dev/docs). The version string must exist in the file name and it is the Stan version.
2. Clean and rebuild all generated files. Fix issues in testing and linting as necessary.
``` shell
$ make clean
$ make all
```
3. Save and commit the changes.
4. Bump the version number of the emacs packages. For example, to bump to 10.0.0. This is the emacs stan-mode package version, which is different than the Stan language version.

``` shell
$ ./update-versions.sh 10.0.0
```

5. If the changes are minor, tag the commit and push the tag for MELPA Stable. If making major changes, it is better to create a pull request on Github to document and discuss.

``` shell
$ git tag v10.0.0
$ git push --tags
```

### Emacs lisp good practices
The packages must meet the MELPA standards. See [""Making your package ready for inclusion""](https://github.com/melpa/melpa/blob/master/CONTRIBUTING.org#making-your-package-ready-for-inclusion). The packages ensure these standards through the following automated examinations, which are mandated in `make all`.

- Testing via [buttercup](https://github.com/jorgenschaefer/emacs-buttercup)
- Linting
  - Package metadata via [package-lint](https://github.com/purcell/package-lint)
  - Emacs lisp code via [elisp-lint](https://github.com/gonewest818/elisp-lint)

For testing only, use `make test` at the top-level or in a subfolder. For linting only, use `make lint` at the top-level (both types) or `make lint-package` and `make lint-elisp` in a subfolder.

### Available top-level make commands

The following make commands are available at the top-level `Makefile`. Each package has its own `Makefile` and common included file `common.mk`, allowing us to work on them individually.

| Command        | Cleaning Equivalent | Action                                                                            |
|----------------|---------------------|-----------------------------------------------------------------------------------|
| `all`          | `clean`             | Run `stan-lang` `deps` `build-src` `local-melpa` `compile` `test` `lint`          |
|                |                     |                                                                                   |
| `stan-lang`    | `clean-stan-lang`   | Create the `stan-lang.json` definition file in `stan-language-definitions`.       |
| `deps`         | `clean-deps`        | Run `cask install` for each package to manage dependencies.                       |
| `build-src`    | `clean-src`         | Build data files for each package.                                                |
| `local-melpa`  | `clean-local-melpa` | Create a local archive `local-melpa` to manage package installability in testing. |
| `compile`      | `clean-elc`         | Byte-compile `*.elc` files for each package.                                      |
| `test`         |                     | Run `buttercup.el` tests for each package.                                        |
| `lint`         |                     | Run both linting for each package.                                                |
| `lint-package` |                     | Run `package-lint.el` to examine package metadata.                                |
| `lint-elisp`   |                     | Run `elisp-lint.el` to examine elisp coding practice.                             |
| `dist`         | `clean-dist`        | Run `cask package` to build package files under `dist` for each package.          |
| `show`         |                     | Show elisp files designated for byte-compilation for each package.                |
| `snippets`     | `clean-snippets`    | Move snippet files under `stan-snippets/snippets/` to `snippets`                  |


### Submodules
- [`stan-language-definitions`](https://github.com/jrnold/stan-language-definitions): Provides language definitions. All packages depend on this for definitions.
- [`local-melpa`](https://github.com/kaz-yos/melpa/tree/local-melpa-stan): Provides a local version of MELPA to make the current development version available via the `package.el` interface. Used to avoid installability issue during linting. Also allows package installation checking.
- [`rstanarm`](https://github.com/kaz-yos/rstanarm): Used to provide examples of complicated stan code in indentation and syntax highlighting tests for the `stan-mode` package.


### TODO items
- eldoc-stan: Correctly ignore , in [,]
- flycheck-stan: Handle end of column
- company-stan: Detect context and modify candidates

## License

All packages are free software under the [GPL v3](http://www.gnu.org/licenses/gpl-3.0.html).

<!--  LocalWords:  stan imenu yasnippet flymake MELPA kbd RET init '
 -->
<!--  LocalWords:  mapc EmacsWiki cd 'load 'stan 'flymake Aquamacs
 -->
<!--  LocalWords:  GPL stanc ' 'load 'stan autocomplete setq 'flymake
 -->
<!--  LocalWords:  lang json el emacs CmdStan flycheck 'stan v3
 -->
<!--  LocalWords:  'ac 'flycheck v1
 -->
",2022-08-12
https://github.com/stan-dev/stan2tfp,"# A Stan-to-TensorFlow Probability Compiler, stan2tfp

[![Build Status](https://jenkins.mc-stan.org/buildStatus/icon?job=stan2tfp%2Fmaster&style=flat-square)](https://jenkins.mc-stan.org/job/stan2tfp/job/master/)


This repo houses the experimental/WIP Stan-to-Tensorflow Probability transpiler.
It uses the frontend and middle-end capabilties of
[stanc3](https://github.com/stan-dev/stanc3) by vendoring that repo as a git
submodule.

This project was originally contained in the same repository as stanc3, but was
moved out following the 2.28.1 release of stanc3 (see [this forum
discussion](https://discourse.mc-stan.org/t/moving-stan2tfp-out-of-stanc3s-repo/24902/)).
This allows development of this package to lag behind the cutting edge without
slowing development of the core C++ backend for Stan.

A Python [wrapper package](https://github.com/adamhaber/stan2tfp) is available
on [PyPi](https://pypi.org/project/stan2tfp/).

The structure of this repository also serves as a template for how further
extensions to the stanc3 compiler can proceed. The stanc3 repository
[exposes](https://mc-stan.org/stanc3/stanc/#modules) packages `stanc.frontend`,
`stanc.middle`, `stanc.common`, and `stanc.analysis` which can be used to build
a new backend and new executable. If a backend reaches a good level of parity
with the C++ backend, it can be considered for formal support within stanc3.

### Notice
This backend currently lacks support for many major features of the Stan
language. If the goal is simply to use Stan from within a Python environment,
[PyStan](https://pystan.readthedocs.io/) and
[CmdStanPy](https://cmdstanpy.readthedocs.io/) both provide Python wrappings to
the C++ backend for Stan.
",2022-08-12
https://github.com/stan-dev/stanc3,"# A New Stan-to-C++ Compiler, stanc3
This repo contains a new compiler for Stan, stanc3, written in OCaml.
Since version 2.26, this has been the default compiler for Stan. See [this wiki](https://github.com/stan-dev/stanc3/wiki/changes-from-stanc2) for a list of minor differences between this compiler and the previous Stan compiler.

To read more about why we built this, see this [introductory blog post](https://statmodeling.stat.columbia.edu/2019/03/13/stanc3-rewriting-the-stan-compiler/). For some discussion as to how we chose OCaml, see [this accidental flamewar](https://discourse.mc-stan.org/t/choosing-the-new-stan-compilers-implementation-language/6203).
We're testing [these models](https://jenkins.flatironinstitute.org/job/Stan/job/Stanc3/job/master/) (listed under Test Results) on every pull request.

[![Build Status](https://jenkins.flatironinstitute.org/job/Stan/job/Stanc3/job/master/badge/icon?style=flat-square)](https://jenkins.flatironinstitute.org/job/Stan/job/Stanc3/job/master/)

## Documentation

Documentation for users of stanc3 is in the Stan Users' Guide [here](https://mc-stan.org/docs/stan-users-guide/using-the-stan-compiler.html)

The Stanc3 Developer documentation is available here: https://mc-stan.org/stanc3/stanc

Want to contribute? See [Getting Started](https://mc-stan.org/stanc3/stanc/getting_started.html)
for setup instructions and some useful commands.

## High-level concepts, invariants, and 30,000-ft view
Stanc3 has 4 main src packages: `frontend`, `middle`, `analysis_and_optimization` and `stan_math_backend`.

```mermaid
flowchart
    Stanc --> Frontend & Analysis & Backend <-.-> Middle
```

The goal is to keep as many details about the way Stan is implemented by the core C++ implementation in the Stan Math backend library as possible.
The Middle library contains the MIR and currently any types or functions used by the two ends.
The entrypoint for the compiler is in `src/stanc/stanc.ml` which sequences the various components together.

### Distinct stanc Phases

The phases of stanc are summarized in the following information flowchart and list.
```mermaid
flowchart TB

    subgraph frontend[Frontend]
        direction TB
        infile>Source file]
        lexer(frontend/lexer.mll)
        parser(frontend/parser.mly)
        typecheck(frontend/Typechecker.ml)
        lower(frontend/Ast_to_Mir.ml)

        infile --> lexer -->|Tokens| parser
        parser -->|Untyped AST| typecheck -->|Typed AST| lower
    end


    subgraph middle[Middle Representation]
        data{{MIR Data Structures}}
    end

    subgraph analysis[Static Analysis and Optimization]
        optimize(analysis_and_optimization/Optimize.ml)
    end

    subgraph backend[Backend]
        codegen(*_backend/*_code_gen.ml)
        transform(*_backend/Transform_Mir.ml)

        transform -.->|MIR with backend specific code| optimize
        transform --> codegen
        optimize -->|Optimized MIR| codegen
    end

    outfile>Output File, e.g. a .hpp]

    middle --- analysis
    frontend ==> middle =====> backend ==> outfile


    click lexer ""https://github.com/stan-dev/stanc3/blob/master/src/frontend/lexer.mll""
    click parser ""https://github.com/stan-dev/stanc3/blob/master/src/frontend/parser.mly""
    click typecheck ""https://github.com/stan-dev/stanc3/blob/master/src/frontend/Typechecker.ml""
    click lower ""https://github.com/stan-dev/stanc3/blob/master/src/frontend/Ast_to_Mir.ml""
    click optimize ""https://github.com/stan-dev/stanc3/blob/master/src/analysis_and_optimization/Optimize.ml""
    click data ""https://github.com/stan-dev/stanc3/tree/master/src/middle""
    click codegen ""https://github.com/stan-dev/stanc3/blob/master/src/stan_math_backend/Stan_math_code_gen.ml""
    click transform ""https://github.com/stan-dev/stanc3/blob/master/src/stan_math_backend/Transform_Mir.ml""
```

1. [Lex](src/frontend/lexer.mll) the Stan language into tokens.
1. [Parse](src/frontend/parser.mly) Stan language into AST that represents the syntax quite closely and aides in development of pretty-printers and linters. `stanc --debug-ast` to print this out.
1. Typecheck & add type information [Typechecker.ml](src/frontend/Typechecker.ml).  `stanc --debug-decorated-ast`
1. [Lower](src/frontend/Ast_to_Mir.ml) into [Middle Intermediate Representation](src/middle/Program.ml) (AST -> MIR) `stanc --debug-mir` (or `--debug-mir-pretty`)
1. Backend-specific MIR transform  (MIR -> MIR) [Transform_Mir.ml](src/stan_math_backend/Transform_Mir.ml)  `stanc --debug-transformed-mir`
1. Analyze & optimize (MIR -> MIR)
1. Code generation  (MIR -> [C++](src/stan_math_backend/Stan_math_code_gen.ml)) (or other outputs, like [Tensorflow](https://github.com/stan-dev/stan2tfp/)).

### The two central data structures

1. `src/frontend/Ast.ml` defines the AST. The AST is intended to have a direct 1-1 mapping with the syntax, so there are things like parentheses being kept around.
The pretty-printer in the frontend uses the AST and attempts to keep user syntax the same while just adjusting whitespace.

    The AST uses a particular functional programming trick to add metadata to the AST (and its other tree types), sometimes called [the ""two-level types"" pattern](http://lambda-the-ultimate.org/node/4170#comment-63836). Essentially, many of the tree variant types are parameterized by something that ends up being a placeholder not for just metadata but for the recursive type including metadata, sometimes called the fixed point. So instead of recursively referencing `expression` you would instead reference type parameter `'e`, which will later be filled in with something like `type expr_with_meta = metadata expression`.

    The AST intends to keep very close to Stan-level semantics and syntax in every way.

2. `src/middle/Program.ml` contains the MIR (Middle Intermediate Language - we're saving room at the bottom for later). `src/frontend/Ast_to_Mir.ml` performs the lowering and attempts to strip out as much Stan-specific semantics and syntax as possible, though this is still something of a work-in-progress.

    The MIR uses the same two-level types idea to add metadata, notably expression types and autodiff levels as well as locations on many things. The MIR is used as the output data type from the frontend and the input for dataflow analysis, optimization (which also outputs MIR), and code generation.

## Design goals
* **Multiple phases** - each with human-readable intermediate representations for easy debugging and optimization design.
* **Optimizing** - takes advantage of info known at the Stan language level. Minimize information we must teach users for them to write performant code.
* **Holistic** - bring as much of the code as possible into the MIR for whole-program optimization.
* **Research platform** - enable a new class of optimizations based on probability theory.
* **Modular** - architect & build in a way that makes it easy to outsource things like symbolic differentiation to external libraries and to use parts of the compiler as the basis for other tools built around the Stan language.
* **Simplicity first** - When making a choice between correct simplicity and a perceived performance benefit, we want to make the choice for simplicity unless we can show significant (> 5%) benchmark improvements to compile times or run times. Premature optimization is the root of all evil.
",2022-08-12
https://github.com/stan-dev/stancon_talks,"<a href=""http://mc-stan.org"">
<img src=""https://raw.githubusercontent.com/stan-dev/logos/master/logo.png"" width=150 alt=""Stan Logo""/>
</a>

# Materials from StanCon

This repository contains materials from StanCon and links to StanCon content hosted elsewhere. Unless otherwise noted, the text in this repository is distributed under the [CC BY 4.0 License](https://creativecommons.org/licenses/by/4.0/legalcode) and code is distributed under the [New BSD License](https://opensource.org/licenses/BSD-3-Clause). Copyright to the authors.

### Contents:

**2020 Virtual Conference**

* [Videos](https://www.youtube.com/playlist?list=PLCrWEzJgSUqzI3goQEAKkDsHg72inmqbe)

**2019 Cambridge (UK)**

* [Abstracts](https://mc-stan.org/events/stancon2019Cambridge/abstracts.html)
* [Videos](https://mc-stan.org/events/stancon2019Cambridge/#recorded-talks)

**2018 Helsinki**

* [Contributed talks](#2018-helsinki-peer-reviewed-contributed-talks)
* [Invited talks](#2018-helsinki-invited-talks) 
* [Tutorials](#2018-helsinki-tutorials)

**2018 Asilomar**

* [Contributed talks](#2018-peer-reviewed-contributed-talks)
* [Invited talks](#2018-invited-talks) 

**2017 NYC**

* [Contributed talks](#2017-peer-reviewed-contributed-talks) 

<br>
  
## StanCon 2018 | August 29-31, Aalto University, Helsinki

StanCon’s version of conference proceedings is a collection of contributed talks based on interactive notebooks. Every submission is peer reviewed by at least two reviewers. The reviewers are members of the [Stan Conference Organizing Committee](https://mc-stan.org/events/stancon2018Helsinki/#committee) and the [Stan Developmemt Team](https://mc-stan.org/about/team/). This repository contains all of the accepted notebooks as well as any supplementary materials required for building the notebooks. The slides presented at the conference are also included.

### 2018 Helsinki Peer reviewed contributed talks


**_Solving ODEs in the wild: Scalable pharmacometrics with Stan_** 

* Authors: Sebastian Weber (Novartis)

Pharmacometric modeling involves nonlinear hierarchical models, which
are most naturally expressed as forced ordinary differential equations
(ODEs). These class of models lead to a number of challenges which
complicate a practical modeling work-flow in Stan mostly due to long
model execution times. This contribution demonstrates at the example
of the drug Warfarin how forced ODEs can be written efficiently in
Stan leading to a doubling in model evaluation speed for the presented
example. Finally, it is demonstrated how the new `map_rect` facility
in Stan can be used to make models scalable to large data sets leading
to substantial speedups in model evaluation time and most importantly
this enables to *scale* Stan's performance as needed.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465996.svg)](https://doi.org/10.5281/zenodo.1465996)

Links: 

* [Video](https://www.youtube.com/watch?v=wcpjZC9AV84&t=1h44m33s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/weber/stancon18-master) 

<br>

**_Analysis of repeated measures data in RStan_** 

* Authors: Marco Munda (Pharmalex)

We illustrate the analysis of repeated measures data in the Bayesian framework using RStan.
In addition to the modelling itself, we further show how to make inference on the primary effect based on a probability of success, and how to predict the longitudinal profile of a future patient, two difficult (if not impossible) tasks from a frequentist perspective.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465988.svg)](https://doi.org/10.5281/zenodo.1465988)

Links: 

* Video not available
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/weber/stancon18-master) 

<br>

**_Define custom response distributions with brms_** 

* Authors: Paul Bürkner (University of Münster)

The **brms** package (Bürkner, 2017a, 2017b) implements Bayesian regression models using the probabilistic programming language **Stan** (Carpenter et al., 2016) behind the scenes. It has grown to be one of the most flexible R packages when it comes to multilevel regression modelling. A wide range of response distributions are supported, allowing users to fit -- among others -- linear, robust linear, count data, survival, response times, ordinal, zero-inflated, hurdle, and even self-defined mixture models all in a multilevel context. Predictor terms can be specified with a simple yet powerful formula syntax. Thanks to **Stan**, even very complex models tend to converge well in a reasonable amount of time. While **brms** comes with a lot of built-in response distributions (usually called *families* in R), there is still a long list of distributions which are not natively supported. The present notebook will explain how to specify such *custom families* in **brms**. By doing that, users can benefit from the modeling flexibility and post-processing options of **brms** even when applying self-defined response distributions.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465958.svg)](https://doi.org/10.5281/zenodo.1465958)

Links: 

* [Video](https://www.youtube.com/watch?v=FoaxA7sJi7w&t=2h13m03s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/buerkner/buerkner_notebook.Rmd) 

<br>


**_Are shots predictive of soccer results?_** 

* Authors: Leonardo Egidi, Francesco Pauli, Nicola Torelli (University of Trieste)

Predicting the outcome of a soccer match is the subject of much debate, and several models based on different assumptions 
have been proposed for modeling the numbers of goals scored by two competing teams. In this case study we adopt a different perspective and propose a Bayesian hierarchical model consisting of three nested multiple outcomes: 
**number of scores**, **number of shots on target** and **number of total shots**. We model the number of scores and the number of shots on target with  <span style=""color:red"">two binomial 
distributions</span> respectively, whereas the total shots follow a negative binomial distributon. Our dataset consists of nine seasons of the English Premier League (EPL): eight seasons---from 2008/2009 to 2015/2016, 3040 matches---represent the train set, whereas the nineth season, 2016/2017 (with the remaining 380 matches), is our test set, used for out-of sample prediction. 

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465963.svg)](https://doi.org/10.5281/zenodo.1465963)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=4h15m39s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/egidi/egidi) 

<br>

**_Flexible models of holiday lift_** 

* Authors: Alex Braylan, Dan Marthaler (Revionics)

We develop a flexible, portable, and interpretable Bayesian model of cyclical holiday effects on time series. Our model uses five parameters for each possible holiday that capture the general shape, magnitude, and peak location offset of each holiday effect. Choice of priors prevents the model from overfitting while still achieving considerable flexibility. We experiment on simulated and real data from Google Trends and demonstrate the model's performance on held-out data.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465943.svg)](https://doi.org/10.5281/zenodo.1465943)

Links: 

* [Video](https://www.youtube.com/watch?v=FoaxA7sJi7w&t=18m15s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/braylan) 

<br>

**_Artificial turf advantage and predictive accuracy in Dutch football_** 

* Authors: Gertjan Verhoeven 

This submission uses Stan to learn about the so-called artificial turf advantage in Dutch football. Two model variants are used to model match outcomes. One of the models is the model from Milad Kharratzadeh presented at Stancon 2017, the other is new to Stan (a dynamic Skellam model). I use out-of-sample forecasts together with the Ranked Probability Score (a proper scoring rule) to learn whether including the artificial turf advantage increases predictive accuracy. Bookmakers odds are used as a benchmark to check the quality of our forecasts.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465994.svg)](https://doi.org/10.5281/zenodo.1465994)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=4h40m34s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/verhoeven/artificial_turf_predictive-master) 

<br>

**_ODE model of gene regulation_** 

* Authors: Martin Modrák (Czech Academy of Sciences)

In this notebook we fit time series of gene expression data with a 
non-linear ODE-based model. Splines are used to model noisily observed 
regulator expression. The ODE is not solved explicitly, it is instead 
transformed to a definite integral and integrated via the trapezoid 
rule. Some interesting reparametrizations are introduced to make the 
model well identified.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465978.svg)](https://doi.org/10.5281/zenodo.1465978)

Links: 

* [Video](https://www.youtube.com/watch?v=wcpjZC9AV84&t=1h24m10s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/modrak/genexpi-stan) 

<br>

**_Predicting New York City school enrollment_** 

* Authors: Jonathan Auerbach, Timothy Jones, Robin Winstanley (Columbia University)

We propose a Bayesian hierarchical Age-Period-Cohort model to predict elementary 
school enrollment in New York City. We demonstrate this model using student 
enrollment data for grades K-5 in each Census Tract of Brooklyn's 20th School 
District over the 2001-02 to 2010-11 school years. Specifically, our model 
disaggregates enrollment into grade (age), year (period), and cohort effects so 
that each can be interpreted and extrapolated over the 2011-12 to 2017-18 school 
years. We find this approach ideal for incorporating spatial information 
indicative of the socioeconomic forces that determine school enrollment in New 
York City.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465936.svg)](https://doi.org/10.5281/zenodo.1465936)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=4h02m50s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/auerbach_jones_winstanley/auerbach_jones_winstanley_notebook.Rmd) 

<br>

**_Analyzing brain taxonomy trees_** 

* Authors: Chris Hammill, Jason Lerch

This talk and notebook introduce the idea of analyzing brain anatomy as a taxonomy of structures, defined by containment, using Stan. This taxonomy imposes dependence between model coefficients for structures and their enclosing structure. The slides and notebook compare this approach to other hierarchical modelling strategies using the mouse brain for illustration.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465967.svg)](https://doi.org/10.5281/zenodo.1465967)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=2h18m48s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/hammill) 

<br>


**_Getting more out of Stan: some ideas from the Haskell bindings_** 

* Authors: Thomas A. Nielsen (Tweag I/O), Dominic Steinitz (Tweag I/O), Henrik Nilsson (University of Nottingham)

We present draft bindings to Stan in Haskell, a purely functional programming language. Unlike in most bindings, our models are encoded as a data type the host language. We show how this can be used to widen the range of computations that can be done based on the Stan model definition. For instance, predictions, posterior predictive checks and residual calculations can be done based on a single model definition.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465992.svg)](https://doi.org/10.5281/zenodo.1465992)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=5h05m28s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/nielsen) 

<br>

**_Dose-finding clinical trial designs in Stan with trialr_** 

* Authors: Kristian Brock (University of Birmingham)

My notebook illustrates two different methods for conducting dose-finding clinical trials.
The first, CRM, escalates dose according to toxicity outcomes only, assuming implicitly that higher doses are more likely to benefit the patient.
The second, EffTox, escalates dose according to efficacy and toxicity outcomes, thus addressing the potential that higher may not always mean better.
These models are implemented in the trialr R-package using Stan.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465952.svg)](https://doi.org/10.5281/zenodo.1465952)

Links: 

* [Video](https://www.youtube.com/watch?v=FoaxA7sJi7w&t=2h32m49s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/brock) 

<br>

**_""The implementation of a model of choice: the (truncated) linear ballistic accumulator._** 

* Authors: Bruno Nicenboim (University of Potsdam)

It is very common in cognitive science and psychology to use experimental tasks that involve making a fast choice among a restricted number of alternatives.  In this notebook, I focus on one influential and relatively simple model that belongs to the class of sequential-sampling models: the linear ballistic accumulator with a drift rate drawn from a normal distribution (restricted to positive values) (S. D. Brown and Heathcote 2008; Heathcote and Love 2012). First, I discuss the motivation for fitting this model using the Stroop task (Stroop 1935) as a case study. Then, I discuss the challenges in the implementation of the model in (R)Stan (Stan Development Team 2017), which might also apply to other hierarchical models with complex likelihood functions. Finally, I show some results that exemplify how the linear ballistic accumulator can be used for examining individual differences.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1465990.svg)](https://doi.org/10.5281/zenodo.1465990)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=1h46m05s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/nicenboim) 

<br>

**_Hierarchical Ornstein-Uhlenbeck type t-processes_** 

* Authors: Ville Laitinen, Leo Lahti (University of Turku)

This work investigates probabilistic time series models that are motivated by applications in statistical ecology. In particular, we investigate variants of the mean-reverting and stochastic Ornstein-Uhlenbeck (OU) process. We provide a hierarchical extension for joint analysis of multiple (short) time series, validate the model, and analyze its performance with simulations. The works extends the recent Stan implementation of the OU process (A 2018), where parameter estimates of a Student-t type OU process are obtained based on a single (long) time series. We have added a level of hierarchy, which allows joint inference of the model parameters across multiple time series.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1471578.svg)](https://doi.org/10.5281/zenodo.1471578)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=2h05m53s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/laitinen) 

<br>

**_GPU optimized math routines in the Stan math library_** 

* Rok &#268;e&#353;novar, Davor Sluga, Jure Dem&#353;ar, Steve Bronder, Erik &#352;trumbelj

The Stan Math library's Hamilton Monte Carlo (HMC) sampler has computationally expensive draws while usually searching the target distribution more efficiently than alternative MCMC methods with fewer iterations. The bottleneck within draws makes Stan a prime candidate for GPU optimizations within samples. This project implements GPU optimizations for the Cholesky decomposition and it's derivative in the Stan Math library [@stanmath2015]. This work is the first known open source implementation of the Cholesky decomposition with a GPU in an HMC setting. Furthermore, the GPU kernels use OpenCL which allows the use of these methods across any brand of GPU. While results show that GPU optimizations are not optimal for small $N\times M$ matrices, large matrices can see speedups of 7.8x while retaining the same precision as models run purely on a CPU.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1471578.svg)](https://doi.org/10.5281/zenodo.1471578)

Links: 

* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=5h46m00s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/Bronder) 

<br>
 
**_Relating Disparate Measures of Coagulapathy Using Unorthodox Data: A Hybrid
  Mechanistic-Statistical Approach_** 

* Authors: Arya A. Pourzanjani, Tie Bo Wu, Benjamin B. Bales, Linda R. Petzold (University
  of California, Santa Barbara)
  
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1489142.svg)](https://doi.org/10.5281/zenodo.1489142)

Links: 

* [Video](https://www.youtube.com/watch?v=wcpjZC9AV84&t=1h02m58s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/arya) 

 
<br>
 
**_Modeling the Effects of Nutrition with Mixed-Effect Bayesian Network_** 

* Authors: Jari Turkia (University of Eastern Finland)

This work proposes Mixed-Effect Bayesian Network (MEBN) as a method for modeling the effects of nutrition. It allows identifying both typical and personal correlations between nutrients and their bodily responses. Predicting a personal network of nutritional reactions would allow interesting applications at personal diets and in understanding this complex system. Brief theory of MEBN is first given, followed by the implementation in R and Stan. A real life dataset from a nutritional study (Sysdimet) is then analyzed with this method and the results are visualized with a responsive JavaScript-visualization.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1489146.svg)](https://doi.org/10.5281/zenodo.1489146)

Links: 

* [Video](https://www.youtube.com/watch?v=FoaxA7sJi7w&t=39m11s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/Turkia) 

<br>
 
**_Using counterfactual queries to improve models for decision-support_** 

* Authors: Sundin, Iiris(1) Peter Schulam(2) Eero Siivola(1) Aki Vehtari(1) Suchi Saria(2) Samuel Kaski(1)
1 Department of Computer Science, Aalto University, Espoo, Finland
2 Department of Computer Science, Johns Hopkins University

In this extended abstract, we generalize active learning to tasks where a human has to choose which action a to take for a target after observing its covariates x ̃ and predicted outcomes p( ̃y|x, a  ̃ ).
An example case is personalized medicine and the decision of which treatment to give to a pa-
tient. We show that standard active learning, which is not aware of the final task, would be
very inefficient, and we introduce a new problem of decision-making-aware active learning. We for-
mulate the problem as finding the query with the highest information gain for the specific decision-
making task, assuming a rational decision-maker. The problem can be solved particularly efficiently
assuming an expert able to answer queries about counterfactuals. We demonstrate the effective-
ness of the proposed method in a binary outcome decision-making task using simulated data, and in a continuous-valued outcome task on the medical dataset IHDP with synthetic treatment outcomes. The outcomes are predicted using Gaussian processes.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1489150.svg)](https://doi.org/10.5281/zenodo.1489150)

Links: 

* [Video](https://www.youtube.com/watch?v=FoaxA7sJi7w&t=0m00s)
* [Notebook, code, slides](2018-helsinki/Contributed-Talks/Sundin.pdf) 


### 2018 Helsinki Invited talks 

**_Hierarchical modelling of galaxy clusters for cosmology_**
    
* Presenter: Maggie Lieu (European Space Agency)
* [Video](https://youtu.be/wcpjZC9AV84?t=890)
* [Slides](2018-helsinki/Invited-Talks/lieu.pdf) 


**_Bad data, big models & statistical methods for studying evolution_**
    
* Presenter: Richard McElreath (Max Planck Institute for Evolutionary Anthropology)
* [Video](https://youtu.be/FoaxA7sJi7w?t=3581)
* [Slides](2018-helsinki/Invited-Talks/mcelreath.pdf) 

**_Identifying the effect of public holidays on daily demand for gas_**
    
* Presenter: Sarah Heaps (Newcastle University)
* [Video](https://www.youtube.com/watch?v=pKZLJPrZLhU&t=585s)
* [Slides](2018-helsinki/Invited-Talks/heaps.pdf) 


**_Esther Williams in the Harold Holt Memorial Swimming Pool_**
    
* Presenter: Daniel Simpson (University of Toronto)
* [Video](https://youtu.be/pKZLJPrZLhU?t=26305)
* [Slides](2018-helsinki/Invited-Talks/simpson.pdf) 

### 2018 Helsinki tutorials

**_Basics of Bayesian inference and Stan_**

* Instructors: Jonah Gabry & Lauren Kennedy
* [Video part 1](https://www.youtube.com/watch?v=ZRpo41l02KQ&t=18s&index=6&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)
* [Video part 2](https://www.youtube.com/watch?v=6cc4N1vT8pk&t=2s&index=7&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)
* [Slides and code](https://github.com/jgabry/stancon2018helsinki_intro)

**_Hierarchical models_**

* Instructor: Ben Goodrich 
* [Video part 1](https://www.youtube.com/watch?v=DzaQiJG3RpA&t=4s&index=8&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)
* [Video part 2](https://www.youtube.com/watch?v=DPnLb5EaCkA&t=0s&index=9&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)
* [Slides](http://mc-stan.org/workshops/stancon2018_hierarchical)

**_Stan C++ development: adding a new function to Stan_**

* Instructors: Bob Carpenter, Sean Talts, and Mitzi Morris. 
* [Video part 1](https://www.youtube.com/watch?v=tIzxfFxCb2Y&t=4s&index=13&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)
* [Video part 2](https://www.youtube.com/watch?v=pHatMUlK6nk&t=1s&index=5&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)

**_Ordinary differential equation (ODE) models in Stan_**

* Instructor: Daniel Lee 
* [Video](https://www.youtube.com/watch?v=hJ34_xJhYeY&t=217s&index=11&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)

**_Productization of Stan_**

Instructor: Eric Novik 
Productization panel discussion: Markus Ojala (Smartly), Tom Nielsen (Tweag.io), Anna Kircher (Lendable), Eric Novik (Generable)
* [Video](https://www.youtube.com/watch?v=4vfilYZ-F3A&t=54s&index=12&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)

**_Model assessment and selection_**

* Instructor: Aki Vehtari 
* [Video](https://www.youtube.com/watch?v=hpr8pxqkCH8&t=1329s&index=10&list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J)
* [Slides and demos](https://avehtari.github.io/modelselection/)

<br>
  
## StanCon 2018 | January 10-12, Asilomar, California  

StanCon’s version of conference proceedings is a collection of contributed talks based on interactive notebooks. Every submission is peer reviewed by at least two reviewers. The reviewers are members of the Stan Conference Organizing Committee and the [Stan Developmemt Team](https://mc-stan.org/about/team/). This repository contains all of the accepted notebooks as well as any supplementary materials required for building the notebooks. The slides presented at the conference are also included.

### 2018 Peer reviewed contributed talks

**_Does the New York City Police Department rely on quotas?_** 

* Authors: Jonathan Auerbach (Columbia University)

This submission investigates whether the New York City Police Department (NYPD) uses productivity targets or quotas to manage officers in contravention of New York State Law. The analysis is presented in three parts. First, the NYPD's employee evaluation system is introduced, and the criticism that it constitutes a quota is summarized. Secondly, a publically available dataset of traffic tickets issued by NYPD officers in 2014 and 2015 is described. Finally, a generative model to describe how officers write traffic tickets is proposed. The fitted model is consistent with the criticism that police officers substantially alter their ticket writing to coincide with departmental targets. The submission concludes by discussing the implication of these findings and offering directions for further research.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284317.svg)](https://doi.org/10.5281/zenodo.1284317)

Links: 

  - [Video](https://youtu.be/5qojKAiirqI)
  - [Notebook, code, slides](2018/Contributed-Talks/01_auerbach) 
  - <a href=""https://github.com/jauerbach""> github.com/jauerbach</a>

  
<br>  

**_Diagnosing Alzheimer’s the Bayesian way_** 

* Authors: Arya A. Pourzanjani, Benjamin B. Bales, Linda R. Petzold, Michael Harrington (UC Santa Barbara)

Alzheimer's Disease is one the most debilitating diseases, but how do we diagnose it accurately? Researchers have been trying to answer this question by building generative models to describe how patient biomarkers, such as MRI scans, psychological tests, and lab tests relate over time to the underlying brain deterioration that's present in Alzheimer's Disease. In this notebook we show how we translated these models to the Bayesian framework in Stan and how this allowed for several model improvements that can ultimately improve our understanding of Alzheimer's and help physicians in diagnosis. In particular, we describe how we hierarchically model patient disease trajectories to obtain stable estimates for patients who lack data. We describe how fitting in Stan yields uncertainties on these disease trajectories, and why that is important for weighing the pros and cons of risky treatment. Lastly, we describe a new method for Bayesian modeling of these monotonic disease trajectories in Stan using I-Splines.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284330.svg)](https://doi.org/10.5281/zenodo.1284330)

Links: 

  - [Video](https://youtu.be/j_JIfNiO9TA)
  - [Notebook, code, slides](2018/Contributed-Talks/02_pourzanjani)
  - [https://github.com/pourzanj/Stancon2018\_Alzheimers](https://github.com/pourzanj/Stancon2018_Alzheimers)
  - <a href=""https://aryastats.com/""> aryastats.com</a>

<br>  

**_Joint longitudinal and time-to-event models via Stan_** 

* Authors: Sam Brilleman, Michael Crowther, Margarita Moreno-Betancur, Jacqueline Buros Novik, Rory Wolfe (Monash University, Columbia University)

The joint modelling of longitudinal and time-to-event data has received much attention in the biostatistical literature in recent years. In this notebook (and talk), we describe the implementation of a shared parameter joint model for longitudinal and time-to-event data in Stan. The methods described in the
notebook are a simplified version of those underpinning the `stan_jm` modeling function that has recently been contributed to the [**rstanarm**](http://mc-stan.org/rstanarm) R package.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284334.svg)](https://doi.org/10.5281/zenodo.1284334)

Links: 

  - [Video](https://youtu.be/8r-Ipt885FA)
  - [Notebook, code, slides](2018/Contributed-Talks/03_brilleman) 
  - [github.com/sambrilleman/2018-StanCon-Notebook](https://github.com/sambrilleman/2018-StanCon-Notebook)
  - <a href=""http://www.sambrilleman.com/""> sambrilleman.com</a>


<br>

**_A tutorial on Hidden Markov Models using Stan_** 

* Authors: Luis Damiano, Brian Peterson, Michael Weylandt 

We implement a standard Hidden Markov Model (HMM) and the Input-Output Hidden Markov Model for unsupervised learning of time series dynamics in Stan. We begin by reviewing three commonly-used algorithms for inference and parameter estimation, as well as a number of computational techniques and modeling strategies that make full Bayesian inference practical. For both models, we demonstrate the effectiveness of our proposed approach in simulations. Finally, we give an example of embedding a HMM within a larger model using an example from the econometrics literature.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284341.svg)](https://doi.org/10.5281/zenodo.1284341)

Links: 

  - [Video](https://youtu.be/oe9PAEI97oI)
  - [Notebook, code, slides](2018/Contributed-Talks/04_damiano) 
  - <a href=""https://github.com/luisdamiano/stancon18""> github.com/luisdamiano/stancon18</a>

  
<br>

**_Student Ornstein-Uhlenbeck models served three ways (with applications for population dynamics data)_** 

* Authors: Aaron Goodman (Stanford University)

Ornstein-Uhlenbeck (OU) processes are a mean reverting process and is used to model dynamics in biology, physics, and finance. I fit an extension of the OU process that is driven by a Lévy process with Student's t-marginals rather than Brownian motion with Gaussian marginals, which allows for heavy-tailed increments. I implement four formulations of the Student-t OU-type model in Stan and compare the sampling performance on both real and simulated population dynamic data. 

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284346.svg)](https://doi.org/10.5281/zenodo.1284346)

Links: 

  - Video (coming soon)
  - [Notebook, code, slides](2018/Contributed-Talks/05_goodman) 
  - [github.com/aaronjg/outype\_t\_process\_stan](https://github.com/aaronjg/outype_t_process_stan)
  - <a href=""https://web.stanford.edu/~aaronjg/""> web.stanford.edu/~aaronjg</a>

  
<br>  

**_SlicStan: a blockless Stan-like language_** 

* Authors: Maria I. Gorinova, Andrew D. Gordon, Charles Sutton (University of Edinburgh) 

We present SlicStan — a probabilistic programming language that compiles to Stan and uses static analysis techniques to allow for more abstract and flexible models. SlicStan is novel in two ways: (1) it allows variable declarations and statements to be automatically shredded into different components needed for efficient Hamiltonian Monte Carlo inference, and (2) it introduces more flexible user-defined functions that allow for new model parameters to be declared as local variables. This work demonstrates that efficient automatic inference can be the result of the machine learning and programming languages communities joint efforts.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284348.svg)](https://doi.org/10.5281/zenodo.1284348)

Links: 

  - [Video](https://youtu.be/WTqnehdFNbo)
  - [Notebook, code, slides](2018/Contributed-Talks/06_gorinova) 
  - [github.com/mgorinova/SlicStan-Paper](https://github.com/mgorinova/SlicStan-Paper)
  - <a href=""http://homepages.inf.ed.ac.uk/s1207807/""> homepages.inf.ed.ac.uk/s1207807</a>, <a href=""https://www.microsoft.com/en-us/research/people/adg/""> microsoft.com/en-us/research/people/adg</a>, <a href=""http://homepages.inf.ed.ac.uk/csutton/""> http://homepages.inf.ed.ac.uk/csutton</a> 


<br>

**_idealstan: an R package for ideal point modeling with Stan_** 

* Authors: Robert Kubinec (University of Virginia)

Item-response theory (IRT) ideal-point scaling/dimension reduction methods that incorporate additional response categories and missing/censored values, including absences and abstentions, for roll call voting data (or any other kind of binary or ordinal item-response theory data). Full and approximate Bayesian inference is done via Stan.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284361.svg)](https://doi.org/10.5281/zenodo.1284361)

Links: 

  - [Video](https://youtu.be/0ZjrLOosXwk)
  - [Notebook, code, slides](2018/Contributed-Talks/07_kubinec) 
  - [https://CRAN.R-project.org/package=idealstan](https://CRAN.R-project.org/package=idealstan)


<br>  

**_Computing steady states with Stan’s nonlinear algebraic solver_** 

* Authors: Charles C. Margossian (Metrum, Columbia University)

Stan’s numerical algebraic solver can be used to solve systems of nonlinear algebraic equations with no closed form solutions. One of its key applications in scientific and engineering fields is the computation of equilibrium states (equivalently steady states). This case study illustrates the use of the algebraic solver by applying it to a problem in pharmacometrics. In particular, I show the algebraic system we solve can be quite complex and embed, for instance, numerical solutions to ordinary differential equations. The code in R and Stan are provided, and a Bayesian model is fitted to simulated data. 

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284375.svg)](https://doi.org/10.5281/zenodo.1284375)

Links: 

  - [Video](https://youtu.be/JhwZIX5ryw0) 
  - [Notebook, code, slides](2018/Contributed-Talks/08_margossian) 
  - [github.com/charlesm93](https://github.com/charlesm93)


<br>

**_Bayesian estimation of mechanical elastic constants_** 

* Authors: Ben Bales, Brent Goodlet, Tresa Pollock, Linda Petzold (UC Santa Barbara)

This outlines a Bayesian approach to resonance ultrasound spectroscopy (RUS), a technique for estimating elastic constants of a material from a sample's measured resonance modes. The notebook includes an example of how to take advantage of custom automatic differentiation in specialized Stan models (either for numerical or efficiency reasons).

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1285265.svg)](https://doi.org/10.5281/zenodo.1285265)

Links: 

  - [Video](https://youtu.be/vOoZBTpN8n4)
  - [Notebook, code, slides](2018/Contributed-Talks/09_bales) 
  - [github.com/bbbales2/stancon_2018](https://github.com/bbbales2/stancon_2018)


<br>

**_Aggregate random coefficients logit — a generative approach_** 

* Authors: Jim Savage (Lendable Marketplace), Shoshana Vasserman (Harvard University).

This notebook illustrates how to fit aggregate random coefficient logit models in Stan, using Bayesian techniques. It’s far easier to learn and implement than the standard BLP algorithm, and has the benefits of being robust to mismeasurement of market shares, and giving limited-sample posterior uncertainty of all parameters (and demand shocks). This comes at the cost of modeling firms’ price-setting process, including how unobserved product-market demand shocks affect prices.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1285268.svg)](https://doi.org/10.5281/zenodo.1285268)

Links: 

  - [Video](https://youtu.be/LDOhRIRRe8M)
  - [Notebook, code, slides](2018/Contributed-Talks/10_savage) 
  - [github.com/khakieconomics](https://github.com/khakieconomics), [github.com/shoshievass](https://github.com/shoshievass)


<br>

**_The threshold test: Testing for racial bias in vehicle searches by police_** 

* Authors: Camelia Simoiu, Sam Corbett-Davies, Sharad Goel, Emma Pierson (Stanford University)

We develop a new statistical test to detect bias in decision making — the threshold test—that mitigates the problem of infra-marginality by jointly estimating decision thresholds and risk distributions.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1285270.svg)](https://doi.org/10.5281/zenodo.1285270)

Links: 

  - [Video](https://youtu.be/vEO-rjAqGW8)
  - [Notebook, code, slides](2018/Contributed-Talks/11_simoiu) 
  - [github.com/camioux/stancon2018](https://github.com/camioux/stancon2018) 
  - [web.stanford.edu/~csimoiu](http://web.stanford.edu/~csimoiu/), [samcorbettdavies.com](https://samcorbettdavies.com/), [cs.stanford.edu/~emmap1](https://cs.stanford.edu/~emmap1/), [5harad.com](https://5harad.com/)


<br>

**_Assessing the safety of Rosiglitazone for the treatment of type II diabetes_** 

* Authors: Konstantinos Vamvourellis, K. Kalogeropoulos, L. Phillips (London School of Economics and Political Science) 

A Bayesian paradigm for making drug approval decisions. Case study in the treatment of Diabetes (Type 2).

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1285274.svg)](https://doi.org/10.5281/zenodo.1285274)

Links: 

  - [Video](https://youtu.be/Gt73VNaZLXA)
  - [Notebook, code, slides](2018/Contributed-Talks/12_vamvourellis) 
  - [github.com/bayesways/case\_studies\_R/tree/master/stancon18](https://github.com/bayesways/case_studies_R/tree/master/stancon18)
  - [personal.lse.ac.uk/vamourel/](http://personal.lse.ac.uk/vamourel/)


<br>

**_Causal inference with the g-formula in Stan_** 

* Authors: Leah Comment (Harvard University)

The potential outcomes framework often uses one or more parametric outcome models to learn about underlying causal processes. In Stan, parameter estimation using observed data takes place in the model block, while simulation-based estimation of causal parameters using the g-formula can be done separately with generated quantities. Bayesian estimation allows for data-driven sensitivity analysis regarding the assumption of no unmeasured confounding. This presentation shows some simple causal models, then outlines a basic sensitivity analysis using prior information derived from an external data source.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1285276.svg)](https://doi.org/10.5281/zenodo.1285276)

Links: 

  - [Video](https://youtu.be/W3gnbG0v4IE)
  - [Notebook, code, slides](2018/Contributed-Talks/13_comment) 
  - [https://github.com/lcomm/stancon2018](https://github.com/lcomm/stancon2018)
  - [scholar.harvard.edu/leahcomment](https://scholar.harvard.edu/leahcomment/)

<br>

**_Bayesian estimation of ETAS models with Rstan_** 

* Authors: Fausto Fabian Crespo Fernandez (Universidad San Francisco de Quito)

Earthquake modeling with Stan. Applied to seismic recurrence in Ecuador in 2016.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1285278.svg)](https://doi.org/10.5281/zenodo.1285278)

Links: 

  - [Video](https://youtu.be/hTswMCRzltQ)
  - [Notebook, code, slides](2018/Contributed-Talks/14_crespo) 
  - [linkedin.com/in/phd-student-fausto-fabian-crespo-fernandez](https://www.linkedin.com/in/phd-student-fausto-fabian-crespo-fernandez-2b457a71/)


<br>

### 2018 Invited talks 

**_Predictive information criteria in hierarchical Bayesian models for clustered data_**
    
* Presenters: Sophia Rabe-Hesketh, Daniel Furr (UC Berkeley)
* [Video](https://youtu.be/FiSw6adfZcY)
* [Slides and code](2018/Invited-Talks/RabeHesketh_Furr) 
* [gse.berkeley.edu/people/sophia-rabe-hesketh](https://gse.berkeley.edu/people/sophia-rabe-hesketh), [github.com/danielcfurr](https://github.com/danielcfurr)

**_ScalaStan_** 

* Presenter: Joe Wingbermuehle (Cibo Technologies)
* [Video](https://youtu.be/OtggQJI4J7U)
* [Slides](2018/Invited-Talks/Wingbermuehle.pdf) 
* <a href=""https://github.com/cibotech/ScalaStan""> github.com/cibotech/ScalaStan</a>

**_Stan applications in physics: Testing quantum mechanics and modeling neutrino masses_**

* Presenter: Talia Weiss (MIT)
* [Slides](2018/Invited-Talks/Weiss.pdf) 
* [https://www.linkedin.com/in/talia-weiss-184753139](https://www.linkedin.com/in/talia-weiss-184753139/)


**_Forecasting at scale: How and why we developed Prophet for forecasting at Facebook_**

* Presenters: Sean Taylor, Ben Letham (Facebook)
* [Video](https://youtu.be/E8z3LObimok)
* [research.fb.com/facebook-at-stancon-2018](https://research.fb.com/facebook-at-stancon-2018/)
* [facebook.github.io/prophet](https://facebook.github.io/prophet/)


**_Stan applications in human genetics: Prioritizing genetic mutations that protect individuals from human disease_**

* Presenter: Manuel Rivas (Stanford University)
* [Video](https://youtu.be/S6FzGHPPxV4) 
* [Slides](2018/Invited-Talks/Rivas.pdf)
* [med.stanford.edu/rivaslab](http://med.stanford.edu/rivaslab.html)

**_Statistics using geometry to show uncertainties and integrate graph information_**

* Presenter: Susan Holmes (Stanford University)
* [Video]( https://youtu.be/W8TxxN8UdDQ)
* [Slides](2018/Invited-Talks/Holmes.pdf)
* [statweb.stanford.edu/~susan](http://statweb.stanford.edu/~susan/)

  
**_A brief history of Stan_** 

* Presenter: Daniel Lee (Generable)
* [Video](https://youtu.be/xJTZKawa-bM)
* [Slides](2018/Invited-Talks/Lee.pdf)
* [github.com/syclik](https://github.com/syclik)


**_Model assessment, model selection and inference after model selection_** 

* Presenter: Aki Vehtari (Aalto University)
* [Video](https://youtu.be/FUROJM3u5HQ)
* [Notebook, code, slides](https://github.com/avehtari/modelselection_tutorial) 
* [users.aalto.fi/~ave/](https://users.aalto.fi/~ave/)
  
  
**_Spatial models in Stan: intrinsic auto-regressive models for areal data_** 

* Presenter: Mitzi Morris (Columbia University)
* [Video](https://youtu.be/bwLkumivtjU)
* [Slides](2018/Invited-Talks/Morris.pdf)
* [Case study](http://mc-stan.org/users/documentation/case-studies/icar_stan.html)
* [github.com/mitzimorris](https://github.com/mitzimorris)

**_Some problems I'd like to solve in Stan, and what we'll need to do to get there_** 

* Presenter: Andrew Gelman (Columbia University) 
* [Video](https://youtu.be/uDB_NF_i5Ps)

<br>

## StanCon 2017 | January 21, Columbia University, New York

StanCon’s version of conference proceedings is a collection of contributed talks based on interactive notebooks. Every submission is peer reviewed by at least two reviewers. The reviewers are members of the Stan Conference Organizing Committee and the [Stan Developmemt Team](https://mc-stan.org/about/team/). This repository contains all of the accepted notebooks as well as any supplementary materials required for building the notebooks. The slides presented at the conference are also included.

### 2017 Peer reviewed contributed talks

**_Twelve Cities: Does lowering speed limits save pedestrian lives?_**      

* Authors: Jonathan Auerbach, Rob Trangucci (Columbia University)

We investigate whether American cities can expect to achieve a meaningful reduction in pedestrian deaths by lowering the posted speed limit. We find some evidence that a lower speed limit does in fact reduce fatality rates, and our estimated causal effect is similar to the traditional before-after analysis espoused by policy analysts. Nevertheless, we conclude that adjusting the posted speed limit in urban environments does not correspond with a reliable reduction in pedestrian fatalities.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1283490.svg)](https://doi.org/10.5281/zenodo.1283490)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=1h58m49s)
  - [Notebook and materials](2017/Contributed-Talks/01_auerbach)
  - [Slides](2017/Contributed-Talks/slides/01_auerbach_stancon_slides.pdf)
  - https://github.com/jauerbach


<br> 

**_Hierarchical Bayesian Modeling of the English Premier League_**    

* Authors: Milad Kharratzadeh (Columbia University)

In this case study, we provide a hierarchical Bayesian model for the English Premier League in the season of 2015/2016. The league consists of 20 teams and each two teams play two games with each other (home and away games). So, in total, there are 38 weeks, and 380 games. We model the score difference (home team goals − away team goals) in each match. The main parameters of the model are the teams’ abilities which is assumed to vary over the course of the 38 weeks. The initial abilities are determined by performance in the previous season plus some variation.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1283496.svg)](https://doi.org/10.5281/zenodo.1283496)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=2h12m12s)
  - [Notebook and materials](2017/Contributed-Talks/02_kharratzadeh); 
  - [Slides](2017/Contributed-Talks/slides/02_kharratzadeh_stancon_slides.pdf)
  - http://www.columbia.edu/~mk3971/


<br>

**_Advertising Attribution Modeling in the Movie Industry_** 

* Authors: Victor Lei, Nathan Sanders, Abigail Dawson (Legendary Entertainment)

We present a Bayesian method for inferring advertising platform effectiveness as applied to the movie industry, and show some possibilities for drawing inferences by analyzing model parameters at different levels of the hierarchy. In addition, we show some common ways to check model efficacy, and possibilities for comparing between different models.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284248.svg)](https://doi.org/10.5281/zenodo.1284248)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=2h25m43s)
  - [Notebook and materials](2017/Contributed-Talks/03_lei)
  - [Slides](2017/Contributed-Talks/slides/03_lei_stancon_slides.pdf)
  - https://github.com/foo-bar-baz-qux


<br>

**_hBayesDM: Hierarchical Bayesian modeling of decision-making tasks_** 

* Authors: Woo-Young Ahn, Nate Haines, Lei Zhang (Ohio State University)

hBayesDM (hierarchical Bayesian modeling of Decision-Making tasks) is a user-friendly R package that offers hierarchical Bayesian analysis of various computational models on an array of decision-making tasks. 

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284262.svg)](https://doi.org/10.5281/zenodo.1284262)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=2h40m20s)
  - [Notebook and materials](2017/Contributed-Talks/04_ahn)
  - [Slides](2017/Contributed-Talks/slides/04_ahn_stancon_slides.pdf)
  - https://ccs-lab.github.io


<br>

**_Differential Equation Based Models in Stan_** 

* Authors: Charles Margossian, Bill Gillespie (Metrum Research Group)

Differential equations can help us model sophisticated processes in biology, physics, and many other fields. Over the past year, the Stan team has developed many tools to tackle models based on differential equations.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284264.svg)](https://doi.org/10.5281/zenodo.1284264)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=2h53m26s)
  - [Notebook and materials](2017/Contributed-Talks/05_margossian)
  - [Slides](2017/Contributed-Talks/slides/05_margossian_stancon_slides.pdf)
  - http://metrumrg.com/


<br>

**_How to Test IRT Models Using Simulated Data_**

* Authors: Teddy Groves (Football Radar)

This notebook explains how to code some IRT models using Stan and test whether they can recover input parameters when given simulated data.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284275.svg)](https://doi.org/10.5281/zenodo.1284275)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=6h3m16s)
  - [Notebook and materials](2017/Contributed-Talks/06_groves)
  - [Slides](2017/Contributed-Talks/slides/06_groves_stancon_slides.html)
  - https://kent.academia.edu/TeddyGroves


<br>

**_Models of Retrieval in Sentence Comprehension_** 

* Authors: Bruno Nicenboim, Shravan Vasishth (University of Potsdam)

This work presents an evaluation of two well-known models of retrieval processes in sentence comprehension, the activation-based model and the direct-access model. We implemented these models in a Bayesian hierarchical framework and showed that some aspects of the data can be explained better by the direct access model. Specifically, the activation-based cannot predict that, on average, incorrect retrievals would be faster than correct ones. More generally, our work leverages the capabilities of Stan to provide a powerful framework for flexibly developing computational models of competing theories of retrieval, and demonstrates how these models’ predictions can be compared in a Bayesian setting.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284286.svg)](https://doi.org/10.5281/zenodo.1284286)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=6h18m1s)
  - [Notebook and materials](2017/Contributed-Talks/07_nicenboim) 
  - [Slides](2017/Contributed-Talks/slides/07_nicenboim_stancon_slides.pdf)
  - http://www.ling.uni-potsdam.de/~nicenboim/
  

<br>

**_Hierarchical Gaussian Processes in Stan_** 

* Authors: Rob Trangucci (Columbia University)

Stan’s library has been expanded with functions that facilitate adding Gaussian processes (GPs) to Stan models. I will share the best practices for coding GPs in Stan, and demonstrate how GPs can be added as one component of a larger model.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284293.svg)](https://doi.org/10.5281/zenodo.1284293)

Links: 

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=6h31m27s)
  - [Notebook and materials](2017/Contributed-Talks/08_trangucci)
  - [Slides](2017/Contributed-Talks/slides/08_trangucci_stancon_slides.pdf)
  - https://github.com/rtrangucci
  

<br>

**_Modeling the Rate of Public Mass Shootings with Gaussian Processes_** 

* Authors: Nathan Sanders, Victor Lei (Legendary Entertainment)

We have used Stan to develop a new model for the annualized rate of public mass shootings in the United States based on a Gaussian process with a time-varying mean function. This design yields a predictive model with the full non-parametric flexibility of a Gaussian process, while retaining the direct interpretability of a parametric model for long-term evolution of the mass shooting rate. We apply this model to the Mother Jones database of public mass shootings and explore the posterior consequences of different prior choices and of correlations between hyperparameters. We reach conclusions about the long term evolution of the rate of public mass shootings in the United States and short-term periods deviating from this trend.

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1284299.svg)](https://doi.org/10.5281/zenodo.1284299)

Links:

  - [Video](https://youtu.be/DJ0c7Bm5Djk?t=6h45m55s)  
  - [Notebook and materials](2017/Contributed-Talks/09_sanders)
  - [Slides](2017/Contributed-Talks/slides/09_sanders_stancon_slides.pdf)
  - https://github.com/nesanders



<br>
",2022-08-12
https://github.com/stan-dev/statastan,"
<a href=""http://mc-stan.org"">
<img src=""https://raw.githubusercontent.com/stan-dev/logos/master/logo.png"" width=200 alt=""Stan Logo""/>
</a>

**StataStan** is the [Stata](http://www.stata.com) interface to [Stan](http://mc-stan.org).


StataStan comprises a command **stan**. If you are using Windows, you will also have to install the command **windowsmonitor**. Queries by email to Robert Grant at [robert@bayescamp.com](mailto:robert@bayescamp.com).

**stan** will fit a Stan model by Hamiltonian Monte Carlo. You can also ask for the posterior mode, which is found by optimization with the BFGS (or L-BFGS) algorithm. We intend to create more Stata commands:
* allowing CODA-style diagnostics and plotting after a model has been fitted and chains stored,
* fitting specific models given variables, matrices, globals -- in the style of the R package rstanarm,
* and a test script.

Current status update (from Robert)
---------

Since version 16, Stata has featured close integration with Python, so you can type Python code inside your Stata script, and pass objects back and forth.

I wrote StataStan in 2014-15 and it is one of those make-do interfaces that are just wrappers for CmdStan. It relies on passing data, model, chains etc via text files, and that has always been a weakness, producing occasional problems dependent on users' OSs, security settings and so on.

My view now is that anyone who wants to use Stan from Stata should do so through PyStan or CmdStanPy. In interface terms, the speed and stability will be better than the -stan- command. I intend to freeze StataStan for Stata versions pre-16 but not to maintain compatibility with Stata going forward. It's hard to guess whether there will be need/demand to maintain compatibility with future versions of CmdStan, so post any such concerns here as issues, as they emerge.


Recent changes
----------------
Version 1.2.4, SSC release 4 March 2021:
* A warning is displayed if Stata version is 16.0 or later, advising the user to consider using PyStan or CmdStanPy via Python-Stata integration instead. This can be suppressed by the new 'nopywarn' option.
* System calls to copy and delete files are now done 'quietly'. Sometimes, the system reported ""Error: file output.csv does not exist"" and the like, which looked pretty alarming.

Version 1.2.3, SSC release 3 March 2017:
* Incorporates changes to Stan output variables from CmdStan (reads in columns after energy__)
* Bug in the load option fixed

Version 1.2.1 & 1.2.2, SSC release 16 Sep 2016, and hotfix 1 Oct 2016
* various bug fixes too tedious to list - see the commits for stan.ado for details

Version 1.2:
* You can run multiple chains with the **chains** option.
* The **chainfile** option can be abbreviated to chainf but no shorter. This avoids confusion with the new **chains** option.
* Whatever name you give in **outputfile** will now have "".csv"" appended on the end. If you run more than 1 chain, you will get consecutively numbered files like output1.csv, output2.csv, etc. **On Mac and Linux machines, the name should contain no spaces. We will try to work around this in future versions.**
* To avoid clashes with existing files, both the working directory and the CmdStan directory get checked for pre-existing output*.csv files.
* The StataStan and CmdStan version numbers are displayed at the beginning of output. The CmdStan number will be used in future versions for back compatibility.

Version 1.1, SSC release 29 Feb 2016:
* print has been replaced with stansummary
* all files are cleaned up from the CmdStan directory and appear in the working directory instead
* added a 'keepfiles' option - without this, wmbatch (in Windows), winlogfile (in Windows), outputfile, .hpp will be deleted. The executable, datafile and chainsfile are retained no matter what.


Getting Started
----------------
1. Download and install [CmdStan](http://mc-stan.org/users/interfaces/cmdstan.html). Make sure you read the installation instructions and platform-specific appendix before installing. In particular, _if you are using 32-bit Windows_, you will need to add a file called 'local' to the 'make' folder before you run the *make* command, which should simply contain the text: *BIT=32*
1. Don't forget to specify the number of CPU cores when you build CmdStan, so you can take advantage of parallel chains and the faster model-fitting that can deliver.
1. Download the .ado and.sthlp files and save them in your Stata personal ado folder (click [here](http://www.stata.com/support/faqs/programming/personal-ado-directory/) if you don't know where this is)
1. Try out the different examples in the stan-example.do file, or under **help stan**
1. Try your own data and model. The Stan modelling manual is essential reading for this! Start small and build up. Options are listed in detail under **help stan**.
1. You can pass your current data (the stuff you see when you type *browse* in Stata) into Stan, but also you can send matrices and global macros, by specifying their names or typing 'all' in the *matrices* and *globals* options. Unlike BUGS / JAGS, Stan just ignores data that your model doesn't use.

Testing
-----------------
StataStan has been tested with CmdStan 2.9 to 2.25, Stata versions from 11.2 to 16.1, and Stata flavors from IC to MP2. We have run it successfully on Linux (Debian), Mac (up to Mojave) and Windows (up to 10).

Other notes
---------------
* We find that specifying the CmdStan directory with a tilde in Mac OSX causes problems, and a complete path is advisable.
* On Mac and Linux, it is a really good idea to have the working directory and cmdstan directory paths without spaces. For parallel chains, this is essential.
* Non-existent globals and matrices, and non-numeric globals, get quietly ignored.
* Missing values are removed casewise by default (but you can change this)
* Users need to take care not to leave output file names as defaults if they have anything precious called output.csv or modes.csv etc. - these files will be overwritten.

Makers
---------
StataStan was mostly made by @robertgrant with valuable contributions from @sergiocorreia, @felixleungsc, @syclik, @danielcfurr, Charles Opondo and @bob-carpenter, plus constant encouragement and strategic direction from @gelman. We also learnt a lot from John Thompson's Stata commands for BUGS. StataStan was initially partly funded (grant holders Andrew Gelman and Sophia Rabe-Hesketh) by the Institute of Education Sciences, USA.

Licensing
---------
StataStan is licensed under BSD.
",2022-08-12
https://github.com/stan-dev/stat_comp_benchmarks,"# Statistical Computation Benchmarks
Benchmark Models for Evaluating Algorithm Accuracy

This repository includes a very preliminary suite of models, specified as Stan programs, for evaluating the accuracy of Bayesian statistical computation algorithms.  

Recall that the only role of a proper Bayesian computational algorithm is to estimate expectations with respect to a given posterior distribution.  The means of each parameter, transformed paramamter, and generated quantity in each benchmark model has been computed to high accuracy for evaluating new algorithms.  A given estimate, `mu_est`, passes only if `(mu_est - mu_true) / sigma_true < 0.25`, in other words if the algorithm can reasonably locate the mean to the center of the posterior.  No validation of posterior variances is considered outside of those explicitly defined in the generated quantities block of some models.

Each Stan program in the benchmark, and any necessary data, are located in the `benchmarks` folder.  Each Stan program is accompanied with a `<model_name>.info` file describing the model.

To test a new algorithm create the folder `empirical_results/<algorithm_name>` and add to it a text file called `<model_name>.fit` for each model you would like to evaluate.  Each `<model_name>.fit` itself should consist of two columns separated with a space -- the first for the parameter name and the second for the estimated expectation value.  Change line 5 of the shell script `evaluate` to `algo=<algorithm_name>` and run from the command line,

```
> ./ evaluate
```

The shell script will compare each parameter for each model included, listing the results as it goes as well as summary information at the end.
",2022-08-12
https://github.com/stan-dev/visual-diagnostics,"Visual Diagnostics
==================

This optional accessory to Stan generates visual diagnostics of a HMC run.  

To use, run your compiled model with the flag '--diagnostics=<diagnostic_file>' 
and then call './createAllDiagnostics <diagnostic_file>'.

The code requires a recent version of Stan (commit 2c593e4a65a73ef2a71dd79a9c58278cd725d4b8 
or higher) and a local bash environment, as well as gnuplot (http://www.gnuplot.info/)
and pdflatex (http://www.tug.org/applications/pdftex/).  The latter is packaged with most
LaTeX distributions.
",2022-08-12
https://github.com/stefanoschmidt1995/BH_geodesics,"# BH motion

This simple script computes the motion of a satellite (test particle) around a BH. The computation can be performed both in GR or within the Netwonian limit.

## How it works

The geodesics around a non-spinning BH (Schwarzschild) are a simple matter: it's just about writing down the conservation of energy and integrating the trajectories. The [wikipedia page](https://en.wikipedia.org/wiki/Schwarzschild_geodesics) will tell you a lot on this.

## How to use it

The code works in python. You will need to be able to run python (better from a terminal) with `numpy`, `scipy` and `matplotlib` installed.

To compute the orbit, you need to write some options in a config_file.ini. Then you just type:

```Bash

    python BH_geodesics.py config_file.ini

```

A standard ini file looks like:
```

[BBH]
L= 5.02
phi_0= 0.
r_0 = 50.9
r_dot_0 = 0.
GR = 1
t_max= 9e4

max_step = 1e3
relative_units = 1
compare = 0
show = 1
save_trajectories = 0

```

Everything is in units of mass (where G = c = 1)
In the ini file, you need to set:

* `L` : angular momentum (in units of M**2)
* `r_0` : initial distance (in units of M)
* `phi_0` : initial angle (dimension less)
* `r_dot_0` : initial radial velocity (dimensionless)
* `GR`: a boolean variable to control whether the Newtonian potential or the GR potential shall be used
* `t_max` : maximum integration time (in units of M)

There are also a bunch of optionals parameters:

* `max_step` (default = 1e4): controls the maximum integration step: if the integration doesn't converge, it may be a good idea to decrase it 
* `relatives_units` (default = 0): if True, `r_dot_0` will be in units of sqrt(2*V(r,L)). This makes the dynamics more redable:
	- if `|r_dot_0|>1` there are unbound orbits (hyperbolic)
	- if `|r_dot_0|<1` we have a bound system (ellyptic)
* `compare` (default = 0): if set to True, both GR and newtonian solutions will be computed and plotted together
* `show`(default = 1): if True, it will plot show some plots
* `save_trajectories` (default = 0): if True, it will save the trajectories to a jpeg file

If you want to display a quick help message, just type `python BH_geodesics.py --help`.

You can also set several configurations in the same file. Make sure each sets of configurations starts with `[SET_NAME]`.

## Examples

In folder `plots`, you can find an ini file with some examples. Feel free to play with them and explore the different possibilities.

Setting a zero initial velocity and negative energy, you will find strongly precessing orbits: the trajectory will look very nice.

<p align=""center"">
  <img src=""https://github.com/stefanoschmidt1995/BH_geodesics/blob/master/plots/nice_elliptic_trajectory.jpeg"">
</p>

If we set a non zero outward radial velocity (still keeping negative total energy) we will obtain strongly precessing, strongly eccentric orbits.

<p align=""center"">
  <img src=""https://github.com/stefanoschmidt1995/BH_geodesics/blob/master/plots/elliptic_trajectory.jpeg"">
</p>

We can also set a positive total energy to obtain an hyperbolic trajectory. Starting far away from the BH and setting a large angular momentum, the path of the object will be slightly deviated.

<p align=""center"">
  <img src=""https://github.com/stefanoschmidt1995/BH_geodesics/blob/master/plots/hyperbolic_trajectory.jpeg"">
</p>

If we start closer to the black hole, the gravitational deviation effect will be much much higher.

<p align=""center"">
  <img src=""https://github.com/stefanoschmidt1995/BH_geodesics/blob/master/plots/strong_hyperbolic_trajectory.jpeg"">
</p>

Note that in every case, the effect of Newtonian physics and those of GR are very different! This is only because a 1/r**3 term in the potential: cool, isn't it?

Of course, GR is not always important. If we take the case of the earth orbiting around the sun, the Newtonian physics works just fine:

<p align=""center"">
  <img src=""https://github.com/stefanoschmidt1995/BH_geodesics/blob/master/plots/earth_sun_trajectory.jpeg"">
</p>

## Author

This was part of the tutorial for a GR course at Utrecht Univerisity. If you want more information, please feel free to send me an email: [stefanoschmidt1995@gmail.com](mailto:stefanoschmidt1995@gmail.com)












",2022-08-12
https://github.com/stefanoschmidt1995/cppinterp,"# cppinterp
``cppinterp`` is a fast C++ based interpolator for Python. It has the same interface of [``numpy.interp(x, xp, fp)``](https://numpy.org/doc/stable/reference/generated/numpy.interp.html) and allows for interpolation of multidimensional function of one variable.
As it is based on C++ execution, its performances matches those of numpy when a ""small"" number of grid points are considered. For a huge number of grid points, ``cppinterp`` can provide a speed up of a factor of around 2.

### Installation
Before to use the package, you need to compile the file ``interp.cpp``. This is done by running ``make``.
This will build a file ``interp.so``, ready to be read by the Python interpreter.
The library can be used by importing file ``cppinterp.py``, which provides an easy-to-use interface to the C++ function.

### Usage
The library provides two functions 
- **iterp**: interpolate a 1D function of one variable, in the same fashion of numpy.
- **iterp_N**: intepolated a N dimensional function of one variable; this can provide a speed up in the execution if the grid new coordinate grid is large.

File ``try_interp.py`` has a ready to use example of how the interpolation is performed.

### Contact me
For more information, you can contact me at [stefanoschmidt1995@gmail.com](mailto:stefanoschmidt1995@gmail.com)

",2022-08-12
https://github.com/stefanoschmidt1995/GWAnomalyDetection,"# GWAnomalyDetection

A pipeline designed to detect anomalies within a noisy GW strain time series: each anomaly might correspond to an interesting signal (or to a glitch).

## Our work

How does noise behave? Can we ""predict"" the noise? Can we distinguish between noise and non-noise? If we understand these issues, we are able to detect some signal buried in noise: it might show up as an anomaly in an otherwise noisy time series.
In order to do so, we need a model that is able to predict future observations of a time series; when the forecast fails, we might be in front of an anomaly. Of course, as the gw signal is tiny, we might need some more sophisticated statistical methods to asses that the failure of the model is due to an anomaly (and not to noise itself).

We are working along two lines:
  * Long-short-term memory (LSTM) networks. A LSTM is a NN specifically designed for forecasting a time series, predicting the future, given the past. If the train the network with a lot of noise from LIGO/Virgo, we hope that the network will be able to do such predictions
  * Autoregressive process based on Maximum Entropy Spectral Analysis. The time series is modelled as and [autoregressive process](https://en.wikipedia.org/wiki/Autoregressive_model), where new observation depend linearly on past observation (plus a gaussian noise term). The method is able to model complex correlation within the data and can be used for forecasting. We use [memspectrum](https://github.com/martini-alessandro/Maximum-Entropy-Spectrum) package for an implementation of the process.
  
Once the two models are accurate enough in predictions, we are not done yet: we need an additional statistics that, given the prediction error of one of the two predictors, computes the probabilty that an anomaly is detected. This part is not trivial and requires careful tuning.

## The repository

This repository keeps to folder:
  * `LSTM`: for everything relevant to LSTM
  * `maxent`: for everything relevant to maximum entropy
  
## TO DOs
A lot of things to do:
  * ...
  * Add here
  
## Authors
+ Melissa López Portilla [m.lopez@uu.nl](mailto:m.lopez@uu.nl)
+ Stefano Schmidt [s.schmidt@uu.nl](mailto:s.schmidt@uu.nl)



",2022-08-12
https://github.com/stefanoschmidt1995/GWPhone,"# How does this work

This repo host the software to stream the sensor data from your phone to your computer. These will be used eventually to plot in real time the gravitational waves signal that your moving phone emits.

Below is some notes on what I've done so far

## Stream data from your phone

You will need the Android App [SensorStream](https://github.com/yaqwsx/SensorStreamer).

Once it's installed in you phone, you need to configure it by specifying:

- A **connection**: this configure the client or server socket to stream the data. You want to choose a server on your favourite port. Make sure that you know your phone IP address as it will be used for later
- A **packet**: this specify which data will be streamed. You just need to stream the accelerometer data and to include the timestamps. Make sure to choose the JSON format

## Install kafka

This is the most tricky part. Luckily, after some digging, I found a nice [repo](https://github.com/mtpatter/time-series-kafka-demo) that tells you how to do that.
It is related to this nice [article](https://towardsdatascience.com/make-a-mock-real-time-stream-of-data-with-python-and-kafka-7e5e23123582): it basically creates a docker image with `kafka` and `Zookeeper`. This does all the magic for you and allows for the whole thing to work.

In details, you can follows this steps:

- Install [`docker`](https://docs.docker.com/engine/install/)
- Create an image for kafka and Zookeeper with `docker compose up --build`. This may take a while. This will install the docker images as defined in `docker-compose.yml` (thanks to [time-series-kafka-demo](https://github.com/mtpatter/time-series-kafka-demo)!)
- Start a local python environment and type: `pip install -r requirements.txt`

## Dealing with Docker

This is another tricky part!

The command `docker compose up` creates and _starts_ the containers specified in the `docker-compose` file. The option `--build` will be needed to build the images (downloading them if required), before doing the actual container creation. This command is equivalent to an installation of the kafka+zookeeper code. It will create two containers, which are the item that actually execute the software, and it will _activate_ it. (still shady why it takes so long for no reason... do we see logs? or are we just stopping the installation?)

The available images can be listed with `docker image ls`. The available _containers_ (built from images) are listed with `docker container ls`. You need to make sure you have two containers running before moving forward: one for zookeeper and another one for kafka.

In short, `docker compose` is a way to manage with a single interface several containers that needs to be together (in our case, kafka and zookeeper). It is based on the `docker-compose` file, which defines the environment: this file must be in the same folder you're working on. You can check this with `docker compose ls`, which lists the compose set being running: you should see one named after the directory of your `docker-compose`.

To stop the execution of the docker compose, you need to type `docker compose stop`: this will shut down the two containers you have previously created.
To start the execution of the environment just type:
```
docker compose start
```

Of course, you could also stop the two containers by hand with `docker container stop [CONTAINER ID]`, where `[CONTAINER ID]` is listed by `docker container ls`.
Clearly, stopping the two containers is equivalent to `docker compose stop`.


## Checking if everything works

To test whether everything works, you can use the two scripts `producer.py` and `consumer.py`.

- Start the compose enviroment (i.e. the required containers) with `docker compose start`.
- Start streaming the data from your phone
- Start the kafka producer with `python producer.py --hostname ip:port` with the appropriate values.
- Start the kafka consumer with `python consumer.py`.

If everything works (and that maybe be not trivial) you should see the streamed data appearing both in the producer and consumer.

## What's next?

In the next days, I will develop the machinery to parse the data and polish them in a nice timeseries from which we can compute the GW signal emitted by your phone. This pipeline could be done with `faust` or maybe with `kafka` itself. Hopefully this should be the _easy_ part :)






",2022-08-12
https://github.com/stefanoschmidt1995/LSN_Stefano_Schmidt,"# LSN_Stefano_Schmidt
",2022-08-12
https://github.com/stefanoschmidt1995/mbank,"# mbank
`mbank` is a code for fast Gravitational Waves template bank generation. It creates a bank of binary black hole (BBH) systems. It is very handy for generating precessing and eccentric banks.

If you want more details, you can take a look at the [documentation](https://mbank.readthedocs.io/en/latest/).
Otherwise, you can keep reading and learn the essentials below.

## How it works
In order to search for a Binary Black Hole (BBH) signal, one needs to come up with a set of templates (a.k.a. bank), signals that will be searched in the noisy data from the interferometers.
Generating a [bank](https://journals.aps.org/prd/abstract/10.1103/PhysRevD.80.104014) is a tedious work, requiring to place huge number of templates so that their mutual distance is as constant as possible. Unfortunately the computation of such distance is highly expensive and, if we want to expand the parameter space covered by the searches, we are **very** interested to get a reliable bank covering an high dimensional space at a decent cost.

This is exactly the purpose of `mbank`: thanks to a cheap approximation to the distance between templates, it provides a very fast bank generation method, which can be successfully employed for high dimensional bank generation. The approximation consist in replacing the complicated distance with a _metric_ distance (i.e. a bilinear form).

The bank generation algorithm works in 4 steps:

1. Defining a metric approximation
2. Computing the metric on a small set of points all around the space (a.k.a. tiling the space)
3. Placing the templates according to the tiling
4. Validate the bank by means of injections

`mbank` is the code that does all of this for you!

## How to install

To install the latest [released](https://pypi.org/project/gw-mbank/) verion (no release has been done yet):

```Bash
pip install gw-mbank
```

To intall the latest version in the github repository, you can type:

```Bash
pip install git+https://github.com/stefanoschmidt1995/mbank
```

Otherwise, you can clone the repo, build a distribution and install the package:

```Bash
git clone git@github.com:stefanoschmidt1995/mbank.git
cd mbank
python setup.py sdist
pip install dist/mbank-0.0.1.tar.gz
```
This will install the source code as well as some executables that makes the bank generation easier (plus the dependencies).

## How to use

To generate a bank you can use the executable `mbank_run`. Make sure you have a PSD file (either in csv file either in ligo xml format).
You will need to choose:
- The BBH variables that you want to vary within the bank (`--variable_format` parameter)
- The minimum match (`--mm`), that controls the average spacing between templates
- The range of physical parameters you want to include in the bank (note that the spins are _always_ expressed in spherical coordinates)
- Low and high frequency for the match/metric computation (`--f-min` and `--f-max`)
- The WF FD approximant (it must be lal)
- Maximum number of templates in each tile: this tunes the hierarchical tiling (`--template-in-tile` argument)
- A coarse grid for tiling: the tiling can be parallelized and performed independently on each split (`--grid-size` argument)
- The placing method `--placing-method` for the templates in each tile ('geometric', 'stochastic', 'pure_stochastic', 'uniform', 'iterative'). The geometric method is recommended.

An example command to generate a simple precessing bank with precession placed only on one BH is:
```Bash
mbank_run \
	--run-name myFirstBank \
	--variable-format Mq_s1z_s2z \
	--grid-size 1,1,2,2 \
	--mm 0.97 \
	--tile-tolerance 0.5 \
	--max-depth 10 \
	--psd examples/aligo_O3actual_H1.txt --asd \
	--f-min 15 --f-max 1024 \
	--mtot-range 20 75 \
	--q-range 1 5 \
	--s1-range 0.0 0.99 \
	--s2-range -0.99 0.99 \
	--plot \
	--placing-method random \
	--livepoints 100 \
	--approximant IMRPhenomPv2 \
	--use-ray 
```
To know more information about the available options type:
```Bash
mbank_run --help
```
This is how the output bank look like:

![](https://github.com/stefanoschmidt1995/mbank/raw/master/docs/img/bank_README.png)

You can also use the metric to estimate the fitting factor for a bunch of injections: 

```Bash
mbank_injections \
	--n-injs 10000 \
	--N-neigh-templates 100 \
	--variable-format Mq_s1z_s2z \
	--tiling-file out_myFirstBank/tiling_myFirstBank.npy \
	--bank-file out_myFirstBank/bank_myFirstBank.xml.gz \
	--psd examples/aligo_O3actual_H1.txt --asd \
	--approximant IMRPhenomPv2 \
	--f-min 15 --f-max 1024 \
	--plot
```

If you specify the `--full-match` option, the match will be recomputed without a metric approximation: in this case, you want to speed things up with something like `--use-ray` and `--cache` (if you have enough memory).
You can also throw some injection chosen from a file: you just need to set an input xml injection file with the `--inj-file` option.

Here's the injection recovery:

![](https://github.com/stefanoschmidt1995/mbank/raw/master/docs/img/injections_README.png)

If you don't feel like typing all the options every time, you can add them to a text file `myFirstBank.ini` and pass it to the command: it will figure out by itself. You can find some example [ini files](https://github.com/stefanoschmidt1995/mbank/tree/master/examples) in the repo. To run them:

```Bash
mbank_run myFirstBank.ini
mbank_injections myFirstBank.ini
```

As you see, the same file can be used for different commands: each command will just ignore any option not relevant for it.


## Contacts

Fore more info, or just to say hello, you can contact me: [stefanoschmidt1995@gmail.com](mailto:stefanoschmidt1995@gmail.com).




















",2022-08-12
https://github.com/stefanoschmidt1995/MLGW,"# MLGW
MLGW is a Machine Learning model to compute the gravitational waves generated by a Binary Black Hole coalescence. It is part of a thesis project at Università di Pisa under the supervision of prof. Walter Del Pozzo.
The model is released as a Python package ``mlgw`` in the PyPI repository: <https://pypi.org/project/mlgw/>.
You can install the package with
``pip install mlgw``

The model outputs the waveform when given the two BHs masses and spins. It implements also the dependence of the waveform on the spherical harmonics.

Version 1 outputs only the 22 dominant mode. The model is presented in this [paper](https://journals.aps.org/prd/abstract/10.1103/PhysRevD.103.043020) (also online as [arXiv:2011.01958](https://arxiv.org/abs/2011.01958) ), where we present its details, we assess its accuracy and we employ it for analysing the whole GWTC-1, the first catalog of GW transients.
Version 2 is suited to deal with an arbitrary numbers of modes, while at the same time it keeps full compatibility with the previous version interface.

To generate a WF:
```Python
import mlgw.GW_generator as generator
generator = generator.GW_generator() #creating an istance of the generator
theta = np.array([20,10,0.5,-0.3, 1.43, 1.3, 2.3]) #physical parameters [m1,m2,s1,s2, d_L, iota, phi]
times = np.linspace(-8,0.02, 100000) #time grid at which waves shall be evaluated
h_p, h_c = generator.get_WF(theta, times) #returns amplitude and phase of the wave
```
You can read much more details about the model in the [thesis](https://github.com/stefanoschmidt1995/MLGW/raw/master/mlgw_package/docs/schmidt_thesis.pdf ""Thesis"").

### How to use the model
The waveforms are generated by the module ``mlgw.GW_generator``. 
For version 2 a number of tuturials are available:
- [generate_WF](https://raw.githubusercontent.com/stefanoschmidt1995/MLGW/master/mlgw_v2/generate_WF.py): generates and plots a WF. It shows basic usage of the model
- [test_HM](https://raw.githubusercontent.com/stefanoschmidt1995/MLGW/master/mlgw_v2/test_HM.py): builds an histogram of the accuracy of the WFs reconstruced by the model, by comparing them to the train model.
- [play_WF](https://raw.githubusercontent.com/stefanoschmidt1995/MLGW/master/mlgw_v2/play_WF.py): interactive WF generator, to display WF dependence on the relevant parameters.

A number of prefitted models are realesed toghether with the package. However, the interested user can build their own model. A model can be built in two steps following the tutorials:
1. [generating dataset](https://raw.githubusercontent.com/stefanoschmidt1995/MLGW/master/mlgw_v2/generate_dataset.py): a dataset of WFs is built, using TEOBResumS model. User should build a dataset for each mode to be included.
2. [fitting the model](https://raw.githubusercontent.com/stefanoschmidt1995/MLGW/master/mlgw_v2/do_the_fit.py): a PCA model should be fitted on the WFs dataset and a MoE model should learn the regression from the orbital parameter to the reduced order representation of the WF. Module ``fit_model`` takes care of that. A WF model must be saved in a single folder, which must contain a subfolder for each mode included.

### Content of the repository
The repository is organised in the following folders:
- **mlgw_v1**: first version of the model (only 22 mode is fitted)
- **mlgw_v2**: second version of the model (higher modes are included) - not released yet
- **tries_checks**: it holds some tests and checks performed to develop the code - the cose here is not guarenteed to work
- **mlgw_package**: it holds code relevant to the package [``mlgw``](https://pypi.org/project/mlgw/ ""mlgw package at PyPI"").
- **precession**: code for the developments of ML methods for including precession in the model.
- **paper**: it holds the documents for the [paper](https://arxiv.org/abs/2011.01958 ""mlgw"").

For more information, you can contact me at [stefanoschmidt1995@gmail.com](mailto:stefanoschmidt1995@gmail.com)

",2022-08-12
https://github.com/stefanoschmidt1995/ML_ODE,"# ML_ODE

``ML_ODE`` provide the code to build a Machine Learning model for solving a Ordinary differential equation (ODE).
An ODE problem consist in finding a function of time `t` ``f(t;x_0, Omega)``, dependent parametrically on `x_0` and `Omega`, such that:

```
f'(t) = F(t,x,Omega)
f(t=0) = x_0
```
in this context `x_0` is the value of the function at some initial time and `Omega` is a set of additional parameters useful to specify the time derivative of the problem.

As it is well known, a neural network can approximate (at least in principle) any real function, so: why not approximating the ODE solution with a Neural Network? In doing so we follow the seminal work [Artificial Neural Networks for Solving Ordinary and Partial Differential Equations](https://arxiv.org/abs/physics/9705023). The idea is further developed in a more modern [paper](https://arxiv.org/abs/2006.14372) and this is where we take inspiration from.
Basically, we use the following ansatz for the solution:

```
f(t;x_0, Omega) = x_0 + (1-exp(-t))*NN(t, x_0, Omega)
```

This ansatz is pretty smart: it satisfies identically the boundary conditions and you can easily check how far `f` is from the actual solution by comparing its time derivative with the function `F(t,Omega)` of the problem. This idea is implemented in the loss function, which takes the following form:

```
L(t,x_0,Omega) = | f'(t;x_0, Omega) - F(t, f(t,_x0, Omega), Omega) |**2 * exp(-lambda*t)
```
Where `lambda` is a regularization constant to ensure that the training is more gradual.
Once you have a loss function, it's all done: Tensorflow takes care of minimizing your loss function with respect to the weights of the NN.

You may wonder why we need a NN to solve an ODE, when we already have plenty of finite difference methods that works pretty well. The answer is simple: *a NN is fast*! This of course saves you a lot of computational resourches and at the same time you get the same degree of accuracy. Not convinced yet? A NN also provides a closed form expression for the solution, and once you have such expression you can differentiate it as many times as you wish: nice, isn't?

## How it works

All the interesting code is wrapped in a single class, which provides you a (almost) ready to use model to fit. It is built to resemble a Keras model. You will find method `fit()` to train the model and method `call()` to evaluate the NN. You can use methods `get_solution()` or `ODE_solution()` to compute the solution that you are looking for.
You can inherit the provided baseclass and customize your own model easily. You only need to specify the following:

 - The network architecture (by writing function `set_model_properties()`), given as a list of dense keras layers (list should be `self._l_list`)
 - Some hyperparameters of the networks: the dimensionality of the problem (`n_vars`), the number of variables Omega (`n_params`), the range in which every variable lives (in list `constraints`), as well as the range of time the model should be trained at. You might also set the value of a regularizer for the loss function, you can safely use a small value, such as 0.1~1. (See lambda parameter in eq.3 [here](https://arxiv.org/abs/2006.14372) for more information)
 - The function `F(t,x,Omega)`. It should be a tensorflow function, which outputs a batch with shape `(None, n_vars)`

That's all. Not simple enough? Below you will find a simple example:

```Python
import ML_ODE
class my_ML_ODE(ML_ODE.ML_ODE_Basemodel):
	""Class for solving a ODE problem. Inherit from ML_ODE.ML_ODE_Basemodel and implements some methods.""

	def set_model_properties(self):
		""This class should initialise some important paramters for the network""
		self.n_vars = 2	#number of variables in the problem
		self.n_params = 2 #number of parameters in the problem
		self.constraints = [(0.,1.),(-1., 1.), (-1., 1.), (-1., 1.),(-1., 1.)] #(t_range, x0_range, x1_range, *params_range)
		self.regularizer = 1. #regularizer for the loss function (not compulsory, default 0)
			#The regularizer act on the loss function L(t,x,Omega) s.t. L' = L * exp(regularizer*t): this is to ensure that earlier times are fitted with more accuracy

	def build_NN(self):
		""This class builds the fully connected NN architechture. Layer sequence should be provided by a list of layers (in self._l_list)""
		self._l_list.append(tf.keras.layers.Dense(128*2, activation=tf.nn.sigmoid) )
		self._l_list.append(tf.keras.layers.Dense(128, activation=tf.nn.sigmoid) )
		self._l_list.append(tf.keras.layers.Dense(self.n_vars, activation=tf.keras.activations.linear))
	
	
	def ODE_derivative(self, t, X, Omega): #actually z, D_L, Omegas (N,2)
		""""""Here we write down the derivative of x: x' = F(t,x,Omega).
		Function should be written in pure tensorflow (input and outputs are tf tensors).
		Inputs:
			t (None,)					times
			X (None, n_vars)			values of the variables
			Omega (None, n_params)		parameters
		Output:
			x_prime (None,n_vars)/(None,)	derivative of the function
		""""""
		return tf.math.multiply(X,Omega) #simple example where F_i(t,x,Omega) = x_i*Omega_i
```

You can find working examples in `examples/example_model.py` and `examples/cosmological_model.py`: the trained models are in folder `examples/example_model` and `examples/cosmological_model`.

## Installing

It's super simple: you only need the file ML_ODE.py, which keeps all the code you need. Get it with:

`wget https://raw.githubusercontent.com/stefanoschmidt1995/ML_ODE/main/ML_ODE.py`

and you're done! You can `import ML_ODE` from every python script you like: that's all. 

If you need also the examples, you might want to clone the whole repository

`git clone https://github.com/stefanoschmidt1995/ML_ODE.git`


## Documentation

Every function is documented (I did my best to make it clear).
Once you have instantiated a model, you can use the python help function for understanding the behaviour of the `ML_ODE` class or of a specific method:

```
help(ML_ODE)
help(ML_ODE.method_you_want_to_know_more_about)

```

Furthermore, you can have a look at the examples I provided.

If still something is not clear, or you wish to have more information, you can contact me at [stefanoschmidt1995@gmail.com](mailto:stefanoschmidt1995@gmail.com)
",2022-08-12
https://github.com/stefvanbuuren/academic,"# academic

This repository contains the source of my academic website. The site uses 
`blogdown` in combination with the `hugo-academic` theme.

Use `blogdown:::serve_site()` to update the website after a source 
file is saved. The terminal command `./deploy.sh` will rebuild the website 
and upload the contents to `stefvanbuuren/stefvanbuuren.github.io`, 
from where it is published at https://stefvanbuuren.name. 

",2022-08-12
https://github.com/stefvanbuuren/AGD,"<!-- README.md is generated from README.Rmd. Please edit that file -->
[AGD: Analysis of Growth Data](http://stefvanbuuren.github.io/AGD/)
===================================================================

The [`AGD`](https://cran.r-project.org/package=AGD) package implements various tools that aid in the analysis of growth data.

Installation
------------

The `AGD` package can be installed from CRAN as follows:

``` r
install.packages(""AGD"")
```

The latest version is can be installed from GitHub as follows:

``` r
install.packages(""devtools"")
devtools::install_github(repo = ""stefvanbuuren/AGD"")
```

Minimal example
---------------

``` r
library(AGD)

# What is the SDS of a height of 115 cm at age 5 years
# relative to Dutch references?
# Calculate for boys and girls:
y2z(y = c(115, 115), x = 5, sex = c(""M"", ""F""))
#> [1] 0.424 0.706

# What are the SDS of the IOTF BMI cut-off values for 
# overweight (boys 2-18) relative to Dutch references?
cutoff <- c(
18.41, 17.89, 17.55, 17.42, 17.55, 17.92, 18.44, 19.10, 
19.84, 20.55, 21.22, 21.91, 22.62, 23.29, 23.90, 24.46,  
25.00)
age <- 2:18
z <- y2z(y = cutoff, x = 2:18, sex = ""M"", ref = nl4.bmi)
plot(age, z, type = ""b"", xlab = ""Age (years"", 
     ylab = ""SDS IOTF (on Dutch reference)"")
```

![Standard deviation score of IOTF overweight cut-off relative to Dutch reference](README-minimal-1.png)
",2022-08-12
https://github.com/stefvanbuuren/BayesAdaptive,"<!-- README.md is generated from README.Rmd. Please edit that file -->
BayesAdaptive: Bayesian response-adaptive designs for multi-arm sequential clinical trials
==========================================================================================

The `BayesAdaptive` package was written by Steffen Ventz and is available from [Steffen Ventz R Packages](http://bcb.dfci.harvard.edu/~steffen/software.html).

This repository starts with `BayesAdaptive` version 1.0 and implements some tweaks to adapt it to current `R version 3.3.3`.

Installation
------------

The `BayesAdaptive` package can be installed from GitHub as follows:

``` r
install.packages(""devtools"")
devtools::install_github(repo = ""stefvanbuuren/BayesAdaptive"")
```

More information
----------------

The package contains a vignette that demonstrates the functionality. Also, see the paper by Ventz et.al. (Ventz et al. 2017).

References
----------

Ventz, S, WT Barry, G Parmigiani, and L Trippa. 2017. “Bayesian Response-Adaptive Designs for Basket Trials.” *Biometrics*. doi:[10.1111/biom.12668](https://doi.org/10.1111/biom.12668).
",2022-08-12
https://github.com/stefvanbuuren/bdsschema,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# bdsschema

<!-- badges: start -->

[![Lifecycle:
superseded](https://img.shields.io/badge/lifecycle-superseded-blue.svg)](https://lifecycle.r-lib.org/articles/stages.html#superseded)
<!-- badges: end -->

The `bdsschema` package is superseded by
[bdsreader](https://github.com/growthcharts/bdsreader).

The goal of `bdsschema` is to define, document and validate the format
for online data exchange of individual health data conform to the
Basisdataset JGZ. See
<https://www.ncj.nl/themadossiers/informatisering/basisdataset/> for the
definition of the Basisdataset JGZ.

## Installation

You can install the development version `bdsschema` by .

``` r
install.packages(""remotes"")
remotes::install_github(""stefvanbuuren/bdsschema"")
```

There is no release on CRAN.
",2022-08-12
https://github.com/stefvanbuuren/fimd,"# fimd
Flexible Imputation of Missing Data - HTML
",2022-08-12
https://github.com/stefvanbuuren/fimdbook,"## Flexible Imputation of Missing Data, Online Version

This repository contains the R Markdown source for the online 
version of *Flexible Imputation of Missing Data, Second Edition*
(https://stefvanbuuren.name/fimd/). 

This repository tracks changes made to the book.

Probably the most useful file is `R\fimd.R`, which contains the 
sources used to perform the analyses in the book.

In case you want to alert me on any errors, inconsistencies 
or possible improvements, please open an issue or submit a pull
request.
",2022-08-12
https://github.com/stefvanbuuren/minihealth,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

# minihealth

<!-- badges: start -->
<!-- badges: end -->

The `minihealth` package is a central component of
[JAMES](https://github.com/stefvanbuuren/james). The package

-   Defines S4 classes `xyz`, `bse`, `ird`, `individual` and `cabinet`
    for analysing individual growth data;
-   Translates `individual`, `donordata` and `bds` formats into each
    other;
-   Extracts data and ranges from `individual` objects.

## Installation

Install the development version `minihealth` by

``` r
install.packages(""remotes"")
remotes::install_github(""stefvanbuuren/minihealth"")
```

There is no release on CRAN.

## Example 1: Automatic Z-score calculation

The S4 class `xyz` stores three variables useful for anthropometric
data:

-   `x`: usually age, but can also be height (in weight-for-height);
-   `y`: measurement, e.g. height or weight;
-   `z`: Z-score of `y` conditional on `x`.

Here are some examples for automatic *Z*-score calculation:

``` r
library(minihealth)
#> Loading required package: nlreferences
#> Loading required package: donorloader

# specify length (in cm) for boy at ages 0, 0.2 and 0.5 years
new(""xyz"", x = c(0, 0.2, 0.5), y = c(51.0, 54.1, 63.4))
#>   age  hgt hgt_z
#> 1 0.0 51.0    NA
#> 2 0.2 54.1    NA
#> 3 0.5 63.4    NA

# at the minimum, specify sex for automatic Z-score calculation
new(""xyz"", x = c(0, 0.2, 0.5), y = c(51.0, 54.1, 63.4), sex = ""male"")
#>   age  hgt  hgt_z
#> 1 0.0 51.0 -0.154
#> 2 0.2 54.1 -2.312
#> 3 0.5 63.4 -1.829

# specify weight (in kg) at same ages
d1 <- new(""xyz"", x = c(0, 0.2, 0.5), y = c(3.2, 5.2, 7.0), 
          sex = ""male"", yname = ""wgt"")
d1
#>   age wgt  wgt_z
#> 1 0.0 3.2 -0.874
#> 2 0.2 5.2 -0.575
#> 3 0.5 7.0 -1.048

# View reference names used to calculate Z-scores in d1
data.frame(d1)
#>   xname yname zname   x   y      z          pkg             refcode
#> 1   age   wgt wgt_z 0.0 3.2 -0.874 nlreferences nl_1997_wgt_male_nl
#> 2   age   wgt wgt_z 0.2 5.2 -0.575 nlreferences nl_1997_wgt_male_nl
#> 3   age   wgt wgt_z 0.5 7.0 -1.048 nlreferences nl_1997_wgt_male_nl
```

It is also possible to perform the reverse calculation, where `z` is
given and `y` is calculated.

``` r
# Standard weight centiles at age 0.5 year of Dutch boys
new(""xyz"", x = rep(0.5, 5), z = -2:2, sex = ""male"", yname = ""wgt"")
#>   age  wgt wgt_z
#> 1 0.5 6.24    -2
#> 2 0.5 7.04    -1
#> 3 0.5 7.90     0
#> 4 0.5 8.83     1
#> 5 0.5 9.82     2

# Extend to grid of ages: 0y, 0.5y and 1y
new(""xyz"", x = rep(c(0, 0.5, 1), each = 5), z = rep(-2:2, 3), 
    sex = ""male"", yname = ""wgt"")
#>    age   wgt wgt_z
#> 1  0.0  2.77    -2
#> 2  0.0  3.15    -1
#> 3  0.0  3.55     0
#> 4  0.0  3.97     1
#> 5  0.0  4.40     2
#> 6  0.5  6.24    -2
#> 7  0.5  7.04    -1
#> 8  0.5  7.90     0
#> 9  0.5  8.83     1
#> 10 0.5  9.82     2
#> 11 1.0  8.14    -2
#> 12 1.0  9.14    -1
#> 13 1.0 10.24     0
#> 14 1.0 11.44     1
#> 15 1.0 12.75     2
```

See the help (“?`xyz-class`”) and the
[centile](https://github.com/growthcharts/centile) package for more
examples.

## Example 2: Automatic brokenstick estimation

``` r
# specify three height measures
boy <- new(""xyz"", x = c(0, 0.2, 0.5), y = c(51.0, 54.1, 63.4), 
           sex = ""male"")
boy
#>   age  hgt  hgt_z
#> 1 0.0 51.0 -0.154
#> 2 0.2 54.1 -2.312
#> 3 0.5 63.4 -1.829

# calculate broken stick estimates at observed ages
new(""bse"", data = boy)
#> donorloader::load_data(dnr = ""smocc_bs"", element = ""hgt"")
#>   age  hgt  hgt_z
#> 1 0.0 50.6 -0.349
#> 2 0.2 54.9 -1.982
#> 3 0.5 63.5 -1.801

# calculate broken stick estimates at all break points
new(""bse"", data = boy, at = ""knots"")
#> donorloader::load_data(dnr = ""smocc_bs"", element = ""hgt"")
#>       age  hgt  hgt_z
#> 1  0.0000 50.6 -0.349
#> 2  0.0767 50.9 -1.640
#> 3  0.1533 53.2 -1.945
#> 4  0.2500 56.5 -2.020
#> 5  0.3333 60.0 -1.599
#> 6  0.5000 63.5 -1.801
#> 7  0.6250 66.5 -1.532
#> 8  0.7500 68.1 -1.740
#> 9  0.9167 70.7 -1.708
#> 10 1.1667 74.3 -1.605
#> 11 1.5000 78.3 -1.588
#> 12 2.0000 84.0 -1.483
#> 13 3.0000 92.4 -1.547
```

See the help (“?`bse-class`”) and the
[brokenstick](https://github.com/growthcharts/brokenstick) package for
more examples.

## Example 3: Bundle all measures by individual

The S4 class `individual` bundles four different types of information,
each of which is coded by its own class:

-   `individualID`: ID information, like name, date of birth;
-   `individualBG`: Background, like sex, gestional age or etnicity;
-   `individualAN`: Bundles person’s anthrometric data, like `hgt` and
    `wgt`;
-   `individualBS`: Bundles person’s brokenstick estimates;
-   `individualRW`: Stores raw data, e.g. milestones;

Creating an instance of class `individual` can be done by hand in two
steps. First, create one or more of the four subclasses, and then  
tie these together, as follows:

``` r
pid <- new(""individualID"", name = c(""Rob"", ""Dorchester""),
           dob = as.Date(""2014-08-22"", ""%Y-%m-%d""), 
           id = as.integer(204))
pbg <- new(""individualBG"", sex = ""male"", hgtf = 185)
pan <- new(""individualAN"",
           hgt = new(""xyz"", 
                     x = c(0, 0.2, 0.5), 
                     y = c(51.0, 54.1, 63.4), 
                     sex = pbg@sex),
           wgt = new(""xyz"", 
                     x = c(0, 0.5), 
                     y = c(3.2, 7.0), 
                     yname = ""wgt"", 
                     sex = pbg@sex))
pbs <- new(""individualBS"",
           bs.hgt = new(""bse"", yname = ""hgt"",
                        data = pan@hgt,
                        at = ""knots"",
                        sex = pbg@sex),
           bs.wgt = new(""bse"", yname = ""wgt"", 
                        data = pan@wgt, 
                        at = ""knots"",
                        sex = pbg@sex))
data <- data.frame(age = c(0.2, 0.5, 0.7),
                   k1430 = c(1, NA, NA),
                   k1431 = c(2, NA, NA),
                   k1437 = c(3, 1, 1),
                   k1438 = c(0, 1, 1),
                   k1439 = c(0, 1, 1))
map <- data.frame(from = c(""k1430"", ""k1431"", ""k1437"", ""k1438"", ""k1439""),
                  to = c(879, 927, 928, 881, 883))
prw <- new(""individualRW"",
           ddi = new(""ird"", mst = data, map = map, instrument = ""ddi""))

rob <- new(""individual"", pid, pbg, pan, pbs, prw)
```

Doing this sequence by hand is somewhat inconvenient. Fortunately, there
are two functions that convert other formats into an object of class
`individual`:

-   `donordata_to_individual()` takes data in `donordata` format, and
    converts it into `individual` format;
-   `convert_bds_individual()` takes data in `bds` format, and converts
    it into `individual` format;

See the respective documentation for more detail. Both functions have
inverse (but lossy) transformations.

## Miscellaneous functionality

-   A `cabinet` is a collection of multiple objects of class
    `individual`;
-   `data.frame(d1)` extracts the data frame from objects of class
    `xyz`;
-   `get_xyz(rob, ""hgt"")` extracts the data frame from objects of class
    `individual`;
-   `get_range(rob)` extract the age range from objects of class
    `individual`.
",2022-08-12
https://github.com/stefvanbuuren/nl,"# nl

This is a repo to redirect `stefvanbuuren.nl` to `stefvanbuuren.name`.
",2022-08-12
https://github.com/stefvanbuuren/pubertyplot,"#install R and the pubertyplot package
R CMD INSTALL pubertyplot_1.0.tar.gz

#install rapache, e.g. using the package
sudo apt-get install libapache2-mod-r-base

#copy the site file from the package and activate
sudo cp -Rf /usr/local/lib/R/site-library/pubertyplot/sites-available/puberty /etc/apache2/sites-available
sudo a2ensite puberty

#restart apache
sudo service apache2 restart

#if anything isn't working, check the error log:
tail /var/log/apache2/error.log
",2022-08-12
https://github.com/stefvanbuuren/RECAPworkshop,"<!-- README.md is generated from README.Rmd. Please edit that file -->
[RECAP workshop: Statistical Methods for combined data sets](https://stefvanbuuren.github.io/RECAPworkshop/)
============================================================================================================

Overview
--------

This site contains materials for the RECAP workshop *Statistical Methods for combined data sets: Theory, techniques and tools* on September 4-5, 2017 in Leiden.

Motivation
----------

Combining data sets generates blocks of missing data. However, most data analysis procedures are designed for complete data, and many will fail if the data contain missing values. Most procedures will therefore simply ignore any incomplete rows in the data, or revert to ad-hoc procedures like replacing missing values with some sort of ""best value"". However, such fixes are based on assumptions, and may introduce serious biases when these assumptions are not met.

This workshop revises practical issues with combining data, and explores the use of multiple imputation as a principled solution.

Contents
--------

The workshop consist of 6 sessions, each of which comprises a lecture followed by a computer practical using `R`:

1.  Session I: Combining Datasets & Missing Data
2.  Session II: Multiple imputation using `mice`
3.  Session III: Creating Comparable Variables
4.  Session IV: Developmental milestones
5.  Session V: Loss-to-Follow-Up
6.  Session VI: Multilevel Analysis

How to prepare
--------------

Please remember to bring your own laptop computer and make sure that you have write-access to that machine (some corporate computers do not allow write access) or that you have the following software and packages pre-installed.

------------------------------------------------------------------------

1.  Download and install the latest version of `R` from [the R-Project website](https://cloud.r-project.org)
2.  Download and install the most recent version of `RStudio Desktop (Free License)` from [RStudio's website](https://www.rstudio.com/products/rstudio/download3/). This is not necessary, per se, but it is highly recommended as `RStudio` delivers a tremendous improvement to the user experience of base `R`.
3.  Install the packages `markdown`, `mice`, `lme4`, `dplyr`, `plyr` and `mlmRev`.

-   You can install packages from within `RStudio` by navigating to `Tools > Install Packages` in the upper menu and entering the names of the package into the `Packages` field. Make sure that the button `Install dependencies` is selected. Once done, click `Install` and you're all set.
-   Or, from within `R` or `RStudio`, copy, paste and enter the following code in the console window (by default the top-right window in `RStudio` / the only window in `R`):

``` r
install.packages(c(""markdown"", ""mice"", ""lme4"", ""dplyr"", ""plyr"", ""mlmRev""))
```

------------------------------------------------------------------------

Workshop materials
------------------

1.  [Lectures](Lectures/RECAP_Workshop_WP5_20170904.pptx)
2.  [Practical I](Practicals/RECAP_Practical_I.html)
3.  [Practical II](Practicals/RECAP_Practical_II.html)
4.  [Practical III](Practicals/RECAP_Practical_III.html)
5.  [Practical IV](Practicals/RECAP_Practical_IV.html)
6.  [Practical V](Practicals/RECAP_Practical_V.html)
7.  [Practical VI](Practicals/RECAP_Practical_VI.html)
8.  [Practical I .Rmd](Practicals/RECAP_Practical_I.Rmd)
9.  [Practical II .Rmd](Practicals/RECAP_Practical_II.Rmd)
10. [Practical III .Rmd](Practicals/RECAP_Practical_III.Rmd)
11. [Practical IV .Rmd](Practicals/RECAP_Practical_IV.Rmd)
12. [Practical V .Rmd](Practicals/RECAP_Practical_V.Rmd)
13. [Practical V data\_July2017.txt](Practicals/data_July2017.txt)
14. [Practical VI .Rmd](Practicals/RECAP_Practical_VI.Rmd)
15. [Unifying perspective](Background/RECAP_D5_1_Unifying_missing_data_perspective_final.pdf)
",2022-08-12
https://github.com/stefvanbuuren/starter-academic,"# academic

This repository contains the source of my academic website. The site uses 
`blogdown` in combination with the `starter-academic` theme.

Updating:

- `contents/publication/xxx` to change or add publications
- `config/_default/config.toml` and friends for configuration

Add a publication:

- `static/publications/` for adding pdf's
- `static/files/citations/` for adding .bib files. 
- run `academic import --bibtex static/files/citations/2020-08-12_new.bib` to create a new directory `contents/publication/[xxx]/` with `.bib` and `.md` files
- change author ""S. van Buuren"" to `admin`
- add line `url_pdf: 'publications/thepdf.pdf'`
- add file `featured.png` with illustration to `contents/publication/[xxx]/` folder

Use `blogdown:::serve_site()` to update the website after a source file is saved. 

The terminal command `./deploy.sh` will rebuild the website 
and upload the contents to `stefvanbuuren/stefvanbuuren.github.io`, 
from where it is published at https://stefvanbuuren.name. 

",2022-08-12
https://github.com/stefvanbuuren/statsref,"Contributions to Wiley StatsRef: Statistics Reference Online
",2022-08-12
https://github.com/SytseGroenwold/advent-of-code-2021,# advent-of-code-2021,2022-08-12
https://github.com/SytseGroenwold/delay-risks-sprints," # Thesis repository
 
 ## Provisional title: Risk detection and risk control through sprint backlog analysis
 This repository holds all the relevant information regarding my Master's Thesis for the University of Amsterdam's Master's Programme Information Studies in the Data Science track.

## data
The folder `data` holds all the datasets. This includes any data sets not yet used but open for consideration to include.
It also holds anything used to extract or preprocess the data.

## src
The `src` folder is where the source code of the machine learning models are located.

## overleaf
The folder `overleaf` holds a copy of the LaTeX document I am using for both my thesis design and the actual thesis report.
It is just a copy of its own repository: 

## worklog
The file `worklog.yml` is a quick overview of all work, done or still to be done.
",2022-08-12
https://github.com/SytseGroenwold/recipe_api,"# Recipe API
Basic Spring Boot API with MongoDB persistent storage to perform CRUD actions on a collection of recipes.
 
## About
This Recipe API is created as part of an assessment for a job application process with ABN AMRO Bank.

Version: 0.1.0  
Author: Sytse Groenwold  
Contact: sytsegroenwold@gmail.com

# Table of contents
- [Requirements](#requirements)
- [How to set up](#how-to-set-up)
- [How to use (API docs)](#how-to-use-api-docs)
- [Design decisions](#design-decisions)

## Requirements
While the versions of these requirements are likely not as strict, these were the versions used during development. 
In case of any issues, try updating to these version first.
* Java 17.0.3
* Maven 3.8.5
* Docker 20.10.10
* MongoDB 5.0.6 (See [How to set up](#how-to-set-up) for more info)

## How to set up
The API requires MongoDB as a backend to start and function.
Steps 1 and 2 are optional, if you choose to use another implementation of MongoDB more convenient.

1. `docker mongo:5.0.6`  
   Downloads the used version of MongoDB docker image.
   Below step should do it automatically, but in case of trouble, see if you can download this image separately. 
2. `docker run -d --name mongo-on-docker -p 27017:27017 mongo:5.0.6`  
    Starts the MongoDB instance inside a docker container. 
    The name can be changed at will. 
3. `mvn clean install`  
    While the below command should do the same, in case you run into issues, ensure the application can be build.
4. `mvn spring-boot:run`  
    After this command the API application is ready to be used. See the API docs on how to use the API.

## How to use (API Docs)
The API should be reachable through your usual preferred method.
Postman was used during the development and is recommended

The API is exposed on port `8081`, thus going to `localhost:8081` in your browser should show you an error page.
This is correct: if the application is not running, you should see a ""cannot connect"" error instead.

The following paths are available in this API:

| Action                             | Explanation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
|------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `get` all recipes                  | path: `/recipes`  <br/>parameters: none  </br> returns: List of recipes in JSON format.                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| `post` a recipe                    | path: `/recipes/add <body: application/json>`  <br/>parameters:  <br/>* `<body> json`: JSON with contents of the recipe to create.<br/>* [optional] `page`: integer with the page number to return. The API by defaults limits the number of recipes returned to 15 per page. Default is 0, the first page.                                                                                                                                                                                                                                              |
| `put` a recipe:                    | path: `/recipes/put <body: application/json>` </br> Parameters: </br> * `id`: id of the recipe in the database to replace <br/> * `<body> json`: Recipe to replace the entry with. Id cannot be changed, but must be supplied! See template below. <br/> Returns: `List<Recipe>` of the old recipe in database and new entry.                                                                                                                                                                                                                            |
| `delete` a recipe                  | path: `/recipes/delete/<id>`</br>parameters:</br>* `id`: id of the recipe in the database to delete.                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| `get` a recipe based on conditions | path: `localhost:8081/recipe/find`</br>parameters:</br>* `page` (default 0): Which page of results you wish to return. Each page holds 15 entries.</br>* `<body> json`: A list of criteria to which the recipe should adhere. The possible keys in the JSON are displayed below. The json should contain only the keys which actually have values. The only difference between this JSON and the Recipe JSON, is the addition of ""notIngredients"", which are the ingredients to exclude from the results. <br/> returns: List of recipes in JSON format. |

The recipes are returned and the json body templates:
> Result and `post`/`put` .json template:
>```
>#recipe.json
>{
>    ""id"": String,
>    ""name"": String,
>    ""diet"": String,
>    ""servings"": Integer,
>    ""ingredients"": String,
>    ""instructions"": String
>}
>```
> `get` method .json template:
> ```
>\#body.json
>\{
>    ""id"": String,
>    ""name"": String,
>    ""diet"": String,
>    ""servings"": Integer,
>    ""ingredients"": String,
>    ""notIngredients: String,
>    ""instructions"": String
>\}
>```

## Design decisions
Any code specific decisions are added as comments in the code directly at the relevant location, avoiding lengthily descriptions and troubles finding what is being referred to.

For the entirety of this code the following applies: it was made with the assessment in mind, with the given timebox of 4-8 hours.
That means that some decisions would not be made in production, or inconsistencies are purposely implemented to display different possibilities and to not overrun the timebox.

**_[Sprint Boot]_** is chosen as the framework, because it is arguably the most popular for REST APIs. 
It is flexible and allows for many actions to be automatically resolved or linked.
Sprint Boot Data also offers a MongoDB module, further simplifying the implementation.
Also, the alternatives are niches and offer things that don't even come close to being useful for this project.

**_[MongoDB]_** was almost an instant decision after reading the assignment scope. 
Recipes lend themselves quite well to the JSON document format, and thus it allows for easy integration of the API.
Using MongoDB also allows for flexible queries: any field can be easily added or excluded in a search and negative searches are natively also supported.
Unfortunately, due to an unresolved bug, the negative lookaround regex query is not working and had to be resolved by writing more application logic.
This could directly be resolved by switching the builtin Spring MongoDB Respository with the MongoDB Java Driver: this was not done in this project, to showcase the benefits of Spring instead.
This section of code also serves as a testament for thinking in quick workaround solutions in where time is of the essence, something I practiced frequently during my standby shifts at ING.

**_[Docker]_** because MongoDB was chosen as a backend, the need arose to facilitate the reviewers. The instructions that explain how MongoDB can be run are therefore added.
But more importantly, it should not be necessary to even run it, as the integration tests are tested against an embedded MongoDB implementation.

**_[Embedded MongoDB]_** The choice for this is described partly above, the other part is that an embedded version of a project' datastore greatly improves the Test Driven Development paradigm.
An embedded datastore for integration testing can be ran at all times and even continuously/automatically, for example after every git commit.
Problems arise earlier and are less likely to end up in production. Before a merge however, the embedded datastore should be swapped out with an actual implementation (easily done in application.properties).

**_[API Documentation]_** is kept concise to avoid confusion. Users only address it in case of issues or to get started, and that latter option is significantly more frequent.
In a more complete project, a choice for a tooling/framework such as Swagger can be considered. For this small scope, the manual approach should be sufficient.",2022-08-12
https://github.com/SytseGroenwold/thesis-data,"# thesis-data
Repository to hold any data sets to possibly use. The final data sets used can be found in the main repository: thesis-repo.
",2022-08-12
https://github.com/SytseGroenwold/uu-catalog-items,# uu-catalog-items,2022-08-12
https://github.com/SytseGroenwold/VU-DMT-2022-group124,"# VU-DMT-2022-group124

Repository of group 124 of Vrije Universiteit's 2022 Data Mining Techniques course.
",2022-08-12
https://github.com/TabeaSonnenschein/Spatial-Agent-based-Modeling-of-Urban-Health-Interventions,"# Spatial-Agent-based-Modeling-of-Urban-Health-Interventions
by Tabea Sonnenschein

All the work done for this repository is part of the EXPANSE project: http://expanseproject.eu/, and embodies a core part of my PhD work.


The GitHub repository contains a set of methods and models.

1) the folder [**GenSynthPop**](https://github.com/TabeaSonnenschein/Spatial-Agent-based-Modeling-of-Urban-Health-Interventions/tree/main/GenSynthPop) contains a fully documented R-package to generate a representative, spatial synthetic agent population combining neighborhood marginal distributions and stratified distributions. This encompasses functions for data preparation and harmonization, for calculating conditional propensities and populating the agent dataframe according to the propensities and neighborhood marginal distributions, and finally, to validate that the resulting agent dataframe distributions correspond to the neighborhood and stratified distributions.

2) the folder [**Routing**](https://github.com/TabeaSonnenschein/Spatial-Agent-based-Modeling-of-Urban-Health-Interventions/tree/main/Routing) contains complete instructions of setting up a local instance of the Open Source Routing Machine (OSRM) and scripts that can be used to access the routing machine from GAMA.

3) the folder [**Preparing Environmental Spatial Layers**](https://github.com/TabeaSonnenschein/Spatial-Agent-based-Modeling-of-Urban-Health-Interventions/tree/main/Preparing%20Environmental%20Spatial%20Layers) contains a set of scripts to (1)fetch point of interest data from Foursquare, (2)extract relevant data on the location of destinations from OSM, (3)fetch and process weather data, (4)extract all vector polygons that have Green space related keys of OSM, and more.

4) the folder [**NLP Knowledge Extraction and Synthesis**](https://github.com/TabeaSonnenschein/Spatial-Agent-based-Modeling-of-Urban-Health-Interventions/tree/main/NLP%20Knowledge%20Extraction%20and%20Synthesis) contains a sequence of scripts and documentation to automatically extract knowledge on significant variable from scientific articles using NLP and deep learning. The extracted knowledge is further synthesized and used to populate an ontology. It is a proof of concept method as a first step to attemp structural model validation.

5) the folder [**GAMA_Models**](https://github.com/TabeaSonnenschein/Spatial-Agent-based-Modeling-of-Urban-Health-Interventions/tree/main/ABM_models/GAMA_Models) contains simulation scripts for agent based models of urban health interventions. For now it contains a transport intervention simulation of Amsterdam.",2022-08-12
https://github.com/TabeaSonnenschein/SubwayExpansionAndAmenityMix,"Autor: Tabea Sonnenschein

These are the codes that were used to prepare, process and analyze the data for the Research Project (my Master Thesis):

### Sonnenschein, T, Scheider, S., Zheng, S. (2021). THE REBIRTH OF URBAN SUBCENTERS - How Subway Expansion Impacts the Spatial Structure and Mix of Amenities in European Cities. Environment and Planning B: Urban Analytics and City Science.

ABSTRACT: Why do some neighborhoods thrive, and others do not? While the importance of the local amenity mix has been established as a key determinant of local livability, its link to urban transport infrastructure remains understudied, partially due to a lack of data. Using spatiotemporal social media data from Foursquare, we analyze the impact of metro stations which opened between 2014 and 2017 on the amenity mix of surrounding neighborhoods in nine European cities: Rome, Milan, Barcelona, Budapest, Warsaw, Sofia, Vienna, Helsinki and Stuttgart. Thereby, we study three properties of the local amenity mix: its density, multifunctionality, and the heterogeneity between amenity types. For this purpose, we propose a new measurement of multifunctionality, which calculates the entropy of the locally present amenity set incorporating the degree of similarity between amenity types. For causal inference, we use Difference-in-Difference Regression based on Propensity Score Matching and Entropy Balancing. Our findings show that in most cities, subway expansion had a significant positive impact on the local amenity density and multifunctionality and that especially the social amenities - Arts & Entertainment, Restaurants and Nightlife - responded strongly. Moreover, considerable agglomeration forces seem to prevail, causing existing subcenters to benefit most from new metro stations.

KEYWORDS: Subway Expansion, Location Theory, Consumer Amenities, Multifunctionality, Similarity-based Diversity, Spatiotemporal Analysis
",2022-08-12
https://github.com/tdprice/storm,"# storm
Things for dealing with the Egmond storm data
",2022-08-12
https://github.com/terrymyc/terrymyc,"### Hi there 👋

I'm a maintainer of [ASReview LAB](https://github.com/asreview/asreview).

- 🔭 I’m currently working on the design and development of the new interactive interface for the upcoming milestone 1.0 release of ASReview LAB.
- :star: Featured pull requests:
  - [Redesign the interface for creating a new project](https://github.com/asreview/asreview/pull/852)
  - [Redesign project analytics page](https://github.com/asreview/asreview/pull/803)
  - [Quick buttons on project table of home page](https://github.com/asreview/asreview/pull/811)
<!-- - 🌱 I’m currently learning ... -->
<!-- - 👯 I’m looking to collaborate on ... -->
<!-- - 🤔 I’m looking for help with ... -->
<!-- - 💬 Ask me about ... -->
- 📫 How to reach me: [twitter.com/terrymyc](https://twitter.com/terrymyc)
<!-- - 😄 Pronouns: ... -->
<!-- - ⚡ Fun fact: ... -->
",2022-08-12
https://github.com/thomvolker/bayesian_statistics,"# bayesian_statistics
Repository for MSBBSS course Bayesian Statistics
",2022-08-12
https://github.com/thomvolker/bes_master_thesis_ms,"# Volker & Klugkist “Combining support for hypotheses over heterogeneous studies with Bayesian Evidence Synthesis: A simulation study”

This repository contains all code (and results) of the simulations. This
repository consists of five folders.

| Folder            | Content                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|:------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| manuscript        | Contains all files belonging to the manuscript <br> - References in `thesis.bib` <br> - Stylesheet for Journal of Mathematical Psychology <br> - Text and code to create figures in `manuscript_volker.Rmd` <br> - Output document `manuscript_volker.pdf` <br> - Required `Latex` packages in `preamble.tex` <br> - Required frontpage in `frontmatter.tex`                                                                                                                                                                                                                    |
| scripts           | All scripts of the simulations. <br> `00_run_simulations.R` - This script loads all required packages (install first, if required) and runs all individual simulation scripts. <br> `[simulation-number]_[simulation-title].R` - All individual simulation scripts are numbered such that they correspond to the simulations in the paper. <br> `functions.R` - Is a separate file that contains functions to simulate data and obtain results. <br> Note that the code to create the figures and tables in the paper are specified in the `.Rmd` file `manuscript_volker.Rmd`. |
| output            | `[simulation-number]_[simulation-title].RData` - Output of each simulation (data.frame with all simulation outcomes).                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| DataCpp           | Contains the self-written package `DataCpp` which is required to simulate multivariate normal data in `C++` when distributing the simulations over different cores (using `R`-packages `future` and `furrr`)                                                                                                                                                                                                                                                                                                                                                                    |
| notes-other-files | Rather self explanatory, this folder contains random files and thoughts that developed somewhere during the project (this ranges from presentations about the topic, the initial project proposal, an intermediate report and some meeting notes).                                                                                                                                                                                                                                                                                                                              |

All data used in this paper is simulated, and is thus not affected by
any privacy or confidentiality concerns. Ethical approval has been
granted by the FETC at Utrecht University (application number 20-0116).

The archive is stored on
[GitHub](https://github.com/thomvolker/bes_master_thesis_ms) to ensure
that all materials are and remain openly accessible.

# Machine and package info

    ─ Session info ───────────────────────────────────────────────────────────────
     setting  value                       
     version  R version 4.1.0 (2021-05-18)
     os       macOS Big Sur 10.16         
     system   x86_64, darwin17.0          
     ui       X11                         
     language (EN)                        
     collate  en_US.UTF-8                 
     ctype    en_US.UTF-8                 
     tz       Europe/Amsterdam            
     date     2022-05-15                  

    ─ Packages ───────────────────────────────────────────────────────────────────
     package       * version    date       lib source                             
     assertthat      0.2.1      2019-03-21 [1] CRAN (R 4.1.0)                     
     backports       1.4.1      2021-12-13 [1] CRAN (R 4.1.0)                     
     bain          * 0.2.4      2020-03-09 [1] CRAN (R 4.1.0)                     
     BFpack        * 0.3.2      2021-02-02 [1] CRAN (R 4.1.0)                     
     boot            1.3-28     2021-05-03 [1] CRAN (R 4.1.0)                     
     broom           0.7.12     2022-01-28 [1] CRAN (R 4.1.2)                     
     cellranger      1.1.0      2016-07-27 [1] CRAN (R 4.1.0)                     
     cli             3.2.0      2022-02-14 [1] CRAN (R 4.1.2)                     
     codetools       0.2-18     2020-11-04 [1] CRAN (R 4.1.0)                     
     colorspace      2.0-3      2022-02-21 [1] CRAN (R 4.1.2)                     
     crayon          1.5.0      2022-02-14 [1] CRAN (R 4.1.2)                     
     DataCpp       * 1.0        2022-05-04 [1] local                              
     DBI             1.1.1      2021-01-15 [1] CRAN (R 4.1.0)                     
     dbplyr          2.1.1      2021-04-06 [1] CRAN (R 4.1.0)                     
     digest          0.6.29     2021-12-01 [1] CRAN (R 4.1.0)                     
     dplyr         * 1.0.8      2022-02-08 [1] CRAN (R 4.1.2)                     
     ellipsis        0.3.2      2021-04-29 [1] CRAN (R 4.1.0)                     
     evaluate        0.15       2022-02-18 [1] CRAN (R 4.1.2)                     
     extraDistr      1.9.1      2020-09-07 [1] CRAN (R 4.1.0)                     
     fansi           1.0.3      2022-03-24 [1] CRAN (R 4.1.0)                     
     fastmap         1.1.0      2021-01-25 [1] CRAN (R 4.1.0)                     
     forcats       * 0.5.1      2021-01-27 [1] CRAN (R 4.1.0)                     
     fs              1.5.2      2021-12-08 [1] CRAN (R 4.1.0)                     
     furrr         * 0.2.3.9000 2021-10-11 [1] Github (DavisVaughan/furrr@4068c95)
     future        * 1.22.1     2021-08-25 [1] CRAN (R 4.1.0)                     
     generics        0.1.2      2022-01-31 [1] CRAN (R 4.1.2)                     
     ggplot2       * 3.3.5      2021-06-25 [1] CRAN (R 4.1.0)                     
     globals         0.14.0     2020-11-22 [1] CRAN (R 4.1.0)                     
     glue            1.6.2      2022-02-24 [1] CRAN (R 4.1.2)                     
     gtable          0.3.0      2019-03-25 [1] CRAN (R 4.1.0)                     
     haven           2.4.1      2021-04-23 [1] CRAN (R 4.1.0)                     
     hms             1.1.1      2021-09-26 [1] CRAN (R 4.1.0)                     
     htmltools       0.5.2      2021-08-25 [1] CRAN (R 4.1.0)                     
     httr            1.4.2      2020-07-20 [1] CRAN (R 4.1.0)                     
     jsonlite        1.8.0      2022-02-22 [1] CRAN (R 4.1.2)                     
     knitr           1.38       2022-03-25 [1] CRAN (R 4.1.2)                     
     lattice         0.20-44    2021-05-02 [1] CRAN (R 4.1.0)                     
     lavaan          0.6-8      2021-03-10 [1] CRAN (R 4.1.0)                     
     lifecycle       1.0.1      2021-09-24 [1] CRAN (R 4.1.0)                     
     listenv         0.8.0      2019-12-05 [1] CRAN (R 4.1.0)                     
     lme4            1.1-27.1   2021-06-22 [1] CRAN (R 4.1.0)                     
     lubridate       1.7.10     2021-02-26 [1] CRAN (R 4.1.0)                     
     magrittr      * 2.0.3      2022-03-30 [1] CRAN (R 4.1.2)                     
     MASS            7.3-54     2021-05-03 [1] CRAN (R 4.1.0)                     
     Matrix          1.3-3      2021-05-04 [1] CRAN (R 4.1.0)                     
     minqa           1.2.4      2014-10-09 [1] CRAN (R 4.1.0)                     
     mnormt          2.0.2      2020-09-01 [1] CRAN (R 4.1.0)                     
     modelr          0.1.8      2020-05-19 [1] CRAN (R 4.1.0)                     
     munsell         0.5.0      2018-06-12 [1] CRAN (R 4.1.0)                     
     mvtnorm         1.1-2      2021-06-07 [1] CRAN (R 4.1.0)                     
     nlme            3.1-152    2021-02-04 [1] CRAN (R 4.1.0)                     
     nloptr          1.2.2.2    2020-07-02 [1] CRAN (R 4.1.0)                     
     parallelly      1.28.1     2021-09-09 [1] CRAN (R 4.1.0)                     
     pbivnorm        0.6.0      2015-01-23 [1] CRAN (R 4.1.0)                     
     pillar          1.7.0      2022-02-01 [1] CRAN (R 4.1.2)                     
     pkgconfig       2.0.3      2019-09-22 [1] CRAN (R 4.1.0)                     
     pracma          2.3.3      2021-01-23 [1] CRAN (R 4.1.0)                     
     purrr         * 0.3.4      2020-04-17 [1] CRAN (R 4.1.0)                     
     R6              2.5.1      2021-08-19 [1] CRAN (R 4.1.0)                     
     Rcpp          * 1.0.8.3    2022-03-17 [1] CRAN (R 4.1.2)                     
     RcppArmadillo * 0.11.0.0.0 2022-04-04 [1] CRAN (R 4.1.2)                     
     readr         * 2.1.0      2021-11-11 [1] CRAN (R 4.1.0)                     
     readxl          1.3.1      2019-03-13 [1] CRAN (R 4.1.0)                     
     reprex          2.0.0      2021-04-02 [1] CRAN (R 4.1.0)                     
     rlang           1.0.2      2022-03-04 [1] CRAN (R 4.1.0)                     
     rmarkdown       2.14       2022-04-25 [1] CRAN (R 4.1.0)                     
     rstudioapi      0.13       2020-11-12 [1] CRAN (R 4.1.0)                     
     rvest           1.0.1      2021-07-26 [1] CRAN (R 4.1.0)                     
     scales          1.1.1      2020-05-11 [1] CRAN (R 4.1.0)                     
     sessioninfo     1.1.1      2018-11-05 [1] CRAN (R 4.1.0)                     
     stringi         1.7.6      2021-11-29 [1] CRAN (R 4.1.0)                     
     stringr       * 1.4.0      2019-02-10 [1] CRAN (R 4.1.0)                     
     tibble        * 3.1.6      2021-11-07 [1] CRAN (R 4.1.0)                     
     tidyr         * 1.2.0      2022-02-01 [1] CRAN (R 4.1.2)                     
     tidyselect      1.1.2      2022-02-21 [1] CRAN (R 4.1.2)                     
     tidyverse     * 1.3.1      2021-04-15 [1] CRAN (R 4.1.0)                     
     tmvnsim         1.0-2      2016-12-15 [1] CRAN (R 4.1.0)                     
     tzdb            0.2.0      2021-10-27 [1] CRAN (R 4.1.0)                     
     utf8            1.2.2      2021-07-24 [1] CRAN (R 4.1.0)                     
     vctrs           0.3.8      2021-04-29 [1] CRAN (R 4.1.0)                     
     withr           2.5.0      2022-03-03 [1] CRAN (R 4.1.0)                     
     xfun            0.30       2022-03-02 [1] CRAN (R 4.1.2)                     
     xml2            1.3.3      2021-11-30 [1] CRAN (R 4.1.0)                     
     yaml            2.3.5      2022-02-21 [1] CRAN (R 4.1.2)                     

    [1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library
",2022-08-12
https://github.com/thomvolker/bes_master_thesis_sasr,"# Volker, Buskens & Raub “The future is made today: Concerns for reputation foster trust and cooperation”

This repository contains all data and code to reproduce the analyses.
Per data set, one script is used for reproducing the original findings
and analyzing the data for the current paper. This folder contains four
subdirectories, of which more information is included below. The data
sets are included in the folder `data`, the analysis scripts and results
objects are in the folder `R`. All objects are named such that they
correspond to the study of which the results emerged.

| Folder      | Content                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
|:------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| thesis      | Contains all files belonging to the manuscript <br> - References in `thesis.bib` <br> - Text and code to create tables in `manuscript_volker.Rmd` <br> - Output document `manuscript_volker.pdf` <br> - Required `Latex` packages in `preamble.tex` <br> - Required frontpage in `frontmatter.tex`                                                                                                                                                                               |
| R           | All scripts to obtain the individual study results. <br> `[study].R` - These scripts reproduce the original findings (sometimes, `Stata` is required. If this is the case, the lines that should be run in `Stata` are commented out in the `R` code. <br> `create-tables.R` - Aggregates the results of the individual studies and reproduces the Tables in the manuscript. <br> - The folder `results` contains the workspaces resulting from the individual analysis scripts. |
| data        | Contains the data files that I obtained from the authors to reproduce these analyses.                                                                                                                                                                                                                                                                                                                                                                                            |
| other-files | Rather self explanatory, this folder contains random files and thoughts that developed somewhere during the project (this ranges from presentations about the topic, the initial project proposal, an intermediate report and some meeting notes).                                                                                                                                                                                                                               |

All data used in this paper stems from a secondary source. Some authors
published the original data on an online repository, while other authors
transferred the data through email. Because these secondary sources are
used, data is not shared with open access, but I will contact the
authors to ask whether the data can be made freely accessible. Ethical
approval has been granted by the FETC at Utrecht University (application
number 21-1922).

# Machine and package info

    ─ Session info ───────────────────────────────────────────────────────────────
     setting  value                       
     version  R version 4.1.0 (2021-05-18)
     os       macOS Big Sur 10.16         
     system   x86_64, darwin17.0          
     ui       X11                         
     language (EN)                        
     collate  en_US.UTF-8                 
     ctype    en_US.UTF-8                 
     tz       Europe/Amsterdam            
     date     2022-05-16                  

    ─ Packages ───────────────────────────────────────────────────────────────────
     package     * version  date       lib source        
     assertthat    0.2.1    2019-03-21 [1] CRAN (R 4.1.0)
     bain        * 0.2.4    2020-03-09 [1] CRAN (R 4.1.0)
     bdsmatrix     1.3-4    2020-01-13 [1] CRAN (R 4.1.0)
     boot          1.3-28   2021-05-03 [1] CRAN (R 4.1.0)
     cli           3.2.0    2022-02-14 [1] CRAN (R 4.1.2)
     collapse      1.7.6    2022-02-11 [1] CRAN (R 4.1.2)
     colorspace    2.0-3    2022-02-21 [1] CRAN (R 4.1.2)
     crayon        1.5.0    2022-02-14 [1] CRAN (R 4.1.2)
     DBI           1.1.1    2021-01-15 [1] CRAN (R 4.1.0)
     digest        0.6.29   2021-12-01 [1] CRAN (R 4.1.0)
     dplyr       * 1.0.8    2022-02-08 [1] CRAN (R 4.1.2)
     ellipsis      0.3.2    2021-04-29 [1] CRAN (R 4.1.0)
     evaluate      0.15     2022-02-18 [1] CRAN (R 4.1.2)
     extraDistr    1.9.1    2020-09-07 [1] CRAN (R 4.1.0)
     fansi         1.0.3    2022-03-24 [1] CRAN (R 4.1.0)
     fastmap       1.1.0    2021-01-25 [1] CRAN (R 4.1.0)
     Formula       1.2-4    2020-10-16 [1] CRAN (R 4.1.0)
     generics      0.1.2    2022-01-31 [1] CRAN (R 4.1.2)
     ggplot2     * 3.3.5    2021-06-25 [1] CRAN (R 4.1.0)
     glue          1.6.2    2022-02-24 [1] CRAN (R 4.1.2)
     gtable        0.3.0    2019-03-25 [1] CRAN (R 4.1.0)
     htmltools     0.5.2    2021-08-25 [1] CRAN (R 4.1.0)
     knitr         1.38     2022-03-25 [1] CRAN (R 4.1.2)
     lattice       0.20-44  2021-05-02 [1] CRAN (R 4.1.0)
     lavaan        0.6-8    2021-03-10 [1] CRAN (R 4.1.0)
     lifecycle     1.0.1    2021-09-24 [1] CRAN (R 4.1.0)
     lme4        * 1.1-27.1 2021-06-22 [1] CRAN (R 4.1.0)
     lmtest      * 0.9-38   2020-09-09 [1] CRAN (R 4.1.0)
     magrittr    * 2.0.3    2022-03-30 [1] CRAN (R 4.1.2)
     MASS          7.3-54   2021-05-03 [1] CRAN (R 4.1.0)
     Matrix      * 1.3-3    2021-05-04 [1] CRAN (R 4.1.0)
     maxLik        1.5-2    2021-07-26 [1] CRAN (R 4.1.0)
     minqa         1.2.4    2014-10-09 [1] CRAN (R 4.1.0)
     miscTools     0.6-26   2019-12-08 [1] CRAN (R 4.1.0)
     mnormt        2.0.2    2020-09-01 [1] CRAN (R 4.1.0)
     munsell       0.5.0    2018-06-12 [1] CRAN (R 4.1.0)
     nlme          3.1-152  2021-02-04 [1] CRAN (R 4.1.0)
     nloptr        1.2.2.2  2020-07-02 [1] CRAN (R 4.1.0)
     pbivnorm      0.6.0    2015-01-23 [1] CRAN (R 4.1.0)
     pillar        1.7.0    2022-02-01 [1] CRAN (R 4.1.2)
     pkgconfig     2.0.3    2019-09-22 [1] CRAN (R 4.1.0)
     plm         * 2.6-1    2022-03-05 [1] CRAN (R 4.1.2)
     purrr       * 0.3.4    2020-04-17 [1] CRAN (R 4.1.0)
     R6            2.5.1    2021-08-19 [1] CRAN (R 4.1.0)
     rbibutils     2.2.7    2021-12-07 [1] CRAN (R 4.1.0)
     Rcpp          1.0.8.3  2022-03-17 [1] CRAN (R 4.1.2)
     Rdpack        2.2      2022-03-19 [1] CRAN (R 4.1.0)
     rlang         1.0.2    2022-03-04 [1] CRAN (R 4.1.0)
     rmarkdown     2.14     2022-04-25 [1] CRAN (R 4.1.0)
     rstudioapi    0.13     2020-11-12 [1] CRAN (R 4.1.0)
     sandwich    * 3.0-1    2021-05-18 [1] CRAN (R 4.1.0)
     scales        1.1.1    2020-05-11 [1] CRAN (R 4.1.0)
     sessioninfo   1.1.1    2018-11-05 [1] CRAN (R 4.1.0)
     stringi       1.7.6    2021-11-29 [1] CRAN (R 4.1.0)
     stringr       1.4.0    2019-02-10 [1] CRAN (R 4.1.0)
     tibble        3.1.6    2021-11-07 [1] CRAN (R 4.1.0)
     tidyr       * 1.2.0    2022-02-01 [1] CRAN (R 4.1.2)
     tidyselect    1.1.2    2022-02-21 [1] CRAN (R 4.1.2)
     tmvnsim       1.0-2    2016-12-15 [1] CRAN (R 4.1.0)
     trend       * 1.1.4    2020-09-17 [1] CRAN (R 4.1.0)
     utf8          1.2.2    2021-07-24 [1] CRAN (R 4.1.0)
     vctrs         0.3.8    2021-04-29 [1] CRAN (R 4.1.0)
     withr         2.5.0    2022-03-03 [1] CRAN (R 4.1.0)
     xfun          0.30     2022-03-02 [1] CRAN (R 4.1.2)
     yaml          2.3.5    2022-02-21 [1] CRAN (R 4.1.2)
     zoo         * 1.8-9    2021-03-09 [1] CRAN (R 4.1.0)

    [1] /Library/Frameworks/R.framework/Versions/4.1/Resources/library
",2022-08-12
https://github.com/thomvolker/dav,"[![Build Status](https://travis-ci.com/rstudio/bookdown-demo.svg?branch=master)](https://travis-ci.com/rstudio/bookdown-demo)

This gitbook contains lectures, practicals and a summary of the required readings for the course data analysis and visualization. 
",2022-08-12
https://github.com/thomvolker/SDA,"# Survey Data Analysis
Github repository for the first year's M&amp;S ReMa course Survey Data Analysis
",2022-08-12
https://github.com/thomvolker/simulate,"This is a minimal example of a book based on R Markdown and **bookdown** (https://github.com/rstudio/bookdown). Please see the page ""Get Started"" at https://bookdown.org/home/about/ for how to compile this example.
",2022-08-12
https://github.com/thomvolker/synthesizeR,"# README
",2022-08-12
https://github.com/thomvolker/Website_Thom,"# Personal webpage Thom Volker
This repository contains all content created for my personal webpage. The website itself can be accessed on [thomvolker.github.io](https://thomvolker.github.io).
",2022-08-12
https://github.com/UiL-OTS-labs/babex,"# babex
The babex baby experiment system
",2022-08-12
https://github.com/UiL-OTS-labs/babylab-admin,"# Babylab Admin

Babylab administration and data warehouse system, written in CodeIgniter.

## Introduction
This CI project is a custom-tailored portal used to keep track of participants, experiments and appointments for the 
babylab, part of the UiL OTS Labs.

## Requirements

- PHP 7.1+ 
- PHP PDO (mysql) extension
- Apache 2
- MySQL(-like) database

This application integrates with a LimeSurvey installation, but this functionality is disabled by default.
If you wish to use this, you have to supply a LimeSurvey 1.x installation. (LimeSurvey 2 has not been tested)

## Installation

Installation is quite straightforward, but needs a functional Apache 2 environment. On request, a puppet module can be 
provided for automated deployments.

### Setting up your code
- Clone this repo to the root of the webserver folder*
- Edit `config.php` (in application/config) to reflect your setup
- Create a file in application/config called `email.php` based upon the `email_default.php` in the same folder. Edit 
this file for your own setup.
- Repeat the last step for `database.php`

If you already have a database set up and up-to-date, you should be ready to go. If you do not, follow the instructions below.

### Setting up your database
- Import `babylab_schema_20140722.sql` into your database. (Skip this if you only need to update your database).
- Enable the migrate controller by commenting out `show_error` in `application/controllers/migrate.php`.
- Open your browser, and navigate to `$host/index.php/migrate/install`
  - You might need to temporarily disable the session library. You can do this by removing `session` from 
`application/config/autoload.php`. Remember to re-enable this library when you're done.
  - If you still encounter errors, try to manually install each migration on it's own by navigating to 
`$host/index.php/migrate/version/$id`
- When you're done, git-revert `migrate.php` to disable migrations (and to make sure you don't commit the enabled file ;-) )

### Adding a user account
Go to your database, and add a user entry to the users table. The password needs to be hashed with BCrypt. You can 
generate one on a command line with the following command:

`php -r 'echo password_hash(""PASSWORD HERE"", PASSWORD_BCRYPT);'`

### Enabling limesurvey
- Open `database.php`
- Make a new database entry (you can copy paste the `$db['default']` database), and call it `survey`.
- Fill in your limesurvey database details.
- Fill in the following constants defined in `database.php`:
  - LS_BASEURL: This is the place where you would find the index page for your limesurvey install
  - SURVEY_DEV_MODE: Set this to false. 

## Language

The main language of this web application is Dutch, as it's aimed towards the mostly avid Dutch-speaking researchers of Utrecht University.
However, there is a full English translation available.

Translations in other languages are possible, of course. Simply make a new folder under `application/language` for the
desired language with the translated files in it. 

## ",2022-08-12
https://github.com/UiL-OTS-labs/BASH-db-synchronise.sh,"# BASH-db-synchronise.sh
Script meant to be used to synchronise research-database for offline systems to the online backup server.
",2022-08-12
https://github.com/UiL-OTS-labs/beepbeep,"# beepbeep

Generate short beeps to dummy test experiments that would use lengthy wav 
fragments.

Python 3 code to generate a lot of short wav files with beeps according to some 
naming convention, so one can easily create a 'dummy experiment' for testing/
debugging.

## Background and rationale

Consider the case of Wil de Bras (fake person). Wil is a linguistics master 
student, who wants to run an experiment in which EEG/ERP signals may or may not
be found to be different based upon some deep language logic. Let's say
sense making language vs non sense making context sentences.

In order to create context, some rather long wav files have been prepared, and
all kinds of conditions have been matched. A trial consists of some 
concatenation of different wav files. One trial can easily take 30 
seconds to play, and in order to have enough power, a total of 504 different, 
wav files have to be in the experiment, all nicely counterbalanced, 
block designed, matched and pseudorandomised, the whole enchilada. 

The exeriment runs in, say, neurobs Presentation.

Let's say we've come up with an experiment with the following type of 
file names:

- 60 wavs of type a
- 60 wavs of type b
- 60 wavs of type c
- 60 wavs of type d

- 66 wavs of type fa
- 66 wavs of type fb
- 66 wavs of type fc
- 66 wavs of type fd

- 2 wavs of type pa
- 2 wavs of type pb
- 2 wavs of type pc
- 2 wavs of type pd

All have nicely been recorded and by hand named to be used programatically in 
Presentation, so all types are in the wav filename, e.g.

Type A yields:

	a01.wav
	a02.wav
	a03.wav
	...
	a58.wav
	a59.wav
	a60.wav

Type B yields:

	b01.wav
	b02.wav
	b03.wav
	...
	b58.wav
	b59.wav
	b60.wav

And so on...

### Questions

- How long would it take to (tech) pilot the entire experiment and check if all
  items are randomised correctly?
- How long would it take to find a bug in the code that only happens every now 
  and then, due to some specifics in randomisation? 
- What if you just want to check if the different wav *types* are nicely 
  generated to form trials, instead of having to judge this by linguistic 
  content?

The anwser is, you (and your technician) have arrived in ~~hell~~. You have to 
run the entire experiment of more than one hour at least 10 times to test under 
what condition some bug might appear and spend an entire day listening to your
lengthy sentences.

## Solution: short beeps
   
For this reason, beep.py has been created. If you know how to make a custom 
dictionary, you can easily adapt this to your type/token condition needs and
build you own dummy version of the experiment with simple, short beeps instead 
of long, complex sentences with one command in a python shell.

# How to use

This is not aimed at single purpose production use, you will have to create your
own dictionary. Or your local techie might be able to help you out.

If you would like the amount of files and types exactly like Wil de Bras, you
can clone or download this repository, make sure you have the right permission,
fire up Konsole, Terminal, or some windows python app and for instance, type:

ipython3
Python 3.7.2 (default, Jan 13 2019, 12:50:15)
Type 'copyright', 'credits' or 'license' for more information
IPython 6.5.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import beep as b
In [2]: b.make_all_beeps()

(TODO for specific UiL OTS labs situation)




",2022-08-12
https://github.com/UiL-OTS-labs/beexybox-python,"# The python interface to the BeexyBox SDK

This contains a python module that loads the libbeexybox shared object. After
that the library is loaded all functions are translated to functions callable
by python. This yields a non-pythonic interface to a beexybox.
The library hence contains a number of classes that provide a better pythonic
interface where the BeexyBox is a type and events are handled via callable
classes.

## To Do
After there is an official package to the beexybox
",2022-08-12
https://github.com/UiL-OTS-labs/det_hab_crit,"# det_hab_crit
Determine the habituation criterion

This is a very small utility that determines at which trial infants should have been counted as habituated
",2022-08-12
https://github.com/UiL-OTS-labs/EDFPlusChecker,"# EDFPlusChecker
Tool to check EDF+ files against Presentation Log files and check, report and/or potentially fix any inconsistencies.

REQUIREMENTS:
Windows OS with .NET 4.5 Environment or higher.

This software is distributed as an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
See the License for the specific language governing permissions and limitations under the License.

To build, make the EDFPlusCheckerGUI the startup project and EDFPlusChecker.GraphicalUserINterface.App the startup object. Cheers!
",2022-08-12
https://github.com/UiL-OTS-labs/eeg-fft,"# eeg-fft
Read edf/bdf files and perform discrete fourier transform to identify dominant
frequencies in the signal. The utility consists of an edf/bdf reader edf.py and
a plotting program: plot-signal.py
The plotting program takes currently at least two arguments. The first is the
name of the edf file, and the remaining shall be integer values that is a valid
zero base index in the edf/bdf file.

So for example you can run the program like:
```console
./fft.py sample-data/note440hz.bdf 0 1
```
Then a plot of the channels 1 and 2 is displayed and the result of the fourier
transform is displayed for these channels of this file.

note440hz is a file that is a EEG recording with all electrodes in a little bit
salty water. Simultaneously a 440 Hz note was played via a shielded active studio
monitor. Another file passive-440.bdf is a recording while a stimulus of 440 hz
is played via a unshielded passive speaker. In this case you can see an
additional peak.

# 440 Hz tone with magnetically shielded active monitor
In this picture you can see a clear peak at 50 Hz, which is the mains frequency in
the Netherlands. Additionally you can see some peak at a number of harmonics of
the 50 Hz frequency. 440 hz is not a multiple of 50 so that can't be an harmonic.
At 440Hz there is not realy a component visible.
![440 Hz active monitor](./output/fft-shielded-note-440Hz.png)
# 440 Hz tone with passive monitor
In contrast with the shielded active monitor there is a clear peak in the fft at
440 Hz. So this indicates that running an experiment with those passive monitors
you pollute the EEG with the signal that is played at the sound monitors.

![440 Hz active monitor](./output/fft-passive-note-440Hz.png)

# Take home message
When doing EEG, prevent the use of unshielded speakers, for they or the cables
will introduce artifacts in your signal.
",2022-08-12
https://github.com/UiL-OTS-labs/essay-annotation,"# essay-annotation
Parses essay annotation into FoLiA XML

This project contains scripts to convert Word documents with a specific markup (called essay annotation) to [FoLiA XML](https://proycon.github.io/folia/).
After the conversion is complete, there are scripts available to output the results to .csv- or .html-format.
You can find details on the scripts below.

## Preprocessing

### Word to plain text

Call `conversions/docx2txt.py` with the specified file to convert the document to plain text files.

### Plain text to FoLiA

Call `conversions/essay2xml.py` to convert the plain-text files to FoLiA.

## Conversions

### FoLiA to HTML

Call `conversions/folia2html.py` to convert the plain-text files to FoLiA.

### FoLiA to .csv

Call `conversions/xml2csv.py` to convert the plain-text files to FoLiA.

### FoLiA to .txt

Call `conversions/xml2txt.py` to convert the plain-text files to FoLiA.

",2022-08-12
https://github.com/UiL-OTS-labs/four-labs-et,"# Multi-lab baby eyetracking experiment

# Requirements
You are expected to be able to run a few commands from the command line. You
have a working version of python2 (unfortunately some of the dependencies don't
work with python3) on your path or know how to run the
python interpreter another way.

# Run the experiment
Make sure you have installed the experiment (see Install experiment below).

Open a terminal and change to the directory, that contains the python script
test.py.

You can navigate to the folder with a file explorer (nautilus or
dolphin). Right click somewhere in the folder and look for some option
that tells roughly: ""open a terminal here"".

Activate the virtual environment.

- source venv2/bin/activate

run the experiment:

- python test.py

And off you go.

# Install experiment

install source tree:

- git clone https://github.com/UiL-OTS-labs/four-labs-et.git

Change to the cloned directory

- cd four-labs-et

Create virtual python environment (use --system-site-packages in order to obtain
pylink from the system environment)

- virtualenv venv2 --system-site-packages

Activate the environment. Prior to the the next command your terminal prompt
looks roughly like:
""`duijn119@im-lab-009-02:~/zep/daan-van-renswoude/four-labs-et$`""
and after:
""`(venv2) duijn119@im-lab-009-02:~/zep/daan-van-renswoude/four-labs-et$`""
notice the prepending of ""(venv3)""

- source venv2/bin/activate

Install the required packages:
First upgrade pip (from the venv)

- pip install --upgrade pip

install packages

- pip install numpy pillow pygame python-pygaze

",2022-08-12
https://github.com/UiL-OTS-labs/fr_mw,"Experiment:
        Self-Paced Reading with Moving Window

Description:
        Purpose of this experiment is to measure word or segment reading times
        in sentences, using a self-paced word/segment revealing mechanism. 
        Participant's task is to read sentences which are presented in a 
        segment-by-segment fashion. Participant reveals next segment by hitting
        a button. RT is measured from the presentation of a segment to button-
        press. Comprehension questions are implemented. Self-paced. 
        
        Output: RT per segment.

        This particular SPR implementation uses a moving window: For each 
        segment the text in the segment is displayed at its normal position 
        in the sentence. The words outside the segment window are displayed 
        as underlines. In other words the visual structure of the whole 
        sentence is visible, but only the words in the segment window are 
        readable.
        
        This is a (no-brainer) alteration of the boilerplate with some 
        grouping/condition specifics for Marco Bril et al.
        

Author:
        Theo Veenker <theo.veenker@beexy.nl>
        Jacco van Elst <j.c.vanelst@uu.nl>

Client:
        Andrea Roijen

Supervisor:
        -
        Dr M. (Marco) Bril

References:
        Just, M. A., Carpenter, P. A., & Woolley, J. D. 1982. 
          Paradigms and processes in reading comprehension. 
          Journal of Experimental Psychology: General, Vol 111: 228-238.


For information on running the experiment and extracting the experiment
results please go the the Zep website at http://www.beexy.nl/zep and check 
out the documentation section. There you'll also find explanations and 
instructions that help you understand and modify a Zep experiment.


DISCLAIMER

This experiment script is released under the terms of the GNU General Public
License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in
the hope that it will be useful, but with absolutely no warranty. It is your
responsibility to carefully study and test the script before using it with 
real participants.
",2022-08-12
https://github.com/UiL-OTS-labs/geyetracker,"# geyetracker
gobject bindings for an eyetracker
",2022-08-12
https://github.com/UiL-OTS-labs/iSpector,"# iSpector
Tool to inspect, extract and analyze eye movement data

## Depencies
iSpector is dependent on a number of python packages in order to run:
1. matplotlib
2. python-qt4 (PyQt4)
3. numpy
4. scipy

## iSpector file structure
This describes the directory layout.
* data/
    * Contains sample data to analyse with iSpector
* eyelog/
    * Contains python files to open/save eye movement log files.
* gui/
    * Contains python files for the gui of iSpector
* iSpector-installer.nsi (windows nsis scrip)
* iSpector.spec (spec file to be used with pyinstaller)
* images/
    * Contains python files for the gui of iSpector
* logo
    * The beautiful iSpector logo. Made in 15 minutes ;-)
* utils
    *  Contains python script for numerous utilities.
",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-audlexdec,"# jspsych-audlexdec
Auditory [Lexical Decision](https://en.wikipedia.org/wiki/Lexical_decision_task) Experiment (template)

# Generic documentation
Please read the [generic documentation](https://github.com/UiL-OTS-labs/jspsych-uil-template-docs) for some context and scope.

# Task description
The most basic auditory lexical decision task: the participant sees a fixation cross and hears a real word or a non existing word (non-word). The task is to respond as quickly as possible and indicate wether the heard word is a real word or not.

Crucial trial phases (sub trial phases):
- Fixation cross
- Auditory item (Decision phase)

### Reference:
        Rubenstein, H., Garfield, L., & Millikan, J.A. (1970). 
        Homographic entries in the internal lexicon. 
        Journal of Verbal Learning and Verbal Behavior, 9, 487≠494.

# Output
The data of _all_ (sub) _trial phases_ are logged in the data, but the output data can be filtered after data collection in many ways.
Please read the [general primer on jsPsych's data](https://github.com/UiL-OTS-labs/jspsych-output) if you are new to jsPsych data output.

Essential output for the _'true experimental'_ purpose in this template are:

- Reaction Time (RT) of the keyboard response in the decision phase
- Correctness of the keyboard response in the decision phase

The crucial trial/sub-trial phase (decision phase) output may look similar to this:

```json
	{
		""rt"": 1335,
		""stimulus"": ""./sounds/hot.wav"",
		""key_press"": 65,
		""condition"": ""REAL_WORD"",
		""word"": ""hot"",
		""filename"": ""./sounds/hot.wav"",
		""id"": 2,
		""trial_phase"": ""present_word"",
		""useful_data_flag"": true,
		""correct_response"": 1,
		""trial_type"": ""audio-keyboard-response"",
		""trial_index"": 15,
		""time_elapsed"": 36569,
		""internal_node_id"": ""0.0-8.0-1.1"",
		""subject"": ""hygb1h3q"",
		""list"": ""my_one_and_only_list"",
		""correct"": true,
		""integer_correct"": 1,
		""key_chosen_ascii"": 65,
		""key_chosen_char"": ""A"",
		""yes_key"": ""A"",
		""no_key"": ""L""
	},
	//(...)
```
Variable name (key) | Description          | Unit  | Type           | Comments                             | jsPsych default | Template default | Plugin name
--------------------|----------------------|-------|----------------|--------------------------------------|-----------------|------------------|------------
""rt""                | Reaction Time        | ms.   | float          | Reaction time in milliseconds        | yes             |                  |            
""stimulus""          | stimulus (audio file)|       | string/html    | For instance: ""./sounds/hot.wav""     | yes             |                  |
""key_press""         | Keyboard response    |       | string/object? | https://en.wikipedia.org/wiki/ASCII  | yes             |                  | html-keyboard-response/audio-keyboard-response
""condition""         | Condition            |       | string         | See ```stimuli.js```                 | no              | yes              |
""word""              | Decision phase item  |       | string/html    | String (represents ""audio"")          | no              | yes              | 
""filename""          | Decision phase item  |       | string/html    | Path to 'word' audio""                | no              | yes              | audio-keyboard-response 
""id""                | ID/code              |       |                | (See `stimuli.js` and `index.html`)  |                 | yes              |                  |
""trial_phase""       | Trial phase          |       |                | (...)                                | no              | yes              | 
""useful_data_flag""  | Filter flag          |       | boolean        |                                      | no              | yes              | 
_""expected_answer""_ | TODO change!         |       | todo           | Now (still) named ""correct_response"" | no              | no/yes/willbe    | 
""trial_type""        | What plugin was used |       |                |                                      | yes             |                  | ""audio-keyboard-response""
""trial_index""       | jsPsych index        |       |                |                            	   | yes             |                  |	
""time_elapsed"".     | jsPsych time object  | ms    | int (/float?)  | For instance: 45062                  | yes             |                  |
""internal_node_id.  | jsPsych node object  |       |                | For instance:""0.0-11.0-1.4""          | yes             |                  |
""subject""           | Subject ID           |       |                | For instance: ""8oo722dq""             |                 | yes              |
""list""              | Stimulus list name.  |       | string         | For instance: ""my_one_and_only_list"" | no              | yes              | 
""correct""           | Scoring result       |       | Boolean        | 'true or false' score of response    |                 | yes              |   
""integer_correct""   | Scoring result       |       | integer        | 1 or 0 in case of corect or incorrect| no              | yes              |   
""key_chosen_ascii""  |                      |       |                | For instance: 65                     | no              | yes              |
""key_chosen_char""   |                      |       |                | For instance: ""A""                    | no              | yes              |
""yes_key""           |                      |       |                | For instance:  ""A""                   | no              | yes              |
""no_key""            |                      |       |                | For instance: ""L""                    | no              | yes              |



# Getting started 
People _affiliated with our lab_ can use the information [from our lab webiste](https://uilots-labs.wp.hum.uu.nl/experiments/overview/) and expand the ""Online experiments using jsPsych"" section for details. Please follow [this how-to](https://uilots-labs.wp.hum.uu.nl/how-to/online-experimenting/).

### Make your experiment ready for use with the data server

### Update access key
In the file `globals.js` is a variable:
```javascript
const ACCESS_KEY = '00000000-0000-0000-0000-000000000000';
```
Before uploading your experiment to the UiL OTS data server, you will need to change this to the access_key that you obtained when your experiment was approved. For elaborate info see `globals.js`.


### Adapting stimuli
- Open the file `stimuli.js` in your plain text editor.
- There is a list, called LIST_1:

```javacript
const LIST_1 = [ // stimuli and timeline variables

```
-  This list can be adapted to your own needs, i.e, you can replace values, make the list longer (don't forget to increment the 'id' values for new items!).
- If you need to implement a more complex design, you should read the `stimuli.js` file (and its comment sections) a little better. 
- For an example of a Latin square design, please have a look [here](https://github.com/UiL-OTS-labs/jspsych-spr-mw).





  

",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-audlexdec-ap,"# jspsych-audlexdec-ap
Auditory [Lexical Decision](https://en.wikipedia.org/wiki/Lexical_decision_task) Experiment with Auditory Prime (template)

# Generic documentation
Please read the [generic documentation](https://github.com/UiL-OTS-labs/jspsych-uil-template-docs) for some context and scope.

# Task Description
One of the variations of an Auditory Lexical Decision task. In this variation, a trial consists of two connected auditory 'test items', one is an auditory _prime_ word (or non-word), the other a second word (or non-word). The words are presented as a pair. After the second word, the participant is asked to respond --as soon as possible -- with keyboard keys and indicate wether _both_ heard words are real words or not.

Crucial trial phases (sub trial phases):
- Fixation cross
- Auditory prime
- Auditory decision (decision phase)


### Reference:
        Rubenstein, H., Garfield, L., & Millikan, J.A. (1970). 
        Homographic entries in the internal lexicon. 
        Journal of Verbal Learning and Verbal Behavior, 9, 487≠494.

# Output
The data of _all_ (sub) _trial phases_ are logged in the data, but the output data can be filtered after data collection in many ways.
Please read the [general primer on jsPsych's data](https://github.com/UiL-OTS-labs/jspsych-output) if you are new to jsPsych data output.

Essential output variables are:

- Reaction Time (RT) of the keyboard response in the decision phase
- Correctness of the keyboard response in the decision phase

The crucial trial/sub-trial phase (decision phase) output may look similar to this:

```json
	{
		""rt"": 1057,
		""stimulus"": ""./sounds/hot.wav"",
		""key_press"": 65,
		""condition"": ""UNRELATED"",
		""word"": ""hot"",
		""word_file"": ""./sounds/hot.wav"",
		""prime"": ""stapler"",
		""prime_file"": ""./sounds/stapler.wav"",
		""id"": 2,
		""trial_phase"": ""present_word"",
		""useful_data_flag"": true,
		""correct_response"": 1,
		""trial_type"": ""audio-keyboard-response"",
		""trial_index"": 16,
		""time_elapsed"": 55755,
		""internal_node_id"": ""0.0-8.0-2.1"",
		""subject"": ""m1aha7y1"",
		""list"": ""my_one_and_only_list"",
		""correct"": true,
		""integer_correct"": 1,
		""key_chosen_ascii"": 65,
		""key_chosen_char"": ""A"",
		""yes_key"": ""A"",
		""no_key"": ""L""
	},
	//(...)
```
Variable name (key) | Description          | Unit  | Type           | Comments                             | jsPsych default | Template default | Plugin name
--------------------|----------------------|-------|----------------|--------------------------------------|-----------------|------------------|------------
""rt""                | Reaction Time        | ms.   | float          | Reaction time in milliseconds        | yes             |                  |            
""stimulus""          | stimulus (html)      |       | string/html    | Path to audio file                   | yes             |                  | audio-keyboard-response
""key_press""         | Keyboard response    |       | string/object? | https://en.wikipedia.org/wiki/ASCII  | yes             |                  | audio-keyboard-response
""condition""         | Condition            |       | string         | See ```stimuli.js```                 | no              | yes              |
""word""              | Decision phase item  |       | string/html    | See ```stimuli.js, index.html```     | no              | yes              | 
""word_file""         | Filename for ""word""  |       | string         | For instance ""./sounds/hot.wav"".     | no              | yes              |
""prime""             | Prime phase item     |       | string/html    | (...)                                | no              | yes              |
""prime_file""        | Filename for ""prime"" |       | string         | For instance ""./sounds/hot.wav"".     | no              | yes              |
""id""                | ID/code              |       |                | (...)                                | yes             |                  |
""trial_phase""       | Trial phase          |       |                | (...)                                | no              | yes              | 
""useful_data_flag""  | Filter flag          |       | boolean        |                                      | no              | yes              | 
_""expected_answer""_ | TODO change!         |       | todo           | Now (still) named ""correct_response"" | no              | no/yes/willbe.   | 
""trial_type""        | What plugin was used |       |                |                                      | yes             |                  | ""audio-keyboard-response""
""trial_index""       | jsPsych index        |       |                |                        	           | yes             |                  |	
""time_elapsed"".     | jsPsych time object  | ms?   | int (/float?)  | For instance: 45062                  | yes             |                  |
""internal_node_id.  | jsPsych node object  |       |                | For instance:""0.0-11.0-1.4""          | yes             |                  |
""subject""           | Subject ID           |       |                | For instance: ""8oo722dq""             |                 | yes              |
""list""              | Stimulus list name.  |       | string         | For instance: ""my_one_and_only_list"" | no              | yes              | 
""correct""           | Scoring result       |       | Boolean        | 'true or false' score of response    |                 | yes              |   
""integer_correct""   | Scoring result       |       | integer        | 1 or 0 for correct or incorrect      |                 | yes              |   
""key_chosen_ascii""  |                      |       |                | For instance: 65                     | no              | yes              |
""key_chosen_char""   |                      |       |                | For instance: ""A""                    | no              | yes              |
""yes_key""           |                      |       |                | For instance:  ""A""                   | no              | yes              |
""no_key""            |                      |       |                | For instance: ""L""                    | no              | yes              |


# Getting started 
People _affiliated with our lab_ can use the information [from our lab webiste](https://uilots-labs.wp.hum.uu.nl/experiments/overview/) and expand the ""Online experiments using jsPsych"" section for details. Please follow [this how-to](https://uilots-labs.wp.hum.uu.nl/how-to/online-experimenting/).

### Make your experiment ready for use with the data server

### Update access key
In the file `globals.js` is a variable:
```javascript
const ACCESS_KEY = '00000000-0000-0000-0000-000000000000';
```
Before uploading your experimentto the UiL-OTS data server, you will need to change this to the access_key that you obtained when your experiment was approved. For elaborate info see `globals.js`.


### Adapting stimuli
- Open the file `stimuli.js` in your plain text editor.
- There is a list, called LIST_1:

```javacript
const LIST_1 = [ // stimuli and timeline variables

```
-  This list can be adapted to your own needs, i.e, you can replace values, make the list longer (don't forget to increment the 'id' values for new items!).
- If you need to implement a more complex design, you should read the `stimuli.js` file (and its comment sections) a little better. 
- For an example of a Latin square design, please have a look [here](https://github.com/UiL-OTS-labs/jspsych-spr-mw).

",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-audlexdec-vp,"# jspsych-audlexdec-vp
Auditory [Lexical Decision](https://en.wikipedia.org/wiki/Lexical_decision_task) Experiment with _Visual Prime_ (template)

# Generic documentation
Please read the [generic documentation](https://github.com/UiL-OTS-labs/jspsych-uil-template-docs) for some context and scope.

# Task Description
Visually primed auditory lexical decision task: the participant sees a fixation cross, visual prime and then hears a real word or a non existing word (non-word). The task is to respond as quickly as possible and indicate wether the heard word is a real word or not (or wether both the visual and the heard word are real words, it depends on the instructions).

Crucial trial phases (sub trial phases):
- Fixation cross
- Visual Item (Prime)
- Auditory item (Decision phase)

### Reference:
        Rubenstein, H., Garfield, L., & Millikan, J.A. (1970).
        Homographic entries in the internal lexicon.
        Journal of Verbal Learning and Verbal Behavior, 9, 487≠494.
        
# Output
The data of _all_ (sub) _trial phases_ are logged in the data, but the output data can be filtered after data collection in many ways.
Please read the [general primer on jsPsych's data](https://github.com/UiL-OTS-labs/jspsych-output) if you are new to jsPsych data output.

Essential output for the _'true experimental'_ purpose in this template are:

- Reaction Time (RT) of the keyboard response in the decision phase
- Correctness of the keyboard response in the decision phase

The crucial trial/sub-trial phase (decision phase) output may look similar to this:

```json
{
    ""rt"": 1210,
    ""stimulus"": ""./sounds/clem.wav"",
    ""key_press"": 76,
    ""condition"": ""NON_WORD"",
    ""word"": ""clem"",
    ""word_file"": ""./sounds/clem.wav"",
    ""prime"": ""flower"",
    ""id"": 4,
    ""trial_phase"": ""present_word"",
    ""useful_data_flag"": true,
    ""expected_answer"": 0,
    ""trial_type"": ""audio-keyboard-response"",
    ""trial_index"": 25,
    ""time_elapsed"": 56913,
    ""internal_node_id"": ""0.0-12.0-2.0"",
    ""subject"": ""y82nwppg"",
    ""list"": ""my_one_and_only_list"",
    ""correct"": true,
    ""integer_correct"": 1,
    ""key_chosen_ascii"": 76,
    ""key_chosen_char"": ""L"",
    ""yes_key"": ""A"",
    ""no_key"": ""L""
},
```

Variable name (key) | Description          | Unit  | Type           | Comments                             | jsPsych default | Template default | Plugin name
--------------------|----------------------|-------|----------------|--------------------------------------|-----------------|------------------|------------
""rt""                | Reaction Time        | ms.   | float          | Reaction time in milliseconds        | yes             |                  |
""stimulus""          | stimulus (html)      |       | string/html    |                                      | yes             |                  |
""key_press""         | Keyboard response    |       | string/object? | https://en.wikipedia.org/wiki/ASCII  | yes             |                  | audio-keyboard-response
""condition""         | Condition            |       | string         | See ```stimuli.js```                 | no              | yes              |
""word""              | Decision phase item  |       | string/html    | See ```stimuli.js, index.html```     | no              | yes              |
""word_file""         | Decsion phase item   |       | string         | Path to ""word"" audio file            | no              | yes              |
""prime""             | Prime phase item     |       | string/html    | (Visual) Prime                       | no              | yes              |
""id""                | ID/code              |       |                |                                      | yes             |                  |
""trial_phase""       | Trial phase          |       |                |                                      | no              | yes              |
""useful_data_flag""  | Filter flag          |       | boolean        |                                      | no              | yes              |
""expected_answer""   |                      |       | todo           |                                      | no              | no/yes/willbe.   |
""trial_type""        | What plugin was used |       |                |                                      | yes             |                  | ""audio-keyboard-response""
""trial_index""       | jsPsych index        |       |                |                            	       | yes             |                  |
""time_elapsed"".     | jsPsych time object  | ms    | int (/float?)  | For instance: 45062                  | yes             |                  |
""internal_node_id.  | jsPsych node object  |       |                | For instance:""0.0-11.0-1.4""          | yes             |                  |
""subject""           | Subject ID           |       |                | For instance: ""8oo722dq""             |                 | yes              |
""list""              | Stimulus list name.  |       | string         | For instance: ""my_one_and_only_list"" | no              | yes              |
""correct""           | Scoring result       |       | Boolean        | 'true or false' score of response    |                 | yes              |
""integer_correct""   | Scoring result       |       | integer        | 1 or 0 for correct or incorrect      |                 | yes              |
""key_chosen_ascii""  |                      |       |                | For instance: 65                     | no              | yes              |
""key_chosen_char""   |                      |       |                | For instance: ""A""                    | no              | yes              |
""yes_key""           |                      |       |                | For instance:  ""A""                   | no              | yes              |
""no_key""            |                      |       |                | For instance: ""L""                    | no              | yes              |



# Getting started 
People _affiliated with our lab_ can use the information [from our lab webiste](https://uilots-labs.wp.hum.uu.nl/experiments/overview/) and expand the ""Online experiments using jsPsych"" section for details. Please follow [this how-to](https://uilots-labs.wp.hum.uu.nl/how-to/online-experimenting/).

### Make your experiment ready for use with the data server

### Update access key
In the file `globals.js` is a variable:
```javascript
const ACCESS_KEY = '00000000-0000-0000-0000-000000000000';
```
Before uploading your experimentto the UiL-OTS data server, you will need to change this to the access_key that you obtained when your experiment was approved. For elaborate info see `globals.js`.


### Adapting stimuli
- Open the file `stimuli.js` in your plain text editor.
- There is a list, called LIST_1:

```javacript
const LIST_1 = [ // stimuli and timeline variables

```
-  This list can be adapted to your own needs, i.e, you can replace values, make the list longer (don't forget to increment the 'id' values for new items!).
- If you need to implement a more complex design, you should read the `stimuli.js` file (and its comment sections) a little better. 
- For an example of a Latin square design, please have a look [here](https://github.com/UiL-OTS-labs/jspsych-spr-mw).

",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-audlexdec-vp-vm,"# jspsych-audlexdec-vp-vm
Auditory [Lexical Decision](https://en.wikipedia.org/wiki/Lexical_decision_task) Experiment with _Visual Masked Prime_ (template)

# Generic documentation
Please read the [generic documentation](https://github.com/UiL-OTS-labs/jspsych-uil-template-docs) for some context and scope.

# Task Description
Visually primed auditory lexical decision task with a visual mask: the participant sees a fixation cross, mask and visual prime and then hears a real word or a non existing word (non-word). The task is to respond as quickly as possible and indicate wether the heard word is a real word or not (or wether both the visual and the heard word are real words, it depends on the instructions.

Crucial trial phases (sub trial phases):
- Fixation cross
- Visual Mask ('#####')
- Visual Prime 
- Auditory item (Decision phase)

### Reference:
        Rubenstein, H., Garfield, L., & Millikan, J.A. (1970). 
        Homographic entries in the internal lexicon. 
        Journal of Verbal Learning and Verbal Behavior, 9, 487≠494.

# Output

The data of _all_ (sub) _trial phases_ are logged in the data, but the output data can be filtered after data collection in many ways.
Please read the [general primer on jsPsych's data](https://github.com/UiL-OTS-labs/jspsych-output) if you are new to jsPsych data output.

Essential output for the _'true experimental'_ purpose in this template are:

- Reaction Time (RT) of the keyboard response in the decision phase
- Correctness of the keyboard response in the decision phase

The crucial trial/sub-trial phase (decision phase) output may look similar to this:

```json
	{
		""rt"": 583,
		""stimulus"": ""./sounds/white.wav"",
		""key_press"": 65,
		""condition"": ""RELATED"",
		""word"": ""white"",
		""word_file"": ""./sounds/white.wav"",
		""prime"": ""snow"",
		""prime_mask"": ""####"",
		""id"": 5,
		""trial_phase"": ""present_word"",
		""useful_data_flag"": true,
		""correct_response"": 1,
		""trial_type"": ""audio-keyboard-response"",
		""trial_index"": 26,
		""time_elapsed"": 46171,
		""internal_node_id"": ""0.0-12.0-3.0"",
		""subject"": ""kd9tsn3y"",
		""list"": ""my_one_and_only_list"",
		""correct"": true,
		""integer_correct"": 1,
		""key_chosen_ascii"": 65,
		""key_chosen_char"": ""A"",
		""yes_key"": ""A"",
		""no_key"": ""L""
	},
	//(...)
```
Variable name (key) | Description          | Unit  | Type           | Comments                             | jsPsych default | Template default | Plugin name
--------------------|----------------------|-------|----------------|--------------------------------------|-----------------|------------------|------------
""rt""                | Reaction Time        | ms.   | float          | Reaction time in milliseconds        | yes             |                  |            
""stimulus""          | stimulus (html)      |       | string/html    | Path to audio file                   | yes             |                  |
""key_press""         | Keyboard response    |       | string/object? | https://en.wikipedia.org/wiki/ASCII  | yes             |                  | audio-keyboard-response
""condition""         | Condition            |       | string         | See ```stimuli.js```                 | no              | yes              |
""word""              | Decision phase item  |       | string/html    | See ```stimuli.js, index.html```     | no              | yes              | 
""word_file""         | Decision phase item  |       | string         | Path to audio file (reflects 'word') | no              | yes              | 
""prime""             | Prime phase item     |       | string/html    | (...)                                | no              | yes              |
""prime_mask""        | Mask item            |       | string/html    | (...)                                | no              | yes              |
""id""                | ID/code              |       |                | (...)                                | yes             |                  |
""trial_phase""       | Trial phase          |       |                | (...)                                | no              | yes              | 
""useful_data_flag""  | Filter flag          |       | boolean        |                                      | no              | yes              | 
_""expected_answer""_ | TODO change!         |       | todo           | Now (still) named ""correct_response"" | no              | no/yes/will be   | 
""trial_type""        | What plugin was used |       |                |                                      | yes             |                  | ""audio-keyboard-response""
""trial_index""       | jsPsych index        |       |                |                            	       | yes             |                  |	
""time_elapsed"".     | jsPsych time object  | ms    | int (/float?)  | For instance: 45062                  | yes             |                  |
""internal_node_id.  | jsPsych node object  |       |                | For instance:""0.0-11.0-1.4""          | yes             |                  |
""subject""           | Subject ID           |       |                | For instance: ""8oo722dq""             |                 | yes              |
""list""              | Stimulus list name.  |       | string         | For instance: ""my_one_and_only_list"" | no              | yes              | 
""correct""           | Scoring result       |       | Boolean        | 'true or false' score of response    |                 | yes              |   
""integer_correct""   | Scoring result       |       | integer        | 1 or 0 for correct or incorrect      |                 | yes              |   
""key_chosen_ascii""  |                      |       |                | For instance: 65                     | no              | yes              |
""key_chosen_char""   |                      |       |                | For instance: ""A""                    | no              | yes              |
""yes_key""           |                      |       |                | For instance:  ""A""                   | no              | yes              |
""no_key""            |                      |       |                | For instance: ""L""                    | no              | yes              |



# Getting started 
People _affiliated with our lab_ can use the information [from our lab webiste](https://uilots-labs.wp.hum.uu.nl/experiments/overview/) and expand the ""Online experiments using jsPsych"" section for details. Please follow [this how-to](https://uilots-labs.wp.hum.uu.nl/how-to/online-experimenting/).

### Make your experiment ready for use with the data server

### Update access key
In the file `globals.js` is a variable:
```javascript
const ACCESS_KEY = '00000000-0000-0000-0000-000000000000';
```
Before uploading your experimentto the UiL-OTS data server, you will need to change this to the access_key that you obtained when your experiment was approved. For elaborate info see `globals.js`.


### Adapting stimuli
- Open the file `stimuli.js` in your plain text editor.
- There is a list, called LIST_1:

```javacript
const LIST_1 = [ // stimuli and timeline variables

```
-  This list can be adapted to your own needs, i.e, you can replace values, make the list longer (don't forget to increment the 'id' values for new items!).
- If you need to implement a more complex design, you should read the `stimuli.js` file (and its comment sections) a little better. 
- For an example of a Latin square design, please have a look [here](https://github.com/UiL-OTS-labs/jspsych-spr-mw).

",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-boilerplates,"# jspsych-boilerplates
A number of boilerplate experiments for use with jsPsych
",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-cam-rec,"# jspsych-cam-rec
Online Baby Observation Experiment using [jsPysch](http://www.jspsych.org) and customised [WebRTC](https://webrtc.org/) video tools in collaboration with [ManyBabies-AtHome](https://manybabies.github.io/).

## NB: Development has concluded

We are happy to anounce that the experiment has succesfully been implemented and our results are in. Big thanks to anyone who contributed or participated! This repository will remain online for posterity and Open Science purposes. Feel free to have a look around.

## Contents
This project aims to bring baby behavior observation (and scoring) to the web.
This project contains several folders and many files:

- Folder `jspsych` contains a [jsPsych](https://www.jspsych.org) web experiment.
- Folder `webrtc` contains a basic application to record and upload a webcam stream.
- File `csv_data_reader.ipynb` is a script to extract salient moments from output data.

## webRTC (video)
In fact, the code in the folder started out as just a copy from this example:
https://github.com/webrtc/samples/tree/gh-pages/src/content/getusermedia/record/js

We added upload and token functionality to facilitate the uploading of participants' webcam recordings and have them labeled under a correct participant ID.

## Development, relations, and scope
- [This how-to](https://uilots-labs.wp.hum.uu.nl/how-to/online-experimenting/) describes the basic flow for creating a web experiment with jsPsych, given UiL OTS affiliation and the use of our custom jsPsych experiment _Data Store_.
- The following [general template/boilerplate documentation](https://github.com/UiL-OTS-labs/jspsych-uil-template-docs) may also be of help to get a sense of how UiL OTS developers have chosen to organise code/functionality in a _modular_ way.
- Note that at this time, we do not (as of yet) have a standard 'template' or 'boilerplate' for the experimental task. In fact, this codebase's folder 'jspsych' may evolve to become a 'template' experiment for presenting (sparse, moving/static) stimuli, and the related [stream recorder](https://github.com/UiL-OTS-labs/streamupload/) will probably evolve as a robust complementary application to be able to score (infant) behavior.
- Due to this heavy development and the additional complexity, the above mentioned 'UiL OTS how-to' for conducting web experiments will _not_ be identical for this repository. For instance: simply running the experiment locally by double-clicking the index.html file will probably not work, due to how many browsers deal with allowing to load and serve files from your PC/laptop without a web server and/or mixing http/https protocols. In short: you either need help from our technicial staff (or maybe a web-development-savvy friend can help you out to some extent), or you will need to invest quite a bit of time in things like using a local web server, general web-development & programming practices and learning to the 'terminal' (command line).
- Some custom jsPsych-tools (like the [UiL Utililty library](https://github.com/UiL-OTS-labs/jspsych-uil-utils)) are developed and versioned/updated at shared location(s) in our data store. So, there's quite a lot of interdependency and possible source of error. Please be _very aware_ of this!

## Mimimal case for adapting/testing integrated functionality: Python development webserver (local development)
While piloting and adapting things from this current combination of experiment and video streaming example, you will need some sort of local web-server to test things. This [python http server](https://pythonbasics.org/webserver/) example may be of help. You will have to know or learn some other things, like how to use the terminal. Note that, by default, the python http server binds to `http://0.0.0.0:8000`, but some browsers don't allow http (vs https), unless it is served from 'localhost' (127.0.0.1), so for developers, while testing locally, use this command to serve the experiment:
```
python3 -m http.server -b localhost
```
Otherwise, you'll get _at least_ an error from `navigator.mediaDevices` when using the stream recorder's fucntionality (in folder webRTC -- and referenced in the jsPsych experiment's pages that deal with the video recording). Additionally, rename the file `globals_developing.js` to simply `globals.js` if testing the experiment locally.

## Binary data upload application
See related project: [streamupload](https://github.com/UiL-OTS-labs/streamupload/)

A proof of concept webcam video uploader is included in `/WebRTC/`. It communicates with `gw-c-lab-web-binary-data-tst.im.hum.uu.nl` to retrieve experiment _tokens_ and upload binary (video) data. Experiment tokens are semi-secret keys embedded in the experiment which serve two purposes:

1. They act as a security measure to prevent anyone from dumping massive files on the binary data store.
2. They categorize the data allowing researchers to find recordings of participants from a certain study and condition.

A token has to be created manually and should be both descriptive and hard to guess. For example: `2021-Phonemes-control-ue5Ubi`, which tells us which experiment and condition we're dealing with without making it trivial to figure out. Depending on the experiment implementation they may be random strings or also be shared with a participant ID.

In the default `/WebRTC/index.html`, there is a input field to paste a token into during testing. But when running a finalized experiment, the input field can be changed to a hidden field easily by experimenters to eliminate this step. See `index_hidden_token.html` for such a template (remember to change the token manually: `change_token_here`).

Tokens can be made by only some people of the UiL-OTS lab staff. Get in touch using the UiL OTS lab website if you would like to run a pilot. We recommend using one token per experimental condition, but using one token per participant or per experiment as a whole is also possible. Or you can host your own Streamupload instance.

## Links

- Media API
https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia

- Sound library
https://www.luftrum.com/free-field-recordings/


## License

All original code is copyright (c) 2021 UiL OTS Labs, Faculty of Humanities, Utrecht University and licensed under the GPLv2. Please see `jspsych/LICENSE` for further license information.

Sounds, images, and videos used or adapted for the experiment are used with permission but may be subject to different license terms and copyright.
",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-concessions,"# jsPsych-concessions
A brief summery on the concessions we have to make in order to work with jsPsych.

# Introduction
In the lab we have very good control of the stimuli that we present using ZEP-x.y.
Zep makes it easy to assign a participant to a given group manually using the
control window. ZEP stores the data into it's database and the user is able
to extract the data to a "".csv"" file on the pc on which can be imported in a
spreatsheet program.

However, with jsPsych we lose some of the control we used to have in the lab.

Here I'm making a difference in two categories of concessions we have to make.
The ones that are fixable to a extend, we can code something together to work
around problems that are more trivially solved in the lab. In contrast there
are also problems that are really hard which I think are unsolvable.

# Concessions for which a work around might exist.
1. Currently we pick groups/lists randomly. Even with the server online it will
   be harder and less controlled than to assign a participant manually to
   a group.
2. It's harder to import .csv files (but this makes it easier in the end)
    - eeg store stimulus parameters in javascript scripts.
3. Storage and retrieval of data on uilots server is not as straightforward.

# Hard Concessions
1. Lack of control on the physical properties of the stimulus.
    - We have no control over how loud the participant sets her audio.
    - We have no prior knowledge on the display resolution and or size nor
      the brightness of the display..
    - A sentence might advance to the next line for participant 1, but not
      for participant 2.
    - etc.
2. German vs English keyboard layout, a response button might appear on a
   different location for another keyboard layout.
3. Keyboard vs touchpad (although it might seem easy to easy to ignore
   mobile devices, many laptops have touchscreens to).
   It would be interesting to see whether there is a reaction time main
   effect of mobile devices versus pc's.
4. It is harder to see whether the participant is making a cup of tea during
   the experiment.
5. Probably few participants have speciallized equipment like EEG or 
   an Eyetracker at home, even if the required jsPscych modules for
   controlling a 

# Disscusion
We lose some control over an experiment by running it online instead of the
lab. Some of the concessions that we have to make are mitigated because its easier
to measure more people online than participants in the lab. Measuring more
people might reduce the noise that exists due to the lack of control.
It depends on how much control you are willing to sacrifice in order to
run an experiment online.
",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-lexical-decision,"# jspsych-lexical-decision
This boilerplate [lexical decision](https://en.wikipedia.org/wiki/Lexical_decision_task)
experiment is designed to run multiple flavors of lexical
decision. The lexical decision may be based on an auditory or visual
stimulus. The target word may be preceded by an optionally masked prime.
This experiment is based on the previous experiments of the Uil-OTS labs:

- **jspsych-vislexdec**       lexical decision (visual target stimulus)
- **jspsych-vislexdec-vp**    lexical decision (visual target stimulus) + visual prime
- **jspsych-vislexdec-vp-vm** lexical decision (visual target stimulus) + visual masked prime
- **jspsych-audlexdec**       lexical decision (auditory target stimulus)
- **jspsych-audlexdec-ap**    lexical decision (auditory target stimulus) + auditory prime
- **jspsych-audlexdec-vp**    lexical decision (auditory target stimulus) + visual prime
- **jspsych-audlexdec-vp-vm** lexical decision (auditory target stimulus) + visual masked prime

however, it is one experiment capable of running all sub flavors above. The list
of stimuli will determine what sub flavor will be presented to the subjects of
this experiment.

# Generic documentation
Please read the [generic documentation](https://github.com/UiL-OTS-labs/jspsych-uil-template-docs)
for some context and scope.

# Task Description
Auditory or visual lexical decision task: the participant first sees a fixation
cross, this marks the onset of a trial. Then optionally an auditory or visual
prime is presented. The visual prime may be subsequently masked. Finally, the
participants hear or see a real or non-existing word (non-word). The task is
to respond as quickly as possible and indicate whether the heard word is a real
word or not.

Crucial trial phases (sub trial phases):
- Fixation cross 
- (optional) Visual Forward Mask ('#####')
- (optional) Visual or auditory Prime
- (optional) Visual Backward Mask
- Target visual or auditory stimulus (Decision phase)

This boilerplate is a short example, and primarily intended to illustrate how
this experiment works. So for the practice phase you see a lexical decision
with a forward mask and a visual prime, and the target word is also presented
on screen. In contrast, in the test phase, there is a backward mask and the
target is presented as a sound. You could interleave all different flavors if
you would want to, however, we strongly suggest making one type of trial for all
practice and test stimuli.

# Getting started
People _affiliated with our lab_ can use the information
[from our lab webiste](https://uilots-labs.wp.hum.uu.nl/experiments/overview/)
and expand the ""Online experiments using jsPsych"" section for details. Please
follow [this how-to](https://uilots-labs.wp.hum.uu.nl/how-to/online-experimenting/).

## Make your experiment ready for use with the data server
### Update access key
In the file `globals.js` is a variable:
```javascript
const ACCESS_KEY = '00000000-0000-0000-0000-000000000000';
```
Before uploading your experiment to the UiL-OTS data server, you will need to
change this to the access key that you obtained when your experiment was
approved. For elaborate info see `globals.js`.

## Adapting stimuli
- Open the file `stimuli.js` in your plain text editor.
- There is a list, called `LIST_1`:

  ```javacript
  const LIST_1 = [ // stimuli and timeline variables
  ```
  For a more elaborate example see the Adding stimuli section below.
- This list can be adapted to your own needs, i.e, you can replace values,
  make the list longer (don't forget to increment the 'id' values for new items!).
- If you need to implement a more complex design, you should read the
  `stimuli.js` file (and its comment sections) a little better.
- For an example of a Latin square design, please have a look
  [here](https://github.com/UiL-OTS-labs/jspsych-spr-mw).

### Adding stimuli
In the file stimuli.js, one needs to add stimuli to one or multiple lists.
One full description of one trial looks like this:
```javascript
const LIST_1 = [
    {
        // We recommend always to use these first tree items.
        id: 1,                      // Always use incrementing id's e.g 1, 2, etc.
        item_type: RELATED,         // Always denote the item_type/condition.
        word: ""palve"",              // Always show which word is being presented
                                    //  as target.
        expected_answer: 0,         // 0 when word is a NONword, 1 if it IS a word.

        // The next items are optional, you may leave them empty """", null, undefined
        // or simply omit them if you don't use them, leaving them out is the
        // preferred method.

        // A visual mask presented prior to the prime. You are free to omit it, then
        // no forward prime will be presented.
        forward_mask: ""#####"",

        // primes Optional you are free to omit both of them.
        visual_prime : ""prime"",     // A word presented on the screen as prime

        // an auditory prime make sure the file exists.
        auditory_prime : ""./sounds/prime.wav"",

        // Similar to the forward mask, only enter it in the stimuli when needed.
        backward_mask : ""#####"",

        // The target stimuli, you should use precisely one of the two options below.

        // A sound file containing the target word
        auditory_target : ""./sounds/palve.wav"",

        // A word that will be displayed on screen.
        visual_target : ""palve""
    }
]
```
The example above shows all options, in order to demonstrate what you would add
to the stimuli. However, consider what would happen when you provide both a
auditory and a visual target word. The experiment has to present them both
and that won't be very handy. So in a very simple visual lexical decision
without priming you can simplify the description of the stimulus list to:
```javascript
const LIST_1 = [
    {
        id: 1,
        item_type: RELATED,
        word: ""palve"",
        expected_answer: 0,
        visual_target : ""palve""
    }
]
```
So this is not as long as one might be scared of ;-).

## In case of using auditory stimuli as prime or target word
In the boilerplate experiment is a *soundtest* stimulus. This sound test is done
in order for the participant to adjust their sound settings on to a comfortable
sound level. This ensures stimuli are not presented to loud or
to soft. This ""test"" is skipped when your stimulus list does not contain
auditory stimuli.

In the `sounds` folder there is a stimulus called **""sound_test.mp3""**. The
boilerplate plays this stimulus. It is an artificial *beep* but at full
volume, hence louder than your auditory stimuli. When recording stimuli
we suggest that you also record a stimulus that is going to replace the beep.
The replacement is recorded in a similar fashion as your test stimuli and
then their sound volume will be roughly equivalent.

If you need to change the name of the stimulus, e.g. to `sound_test.wav`, you
need to make the experiment aware of this change. In the file `globals.js` is
the following variable:

```javascript
// Test stimulus name for the test audio.
const AUDIO_TEST_STIMULUS = ""./sounds/sound_test.mp3"";
```
One would alter this to become:
```javascript
// Test stimulus name for the test audio.
const AUDIO_TEST_STIMULUS = ""./sounds/sound_test.wav"";
```

# Output
The data of _all_ (sub) _trial phases_ are logged in the data, but the output
data can be filtered after data collection in many ways.
Please read the
[general primer on jsPsych's data](https://github.com/UiL-OTS-labs/jspsych-output)
if you are new to jsPsych data output.

Essential output for the _'true experimental'_ purpose in this template are:

- Reaction Time (RT) of the keyboard response in the decision phase
- Correctness of the keyboard response in the decision phase

The crucial trial/sub-trial phase (decision phase) output may look similar to
this:

```json
{
    ""rt"": 848,
    ""stimulus"": ""./sounds/white.wav"",
    ""response"": ""l"",
    ""trial_type"": ""audio-keyboard-response"",
    ""trial_index"": 54,
    ""time_elapsed"": 86637,
    ""internal_node_id"": ""0.0-12.0-6.7-0.7"",
    ""subject"": ""9mvxuzsx"",
    ""list"": ""list1"",
    ""id"": 5,
    ""condition"": ""RELATED"",
    ""word"": ""white"",
    ""expected_answer"": 1,
    ""visual_prime"": ""snow"",
    ""backward_mask"": ""####"",
    ""auditory_target"": ""./sounds/white.wav"",
    ""useful_data_flag"": true,
    ""answer"": 1,
    ""correct"": true,
    ""integer_correct"": 1,
    ""pressed_key"": ""L""
}
```

Some of the output variables may be present or absent based on the
changes you've made to the experiment, e.g. when you don't present a prime, it
won't be in the output. They have a checkmark in the conditional column.

| Variable name (key) | conditional | Description          | Unit   | Type        | Comments                                              | jsPsych default |
|---------------------|-------------|----------------------|--------|-------------|-------------------------------------------------------|-----------------|
| rt                  |             | Reaction Time        | ms.    | int         | Reaction time in milliseconds                         | yes             |
| stimulus            |             | stimulus (html)      |        | string/html | Path to audio file/ or html                           | yes             |
| response            |             | The pressed button   | letter | string/null | Denotes the key pressed/or absence.                   | yes             |
| trial_type          |             | What plugin was used |        |             | ""html-keyboard-response"" or ""audio-keyboard-response"" | yes             |
| trial_index         |             | trial_index          | count  | index       | every stimulus is a trial using jspsych               | yes             |
| time_elapsed        |             | jsPsych time object  | ms     | number      | For instance: 45062                                   | yes             |
| internal_node_id.   |             | jsPsych node object  |        |             | For instance:""0.0-11.0-1.4""                           | yes             |
| subject             |             | Subject ID           |        |             | A random string of character or id from dataserver    | no              |
| list                |             | Stimulus list name.  |        | string      | For instance: list1                                   | no              |
| id                  |             | ID/code              |        | number      | identifies a stimulus within a list                   | no              |
| condition           |             | Condition            |        | string      | See ```stimuli.js```; the condition of this trial     | no              |
| word                |             | Decision phase item  |        | string      | See ```stimuli.js``` the word in plain utf8 text      | no              |
| expected_answer     |             | 1 word or 0 non word |        | int         | Signifies what would be the correct response          | no              |
| forward_mask        | ✓           | the forward mask     |        | string      | The presented forward mask                            | no              |
| visual_prime        | ✓           | visual prime         |        | text        | A prime that is displayed on screen                   | no              |
| auditory_prime      | ✓           | auditory prime       |        | string/path | An audible prime                                      | no              |
| backward_mask       | ✓           | backward_mask        |        | string      | The presented backward mask                           | no              |
| auditory_target     | ✓           | Target word          |        | string/path | The sound file that plays the target                  | no              |
| visual_target       | ✓           | Target word          |        | string      | The target word presented on screen                   | no              |
| useful_data_flag    |             | Filter flag          |        | boolean     | may be used to filter useful data during analysis     | no              |
| answer              |             | The provided answer  |        | int         | 1 or 0 if the participant responsed word or non-word  | no              |
| correct             |             | Scoring result       |        | Boolean     | 'true or false' score of response                     | no              |
| integer_correct     |             | Scoring result       |        | integer     | 1 or 0 for correct or incorrect                       | no              |
| pressed key         |             | the key pressed      |        | string/null | An upper cased letter for true or false.              | no              |

Good luck, happy experimenting!

# Reference:
    Rubenstein, H., Garfield, L., & Millikan, J.A. (1970).
    Homographic entries in the internal lexicon.
    Journal of Verbal Learning and Verbal Behavior, 9, 487≠494.

",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-output,"# jspsych-output
A short description of the output of jsPsych

## Introduction

This is a short introduction to the output generated by jsPsych experiments. jsPsych 
is the software used by researchers in the Uil OTS labs to run online experiments. 

jsPsych comes with its internal ""database"" for the research data that is collected during one run of
the experiment. jsPsych is rather verbose in its output, and everything
that is presented to the participant is treated as a single trial. This means
that for instance the fixation cross
that starts your trial is - in the output of jsPsych - a trial of its own. This
is not ideal, but it is how jsPsych works internally.

This document explains where you can find the relevant documentation in
the jsPsych documentation to understand the output that is generated by your
experiment. We provide the relevant links and a little guidance in order to
understand the output.

## jsPsych general trial information
Each jsPsych trial is started via a jsPsych plugin. All plugins will write the
following four variables to the output:

1. **trial_type**
2. **trial_index**
3. **time_elapsed**
4. **internal_node_id**

Information can be obtained from the [data-collected-from-plugins][1] section in the
jsPsych documentation. The most significant variable is the first, **trial_type**
because that determines where you can look for the meaning of the other variables of
the same jsPsych trial. For example, in a visual lexical decision task with a visual
prime, a trial starts with a fixation cross, then a prime and then a (non-)word
again. In terms of jsPscych those are 3 trials. the cross, the prime and the
(non-)word. They are all presented with the same plugin called:
*html-keyboard-response*. In order to see the meaning of the other variables
you'll have to look up the plugin of that trial in the [list of plugins][2]. All
the plugins are prefixed with ""jspsych-"", so you'll be looking for the plugin:
*jspsych-html-keyboard-response*. Once you have found it, you need to
look for the paragraph *Data Generated*. In the panel on the right hand side
of the web page you can find a link to the section.
There are 3 variables listed:

1. **key_press**
2. **rt**
3. **stimulus**

So for the html-keyboard-response-plugin, in addition to the 4 variables above, the
**key_press**, **rt** and **stimulus** are stored in the output. As you can see,
the type and meaning of the variable are also listed in the table.

## Data added by the boilerplates of the UiL OTS labs
Scripts using jsPsych can add some own data to each trial. The UiL OTS
boilerplate experiments also add some variables in the hope they are useful for
analyzing the experiment data.

In addition to the 7 variables (in the case of the html-keyboard-response) above,
the boilerplate experiments of the UiL OTS labs add two additional
variables to each trial. This is done for example by:

```javascript
jsPsych.data.addProperties (
    {
        subject : subject_id,
        list : list_name
    }
);
```
The two variables added by the experiment script are:

1. **subject** A random id generated when the experiment starts.
2. **list** The chosen list from the available lists in the experiment.

## Experiment specific variables

Different experiments need different output. The UiL OTS Self Paced Reading
(SPR) boilerplate  for example records up to 15 reaction times per trial, 
whereas the lexical decision (LD) templates only record one.
You'll have to look this up in the documentation of each boilerplate.

[1]:<https://www.jspsych.org/7.1/overview/plugins/#data-collected-by-all-plugins>
[2]:<https://www.jspsych.org/7.1/plugins/list-of-plugins>

",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-self-paced-reading-mw,"# Deprecation warning
We suggest to use the following experiment instead: https://github.com/UiL-OTS-labs/jspsych-spr-mw
This experiment will not receive any future updates and is marked as READONLY.

# jspsych-self-paced-reading-mw
Self-paced reading with moving window implementation, programmed in jsPsych (JavaScript)

# How to use
See instruction and some more context at https://github.com/UiL-OTS-labs/jspsych-vislexdec-vp , also for setting up a version with your own folder.

In the current development case and stage, the script paths point directly to our own paths for the current development version, so if you have a working internet connection, locally double clicking on the file index.html should start the boilerplate/template in your default browser.

# Todo
- The conditional workflow given incidence of check questions/statements
- Better layouting and such
- Docs (as soon as more generic)

",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-spr-mw,"# jspsych-spr-mw

## A self paced reading with moving window experiment using jsPsych
This is a small boilerplate experiment for Self Paced Reading (SPR) with a
moving window. Participants read sentences formatted in lines. Single
words or groups of words are revealed when participants press a key (space-bar). An occasional
question is asked to see whether they comprehend the text they have been
reading.

## Generic documentation
Please read the [generic documentation](https://github.com/UiL-OTS-labs/jspsych-uil-template-docs) for some context.

## Make your experiment ready for use with the data server

### Update access key
The file `globals.js` contains a variable:
```javascript
const ACCESS_KEY = '00000000-0000-0000-0000-000000000000';
```
For uploading to the UiL-OTS data server you will need to change
this to the access_key that you obtained when your experiment
was approved. Your personal access key should look identical, but
with all the '0' changed. For elaborate info see `globals.js`.

### Adapting stimuli
The file `stimuli.js` contains a number of variables you can tweak
in order to adapt the stimuli the participants sees. There
are by default 2 lists. Each list gets its own
set of stimuli assigned. These lists can be used to implement
a Latin square design. The boilerplate has sentences in an
active, passive and filler condition. The two lists complement
each other, so an active stimulus in the first list is complemented
with a passive stimulus in the second list. Participants get one of these
lists assigned randomly. You can add additional lists if your experiment
requires this. This would make it more difficult
to implement the Latin square.

### Presenting multiple words as one group
The boilerplate experiment treats every word in the stimuli as a
group of its own containing one word. Sometimes it is handy to group
multiple words together, to shorten the time it takes to complete the
experiment for example. This is possible, but must be enabled. The file
`globals.js` contains another variable:
```javascript
const GROUPING_STRING = null;
```
To enable grouping you must define a useful delimiter between groups.
A little bit further in the file there's a commented version of this:
```javascript
const GROUPING_STRING = ""/"";
```
So in order to enable grouping, comment the first version and uncomment
the latter. In theory you can fill out any string instead of the `""/""`
(useful in case you need to use / in a stimulus).
Notice the string is turned into a regular expression in order to split
the stimulus into parts and to remove the `/` in the case described here.
```javascript
re = RegExp(GROUPING_STRING,'gu');
```
So make sure if you are going to be creative, that the expression is valid.

## Output
For some general information about understanding the output of jsPsych you
can visit the `README.md` of our [jspsych output][1] github page. This 
boilerplate creates its own plugin in order to make an SPR stimulus. Currently this
stimulus is not listed in [jspsych plugin list][2], because it is not part of
the official jsPsych source. So here we describe which output is specific for
the SPR.

### SPR stimulus
The boilerplate has support for 15 groups of words. Remember that a group can
consist of a single word. Groups starting with a `#` in the stimulus list are
added to the output. Each reaction time is in ms. If less than 15 groups are
marked with a `#`, the remaining reactiontimes are set to -1 as an indication
that no response has been given for that word; this marks it as
invalid. The variables are listed as **rt1**, **rt2** **..** **rt15** in the output.
The fixation cross that lures the eyes of the participant to the start of the
sentence is also implemented using the same spr-moving-window plugin, with the
single group/word '+'. All RTs for this trial will be set to -1.
In addition to the RTsm three variables are added to the output:

1. **id**
2. **item_type**
3. **uil_save**

The **id** (which you assign in `stimulus.js`) of the stimulus identifies 1
specific stimulus from your list. It is typically 1 to n, where n is the number
of items in your list. 

The **item_type** is added to the output, also defined by you in `stimuli.js`.
In contrast the item_type of the fixation cross is always ```FIX_CROSS```.

**uil_save** is added to the output variables to indicate whether this
json opbject or one line in the output of the csv is worth saving, in other
words is really mandatory for the analysis of your experiment. If it is defined
for a trial it is either true or false - a boolean. You can use this boolean 
to filter your data. You can first check whether the uil_save is set and
if it is set, you can examine whether it is true and if not, just throw the
row-in-the-csv or JSON object away.

### question stimulus
The question stimulus is a 'html-button-response' type of stimulus. From the
information of [jspsych output][1] you should be able to figure out what jsPsych
adds for each trial. In addition to what jsPsych adds, we add the following
output variables:

1. **id** (similar to the id in the section of the SPR stimulus above)
2. **item_type** (similar to the id in the section of the SPR stimulus above)
3. **expected_answer**: the answer that would be correct.
4. **uil_save** (similar to the id in the section of the SPR stimulus above)
5. **answer** the answer that the participant gave; one of [false, true].
6. **correct** whether the given answer was the correct answer; one of [false, true].
7. **integer_correct** whether the given answer was the correct answer; one of [0, 1].

## Good luck, hopefully this experiment is useful to you.

[1]:<https://github.com/UiL-OTS-labs/jspsych-output>
[2]:<https://www.jspsych.org/plugins/overview/#list-of-available-plugins>
",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-stroop,"# jspsych-stroop
Strooptask using jspsych

## Description
This task is a general form of a STROOP task. Currently, It uses a CONGRUENT 
stimuli e.g. the word ""red"" in a reddish font type and INCONGRUENT e.g.
""blue"" in a yellowish font type. Just like a regular font type. The general
flow is as follows:
- A participant is welcomed to the task
- The participant should agree with the informed consent form.
  - If the participant doesn't agree, the experiment terminates.
- Some general information (gender, age, etc.) is gathered from the participant
- The participant is instructed on how to do the strooptask
- The participant practices a number of trials
  - The participant should achieve a given percentage correct, otherwise
    the practice block repeats.
- The participant is given final instructions.
- The participant runs through all of the trials of the stroop task
- The data is stored on the data server and the experiment finishes.

Currently, there is support for multiple lists borrowed from the
[jspsych-spr-mw](https://github.com/UiL-OTS-labs/jspsych-spr-mw) experiment.
In the moving window experiment this is used to create a latin square.

## Generic documentation
Please read the [generic documentation](https://github.com/UiL-OTS-labs/jspsych-uil-template-docs)
for some context of the use of jsPsych in the UiL-OTS laboratories.

## Adapting the stroop task
In its current state the stroop task is a very short experiment, hence many stimuli
should be added. The stimuli to be presented are in three lists by default.
The practice items which every participant encounters and 2 sets of lists for
the test items: LIST_GROUP1 and LIST_GROUP2, one of these list is assigned
randomly to a participant. These lists can be extended so the experiment
gets some body. It should be quite easy to remove one list or add 
additional. For more info see the comments/documentation in stimuli.js.

### Update access key
In the file `globals.js` is a variable:
```javascript
const ACCESS_KEY = '00000000-0000-0000-0000-000000000000';
```
For uploading to the UiL-OTS data server you will need to change
this to the access_key that you obtained when your experiment
was approved. Your personal access key should look identical, but
with all the '0' changed. For elaborate info see `globals.js`.

For general information about the how to work with the UiL-OTS 
data and experiment server (UiL OTS student and employees only)
see: [online experimenting][3]

## Output
For some general information about understanding the output of jsPsych you
can visit the `README.md` of our [jspsych output][1] github page.

## License
This software is free software LICENSED under the GNU General Public License v3.0
for more details see the LICENSE file in the repository.

[1]:<https://github.com/UiL-OTS-labs/jspsych-output>
[2]:<https://www.jspsych.org/plugins/overview/#list-of-available-plugins>
[3]:<https://uilots-labs.wp.hum.uu.nl/how-to/online-experimenting/>
",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-stroop-zinnen,"# jspsych-stroop-zinnen
Strooptask using jsPsych with sentences as stimuli.

## Description
This task is an adaptation of the general form of a STROOP task. Currently,
the task presents sentences word after word where the last word requires a
response from the participant. So the first words are in a default color,
whereas the last has a specific color, which requires a forced choice from the
participant. It uses CONGRUENT stimuli e.g. the word ""red"" in a reddish font
type and INCONGRUENT e.g. ""blue"" in a yellowish font type. Just like a regular
font type. The flow of the experiment is as follows:
- A participant is welcomed to the task
- The participant should agree with the informed consent form.
  - If the participant doesn't agree, the experiment terminates.
- Some general information (gender, age, etc.) is gathered from the participant
- The participant is instructed on how to do the strooptask
- The participant practices a number of trials
  - The participant should achieve a given percentage correct, otherwise
    the practice block repeats.
- The participant is given final instructions.
- The participant runs through all of the trials of the stroop task
- The data is stored on the data server and the experiment finishes.

Currently, there is support for multiple lists borrowed from the
[jspsych-spr-mw](https://github.com/UiL-OTS-labs/jspsych-spr-mw) experiment.
In the moving window experiment this is used to create a latin square.

## Generic documentation
Please read the [generic documentation](https://github.com/UiL-OTS-labs/jspsych-uil-template-docs)
for some context of the use of jsPsych in the UiL-OTS laboratories.

## Adapting the stroop task
In its current state the stroop task is a very short experiment, hence many stimuli
should be added. The stimuli to be presented are in three lists by default.
The practice items which every participant encounters and 2 sets of lists for
the test items: LIST_GROUP1 and LIST_GROUP2, one of these list is assigned
randomly to a participant. These lists can be extended so the experiment
gets some body. It should be quite easy to remove one list or add 
additional. For more info see the comments/documentation in stimuli.js.

### Update access key
In the file `globals.js` is a variable:
```javascript
const ACCESS_KEY = '00000000-0000-0000-0000-000000000000';
```
For uploading to the UiL-OTS data server you will need to change
this to the access_key that you obtained when your experiment
was approved. Your personal access key should look identical, but
with all the '0' changed. For elaborate info see `globals.js`.

For general information about the how to work with the UiL-OTS 
data and experiment server (UiL OTS student and employees only)
see: [online experimenting][3]

## Output
For some general information about understanding the output of jsPsych you
can visit the `README.md` of our [jspsych output][1] github page.

## License
This software is free software LICENSED under the GNU General Public License v3.0
for more details see the LICENSE file in the repository.

[1]:<https://github.com/UiL-OTS-labs/jspsych-output>
[2]:<https://www.jspsych.org/plugins/overview/#list-of-available-plugins>
[3]:<https://uilots-labs.wp.hum.uu.nl/how-to/online-experimenting/>
",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-uil-template-docs,"# jsPsych-uil-template-docs
- This page provides generic background information on UiL OTS 'templates' (or 'boilerplates') for [jsPsych](https://www.jspsych.org) experiments (mostly linguistic experimental paradigms) and some additional tools that can be reused. 
- Ideally, any new template links to this overview in its README.md, so that we can keep core documentation in one place.

# Background and Rationale
This documentation provides information about using some templates that we developed at the UiL OTS labs for (time critical reaction time) paradigms and tools needed to make your own online experiment using [jsPsych](https://www.jspsych.org/). The aims for this repository are:

- Keep documentation on a selection of code bases in _one place_.
- Separate _generic_ documentation from _template-specific_ documentation.
- Give some _background information_ to help Students, Researchers, and Developers plan online experimenting.


## Standard components
At this point, __every__ UiL OTS jsPsych template comes with the following 'default' components included: 

### 1. Consent
A consent page placeholder is included before the experiment starts. It is up to the specifics of your own goals (and organisation) what information should be given. A consent page must always implement a combination of a consent statement, a checkbox that must be checked _and_ a _button_ that needs to be clicked to give consent. See `consent.js` for (placeholder) content.

### 2. Survey questions
You can adapt the survey questions to your own needs, but it is not as easy as it may seem. Please think well about what you want to accept as 'valid' in your survey. Read up about [input (data) validation](https://en.wikipedia.org/wiki/Data_validation). For instance, you may want an e-mail address to be in the form of `someone@somewebsite.com` and not allow people to fill out `whatever` in a survey field. Input validation is important for quality of online research. Also think about data management, and make sure to formulate your survey questions so that they will provide you with answers you can and will actually use (so, be very specific in your questions). See `survey.js`.

### 3. Generic UiL utility library
This [utility library](https://github.com/UiL-OTS-labs/jspsych-uil-utils) was created to enable:

- Mobile/Tablet detection (not the type of devices we want participants to use).
- Restrained (or pseudo-) randomisation.

## Optional and or additional components
Some templates have additional files and folders included, for example:

### 1. Plugins
Sometimes, a custom plugin is used, for instance in the [Self-paced reading](https://github.com/UiL-OTS-labs/jspsych-spr-mw) template. See the folder `plugins` and its contents.
### 2. Keyboard setting/testing procedures
The Lexical Decision Templates use a custom keyboard setting procedure. See `keyboard.js`
### 3. Audio setting/testing procedures
Some templates use _audio_, if they do, there is or will be a simple testing procedure.

# Output (data)
Specific information about the output (data) of each template experiment can be found in its README.md. In addition, there is a general primer describing jsPysch's way of ""dealing with data"" and some of the UiL OTS template's defaults for data output. Please read that [primer on data output](https://github.com/UiL-OTS-labs/jspsych-output), especially if this is your first time using one of these templates!

In a nutshell a _reminder_ to summarize with regard to experiment data output:
- jsPsych experiments _always return all_ data for _each type_ of stimulus/trial/trial phase in 'trial' objects, that have _keys_ and _values_.
- Many of _our_ templates add data (subject ID & list ID as _key, value_ pairs) to all the aforementioned objects, so the (sub)trial-data always have this information available for analysis.
- In the case of a trial or trial phase where the output contains relevant variables (experimental item, Reaction Time, etc), we sometimes add a key ""useful_data_flag"" with a value 'true' in the experiment; note that this may or may not include ""practice trials"", so always keep checking your code and your output!
- The above mentioned use of _flags_ can be of use for filtering your data.

# Documentation that should minimally be in _every template_ (developer requirements)
Certain information should be in every template. The elements below should be in every repository's README.md (Markdown):
The text within ```< >``` are 'placeholder text', the ones without them should be _'as is'_.
```markdown
# <Template name>
<A human redadable intuitive description related to the template name.>

# Generic documentation
Please read the [generic documentation](https://github.com/UiL-OTS-labs/jspsych-uil-template-docs) for some context and scope.
UiL-OTS-affiliated students and researchers should follow [this how-to](https://uilots-labs.wp.hum.uu.nl/how-to/online-experimenting/). 

# Paradigm-specific template related documentation
<If there are clearly shared properties of templates (a general paradigm), like in the case of the Lexical Decision Paradigm, they may be referenced here.>

# Task Description
<Optionally, some scope information about the paradigm or shared organisation, if relevant.>

### Short description
<A short description of what _this specific template_ does and what output is given.>

### Longer Description
<A longer description that reflects on, for instance a paradigm's variations.>

# Output (data)
The data of all (sub) trial phases are logged. Please read the [general primer on jsPsych's data output](https://github.com/UiL-OTS-labs/jspsych-output) if you are new to jsPsych data output.

<Specific template output variables and how to adapt your stimuli.js and/or other files regarding data output.>

# Getting started
<How to use/start the experiment.>

```
# Functional code separation in different files (modular structure)
Some of the javascript (jsPsych) code (that is used by the experiments' 'index.html') is separated into separate files/modules. After unzipping a template download, you will often find the following files/folder structure.

```
template_download_main_folder\
                              | plugins/    
                              |        |
                              |         \ someplugin.js          # _Only_ if there is a custom plugin needed, you will find this folder 'plugins'.
                              | consent.js                       # Informed consent related code.
                              | globals.js                       # Global template settings/constants (buttonlabels/html messages, etc.).
                              | index.html                       # The main experiment file, always called 'index.html'. Always there!
                              | instructions.js                  # Instruction-related code.
                              | stimuli.js                       # Stimuli and their designs. Experimental, often Reaction Time (RT) critical!
                              | survey.js                        # Survey tools. 
                             /
```
As a general rule, we design templates in such a way, that the main experiment's file (index.html) remains rather 'minimal'. That is: not too many lines of code in the main experiment's index.html, but bundle shared functionalities in separate files with functional, descriptive names.
                             
# Context and scope of current developments
The first templates were developed for the liguistics RMA course [Experimental Design and Data Analysis ()](https://osiris.uu.nl/osiris_student_uuprd/OnderwijsCatalogusSelect.do?selectie=cursus&cursus=TLRMV16108&collegejaar=2020&taal=nl). Given this context, we've made the templates so that they behave quite similarly to the  [ZEP templates](https://www.beexy.nl/zep/wiki/doku.php?id=templates:lexical_decision) that are used in the UiL OTS labs for traditional, lab bound research. However, although JavaScript and ZEP may share some features (like coding syntax, code organisation), they are of course not the same.

# List of jsPsych-based template repositories 
###### (Developers: keep this list _updated_)

### Self-Paced Reading with Moving window Template
- Word-by-word self-paced reading, moving window [jspsych-spr-mw](https://github.com/UiL-OTS-labs/jspsych-spr-mw)

### Visual Lexical Decision Templates
- Visual Lexical Decision basic version: [vislexdec](https://github.com/UiL-OTS-labs/jspsych-vislexdec)
- Visual Lexical Decision with Visual Prime: [vislexdec-vp](https://github.com/UiL-OTS-labs/jspsych-vislexdec-vp)
- Visual Lexical Decision with Visual Masked Prime: [vislexdec-vp-vm](https://github.com/UiL-OTS-labs/jspsych-vislexdec-vp-vm)

### Auditory and/or Visual Lexical Decision Templates
- Auditory Lexical Decision basic version: [audlexdec](https://github.com/UiL-OTS-labs/jspsych-audlexdec)
- Auditory Lexical Decision with Visual Prime: [audlexdec-vp](https://github.com/UiL-OTS-labs/jspsych-audlexdec-vp)
- Auditory Lexical Decision with Visual Masked Prime: [audlexdec-vp-vm](https://github.com/UiL-OTS-labs/jspsych-audlexdec-vp-vm)
- Auditory Lexical Decision with Auditory Prime: [audlexdec-ap](https://github.com/UiL-OTS-labs/jspsych-audlexdec-ap)

## Generic 'UiL' utility library (used in most templates)
A tool that may help with things like restrained randomisation, detecting mobile phone/tablets and other reusable functionality.
- [jspsych-uil-utils](https://github.com/UiL-OTS-labs/jspsych-uil-utils)

## Output documentation
As mentioned before, please read the [primer on data output](https://github.com/UiL-OTS-labs/jspsych-output)

## Miscallenous jspsych related repositories

### Documentation on the concessions we have/had to make
- [jspsych-concessions](https://github.com/UiL-OTS-labs/jspsych-concessions)

### Tools to help with csv/json conversions
- [jspsych-boilerplates](https://github.com/UiL-OTS-labs/jspsych-boilerplates)

# General Overview

## jsPsych is _not_ a 'programming language'

jsPsych uses a _cominbation_ of a _markup_ language (html), a _styling_ language (css) and JavaScript, this last one _is_ a [programming language](https://developer.mozilla.org/en-US/docs/Web/JavaScript).

How some concepts relate to each other:

### HTML
- HTML is a [markup language](https://en.wikipedia.org/wiki/Markup_language), not a programming language
- All HTML files have a similar structure
- With the ```<script>somescipt.js</scipt>``` tags, you can use Javascript in HTML

### Javascript
- Javascipt is a (web) _programming language_.
- You can use functions, create custom code with interaction! 
- There is no ""official standard"" for coding Javascipt, but it is very powerful  

### CSS
- CSS mainly deals with styling, like layouts, fonts, colors, backgrounds.
- CSS evolved from being primarily a styling language, but is also becoming a bit more like a 'programming language', in a way.

### jsPsych
- jspsych _uses_ Javascript code for a specific experimental purpose and the functions from this library need to be imported in the top of your html file before you can use them.
- You _could_ run an experiment in your browser while being offline, if you only refer to local sources that are imported in the relative path.
- If you would need or want to load scripts from an online location in your experiment, you _could_. 
  - But then, do make sure that the locally run code *does* have a working internet connection!

## You will become a 'web developer' 
jsPsych was developed with scientists in mind, not developers. In essence, the jsPsych library tries to let you focus on mainly writing a html file and (re-)use simple JavaScript-style code blocks for trials or trial parts. However, once you get started with jsPsych, you will become a bit of a 'web developer' anyway, in the sense that:

- You will need _tools_ for editing html files, JavaScript code blocks and stylesheets (not Word, Pages, LibreOffice, but a plain text editor).
- You will need to learn about _debugging_ and the debugging capabilities of (most) web-browsers.
- You will be _confused_ when things 'don't work'.
- You will need to _accept_ that you cannot _control everything_.

### You need _at least_ a _plain text editor_
- Windows users may want to download [Notepad++](https://notepad-plus-plus.org/), or try out the free Visual Studio Code IDE (todo)
- Mac users can use TextEdit in plain text mode (preferences) or try out Visual Studio Code, use XCode, Sublime Text, etc.
- (Linux users will usually find their way with this step.)

## 'The lab' vs 'The Web': limitations and opportunities
Traditionally, we've used [ZEP](https://www.beexy.nl/zep/wiki/doku.php) in the UiL OTS labs for time critical experimentation and there are many templates to start with using ZEP. ZEP was designed in house and has been designed to accurately sync sound, visuals/text and/or other hardware (eye tracking, EEG, EMG, etc) in a 'traditional' research lab setup. By that we mean:

- A quite controlled/controllable environment in terms of hardware, software and possible distractions for participants.
- Physically being bound to the lab.
- Relatively small samples, optimised for ""Wilhelm Wundt"" style traditional research.

The lab-based situation was limited due to the COVID-19 pandemic, which is why we started working on web-based alternatives. Some general remarks about this:

- 'The web' cannot offer the accuracy and precision needed for certain paradigms.
- Variations in hardware, software, internet speed, random noise and distractions and many other aspects may cause noise in the data at multiple levels.
- Especially when sounds _and_ images need to be synced, be sure to define means to verify or falsify presentation syncing and test well.

On the other hand:

- The _principles_ of most paradigms can still be taught and learned.
- You could potentially get a lot more data, which may to some extent compensate noise and little control.
- You can find a lot of code snippets and examples online, there is a huge user base for jsPsych and many plugins for certain paradigms can be used or adapted to certain needs.

## General jsPsych info and tutorials
Before you start editing one of these templates, reading up about jsPsych will usually be a good primer. We encourage anyone to get a gist of the 'simple' (but powerful!) way of doing things by following the recommendations given at jsPsych's [site](https://www.jspsych.org/) and to follow the first two tutorials. The basic things like how some `index.html` file imports from the jspsych library, where and how plugins can be used are very relevant to understand. 

## The templates in a 'jsPsych' perspective
As always, the easy things tend to be easy to read (lines of code and examples) and understand, but in order to conduct a full-fledged online experiment, you need to invest a lot more time to make things work. This is why we chose to equip this selection of templates with the aforemetioned 'standard components' (consent, survey, a shared common utilities library). Also, by default, the templates implement stimulus lists with a so-called _'timelineVariable'_ implementation.

## Modes of using jsPsych (and the templates)
There are multiple ways to test, run and configure jsPsych-based experiments:

- Locally on your (or a participant's) PC, by double-clicking the `index.html` file.
- Full web server implementation (getting a link to the experiment, served by ICT&Media).

These two modes will likely confuse those who are not web developers, but the bottom line is: 

- A locally developed experiment will _not guarantee that the same experiment will run in a server setup_, or vice versa! 

Many browsers and versions have their own defaults for security like 'autoplay', allowing sounds, pop-ups and alerts, importing images from local sources, etc. It is impossible to know 100% sure if things will work in your browser in advance, so be prepared to deal with some confusion.

## For 'real' online data collection, you will need a server-based setup
While you could -- in principle -- use jsPsych offline in the traditional lab context, it will typically not make a lot of sense. Especially if you want to store your research data for an experiment in one place, a web-server setup is the way to go. 

# From template to your own online experiment

## Prerequisites and scope limitation
This documentation is primarily aimed at people _affiliated with the UiL OTS labs_ (Utrecht University: Linguistics students and researchers). The infrastructure and support for doing online experiments will be _only availble for those people_.

- People _affiliated with our lab_ can use the information [from our lab webiste](https://uilots-labs.wp.hum.uu.nl/experiments/overview/) and expand the ""Online experiments using jsPsych"" section for details. Please [follow this how-to](https://uilots-labs.wp.hum.uu.nl/how-to/online-experimenting/). 

- People _not affiliated with our lab_ can of course use the templates, but will have to do without (support for, access to) the [UiL OTS Experiment Datastore](https://experiment-datastore.lab.hum.uu.nl/experiments/).

With the above scope limitations, the bigger picture overview is:

step | Description                                                          | Comment
-----|----------------------------------------------------------------------|----------------------------------------------------------------------------
1    | Choose a template to base your experiment on.                        | Read the template docs.
2    | Edit the template to your needs and test it _locally_.               | First, focus on the general flow and look and feel.
3    | Log in to the Experiment data store and _read the docs_.             | [Experiment data store](https://experiment-datastore.acc.lab.hum.uu.nl)
4    | Make your experiment ready for _online_ (web server) usage.          | See the documentation [on our lab webiste](https://uilots-labs.wp.hum.uu.nl/how-to/online-experimenting/)
5    | Upload your experiment, 'open' it and test again.                               | 'Opening' the experiment allows it to write data to the data server. Focus on data output is advised in this step.
6    | Share the link to your experiment.                        | Test very well before doing this (nobody likes failing online experiments or unusable research data)!

Step 4 and 5 will generally be alternated iteratively, until all is working well on _all_ aspects: 

- Flow/look and feel, routing, timing, survey & consent.
- Data output and usability. 
- Online specifics for optimising audio presentation, keyboard setting, media and media preloading.

In short: _everything(!)_ that you consider relevant to solving an actual problem with your experiment.

Detailed descriptions:

# 1. Choosing a template
There are quite a few variations for the Lexical Decision Experiments, read the template's specific documentation `README.md` to make an informed decision.

# 2. How and where to edit templates
Editing templates is largely self-explanatory: stimuli are edited in `stimuli.js`, global settings in `globals.js`, and instructions in `instructions.js`. Some hints can be found in the comments in the files themselves (comments are preceded with two forward slashes `//`). When changing things, make sure to frequently run the experiment again to make sure it still works (and that you haven't messed up the syntax by accidentally deleting brackets, quotes and such). 

_Some_ more details about this follow below.

## 2.1 Imports in the head section 'index.html' 
#### Imports the jsPsych library, some typically used jsPsych plugins, the jsPsych style sheet and also 'our' custom template libraries & files.

Javascript libraries --among other things-- are imported in the index.html's so-called _head section_. Somewhere in between the ```<head>``` & ```</head>``` tags, you will find import lines that may look like this:

```html
<script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/jspsych.js""></script>
```

In the above case, the import is in fact an example using the custom [Experiment Datastore](https://experiment-datastore.acc.lab.hum.uu.nl) path as can be used with our current server path to the general jsPsych JavaScript library. If you should want to use your own, _relative path_ to a different version of jsPsych, it could look like this, for example:

```html
<script src=""jspsych/6.1.2/jspsych.js""></script>
```

Configuring your own local paths to jsPsych's core scripts like in the latter examples is generally discouraged, because it may lead to difficult problems if you are not a web developer and things go wrong. If making an exception to this solves an actual problem, lab support may be able to help you out. For instance, maybe a newer version of jsPysch would add a crucial new feature that you want to use.

#### A typical _head section_ import structure
Let's have a look at one of the current templates, the Auditory Lexical Decsion with Visual Prime and what the very start of that file (just until the end of the head section) looks like and walk through that step by step:

```html
1.  <!DOCTYPE html>
2.  <html>
3.     <head>
4.     <meta charset=""UTF-8"">
5.    
6.     <title>Auditory Lexical Decision Experiment with Visual Prime</title>
7.
8.     <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/jspsych.js""></script>
9.     <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-html-keyboard-response.js""></script>
10.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-html-button-response.js""></script>
11.    
12.    <!-- Audio playback &response libraries (audio) -->
13.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-audio-button-response.js""></script>
14.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-audio-keyboard-response.js""></script>
15.    
16.    <!-- Generic check/ask libraries (consent, instructions & surveys) -->
17.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-external-html.js""></script>
18.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-instructions.js""></script>
19.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-survey-html-form.js""></script>
20.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-survey-multi-choice.js""></script>
21.    
22.    <!-- Generic jspsych style sheet -->
23.    <link href=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/css/jspsych.css"" rel=""stylesheet"" type=""text/css""/>
24.
25.    <!-- Uil OTS libraries -->
26.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/uil-utils/dev/jspsych-uil-utils.js""></script>
27.
28.    <!-- Uil OTS scripts -->
29.    <script src=""stimuli.js""></script>
30.    <script src=""globals.js""></script>
31.    <script src=""instructions.js""></script>
32.    
...   
...    </head>
 ```
 
In order for your experiment to use _only_ jsPsych's core library, the import is on line 8 from the previous example:

```html
<script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/jspsych.js""></script>
```

Additionally, the default 'look and feel' for common jsPsych experiment layouts is bundled in a 'styling' file, a .css file. This file is imported on line 22 in the above line-numbered example. If you don't import it, things will not be layouted like jsPsych experiments usually 'work', so it should also always be there when you start with your own experiment: 

```html
<link href=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/css/jspsych.css"" rel=""stylesheet"" type=""text/css""/>
```

Let us have a look at some ```<script>```-```</script>``` imports in between lines 8 and 23 and reflect on what they do to make the jsPsych library actually do things like things that typically define (reaction time) experimental linguistics experiments: key-presses, questions, audio fragments and text presentations, amongst others.

On line 9 and 10 we read:

```html
9.     <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-html-keyboard-response.js""></script>
10.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-html-button-response.js""></script>
```
These are some of the most basic jspsych interaction _plugins_. The first one deals with a participant keyboard responses, the other one with mouse interactions with clickable buttons, found in most experiments.

The ```jspsych-html-button-response``` button response plugin is generally _always_ recommended as a first 'participant interaction trial', since most browsers don't automatically allow the playing of sounds or videos with sound without such an interaction element at the start of your experiment. This type of interaction could also be part of a so-called 'instruction' plugin, so it depends on preference.

From lines 12-14, we may gather:
```html
12.    <!-- Audio playback &response libraries (audio) -->
13.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-audio-button-response.js""></script>
14.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-audio-keyboard-response.js""></script>
```

1. (line 12) Certain tags of the lines in ""header"" code can make it a comment, like ```<!--this is a comment -->```.
2. (line 13) There is a __keyboard-based audio interaction__ jsPsych _plugin_ that is used in this template.
3. (Line 14) There is also a __mouse button-based audio interaction__ jsPsych _plugin_ that is used in this template.

Lines 16-20 deal with other plugins used by our templates, they are usually _all_ included:

```html
16.    <!-- Generic check/ask libraries (consent, instructions & surveys) -->
17.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-external-html.js""></script>
18.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-instructions.js""></script>
19.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-survey-html-form.js""></script>
20.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-survey-multi-choice.js""></script>
```

- Line 17 imports the `jspsych-external-html` plugin, which is used for the consent page (placeholder html).
- Line 18 imports the `jspsych-instructions` plugin, specialised in (multi-page) clickable navigation instructions.
- Line 19 imports a html plugin for survey questions.
- Line 20 imports a multiple choice plugin, also for survey purposes.

We already mentioned the default css _link_ used by jsPsych (line 22). It's not a Javascript library or plugin, but a place where styling is defined. It's placed _below_ all _standard jsPsych_ plugins, but _before_ our custom template-related scripts. The comments should also make this clear. So, we continue with the 'UiL OTS custom template' imports:

```html
...
25.    <!-- Uil OTS libraries -->
26.    <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/uil-utils/dev/jspsych-uil-utils.js""></script>
27.
28.    <!-- Uil OTS scripts -->
29.    <script src=""stimuli.js""></script>
30.    <script src=""globals.js""></script>
31.    <script src=""instructions.js""></script>
32.    
...   
...    </head>
```

Line 26 imports the UiL OTS utility library.
Lines 29-31 import 'non-jsPsych' _default_ files, they import the aforementioned custom template-related javascript files. These _are_ __relative__ imports. To clarify: in case you would remove the stimuli.js file from your template experiment folder, this would result in errors if you do not also remove the import statement. 

#### In _general_: import only what you really need
You may have noticed that a standard template already has quite a few lines for standard things. This is mainly because they were designed to be 'complete experiments'. If for instance, if you would _not_ want to include any survey questions, it's best to delete the imports related to them, and of course also the parts in the index.html that relate to the survey blocks. Read on to find out about the next important section of an index.html experiment file.

## 2.2 The _script section_ of 'index.html'
#### javaScript code blocks, (sub-)trial definitions, trial procedures and other code organisation in an index.html file
After looking at what is imported in between the `<head>` tags, let's now look at what is in the next important section for using jsPsych: the part where all these files and libraries are actually used to _do_ things in the participants browser. Note, this will not be very in-depth at this point.

Global structure of the script section:

section/block | Description
--------------|-------------------------------------------------------------------------------------------------------
A             | Stimuli loading, using stimuli.js function(s), media preloading routines.
B             | Experiment logic_ variables (don't touch these, usually).
C             | Custom (template-related) Javascript _functions_ (if necessary, like with the keyboard setting procedure).
D             | Data preparation: _jsPsych-specific_ way of defining data that should be added to all trials.
E             | Trials and trial elements:_jsPsych-specific_ definition for trials/trial phases and other functional block definitions (instructions, survey blocks, consent page).
F             | Procedures: _jsPsych-specific_ _procedures_ typically use a combination of parts as defined in block E.
G             | The timeline, where the experiment's building blocks are added to form your experiment flow.
H             | The _init_-block: with this part, the experiment starts -for real- by 'executing the timeline'.

# 3. The experiment data store
Most information should be availble once you have logged in to the [Experiment Data Store](https://experiment-datastore.acc.lab.hum.uu.nl). 

# 4. Making the experiment ready for online testing
Most generally, once you have requested/defined your experiment folder, you have to copy the _Access key_ for your experiment to be the new value for the ACCESS_KEY constant in _your version_ of the `globals.js` file. So, say you have been given the code `N278123456-%^&&8888*(*7777--090900!#$%1234`, implement it like this:
```javascript
// ACCESS_KEY needs to be used for server setup (data store)
const ACCESS_KEY = 'zeeekretkeey'; 
```
And change it to:

```javascript
// ACCESS_KEY needs to be used for server setup (data store)
const ACCESS_KEY = 'N278123456-%^&&8888*(*7777--090900!#$%1234'; 
```
Note the _quotes_!

# 5. Sharing your experiment
Running your experiment _locally_ on your own PC will usually _also_ still work after editing that value the `globals.js` file, but of course, the data is not saved like on the server, you will just get a bunch of output in your browser window. Please do make sure to keep track of your edits. Once you are sure everything works, you can invite your participants!

# General best practices for jsPsych experiments 

## Audio
In the case of web server setup, it is as good idea to initialise jsPysch with ```use_webaudio = true```, in case you use audio stimuli. This is typically faster than when set to false. This seems to be be redundant now, since we can do such things using ```jspsych-uil-utils```.

## Preload media, like images, video, audio
In general, since timing is important, please make sure to [pre-load all media files](https://www.jspsych.org/overview/media-preloading/). This is because we will typically use trials with a timelineVariables setup. In the auditory lexical decision templates, you will find pre-loading routines for the audio files used in those.

## Always start an experiment with a _html-button-response_ interaction part
Browsers will often disallow auto-playing sound/video if there is no user activity related to a _mouse click_. It would be a shame to start of the experiment with errors of this type. An _instruction_ (plugin) with a mouse button response (or a multi-page instuction) will also fix this potential error.

# Pseudo-random vs true random

You can find a trial procedure using a custom function from the 'uil-utils' library. A default restriction for pseudorandomisation is configured in the templates as a constant in each ```globals.js``` file, with the string ```const MAX_SUCCEEDING_ITEMS_OF_TYPE = 2```. 

The big caveat for pseudorandomisation, especially with the current small number of items in templates (only _two_ items in the practice items): the utility function _tries_ to shuffle your items given this 'max 2-succeeding items restriction' for X (say, 10) times when your index.html initialises the experiment. If it (the function) fails to do so, --in this case, because you have too few items to make it happen-- your experiment will _not_ start (because of this underlying error, not visible in the browser).

In the lexical decision templates index.html file, find the sections in the timeline part (block G, pretty down below) where you can either use 'real' randomisation or pseudorandomisation given the criterion configured in ```globals.js```. Example taken from [vislexdec-vp](https://github.com/UiL-OTS-labs/jspsych-vislexdec-vp):

```javascript

        //////////////// timeline /////////////////////////////////

        let timeline = [];

        // it's best practice to have *mouse click* user I/O first
        timeline.push(start_screen);
        
        // survey
        timeline.push(survey_procedure);
        
        // kb layout
        timeline.push(select_keyboard_layout);
        
        // kb important keys
        timeline.push(keyboard_set_key_left_procedure);
        timeline.push(keyboard_set_key_right_procedure);
        
        // task instruction
        timeline.push(instruction_screen_practice);

        // a keyboard dominant hand configured key continue/prepare flow
        timeline.push(participant_keyboard_control_start);
        
        timeline.push(practice_procedure);
        timeline.push(well_done_screen);

        // ""get ready""
        timeline.push(participant_keyboard_control_start);

        // NOTE options below! comment/uncomment for regular vs restrained randomization
        // true randomness is better for the current template's amount of items...

        timeline.push(trial_procedure_pseudorandom); // don't do this with little stimuli
        //timeline.push(trial_procedure_random);
        
        timeline.push(end_screen);
 ```
 Below the ```//NOTE options below! (...)``` comment, a trial procedure is called that uses _trial_procedure_pseudorandom_. Commented out below that one, you find a different one, called _trial_procedure_random_. To go for 'real random', comment out ```timeline.push(trial_procedure_pseudorandom);``` (change to ```//timeline.push(trial_procedure_pseudorandom);```and uncomment the other one below (change ```//timeline.push(trial_procedure_random);``` to ```timeline.push(trial_procedure_random);```.
 
 The definitions of the procedures are --in this example-- only slightly above section G:
```javascript

         let trial_procedure_pseudorandom = {
            timeline:[
                present_fixation,
                present_prime,
                present_word,
            ],
            timeline_variables: uil.randomization.randomizeStimuli(
                stimuli.table,
                MAX_SUCCEEDING_ITEMS_OF_TYPE,
                'item_type',
            ),
            randomize_order: false // this should be false if uil randomization is used...
        };

        let trial_procedure_random = {
            timeline:[
                present_fixation,
                present_prime,
                present_word,
            ],
            timeline_variables: stimuli.table,
            randomize_order: true // this should be true if you want jsPsych's randomization
        };
```




  

",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-uil-utils,"# jspsych-uil-utils
A small set of utilities that can be reused throughout jsPsych style experiments.

# Introduction
In the lab we are moving research online. We use javascript in combination with
the jsPsych library: https://www.jspsych.org/ . jsPsych offers great functions
to present stimuli to the participant at their web capable device.

This library intends to add some small utility functions that can be used
throughout multiple experiments. For example in order to use the best sound
library with jsPsych we can add a function that uses the best api given the
protocol used inside of the browser file:// vs https:// or http:// .

All functions are inside a ""namespace"" called 'uil'. In such way that we can
directly see that it comes from our library.

For the students using our this library it might be convenient to test their
experiment using the file:// protocol, since most if not all jsPscych
documentation uses this in examples. So we choose old style modularization
instead of:

    <script type=""module""/>

# Usage
the main file jspsych-uil-utils.js should be included after jsPsych, since
it might depend on variables defined in jsPsych.

The next example is taken from the uilots visual lexical descision with visual
prime boiler plate :

```html
<html>
    <head>
        <title>My experiment title</title>
        <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/jspsych.js""></script>
        <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-html-keyboard-response.js""></script>
        <script src=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/plugins/jspsych-html-button-response.js""></script>
        <link href=""https://web-experiments.lab.hum.uu.nl/jspsych/6.1.0/css/jspsych.css"" rel=""stylesheet"" type=""text/css""/>

        <!-- Uil OTS libraries -->
        <script src=""https://web-experiments.lab.hum.uu.nl/jspsych-uil-utils/jspsych-uil-utils.js""></script>

        <!-- Uil OTS scripts -->
        <script src=""stimuli.js""></script>
        <script src=""globals.js""></script>
        <script src=""instructions.js""></script>
    </head>
    <body>
    </body>
    <script>
        
/* Create timeline */

jsPsych.init(
    {
        timeline: timeline,
        on_finish : function () {
            uil.saveData(ACCESS_KEY);
            // or to save to the acceptation server:
            // uil.saveData(ACCESS_KEY, true);
    }
);

    </script>
</html>
```
",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-vislexdec,"# jspsych-vislexdec
Visual [Lexical Decision](https://en.wikipedia.org/wiki/Lexical_decision_task) Experiment (template)

# Generic documentation
Please read the [generic documentation](https://github.com/UiL-OTS-labs/jspsych-uil-template-docs) for some context and scope.

# Task Description
The participant first sees a fixation cross, then a word or a non word is presented. Participants are instructed to respond as quickly as possible to make the decision wether the string is a word or not, using the keyboard.

- The _'test stimulus'_ (called 'word') is a string of letters and can be a real word or a nonword.

Crucial trial phases (sub trial phases):
- Fixation cross
- Visual item (decision phase)

### Reference:
          Rubenstein, H., Garfield, L., & Millikan, J.A. (1970). 
          Homographic entries in the internal lexicon. 
          Journal of Verbal Learning and Verbal Behavior, 9, 487≠494.

# Output
The data of _all_ (sub) _trial phases_ are logged in the data, but the output data can be filtered after data collection in many ways.
Please read the [general primer on jsPsych's data](https://github.com/UiL-OTS-labs/jspsych-output) if you are new to jsPsych data output.

Essential output for the _'true experimental'_ purpose in this template are:

- Reaction Time (RT) of the keyboard response in the decision phase
- Correctness of the keyboard response in the decision phase

The crucial trial/sub-trial phase (decision phase) output may look similar to this:

```json
	{
		""rt"": 700.0000000000073,
		""stimulus"": ""<p class='stimulus'>thwurp</p>"",
		""key_press"": 65,
		""condition"": ""NON_WORD"",
		""word"": ""thwurp"",
		""id"": 3,
		""trial_phase"": ""present_word"",
		""useful_data_flag"": true,
		""correct_response"": 0,
		""trial_type"": ""html-keyboard-response"",
		""trial_index"": 27,
		""time_elapsed"": 45062,
		""internal_node_id"": ""0.0-11.0-1.4"",
		""subject"": ""8oo722dq"",
		""list"": ""my_one_and_only_list"",
		""correct"": false,
		""integer_correct"": 0,
		""key_chosen_ascii"": 65,
		""key_chosen_char"": ""A"",
		""yes_key"": ""A"",
		""no_key"": ""L""
	},
	//(...)
```

Variable name (key) | Description          | Unit  | Type           | Comments                             | jsPsych default | Template default | Plugin name
--------------------|----------------------|-------|----------------|--------------------------------------|-----------------|------------------|------------
""rt""                | Reaction Time        | ms.   | float          | Reaction time in milliseconds        | yes             |                  |            
stimulus""           | stimulus (html)      |       | string/html    |                                      | yes             |                  |
""key_press""         | Keyboard response    |       | string/object? | https://en.wikipedia.org/wiki/ASCII  | yes             |                  | html-keyboard-response
""condition""         | Condition            |       | string         | See ```stimuli.js```                 | no              | yes              |
""word""              | Decision phase item  |       | string/html    | See ```stimuli.js, index.html        | no              | yes              | 
""id""                | ID/code              |       |                | (...)                                | yes             |                  |
""trial_phase""       | Trial phase          |       |                | (...)                                | no              | yes.             | 
""useful_data_flag""  | Filter flag          |       | boolean        |                                      | no              | yes              | 
_""expected_answer""_ | TODO change!         |       | todo           | Now (still) named ""correct_response"" | no              | no/yes/willbe.   | 
""trial_type""        | What plugin was used |       |                |                                      | yes             |                  | ""html-keyboard-response""
""trial_index""       | jsPsych index        |       |                |                        	           | yes             |                  |	
""time_elapsed"".     | jsPsych time object  | ms?   | int (/float?)  | For instance: 45062                  | yes             |                  |
""internal_node_id.  | jsPsych node object  |       |                | For instance:""0.0-11.0-1.4""          | yes             |                  |
""subject""           | Subject ID           |       |                | For instance: ""8oo722dq""             |                 | yes              |
""list""              | Stimulus list name.  |       | string         | For instance: ""my_one_and_only_list"" | no              | yes              | 
""correct""           | Scoring result       |       | Boolean        | 'true or false' score of response    |                 | yes              |   
""integer_correct""   | Scoring result       |       | integer        | 1 or 0 for correct or incorrect      |                 | yes              |   
""key_chosen_ascii""  |                      |       |                | For instance: 65                     | no              | yes              |
""key_chosen_char""   |                      |       |                | For instance: ""A""                    | no              | yes              |
""yes_key""           |                      |       |                | For instance:  ""A""                   | no              | yes              |
""no_key""            |                      |       |                | For instance: ""L""                    | no              | yes              |

# Getting started 
People _affiliated with our lab_ can use the information [from our lab webiste](https://uilots-labs.wp.hum.uu.nl/experiments/overview/) and expand the ""Online experiments using jsPsych"" section for details. Please follow [this how-to](https://uilots-labs.wp.hum.uu.nl/how-to/online-experimenting/).

### Make your experiment ready for use with the data server

### Update access key
In the file `globals.js` is a variable:
```javascript
const ACCESS_KEY = '00000000-0000-0000-0000-000000000000';
```
Before uploading your experimentto the UiL-OTS data server, you will need to change this to the access_key that you obtained when your experiment was approved. For elaborate info see `globals.js`.


### Adapting stimuli
- Open the file `stimuli.js` in your plain text editor.
- There is a list, called LIST_1:

```javacript
const LIST_1 = [ // stimuli and timeline variables

```
-  This list can be adapted to your own needs, i.e, you can replace values, make the list longer (don't forget to increment the 'id' values for new items!).
- If you need to implement a more complex design, you should read the `stimuli.js` file (and its comment sections) a little better. 
- For an example of a Latin square design, please have a look [here](https://github.com/UiL-OTS-labs/jspsych-spr-mw).

",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-vislexdec-vp,"# jspsych-vislexdec-vp
Visual [Lexical Decision](https://en.wikipedia.org/wiki/Lexical_decision_task) Experiment with _Visual Prime_ (template)

# Generic template documentation
Please read the [generic documentation](https://github.com/UiL-OTS-labs/jspsych-uil-template-docs) for context and scope.

# Task Description
The participant first sees a fixation cross, then a prime is presented, followed by the test stimulus. Particpants are instructed to respond as quickly as possible to make the decision if both strings in the pair are words --or not--, using the keyboard.

- The _prime_ is a string of letters and can be a _real word_ or a _nonword_.
- The _'test stimulus'_ is a string of letters and can also be a _real word_ or a _nonword_.

In this version, a trial consists of subsequently presenting _two_ words or nonwords. The participant needs to make a swift decision whether the _pair_ of presented sets of letters - the test stimuli - are _both_ acutal words or not. Only in this _last phase_ (the decision phase), as soon as the item is being presented, the participant can respond with the keyboard.

Crucial trial phases (sub trial phases):
- Fixation cross
- Visual Prime
- Lexical decision (decision phase)

### Reference:
          Rubenstein, H., Garfield, L., & Millikan, J.A. (1970). 
          Homographic entries in the internal lexicon. 
          Journal of Verbal Learning and Verbal Behavior, 9, 487≠494.

## Output
The data of _all_ (sub) _trial phases_ are logged in the data, but the output data can be filtered after data collection in many ways.
Please read the [general primer on jsPsych's data](https://github.com/UiL-OTS-labs/jspsych-output) if you are new to jsPsych data output.

Essential output for the _'true experimental'_ purpose in this template are:

- Reaction Time (RT) of the keyboard response in the decision phase
- Correctness of the keyboard response in the decision phase

The crucial trial/sub-trial phase (decision phase) output may look similar to this:

```JSON
    {
        ""rt"": 368.0000000000073,
        ""stimulus"": ""<p class='stimulus'>crawse</p>"",
        ""key_press"": 76,
        ""condition"": ""NON_WORD"",
        ""word"": ""crawse"",
        ""prime"": ""piano"",
        ""id"": 2,
        ""trial_phase"": ""present_word"",
        ""useful_data_flag"": true,
        ""correct_response"": 0,
        ""trial_type"": ""html-keyboard-response"",
        ""trial_index"": 25,
        ""time_elapsed"": 44933,
        ""internal_node_id"": ""0.0-11.0-2.1"",
        ""subject"": ""f9pdgg60"",
        ""list"": ""my_one_and_only_list"",
        ""correct"": true,
        ""integer_correct"": 1,
        ""key_chosen_ascii"": 76,
        ""key_chosen_char"": ""L"",
        ""yes_key"": ""A"",
        ""no_key"": ""L""
    },
 //...
```

Variable name (key) | Description          | Unit  | Type           | Comments                             | jsPsych default | Template default | Plugin name
--------------------|----------------------|-------|----------------|--------------------------------------|-----------------|------------------|------------
""rt""                | Reaction Time        | ms.   | float          | Reaction time in milliseconds        | yes             |                  |            
stimulus""           | stimulus (html)      |       | string/html    |                                      | yes             |                  |
""key_press""         | Keyboard response    |       | string/object? | https://en.wikipedia.org/wiki/ASCII  | yes             |                  | html-keyboard-response
""condition""         | Condition            |       | string         | See ```stimuli.js```                 | no              | yes              |
""word""              | Decision phase item  |       | string/html    | See ```stimuli.js, index.html```     | no              | yes              | 
""prime""             | Prime phase item     |       | string/html    | (...)                                | no              | yes              |
""id""                | ID/code              |       |                | (...)                                | yes             |                  |
""trial_phase""       | Trial phase          |       |                | (...)                                | no              | yes              | 
""useful_data_flag""  | Filter flag          |       | boolean        |                                      | no              | yes              | 
_""expected_answer""_ | TODO change!         |       | todo           | Now (still) named ""correct_response"" | no              | no/yes/willbe.   | 
""trial_type""        | What plugin was used |       |                |                                      | yes             |                  | ""html-keyboard-response""
""trial_index""       | jsPsych index        |       |                |                        	       | yes             |                  |	
""time_elapsed"".     | jsPsych time object  | ms?   | int (/float?)  | For instance: 45062                  | yes             |                  |
""internal_node_id.  | jsPsych node object  |       |                | For instance:""0.0-11.0-1.4""          | yes             |                  |
""subject""           | Subject ID           |       |                | For instance: ""8oo722dq""             |                 | yes              |
""list""              | Stimulus list name.  |       | string         | For instance: ""my_one_and_only_list"" | no              | yes              | 
""correct""           | Scoring result       |       | Boolean        | 'true or false' score of response    |                 | yes              |   
""integer_correct""   | Scoring result       |       | integer        | 1 or 0 for correct or incorrect      |                 | yes              |   
""key_chosen_ascii""  |                      |       |                | For instance: 65                     | no              | yes              |
""key_chosen_char""   |                      |       |                | For instance: ""A""                    | no              | yes              |
""yes_key""           |                      |       |                | For instance:  ""A""                   | no              | yes              |
""no_key""            |                      |       |                | For instance: ""L""                    | no              | yes              |


# Getting started 
People _affiliated with our lab_ can use the information [from our lab webiste](https://uilots-labs.wp.hum.uu.nl/experiments/overview/) and expand the ""Online experiments using jsPsych"" section for details. Please follow [this how-to](https://uilots-labs.wp.hum.uu.nl/how-to/online-experimenting/).

### Make your experiment ready for use with the data server

### Update access key
In the file `globals.js` is a variable:
```javascript
const ACCESS_KEY = '00000000-0000-0000-0000-000000000000';
```
Before uploading your experimentto the UiL-OTS data server, you will need to change this to the access_key that you obtained when your experiment was approved. For elaborate info see `globals.js`.


### Adapting stimuli
- Open the file `stimuli.js` in your plain text editor.
- There is a list, called LIST_1:

```javacript
const LIST_1 = [ // stimuli and timeline variables

```
-  This list can be adapted to your own needs, i.e, you can replace values, make the list longer (don't forget to increment the 'id' values for new items!).
- If you need to implement a more complex design, you should read the `stimuli.js` file (and its comment sections) a little better. 
- For an example of a Latin square design, please have a look [here](https://github.com/UiL-OTS-labs/jspsych-spr-mw).

",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-vislexdec-vp-mask,"# jspsych-vislexdec-vp-mask
Visual Lexical Decision with masked prime

# Task Description
This is one of many 'standard' variations of a visual lexical decision task. The 'parent' of this boilerplate/template is the general visual [Lexical Decision With Visual Prime](https://github.com/UiL-OTS-labs/jspsych-vislexdec-vp/). You may want to visit there first and come back here, especially if if you need general info about how to use and set up these kinds of boilerplates.

In its current state, the additional sub trial phases are:

- A _pre prime mask phase_ (with '#####')
- A _post prime mask phase_ (with '&&&&&')

The general Lexical Decision with Visual Prime trial structure is: 

1. Fixation cross (fixed duration, no user input)
2. Prime presentation (fixed duration, no user input)
3. Response item presentation and repsonse interval (last word/non-word, user keypress interval)

This version can be used to do

1. Fixation cross (fixed duration, no user input)
2. __Pre prime mask presentation__ (fixed duration, no user input)
3. Prime presentation (fixed duration, no user input)
4. __Post prime mask presentation__ (fixed duration, no user input)
5. Response item presentation and repsonse window (last word/non-word, user keypress interval)

## Globals for (sub) trial phases
Settings can be specified for durations. You can find them in ```globals.js``` 

## Stimuli
The easiest and most customisable way to implement masks, is to define them as timeline variables in ```stimuli.js``` for each stimulus/trial specification. A _function_ to do approximately the same is not difficult to add, but this verbose way of defining it in the stimulus.js file leaves it up to the creativity of the user to think up additional categories and/or functionalities of masked priming. In this boilerplate, we've defined to use a monospaced font, so
that the layouting of masks and prime will be consistent, _if_ the user takes into account to use _equal lengt as the prime_ for each masking string. 

## Things that differ from the parent boilerplate
- In ```index.html```
    - New sub trial phases:
		- present_pre_prime_mask
		- present_post_prime_mask	
	- Additions of the new sub trial phases' in the timeline procedures: 
		- practice_procedure
		- trial_procedure
	- Some new styling elements 
		- Monospace stimulus
		- Better margins and layouting for instructions
- Instructions in ```instructions.js``` have changed a bit
- Stimuli in ```stimuli.js``` have new key-values for post and pre prime phases
- New globals in ```globals.js```

## Todo
- Check/ask for instruction content and defaults.
- Discuss function version pro/cons: different repos?
- Make docs for how and where to edit boilerplate?



",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-vislexdec-vp-vm,"# jspsych-vislexdec-vp-vm
Visual [Lexical Decision](https://en.wikipedia.org/wiki/Lexical_decision_task) experiment with _Masked_ _Visual Prime_ (template) 

# Generic documentation
Please read the [generic documentation](https://github.com/UiL-OTS-labs/jspsych-uil-template-docs) for some context and scope.

# Task Description
This is one of many 'standard' variations of a visual lexical decision task.

The participant first sees a fixation cross, then a (forward) _mask_ is presented, followed by the a visual _prime_ and then the response item (decision phase). Particpants are instructed to respond as quickly as possible to make the decision if the (last) word, written in UPPER CASE, is a word or not, using the keyboard.

- The (forward) _mask_ is a string of characters of (preferably) the same amount of characters as the prime.
- The _prime_ is a string of letters and can be a real word or a nonword.
- The _'test stimulus'_ is a string of letters and can also be a real word or a nonword.

A reference that descibes this variant's method:

Jacob, G., Heyer, V., & Veríssimo, J. (2018). Aiming at the same target: A masked priming study directly comparing derivation and inflection in the second language. International Journal of Bilingualism, 22(6), 619–637. https://doi.org/10.1177/1367006916688333


Crucial trial phases (sub trial phases):
- Fixation cross
- Visual Mask
- Visual Prime
- Lexical decision (decision phase)

Only in this _last phase_, as soon as the item is being presented, the participant can respond with the keyboard.

# Output
The data of _all_ (sub) _trial phases_ are logged in the data, but the output data can be filtered after data collection in many ways.
Please read the [general primer on jsPsych's data](https://github.com/UiL-OTS-labs/jspsych-output) if you are new to jsPsych data output.

Essential output for the _'true experimental'_ purpose in this template are:

- Reaction Time (RT) of the keyboard response in the decision phase
- Correctness of the keyboard response in the decision phase

The crucial trial/sub-trial phase (decision phase) output may look similar to this:

```json
{
    ""rt"": 330.00000000000364,
    ""stimulus"": ""<p class='stimulus'>PALVE</p>"",
    ""key_press"": 76,
    ""condition"": ""PRACTICE"",
    ""word"": ""PALVE"",
    ""prime"": ""onion"",
    ""prime_mask"": ""#####"",
    ""id"": 1,
    ""trial_phase"": ""present_word"",
    ""useful_data_flag"": true,
    ""expected_answer"": 0,
    ""trial_type"": ""html-keyboard-response"",
    ""trial_index"": 13,
    ""time_elapsed"": 28717,
    ""internal_node_id"": ""0.0-8.0-3.0"",
    ""subject"": ""t3kfvvyq"",
    ""list"": ""my_one_and_only_list"",
    ""correct"": true,
            ""integer_correct"" : 1,
    ""key_chosen_ascii"": 76,
    ""key_chosen_char"": ""L"",
    ""yes_key"": ""A"",
    ""no_key"": ""L""
},

```

Variable name (key) | Description          | Unit  | Type           | Comments                             | jsPsych default | Template default | Plugin name
--------------------|----------------------|-------|----------------|--------------------------------------|-----------------|------------------|------------
""rt""                | Reaction Time        | ms.   | float          | Reaction time in milliseconds        | yes             |                  |
stimulus""           | stimulus (html)      |       | string/html    |                                      | yes             |                  |
""key_press""         | Keyboard response    |       | string/object? | https://en.wikipedia.org/wiki/ASCII  | yes             |                  | html-keyboard-response
""condition""         | Condition            |       | string         | See ```stimuli.js```                 | no              | yes              |
""word""              | Decision phase item  |       | string/html    | See ```stimuli.js, index.html```     | no              | yes              |
""prime""             | Prime phase item     |       | string/html    | (...)                                | no              | yes              |
""prime_mask""        | Mask item            |       | string/html    | (...)                                | no              | yes              |
""id""                | ID/code              |       |                | (...)                                | yes             |                  |
""trial_phase""       | Trial phase          |       |                | (...)                                | no              | yes              |
""useful_data_flag""  | Filter flag          |       | boolean        |                                      | no              | yes              |
""expected_answer""   | TODO change!         |       | todo           | Now (still) named ""correct_response"" | no              | no/yes/willbe.   |
""trial_type""        | What plugin was used |       |                |                                      | yes             |                  | ""html-keyboard-response""
""trial_index""       | jsPsych index        |       |                |                            	       | yes             |                  |
""time_elapsed"".     | jsPsych time object  | ms    | int (/float?)  | For instance: 45062                  | yes             |                  |
""internal_node_id.  | jsPsych node object  |       |                | For instance:""0.0-11.0-1.4""          | yes             |                  |
""subject""           | Subject ID           |       |                | For instance: ""8oo722dq""             |                 | yes              |
""list""              | Stimulus list name.  |       | string         | For instance: ""my_one_and_only_list"" | no              | yes              |
""correct""           | Scoring result       |       | Boolean        | 'true or false' score of response    |                 | yes              |
""integer_correct""   | Scoring result       |       | integer        | 1 of 0 for correct or incorrect      |                 | yes              |
""key_chosen_ascii""  |                      |       |                | For instance: 65                     | no              | yes              |
""key_chosen_char""   |                      |       |                | For instance: ""A""                    | no              | yes              |
""yes_key""           |                      |       |                | For instance:  ""A""                   | no              | yes              |
""no_key""            |                      |       |                | For instance: ""L""                    | no              | yes              |



# Getting started 
People _affiliated with our lab_ can use the information [from our lab webiste](https://uilots-labs.wp.hum.uu.nl/experiments/overview/) and expand the ""Online experiments using jsPsych"" section for details. Please follow [this how-to](https://uilots-labs.wp.hum.uu.nl/how-to/online-experimenting/).

### Make your experiment ready for use with the data server

### Update access key
In the file `globals.js` is a variable:
```javascript
const ACCESS_KEY = '00000000-0000-0000-0000-000000000000';
```
Before uploading your experimentto the UiL-OTS data server, you will need to change this to the access_key that you obtained when your experiment was approved. For elaborate info see `globals.js`.


### Adapting stimuli
- Open the file `stimuli.js` in your plain text editor.
- There is a list, called LIST_1:

```javacript
const LIST_1 = [ // stimuli and timeline variables

```
-  This list can be adapted to your own needs, i.e, you can replace values, make the list longer (don't forget to increment the 'id' values for new items!).
- If you need to implement a more complex design, you should read the `stimuli.js` file (and its comment sections) a little better. 
- For an example of a Latin square design, please have a look [here](https://github.com/UiL-OTS-labs/jspsych-spr-mw).

",2022-08-12
https://github.com/UiL-OTS-labs/jspsych-vwp-webgazer,"# jspsych-vwp-webgazer

Work-in-progress boilterplate for Visual World Paradigm experiments using WebGazer
",2022-08-12
https://github.com/UiL-OTS-labs/jspsych_timing,"# jspsych_timing
A small project to discover the quality of the timing of jspsych
",2022-08-12
https://github.com/UiL-OTS-labs/K02_EEG,"# K02_EEG
Docs for EEG setup in K02

# EEG setup K02

## Todo

- Test plans/reports pre-pilot (latency, sound card, etc)
- Technical drawing (digital version, Keynote master slide added here, should also be Powerpoint/LibreOffice editable versions?)
- Pictures & info of comopnenent (links of all components in keynote file --> export MD?) 
- Manual for Presentation experiment + triggers
- ~~Manual for ZEP?~~


# Overview of setup (update export after new edits)
![image info](https://github.com/UiL-OTS-labs/K02_EEG/blob/main/baby-eeg/baby-eeg.001.png)
![captions](https://github.com/UiL-OTS-labs/K02_EEG/blob/main/baby-eeg/baby-eeg.002.png)
![infographic](https://github.com/UiL-OTS-labs/K02_EEG/blob/main/baby-eeg/baby-eeg.003.png)


## General

### Caregiver + infant setup
- This lab was built as an _infant_ EEG lab, but the lab could be used for all ages (default documentation is for for caregiver + child (participant) scenario.
- Participants are _capped_ and connected with EEG equipment.
- Participant and caregiver will be inside the sound-proof cabin during an experiment. 

### EEG system details
- [BioSemi Active Two system](https://www.biosemi.com/products.htm)
- The amplifier can handle _two_ 32-channel electrode sets (matches the EEG lab in K13).
- [Manual for handling electrode sets](https://resources.lab.hum.uu.nl/resources/Electrodes_handling.pdf
- For more technical details about ther BioSemi Equipment: see the sections about EEG equipment [here](https://uilots-labs.wp.hum.uu.nl/facilities/eeg-lab/). 
- Better info will follow (TODO)

### Masking sounds & talkback (Crimson 3 monitoring interface + HP4 headphone amplifier)
- Caregiver can wear a headphone with masking sounds, played on mp3-player and the Crimson 3 monitoring interface.
- Using the Crimson 3, experimenter and caregiver can communicate using the 'talkback' button on the Crimson. 
	- Only if the experimenter presses the button, there is two-way communication. 
	- The microphone _inside_ the cabin is for the caregiver. 
	- Inside the Crimson is a microphone for the experimenter. 
	- The Crimson is --in this setup-- _not_ connected with USB to any PC, it works in _standalone_ mode.
	- The headphone loudness for the caretaker can best be tested before the experiment and can be adjusted using the headphone amplifier knobs.

### Visually masking the caregiver (polarised glasses)
- In case of visual stimuli, the caregiver may wear a pair of special polarised glasses that will mask the display contents (made by Chris van Run).

### Screens and their functionality
- Each PC (presentation PC and Recording PC) has connections for a _Screen A_ (experimenter section, outside the cabin) and a _Screen B_ (inside the cabin).
- The screen B for the Recording PC is _portable_. After capping and/or signal checking inside the cabin, the display may be taken back to its charging place at the experimenter desk, or otherwise moved/closed. Connect the portable screen to the DisplayPort to HDMI + mini HDMI adapter to use it.  

## System components

## Humans
- Participant
- Caregiver/parent
- Experimenter (and/or assistant)

## EEG system
- EEG Amplifier (EEG-a): where the bubndles are connected.
- EEG Trigger Box (EEG-t): where triggers of presentation program and recording are integrated.
- EEG Charger (EEG-c): charges the EEG battery pack(s)
- EEG battery (EEG-bat): there are two batteries: always keep the other battery charged.
- EEG Cap (EEG-c): caps are in the drawers (todo).
- EEG bundles (EEG-bun): electrode bundles: handle with care!
- EEG Optical cable (EEG-opt): thin orange cable that connects EEG-a to EEG-t).

### PC's
- [Presentation PC](todo:Specs) (stimuli)
- [Recording PC](todo:specs) (bio-signals, triggers from Presentation PC, other recordings)

### Displays (screens)
- Presentation screen A (experimenter)
- Presentation screen B (cabin)
- Recording screen A (experimenter) 
- Recording screen B (for the experimenter, inside or outide the cabin, portable screen).

### Audio components
- Speakers A (left channel), B(center, summed mono channel) and C (right channel)
- Crimson 3 monitoring/talkback interface (standalone mode)
- HP4 Headphone amplifier
- MP3 player (for masking sounds)
- Direct Input (DI) + channel switch box (don't touch)
- Mono summing box (DI)
- Stimulus PC sound card
	
#### Audio cables
- F/M XLR, balanced mono audio signals (speakers A, B, C and up to 2 microphones)
- M/F jack (stereo headphone amplifier extension)
- M mini jack to M RCA L/R cable (connect MP3 player to RCA input of Crimson) 
- Mini jack to left/right XLR splitter (connect Presentation mini jack audio out to DI's ad speakers inside cabin
- Small XLR connectors (DI connections)
- Dual L/R M-M jack connector (connect the Headphone Amplifier to Crimson)

### Data cables/connectors
- Ethernet
- USB 2.0 extension
- USB 3.0 extension
- Display Port cables
- Display port to mini HDMI connector (portable screen)

	


PRES-USB1 | REC-USB-1 | EEG Amp | EEG trigger box | EEG Optical Cable | EEG bundle(s) | 
----------|-----------|---------|-----------------|-------------------|---------------| 
0         | 0         | 0       |                 |                   |               |
",2022-08-12
https://github.com/UiL-OTS-labs/libeye,"# libeye
A small library that helps processing eye movement data

libeye has a dependency on CMake in order to install the library. This is a very
short explanation of how to start building the library. Create an empty directory
and change to that directory. Type cmake <path to libeye> -DCMAKE_BUILD_TYPE=Release
Replace Release by Debug for a debug build.

directory structure:

--libeye            libeye root directory
    |
    |-- log         This contains the logging facilities
    |
    |-- bin         This constains the sources for the executabels
    |
    |-- bindings    This directory contains binding to other languages
    |
    |-- samples     This directory contains some test sample for the library.
",2022-08-12
https://github.com/UiL-OTS-labs/limesurvey-file-collector,"# limesurvey-file-collector
Collects uploaded files from a LimeSurvey survey. Simple script that will require some modifications for your project. But hey, at least you don't have to start from scratch! :-)
",2022-08-12
https://github.com/UiL-OTS-labs/manybabies,"# Head-turn Preference Procedure (Manyababies 1 implementation)
You can find the source code for this experiment at
[GitHub](https://github.com/UiL-OTS-labs/ZEP2-headturn-preference-BOILERPLATE/tree/implementation/manybabies1).
*   Originally authored by: [Theo Veenker](theo.veenker@beexy.nl)
*   Adapted by (heavily): [Chris van Run](C.P.A.vanrun@uu.nl)
*   For clients:
    *   [Maartje de Klerk](https://www.uu.nl/medewerkers/MdeKlerk/0)
    *   [Caroline Junge](https://www.uu.nl/staff/CMMJunge/0)


## Description:
Purpose of this ZEP-based experiment is to see whether an infant participant can
detect a difference between two types of auditory stimuli. The infant sits on
the caregiver lap facing a wall on which a green light, an invisible speaker and
a camera is mounted. On each side wall a red light (or other visual stimuli) and
an invisible speaker are mounted. In this implementation there is a
familiarization phase and a test phase.

For each trial in the test phase the infant's attention is drawn to one the side
lights (blinking). When the infant looks at the blinking light a sequence of
sound stimuli starts and a timer runs as long as the infant keeps looking at the
blinking light. The trial ends when the infant looks away too long (or when a
particular number of stimuli have been presented). In the familiarization phase
that precedes the test phase a similar contingency procedure is used but only
for the lights; the sound stimuli once started continue until all have been
presented.

The researcher indicates a look start by pressing the `RETURN` key and a look
end by pressing the `ESCAPE`. This can alternatively be done via the BeexyBox B.

## Implementation details
This version of the HPP experiment has been adapted to suit the requirements of
the [manyababies 1 experiment](http://manybabies.stanford.edu/).


## Output
In this implementation the front and side lights will be presented via three
computer monitors. Therefore this implementation requires a quad head videocard.

The familiarization-items presentation order is random. The test-items order is
pseudorandomized such that there are never more than two of novel-type items
(NOVEL) or familiar-type items (FAMIL) that appear in sequence.

Generate output in comma-separated files by running `zepdb2extract`. See the usage
description on how to use zepdb2exact (`zepdb2extract -h`).

Output tables:
*   *Table 1*: stimuli details, looking time, not looking time, number of look
    aways (summary).
*   *Table 2*: looking/no-looking epochs per trial including stimuli details and
    duration.

## About Zep (2.0)
For information on running the experiment and extracting the experiment
results please go the Zep website at <http://www.beexy.nl/zep> and check
out the documentation section. There you'll also find explanations and
instructions that help you understand and modify a Zep experiment.

## Modifying or Customizing this Experiment
*Config Looking Times*. For maximum looking-time settings see `test/defs.zm` and
`familiarization/defs.zm`.

*Add stimuli*. Add the WAV sound files to `stimuli/sounds` directory and edit the
stimuli lists at `stimuli/EXAMPLE.csv`

*Edit stimuli lists*. Every line represents one experiment trial. Each variable
separates by a semicolon (`;`).
*   `id` is a unique reference for human convenience. `
*   `trial_type` adult-directed speech (ADS), infant-directed speech (IDS) or training.
*   `sndfn` is the soundfile that plays (e.g. 'test.wav').
*   `sound_direction` dictates which direction the sound should come from and
    should be one of the following values:
    *   `FRONT_SIDE`/`LEFT_SIDE`/`RIGHT_SIDE`: self explanatory
    *   `LEFT_AND_RIGHT_SIDE`: plays both in the left and right channel
        simultaneously.
    *   `PSEUDO_RANDOM_SIDE`: pseudo-random side with the restriction that one
        side not allowed more than twice in a row. Note that this does not count
        manually set sides.

*Fine tune your hardware setup*. As mentioned above a quad-head setup is
required (four monitors). Window- and head -linking settings is read from
`modules/baby_windows3_settings.zm`. You can find out the order of heads and
location by running `zepdpyinfo` in a terminal. Sound settings are in
`modules/sound_settings.zm`. You can find out the specifics of the sound card(s)
installed by running `zepsndinfo` in a terminal. This script assumes that the
order of channels are front left, front right, rear/side left, rear/side right.

## About the Hardware Setup
Optimally, the setup should use a *single* graphical card that supports at least
five monitor outputs. Using more than one graphical card leads to asynchronous
visual blanks and minor visual artifacts. A *stimulus-presentation computer*
should have this card installed along with an good-quality sound card. The
stimulus-presentation computer should output to five monitors. One to the
researcher, three to the participant (i.e. left, middle, right) and one to a
video-capture card in a *recording computer*. The researcher-display signal and
the video-capture signal should be duplicates. The recording computer can then
apply a chroma-key to the display signal, overlay it on a video feed and save
the result for [off-line
analysis](https://github.com/UiL-OTS-labs-backoffice/UiL-OTS-Video-Coding-System).
To get the multi-head setup to function under Kubuntu (Linux/GNU OS) we had to
use the Nvidia's
[baseMosaic](http://nvidia.custhelp.com/app/answers/detail/a_id/3580/~/how-to-configure-mosaic-on-linux)
feature.

The critical hardware/software used in the UiL-OTS lab is as follows:
*   3 x Nvidia NVS 310 GPUs (though we recommend using a single NVS 810 instead)
*   Asus Xonar Dx (sound card)
*   BlackMagic Decklink (video-capture card)
*   Logitech Webcam C920
*   BeexyBox B (response box; alternatively one can use the keyboard)
*   Open Broadcaster Studio (software)
*   Zep 2.0.9

![Hardware Setup](.hardware_setup.jpg)


## References
*   Kemler-Nelson, D. G., Jusczyk, P. W., Mandel, D. R., Myers, J., Turk, A. &
    Gerken, L. (1995). The Head-turn Preference Procedure for testing auditory
    perception. Infant Behavior and Development 18, 111-116.
    [doi](https://doi.org/10.1016/0163-638395900128)
*   KERKHOFF, A., DE BREE, E., DE KLERK, M., & WIJNEN, F. (2013). Non-adjacent
    dependency learning in infants at familial risk of dyslexia. Journal of Child
    Language, 40(1), 11-28. [doi](https://doi.org/10.1017/S0305000912000098)

## Disclaimer
This experiment script is released under the terms of the GNU General Public
License (see <http://www.gnu.org/licenses/gpl-2.0.html>). It is distributed in
the hope that it will be useful, but with absolutely no warranty. It is your
responsibility to carefully study and test the script before using it with real
participants.
",2022-08-12
https://github.com/UiL-OTS-labs/marlisa-word-judgement,"# marlisa-word-judgement
Judge which of the two utterances is the best pronounced.

This is not an experiment. The idea of this ""experiment"" is to ask from
native speaker to judge which of two utterances of a word is the most accurate.

## Trial
Each trial starts with two utterances of a word after each other. The task for
the participant is to judge whether the first or second sounds best according
their native language. They can see the word written while it is spoken. After
the utterances have finished, two repeat buttons appear for each stimulus.
Clicking them will enable to repeat one word. After they have chosen which
sounded best a continue buttons appears so they can continue to the next trial.

## Note
It is possible to divide the task over different sessions because the 
native speakers are about to hear many words. After each session the data for
that sessions is stored.

",2022-08-12
https://github.com/UiL-OTS-labs/matlab-zep-eyelinkdata-plot-trials,"# matlab-zep-eyelinkdata-plot-trial

## Matlab code to inspect zep-based experiment data using the Eyelink 1000

The output of a zep-eyelink experiment consists of a special type of .csv file in which
events from the zep program, time and position data from the eyelink 1000 eye tracker (and
possibly other custom defined data) are are integrated.

This is now a simple set of m-files that can be used to inspect these data visually. It
will probably grow into something bigger, aiming at baby research in which an eye tracker
is used as a device to objectify infant attentional behavior. Most eye trackers are a
black box: i.e. in the case of periods of missing data, it is impossible to know what was
the exact reason for data loss, esecially when infants are the subjects. Was it a blink? 
Was the child turning towards the caretaker or experimenter? Did a foot occlude the eye 
image analysed by the eye tracker?

In order to assess these events in greater detail, the plan is to eventually use this
code as a starting point for synchronising eye tracking data files and participant video,
with the goal to automate the task of selecting usable trials and improving baby eye 
tracking workflows. This is research and development code. Production implementation is
not the goal.

## Functionality
For now, this is just a simple, quite generic plotting tool that shows graphs like this:

```

                 zoomed out                  zoomed in
                 screen pixel based          to screen
                 x-y scatterplot             x-y plot in degrees

                        |                       |
                        V                       V
		-----------------------      -----------------------
		|                      |    |                       |
		|      |---------|     |    |                       |
		|      |   *---* |     |    |      O=======O        |
		|      |---------|     |    |                       |
		|                      |    |                       |
		|----------------------|    |-----------------------|

		|---------------------------------------------------|
	x/t	|_________/---------------------\_____              |  x signal/time (degrees)
                |---------------------------------------------------|

		|---------------------------------------------------|
	y/t	|-------------------------------------              |  y signal/time (degrees)
	        |---------------------------------------------------|

		|---------------------------------------------------|
	v/t	|         /\                    /\                  |  velocity/time deg/s
                |------^=/  \=====-=-==`-====--/- \^=---------------|

```

# Usage

* Input:   Data files in the 'data' directory.
* Output:  High resolution plots of trial periods in the data file.
           In 'inspect' directory.

* Check/adapt some paths/settings in the settings section of main.m
* Run ""main.m""

If you need help: get in touch with Jacco van Elst at lab support UiL OTS

			e-mail: j.c.vanelst %%%%% uu.nl

# Info, credits & references

Programmed using Matlab 2018a. Should work fine on versions 2014 and above (this is a
guesstimation).

## Matlab code
Some parts in the code have been re-used/adapted from work of Ignace Hooge,
Roy Hessels, Diederick Niehorster, Jeroen Benjamins @Utrecht University

## ZEP info
Veenker, T.J.G. (2018). The Zep Experiment Control Application (Version 1.14)
[Computer software]. Beexy Behavioral Experiment Software.
Available from http://www.beexy.org/zep/


",2022-08-12
https://github.com/UiL-OTS-labs/multimodal_jspsych,"# multimodal_jspsych
Multimodal web experiment made with jsPsych 6.1

Starting from the examples in the (jsPsych)[https://github.com/jspsych/moving-research-online-workshop], let's build a web experiment

output example as of now:

		""rt"": null,
		""stimulus"": ""_video/yi4h.mp4"",
		""key_press"": null,
		""target_audio"": ""_audio/_b3s.wav"",
		""target_video"": ""_video/yi4h.mp4"",
		""target_image"": ""_image/_b3i.png"",
		""condition"": ""Emotional Tone 4"",
		""trial_part"": ""single_target_learn"",
		""task_part"": ""test_learn_nine"",
		""trial_type"": ""video-keyboard-response"",
		""trial_index"": 1,
		""time_elapsed"": 6564,
		""internal_node_id"": ""0.0-1.0-0.0"",
		""subject"": ""t9m8zkbxl"",
		""emotion_condition"": ""emotional""


	{
		""rt"": 1946.0400000098161,
		""stimulus"": ""_audio/beep.mp3"",
		""button_pressed"": ""3"",
		""new_correct_index"": 4,
		""target_audio"": ""_audio/_b3s.wav"",
		""target_image"": ""_image/_b3i.png"",
		""condition"": ""Emotional Tone 4"",
		""trial_part"": ""nine_target_choose_neutral"",
		""task_part"": ""test_learn_nine"",
		""target"": ""_b3"",
		""trial_type"": ""audio-button-response-random"",
		""trial_index"": 2,
		""time_elapsed"": 8518,
		""internal_node_id"": ""0.0-1.0-1.0"",
		""subject"": ""t9m8zkbxl"",
		""emotion_condition"": ""emotional"",
		""accuracy"": false
	},


some things are stilla bit weird, but well...carry on

(TIMESTAMP(14)	YYYYMMDDHHMMSS))

booleans? 
https://stackoverflow.com/questions/289727/which-mysql-data-type-to-use-for-storing-boolean-values

So let's just start with these MySQL/Mariadb tables/types:


NAME                     | Type (general)         | MySQL type
-------------------------|------------------------|-----------------
rt 	      	             | float                  | DECIMAL
stimulus                 | string                 | VARCHAR
button_pressed           | integer/string/???     | CHAR(1) / tinyint? JS: ""1""
new_correct_index        | integer                | TINYINT
target_audio             | string                 | VARCHAR
target_image             | string                 | VARCHAR
condition                | string                 | VARCHAR
trial_part               | string                 | VARCHAR
task_part                | sting                  | VARCHAR
target                   | string                 | VARCHAR
trial_type               | string                 | VARCHAR
trial_index              | integer                | TINYINT 
time_elapsed             | integer (ms?)          | INTEGER
internal_node_id         | string                 | VARCHAR
subject                  | string                 | VARCHAR
emotional_condition      | string                 | VARCHAR
accuracy                 | boolean                | BOOL (tinyint)

(I suspcect we may want to also store timestamp formats, browser info etc? )



",2022-08-12
https://github.com/UiL-OTS-labs/ppn-backend,"# Adult participant administration system BACKEND

Adult participant administration system, written in Django.

## Introduction

This Django project is part of a two-application system used to keep track of 
adult participants, experiments and appointments for the UiL OTS Labs.

This project represents the backend application, which is for system 
administrators only.

## Requirements

* Python 3.9+ (3.8 might work, untested)
* Pip (for installing dependencies, see requirements.txt for details)
* A WSGI capable web server (not needed for development)
* A SQL database (tested with SQLite and MySQL)

## Installation

For production/acceptation deployment, please see our Puppet script. (Hosted on 
our private GitLab server).

Development instructions:
* Clone this repository
* Install the dependencies using pip (it is recommended to use a virtual 
  environment!). ``pip install -r requirements.txt``
* Run all DB migrations ``python manage.py migrate`` and ``python manage.py migrate --database auditlog``
* Edit ``ppn_backend/settings.py`` to suit your needs.
* Create a super user using ``python manage.py createsuperuser``
* Compile the translation files using ``python manage.py compilemessages``
* You can now run a development server with ``python manage.py runserver``

Note: you probably also want to set your new super user as the main admin, 
otherwise the frontend probably won't like you. (Read: it will error without a 
Main Admin)
(You can change this in the 'Admin' section in the menu).

Note 2:
As the ``runserver`` command defaults to using ``localhost:8000`` you will need 
to specify a different port for either the frontend or backend. If you're using
the default settings, the backend should run at port 9000. You can change 
the port the application will listen on by specifying it as an argument.

For example: ``python manage.py runserver 9000`` will set the port used to 9000

## A note on dependencies
We use pip-tools to manage our dependencies (mostly to freeze the versions 
used). It's listed as a dependency, so it will be installed automatically.

``requirements.in`` lists the actual dependency and their version constraints. 
To update ``requirements.txt`` just run ``pip-compile -U``. Don't forget to test 
with the new versions!",2022-08-12
https://github.com/UiL-OTS-labs/ppn-frontend,"# Adult participant administration system FRONTEND

Adult participant administration system, written in Django.

## Introduction

This Django project is part of a two-application system used to keep track of 
adult participants, experiments and appointments for the UiL OTS Labs.

This project represents the frontend application, which is used by experiment 
leaders and participants.

## Requirements

* Python 3.9+ (3.8 might work, untested)
* Pip (for installing dependencies, see requirements.txt for details)
* A WSGI capable web server (not needed for development)
* A SQL database (tested with SQLite and MySQL)

## Installation

For production/acceptation deployment, please see our Puppet script. (Hosted on 
our private GitLab server).

Development instructions:
* Configure a working backend first!
* Clone this repository
* Install the dependencies using pip (it is recommended to use a virtual 
  environment!). ``pip install -r requirements.txt``
* Edit ``ppn_backend/settings.py`` to suit your needs. (Make sure you update the 
  backend location setting to your local setup!)
* Run all DB migrations ``python manage.py migrate``
* Compile the translation files using ``python manage.py compilemessages``
* You can now run a development server with ``python manage.py runserver``

The frontend does need a running backend to talk to! Otherwise it will be very 
unhelpful towards you.

## A note on dependencies
We use pip-tools to manage our dependencies (mostly to freeze the versions 
used). It's listed as a dependency, so it will be installed automatically.

``requirements.in`` lists the actual dependency and their version constraints. 
To update ``requirements.txt`` just run ``pip-compile -U``. Don't forget to test 
with the new versions!",2022-08-12
https://github.com/UiL-OTS-labs/psycho-movie,"# psycho-movie
A custom build experiment for Imke Kruitwagen
",2022-08-12
https://github.com/UiL-OTS-labs/psylib,"# psylib
A C object oriented library, that may be helpful for implenting psychological experiments
",2022-08-12
https://github.com/UiL-OTS-labs/pyteensy,"# pyteensy
Python module interface to a teensy for accurate timing of stimulus events in psychological experiments
",2022-08-12
https://github.com/UiL-OTS-labs/PYTHON-luying-hou-renamescript,"# PYTHON-luying-hou-renamescript
Provides a very simple python script to copy and rename wav files based on a comma seperate value file. 
To the specifications of Luying Hou. In short, only the last attempt of a participant is taken and the trial number is replaced by the correct id number. 

# Usage
`python main.py [data dir] [csv dir] [output dir]`

Default directories are:

`[data dir] = data`

`[csv dir] = id`

`[output dir] = output`
",2022-08-12
https://github.com/UiL-OTS-labs/PYTHON-pl4curvefitter-carolien-van-den-hazelkamp,"#PL4fitter
Provides a least-square curve fitting implementation coded in python for a sigmoidal curve that fits the 4PL logistic equation:

 `((A-D)/(1.0+((x/C)**B))) + D`

*A is the minimum asymptote, B is the steepness, C is the inflection point and D is the maximum asymptote.*

In this implementation A is pushed towards zero and B is pushed to be postive (i.e. residual is set to 1e8).

# Dependencies
The script relies on python and several libraries. Likely libraries and the depencies you'll need to install are matplotlib, numpy and scipy. The best suggestion is to download a total package. I suggest Canopy (Express free)
 `https://store.enthought.com/`.

# Usage
 `python pl4fitter.py arg1`
Where arg1 is an optional CSV filename (See *exampledata.csv* for the format of the input document)

# Authors
Internals are inspired by 
 `http://people.duke.edu/~ccc14/pcfb/analysis.html`

Coded by **Jan de Mooij** and fine-tuned by **Chris van Run**
",2022-08-12
https://github.com/UiL-OTS-labs/PYTHON-sigmoidalcurvefitter-carolien-van-den-hazelkamp,"#sigcurvefitter
Provides a least-square curve fitting implementation coded in python for a sigmoidal curve that fits the sigmoidal curve equation:

`d(time) = lambda*(1-math.exp(-beta*(time-delta)))`
If time > delta, d(time) is 0. 
* En dan is lambda de asymptote, delta de intercept met de x-as (waar accuracy van kans afwijkt) en beta de steilheid van de curve. *



# Dependencies
The script relies on python and several libraries. Likely libraries and the depencies you'll need to install are matplotlib, numpy and scipy. The best suggestion is to download a total package. I suggest Canopy (Express free)
 `https://store.enthought.com/`.

# Usage
 `python sigcurvefitter.py arg1`
Where arg1 is an optional CSV filename (See *exampledata.csv* for the format of the input document)

# Authors
Internals are inspired by 
 `http://people.duke.edu/~ccc14/pcfb/analysis.html`

Coded by **Jan de Mooij** and fine-tuned by **Chris van Run**
",2022-08-12
https://github.com/UiL-OTS-labs/Qualtrics-Pseudo-Randomizer,"# Qualtrics Pseudo randomizer

Example call:
```var ran = new Randomizer(true, 2, 'general', true);```
",2022-08-12
https://github.com/UiL-OTS-labs/relative_clauses_in_french,# relative_clauses_in_french,2022-08-12
https://github.com/UiL-OTS-labs/sddm-theme-utrecht-university,"# SDDM theme Utrecht University

This is a quickly put together sddm theme set in the Utrecht University style. It is originally build for the Utrecht Institute for Linguistics.

## Test instruction:
Testing can easily be done by running sddm-greeter --theme _theme-directory_. From the top level directory this is:

`sddm-greeter --theme sddm-theme-utrecht-university/usr/share/sddm/themes/utrecht-university/`

## Build instruction:
* Open a terminal and navigate one level above the sddm-theme-utrecht-university-* folder
* Run `fakeroot dpkg-deb --build sddm-theme-utrecht-university*`
* You should now have an debian package that you can install by running `sudo dpkg -i sddm-theme-utrecht-university*.deb`.
",2022-08-12
https://github.com/UiL-OTS-labs/streamupload,"# streamupload
Proof-of-concept app for storing binary experiment data, such as (streaming) video. Its current primary intended use case is with [jspsych-cam-rec](https://github.com/UiL-OTS-labs/jspsych-cam-rec).

## Administration
Basic administrative functions are currently provided by the default Django admin interface. It can be found at https://<base_url>/admin/. Staff users can use the admin interface to add or remove users, groups, tokens, and uploads. Non-staff users can view and download uploaded files.

## To-do

- Connect to university LDAP
- Easier differentiation of participants
- Management of uploads by non-staff users
",2022-08-12
https://github.com/UiL-OTS-labs/test-zep,"# test-zep
A number of files/programs that belong to te article about ZEP.

The purpose of this article is to get a publication in a peer reviewed article. We 
Hope that this creates extra some extra momentum for Zep. There are many tools
one can use to create psychological/psycholinguistic experiments. Zep is special
to those, because it is a relatively simple language to master and does the very
difficult stuff under the hood. All files in this repository Fall under the GPL-2.0
open source software licence, for details see the LICENSE file at the root directory.

The articile aims to present Zep major version 2.0. Hence, the scripts in this repository
should be run with zep-2.x .

# Directory structure
The build tree has a number of directories with different purposes.
## eda
This directory contains some helper files to create electronic schematics of the
experiment setup. This presents an overview of how the different electronic connections
should be made.
## firware
This directory contains the firmware used to do the measuring with a teensy device. More
information about Teensy devices can be found at: https://www.pjrc.com/teensy/ . In short
a teensy device is a ARM-Cortex M4 development board. This allows us to time changes on
electronic sensors in a realtime fashion.
## utils (deprecated)
This directory contains small utilities used internally to help ourselves.
## zep-scripts
This folder contains 2 scripts in these scripts these script generate the data presented in
the article. These scripts use the zep teensy plugin to communicate with the teensy device.

When Zep connects with a teensy device, the device resets its time to 0, every x µs the timestamp
is increased. Currently x = 10µs. Zep only gets events about electronic sensor events and
cannot adjust the timing. Each event has a teensy timestamp, a pin value and the logical readout
value of such pin. This makes the teensy an objective time measure tool.
",2022-08-12
https://github.com/UiL-OTS-labs/textspeak,"# Textspeak
A lilt gist/script to process a list of strings to mp3 files with the strings read aloud, using gTTS 

Create linguistic/test-stimuli using python and the **[Google Text-to-Speech](https://pypi.org/project/gTTS/)** module.

## Requires
- **[Python](https://python.org)** installed
- **gTTS** installed

## Limitations
- No voice options
- Minimal options overall

## Usage
- Edit the list to words you want spoken aloud, called *mytextlist* in the file **ts.py**
- Save it
- Run it using, for instance using the command: 

    python3 ts.py
    
Output mp3's will be in the same directory as ts.py


",2022-08-12
https://github.com/UiL-OTS-labs/uil-sync,"# uil-sync
Syncing data from a pc to a project folder
",2022-08-12
https://github.com/UiL-OTS-labs/uil_psychopy,"# UiL-PsychoPy
A number of utilities to run a psychopy experiment in the lab
",2022-08-12
https://github.com/UiL-OTS-labs/vislexdec_vp_masked,"# vislexdec_vp_masked
",2022-08-12
https://github.com/UiL-OTS-labs/web-experiment-datastore,"# Web experiment datastore

This Django project provides a generic datastore, intended to store data from
web experiments. 

# Requirements
- Python 3.9+
- Pip (see requirements.txt)
- A WSGI capable web server (not needed for development)
- A SQL database (tested with SQLite and MariaDB)
- When using mariadb: client and dev libraries

# Translations

Against Django standard practice we use _translation keys_ to provide 
translations, instead of wrapping text in one language in gettext calls. While 
this adds the need for translation files for every supported language, it fastly
reduces whitespace weirdness and other issues with the standard approach. 

Keys are formatted in a standard way:

``{django_app}:{location}:{item}(:{optional_extra})*``

- django_app: The django app this key is part of
- location: the location this key presides in, for example:
    - model: it's a model field description
    - form: it's part of a form
    - {template}: it's part of a template called {template}
- item: Identifier for this exact string, for example:
    - header: it's the view's header
    - {field}: the name of the model field
- optional_extra: 0-n extra keys to differentiate different strings for the same
  item. For example, a model field sometimes has a help_text in addition to a
  verbose_name. In that case, the optional_extra should be ``:help_text``

# Installation

For production/acceptation deployment, please see our Puppet module. 
(Hosted on our private GitLab server).

Development instructions:
* Clone this repository
* Install the dependencies using pip (it is recommended to use a virtual 
  environment!). ``pip install -r requirements.txt``
* Run all DB migrations ``python manage.py migrate``
* Run all auditlog migrations ``python manage.py migrate --database auditlog``
* Edit ``ppn_backend/settings.py`` to suit your needs.
* Create a super user using ``python manage.py createsuperuser``
* Compile the translation files using ``python manage.py compilemessages``
* You can now run a development server with ``python manage.py runserver``


## A note on dependencies
We use pip-tools to manage our dependencies (mostly to freeze the versions 
used). It's listed as a dependency, so it will be installed automatically.

``requirements.in`` lists the actual dependency and their version constraints. 
To update ``requirements.txt``, edit ``requirements.in`` and run 
``pip-compile -U``. Don't forget to test with the new versions!
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Advanced-Digitspan-BOILERPLATE,"Experiment:
        Visual Forward Digit Span

Description:
        Memory span test to measure the longest list of digits that a 
        participant can recall after presentation. The participant is visually 
        presented a list of digits (one more for each trial). Alternatively it 
	is possible to provide audio feedback. The participant's
        task is to type in the digits he/she saw in presented order or reversed order. 
	Trials repeat until a mistake is made or until a maximum number of trials 
        done. Self-paced. Output: digit span.

Author:
        Chris van Run (UiL-OTS) <C.P.A.vanrun@uu.nl>

Client:
        -

Supervisor:
        -

References:
        -


You can start up the experiment by typing:

        zep digitspan

For more information on running the experiment and extracting the experiment
results please go the the Zep website at http://www.hum.uu.nl/uilots/lab/zep
and check out the documentation section. There you'll also find explanations
and instructions that help you understand and modify a Zep experiment.


DISCLAIMER

This experiment script is released under the terms of the GNU General Public
License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in
the hope that it will be useful, but with absolutely no warranty. It is your
responsibility to carefully study and test the script before using it with 
real participants.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Animations,"# Animation Sniplet

This is a test sandbox for CSV based animation of ImageShapes of ZEP. Tested in 1.10

This entails that you define the movements to be made relative to the current situation (at the start of the animation). You do this in an csv. Examples of animations can be found in:
 stimuli/animations/

Note that everything is defined relative. For instance, setting x to 10px will INCREASE the x with 10px. Defining it as 0px or 0% equals the values at the start of the animation. That way, returning to the start position can be done by setting everything to 0.

# Member Details

Some of the variables (i.e. x, y, width, height) can be defined in percentage.

The *rotation* is clockwise and in radials (i.e. 2pi equals one full rotation).

The *y axis* is orientated to be positive towards the bottom. That is, the [0;0] point is in the top left and [0;10] is 10px lower. 

# Bugs

Changing height or width while in a rotation influences the picture.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Artificial-Language-Learning,"# ZEP-Artificial-Language-Learning
Experiment:
        Artificial Language Learning Task

Description:
        Purpose of this experiment is to let subject hear two different 
        non-existing words and have them choose which word is correct.

        This experiment is self paced.

        Output: 
                Response value;
                Reaction time (milliseconds); 
                Correctness (Yes/No);
                Number of times repeated;

Author:
        Jan de Mooij (UiL-OTS) <A.J.deMooij@uu.nl>

Client:
        -

Supervisor:
        -

References:
        -


You can start up the experiment by typing:

        zep artlanlrn.zep

For more information on running the experiment and extracting the experiment
results please go the the Zep website at http://www.hum.uu.nl/uilots/lab/zep
and check out the documentation section. There you'll also find explanations
and instructions that help you understand and modify a Zep experiment.


DISCLAIMER

This experiment script is released under the terms of the GNU General Public
License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in
the hope that it will be useful, but with absolutely no warranty. It is your
responsibility to carefully study and test the script before using it with 
real participants.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Auditory-Discrimination-Josje-Verhagen,"# Experiment: Auditory Discrimination (AB)

# Description:
Purpose of this experiment is to measure a participant's ability to
discriminate (speech) sounds. For each trial two sound stimuli are
presented. Participant's task is to indicate which of the two stimuli
are part of the lanuage they previously heard.
Self-paced. Output: chosen value, RT and
correctness.

# Pseudorandomisation
* Participant field `Counterbalance_order_yes_no` enables the swap of the first triplet and second triplet order and also swaps the expected answer.
* No more than two trails in a row of the same type.
# Author(s)
* Theo Veenker <theo.veenker@beexy.nl>
* Chris van Run <C.P.A.vanrun@uu.nl>

# Client:
        -

# Supervisor:
[Josje Verhagen](https://www.uu.nl/medewerkers/jverhagen)

# References:
        -


For information on running the experiment and extracting the experiment
results please go the the Zep website at http://www.beexy.nl/zep and check
out the documentation section. There you'll also find explanations and
instructions that help you understand and modify a Zep experiment.


# DISCLAIMER

This experiment script is released under the terms of the GNU General Public
License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in
the hope that it will be useful, but with absolutely no warranty. It is your
responsibility to carefully study and test the script before using it with
real participants.
",2022-08-12
https://github.com/UiL-OTS-labs/zep-baby-habituation,"# zep-baby-habituation
Baby experiment using the visual habituation paradigm

# This task is used to run a habituation paradigm.
During each trial the gaze of the participant is lured by a attention grabber.
The attention grabber is a series of pictures that mimics a movie of a
laughing baby. When a participant is looking to the grabber the experimenter
Presses a button to start the main part of the trial. In each trial the
baby is looking to a bulls eye stimulus and hears repeated words.
If the baby stops looking the experimenter presses a button and if the
experimenter doesn't cancel this action the trial will finish in 2 seconds.
During a trial the cumulative looking and not looking time is recorded.


## Habituation
In the habituation phase trials are presented as described above. After
the sixth trial on we examine whether the infant meets the habituation
criterion. This criterion is met once the mean cumulative looking time of the
last three trial is less then 65% of the first threesome. Once the criterion
is met the test phase is started.

## Test phase
Two trials are presented.

## Attention phase
One trial is presented with a completely deviant stimulus. Generally it is
expected that a attentive baby will exhibit an increase in looking time,
since a novel stimulus is interesting.

## finish
The experiment finishes after a happy song is played along with some infant
friendly pictures.
",2022-08-12
https://github.com/UiL-OTS-labs/zep-causal-esp-mw,"Experiment:
        Self-Paced Reading with Moving Window

Description:
        Purpose of this experiment is to measure word or segment reading times
        in sentences, using a self-paced word/segment revealing mechanism. 
        Participant's task is to read sentences which are presented in a 
        segment-by-segment fashion. Participant reveals next segment by hitting
        a button. RT is measured from the presentation of a segment to button-
        press. Comprehension questions are implemented. Self-paced. Output: RT per segment.

        This particular SPR implementation uses a moving window: For each 
        segment the text in the segment is displayed at its normal position 
        in the sentence. The words outside the segment window are displayed 
        as underlines. In other words the visual structure of the whole 
        sentence is visible, but only the words in the segment window are 
        readable.

Author:
        Theo Veenker <theo.veenker@beexy.nl>

Client:
        -

Supervisor:
        -

References:
        Just, M. A., Carpenter, P. A., & Woolley, J. D. 1982. 
          Paradigms and processes in reading comprehension. 
          Journal of Experimental Psychology: General, Vol 111: 228-238.


For information on running the experiment and extracting the experiment
results please go the the Zep website at http://www.beexy.nl/zep and check 
out the documentation section. There you'll also find explanations and 
instructions that help you understand and modify a Zep experiment.


DISCLAIMER

This experiment script is released under the terms of the GNU General Public
License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in
the hope that it will be useful, but with absolutely no warranty. It is your
responsibility to carefully study and test the script before using it with 
real participants.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Cyberball-elisabeth-nieuwburg,"Cyberball
=========

Cyberball is a virtual ball-toss game that can be used for research on ostracism, social exclusion or rejection. It has also been used to study discrimination and prejudice. Four players (three computer controlled and one human) play a game of tossing a ball around. After a set number of throws two of the computer-controlled players will both start to ignore another computer-controlled player.

* Uses vector-based graphics for high visual fidelity.
* Ball behavior during throw has visual distinct spin, height, and speed.
* All throws can be balanced prior to the exclusion phase (i.e each bot receives similar number of throws).
* Includes a small practice.
* Allows the player to input name and select gloves.
* Names of players and faces differ based on the gender of the participant.

Settings
========
Most settings can be changed in the
`./game/settings.zm`
    
Additions
=========
In addition the script now allow to specify specific throw sequences. These can be adjusted in
`./stimuli/throws/`
And are simple .csv files.

You can now also add overlay texts to be shown at specific throws
`./stimuli/overlay_texts/`

Several 7-scale rating questions have been added see `./stimuli/rating_questions.csv` 

Groups
======
There are four groups. Only `C` does not show the conditional instructions.
* `C` :- Control
* `AC` :- Active Control
* `POSR` :- Positive Repraisal
* `NEGR` :- Negative Repraisal
 
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-descriptive-stats,"# ZEP-descriptive-stats
provide stats for arrays of real number.

# Goal
Zep doesn't seem to come with functions to obtain statistics for arrays.
This module attempts to write some functions to calculate simple descriptive statistics.

- Sum of squares around the mean
- Variance
- Standard Deviation
- Standard Error
- Standard of the mean.
- mean
- meadian 
- min
- max

",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-detection-task-xin-li,"## Overview
Auditory discrimination. Participant is to learn which audio fragments are part
of the language and which are not.

Current button setup is SPACE_BAR continue and LEFT and RIGHT SHIFT for the choice.

Alternative one can use an attached [BeexyBox](http://www.beexy.org/responseboxes/) which will result in more accurate reaction time measurements.

## Instructions on how to Zep
* [Installing an Experiment](https://www.beexy.nl/zep/wiki/doku.php?id=experiment:installing)
* [Running an Experiment](https://www.beexy.nl/zep/wiki/doku.php?id=experiment:running)
* [Extracting Experiment Results](https://www.beexy.nl/zep/wiki/doku.php?id=experiment:results)

## Pseudorandomisation
For word_leaning:
* Two similar images are not allowed; even if it loops.
*
For detection_practice:
* No more than two of the same expected answer trials in sequence
*
For detection_test:
* No two of the same target audio files in sequence.

## Particpant fields
* Left or right-handedness (this will swap the buttons)
* Language (NL or BJ)

## Adding Stimuli
See `modules/stimuli.zm` for how stimuli should be configured.

## Custom Instruction Text
Edit the variables found in `modules/texts_en.zm` to change the instructions shown to the participant.

## Misc settings
See `test/defs.zm` for settings.

The width of images can be globally set by changing the value of `IMAGE_WIDTH_PX` to some pixel value.

## DISCLAIMER
This experiment script is released under the terms of the GNU General Public License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in the hope that it will be useful, but with absolutely no warranty. It is your responsibility to carefully study and test the script before using it with real participants.

## Request details
### Script Author
[C. van Run, MSc](http://www.uu.nl/staff/CPAvanRun)
### Client
[Xin Li, MA](https://www.uu.nl/staff/XLi3/0)
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Discrimination-ABX-Experiment,"Experiment:
        Match-to-sample Auditory Discrimination (ABX)

Description:
        Purpose of this experiment is to measure a participant's ability to
        discriminate (speech) sounds. For each trial three sound stimuli are
        presented. Participant's task is to indicate whether the third stimulus
        is a replication of the first stimulus or a replication of the second
        stimulus. Auto-paced. Output: chosen value, RT and correctness.

Author:
        Theo Veenker <theo.veenker@beexy.nl>

Client:
        -

Supervisor:
        -

References:
        -


For information on running the experiment and extracting the experiment
results please go the the Zep website at http://www.beexy.nl/zep and check 
out the documentation section. There you'll also find explanations and 
instructions that help you understand and modify a Zep experiment.


DISCLAIMER

This experiment script is released under the terms of the GNU General Public
License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in
the hope that it will be useful, but with absolutely no warranty. It is your
responsibility to carefully study and test the script before using it with 
real participants.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-emotional-mandarin,"# ZEP-emotional-mandarin

A 4AFC speech perception exeriment that investigates emotional context effects 
on word meanings in spoken mandarin Chinese.

## Context & Purpose

In tonal languages (mainly Asian languages) the 'tone' of speech sounds is a 
crucial aspect for their meaning. Due to this, L2 learners usually struggle a 
lot when learning such a language, since most European languages use tone only 
for intonation, not word meaning. To complicate matters even more, the 
emotional context (and intonation too, regardless) may also play an important 
role, even in tone languages. 

## Task 

In this study, Native Manadarin speakers are compared to Dutch learners of 
Mandarin (L2). Simple sentences have been prepared by professional voice 
actors, reading target nonwords embedded in carrier sentences. Participants
ara asked to select the **tone** of a target nonword. The latin square design 
is implemented.

# Conditions

Conditions are mixed ""latin squared"" variations of this matrix:

**Condition               |   I         |    II       |   III       |   IV  
--------------------------|-------------|-------------|-------------|-----------
**Target** syllable       |   mong      |    ra       |   ging      |   bu  
**Tone**                  |  T1         |  T2         |   T3        |   T4 
**Carrier sentence**      | ""Point""     | ""Read""      | ""Practice""  | ""Write""
**Emotion**               | Neutral     | Happy       |   Sad       | Angry


Programmed by UiL OTS, program created for Yachan Lian

",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-entr,"# ZEP-entr
Discovering the effects of entropy in stimulus input

# Goal
Studying the effects of entropy on learning of words.

# Notes
This experiment is an adaptation of an existing experiment
On request of the experimentor the stimuli are not tracked online
Hence, they should be added prior to starting the experiment.
",2022-08-12
https://github.com/UiL-OTS-labs/zep-flanker-bilingual,"The target is flanked by non-target stimuli which correspond either to the same
directional response as the target (congruent flankers), to the opposite
response (incongruent flankers), or to neither (neutral flankers).
The test can be used to assess the ability to suppress responses that are
inappropriate in a particular context.

Randomisation is according to these rules:

* No more then 3 consecutive sides (direction).
* No more then 2 consecutive flanker (congruency).
* Never two consecutive trials that have the same congruency AND side.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Flanker-Task-BOILERPLATE,"The target is flanked by distracting stimuli which correspond either to the same
directional response as the target (congruent flankers), to the opposite
response (incongruent flankers), or to neither (neutral flankers).
The test can be used to assess the ability to suppress responses that are
inappropriate in a particular context.

The task is response contingent, that is the target + flankers are displayed
until the participant responses. After the response the new target is displayed
according the Response Stimulus Interval (RSI) in the table with test items.
In the current example trials with different RSI values exist, 200, 500 and 700
ms respectively.

Randomization is according to these rules:

* No rules, they are pseudo randomized with no constraints on item order.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Four-Image-Visual-World-BOILERPLATE,"#Experiment:
Four Image Visual World

#Description:
Purpose of this experiment is to record participant's eye-movements
while he/she is listening to a spoken utterance and looking at a
screen displaying a semi-realistic scene. For each trial a scene
is displayed and an utterance relating to the scene is played.
Participant's task is to carefully look and listen. Self-paced.
Output: Eye-Tracking data as collected by the eye-tracker.

This experiment has slightly different response values if you compare it whith
the visworld1 and visworld2 experiments, those contain 1 and 2 pictures
respectively, whereas this experiment contains 4. Hence, the participant can
select 4 images. They can do this with the keypad buttons 4, 5, 1 and 2.
Or a 4 button Beexy box.

#Pseudo randomization
Current pseudo randomization rules:
* First item must be a filler item.
* The maximum number of subsequent non-filler items is 3.
* The maximum number of subsequent same type non-filler items is 2.
You can test the pseudo randomization by running
 `zep test_pseudorandomisation.zp`

#Regions of interest
You can make regions of interest selectable (i.e. equivalent with a button) by setting
 `SELECT_ITEMS_BY_MOUSE = true;`
in defs.zm

You can highlight the regions of interest (default configured to overlap with pictures) by setting
 `SHOW_REGION_OF_INTEREST = true;`
in defs.zm. This allows you to double check if they are what you expect them to be.

#Screen captures for post processing
To generate pictures of the screen for region-of-interest selection run
 zep quick_show_and_save_pictures.zp
This will output pictures (PNG) in
 `./data/visworldx/img/`
 You might need to change the name of the directories based on your post processing of choice.

#Author:

#Client:

#Supervisor:

#References:
Huettig, F., Rommers, J., Meyer, A.S. (2011).
Using the visual world paradigm to study language processing:
a review and critical evaluation.
Acta Psychologica.

#DISCLAIMER

This experiment script is released under the terms of the GNU General Public
License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in
the hope that it will be useful, but with absolutely no warranty. It is your
responsibility to carefully study and test the script before using it with
real participants.
",2022-08-12
https://github.com/UiL-OTS-labs/zep-fr-mw-listen,"# zep-fr-mw-listen
Moving window listening task (French)
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-htp2020,"# zep-baby-head-turn-paradigm
Baby experiment using the head turn paradigm

# This task is used to run a head turn paradigm.


## Familiarization
In the habituation phase trials are presented as described above. After
the sixth trial on we examine whether the infant meets the habituation
criterion. This criterion is met once the mean cumulative looking time of the
last three trial is less then 65% of the first threesome. Once the criterion
is met the test phase is started.

## Test phase


#finish
The experiment finishes after a happy song is played along with some infant
friendly pictures.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-hybride-visual-fixation,"# Hybrid Visual Fixation Experiment

The hybrid visual fixation or habituation (`hvf`) can be used to access
perception in younger children. It involves measuring looking times via scoring
a video feed of the child. This experiment has been requested by
[Maartje de Klerk](http://www.uu.nl/staff/MdeKlerk/0) and has been programmed by
 [Chris van Run](http://www.uu.nl/staff/CPAvanRun) and -- later on --
[Jacco van Elst](https://www.uu.nl/staff/JCvanElst).

# Parts & Phases
There are two main parts.
- Attention testing : assesses the current attention span of the participant.  
- Fixation testing : assesses the amount of fixation on certain auditive
tokens.  

These are presented in four phases:  
- Pre-test phase (attention testing)  
- Habituation phase (fixation testing)  
- Test phase (fixation testing)  
- Post-test phase (attention testing)  

# Specific definitions (for the linguistics-naive programmer)    

I (Jacco) have been confused by the use of the words 'token', 'trial', 'type'
etc. in the context of, eg. linguistics vs. programming, and maybe even more
so by the HVF paradigm. For the purpose of clarity, I've tried to be overly
concise here. Recommended quick reading for novices in this field, like me:

[wikipedia article on type-token disctinction](https://en.wikipedia.org/wiki/Type%E2%80%93token_distinction)

- **Tokentuple** : a pair of sounds that defines a unique unit within a trial.
- **Type** : a sound type, eg. a Dutch native non-word 'feep', phonetically
/fe:p/
- **Token** : a 'version' of a specific sound type, which could in practice
mean ('feep' as example):
	- Tokens are recorded versions of the type 'feep' as pronounced by
	**one speaker** (one speaker verbalized a few instances of /fe:p/, these
	recordings are all different tokens of the same type).
	- Tokens are recorded versions of the type 'feep', as pronounced by
	**different speakers**, i.e.**multiple speakers** verbalized the type
	'feep', /fe:p/
	- Two tokens in a trial could thus be different 'versions' based upon
	either/or/both aspects: in this case; speaker and recorded 'take'.
	- (Even if two clearly different types are in a tokentuple (/fe:p/ by
	speaker one, /fa:p/ by speaker two), one will always call them **both**
	tokens, simply because they *are*. This tends to be confusing.)
- **Trial**: in the HVF experiment, a trial consists of quite some repetitions
of a smaller sub-unit, which is a bit ill defined. I introduced the word,
'tokentuple' for it here, in lack of a better word at the moment. During a
trial, a single image (visual) is always accompanied by an amount of repetitions
of the same tokentuple (a pair of sounds). The trial is finished when the child
is no longer looking at the screen. This can be after only one repetition, or
only after 30. Slightly confusing, these repetitions are often referred to as
*trial repetitions*.



# Input
The `stimuli` directory contains are csv files which are used for the fixation
testing. Other stimuli are defined in the `stimuli.zm` module in `./modules`,
`./test` and `./attention_test` directories. The other input is gathered from
the `global_defs.zm` and `defs.zm` modules. The name of the input-csv files are
expected to be build from the three determinators that are present in the
participant data:

* contrast (native/nonnative)
* first alteration (alt/nalt)
* group (one or two)

Determined by a *combination of contrast and group* as filled out in the
participant GUI, a certain csv file with tokens is selected (A1/A2/B1/B2).
If the contrast is **native**, a **type A csv file** is selected, and if
nonnative, a **type B csv file**. Which one of the two versions is habituated
-- and after that -- tested with alternations, depends on the value of
**group** (`group_one_two`) as given in the participant info.



The following participant info is relevant for the study:

Name                       | Description                              | Valid choices           | Comments
---------------------------|------------------------------------------|-------------------------|-----------------------------------------------------------------------------------------------
participant ID             | Some name or code                        | Baby1, B017, etc.       |
created                    | ISO time stamp                           | 2023-03-14 03:14:59     | Automatically filled
gender_m_f                 | Gender                                   | ""m"" or ""f""              |
age_months                 | Age in months                            | All integers above 0    | Careful with this value
type_risk_control          | Whether subject is 'special'              | ""risk"" or ""control""     | E.g. dyslexia in family
contrast_native_nonnative  | Are both sounds native?                 | ""native"" or ""nonnative"" | If native, select 'type A' csv, otherwise type B!
first_alternation_alt_nalt | Whether the first trial alternates or not | ""alt"" or ""nalt""         | If ""nalt"", the first trial does not alternate, but the second does. And vice versa.
group_one_two              | Determines which token is trained/tested | ""one"" or ""two""          | I.e.: train on ""faap"" and contrast with ""feep"", or vice versa. (native example, given Dutch language).

# CSV file names and contents in relation to the current experiment goal

Each file name needs to be in a test or a habituation version (prefix: 'hab_'
or 'test_').Examples are: `test_native_alt_A1.csv` or `hab_B2.csv`.
*Test type* csv files contain two sounds that are contrasted, *habituation
type* files contain. In our current operationalization, the ""not on first
token alternating"" only determines whether **the first pair of tokens** is a
real contrast or not. So, for testing, an ""alt"" .csv in the current
operationalization implies contrasts on ID 1, 5, 8 and 12, whereas ""nalt""
implies contrasts on ID 2, 5, 8 and 12.

# Tokens, speakers and trials (specific)
In the current setup, 4 speakers have been recorded, reading the tokens aloud.
Of one speaker, multiple versions of the same type are used in the testing
phase, so we have the same voice, reading the word aloud a bit differently.
~~In this operationalization, sound file names in the .csv file contents like
""feep1.wav"" and ""feep5.wav"" imply: ttype ""feep"", the *same voice*,
*different versions*, i.e. different tokens.~~

**Update**: a new naming convention has been introduced in order to work towards a more generic way of using and reusing this type of experiment.

The new format for naming stimuli becomes:

Speaker*speakerID*\_*type*\_*token*

eg.

	'Speaker1_faap_1' ---------> The first token by speaker 1 for type 'faap'
	'SpeakerJohnDoe_sEn_23' ---> The 23d token by speaker JohnDoe for type 'sEn'


This way, sounds and coding of sounds is better identifiable at a human **and** computer science level.


# Example

#### Habituation on 'feep' tokentuples
feep1 = sound 'feep' by speaker 1, tokens in tuple are identical.

id | sound 1              | sound 2
---|----------------------|-------------------------------------
1  | Speaker1_feep_1.wav  | Speaker1_feep_1.wav
2  | Speaker2_feep_1.wav  | Speaker2_feep_1.wav
3  | Speaker3_feep_1.wav  | Speaker3_feep_1.wav


... (etc. until habituated. The second token (Speaker1_feep_2) is not used during
habituation)

Ie.: for each item in the csv/list, a tokentuple consists of two
identical speech sounds (types), made by the same speaker (voice).

#### Testing contrasts for type ""feep"" (contrast with ""faap"").
In the testing phase, a different flow starts. Let's say this is a ""nalt""
version. Here, *token (version) 2* is *new* to the participant (so the contrast is on **type only**), with the newest type always presented first in the token-tuple.

id | sound 1    			      | sound 2   			      | extra (not in .csv)
---|------------------------|-----------------------|----------------------------------------
1  | Speaker1\_feep**\_2**  | Speaker1\_feep**\_2**  | Note the 'feep' token (version) 2 has not been heard before (token 1 was used in habituation)
2  | Speaker1\_**faap**\_2  | Speaker1_feep_2       |  the first actual contrast in terms of types
3  | Speaker1_feep_2        | Speaker1_feep_2       |
4  | Speaker1_feep_2        | Speaker1_feep_2       |
5  | Speaker1\_**faap**\_2  | Speaker1_feep_2       | alternation (type  contrast) on 5
6  | Speaker1_feep_2        | Speaker1_feep_2       |
7  | Speaker1_feep_2        | Speaker1_feep_2       |
8  | Speaker1\_**faap**\_2  | Speaker1_feep_2       | alternation on 8
9  | Speaker1_feep_2        | Speaker1_feep_2       |
10 | Speaker1_feep_2        | Speaker1_feep_2       |
11 | Speaker1_feep_2        | Speaker1_feep_2       |
12 | Speaker1\_**faap**\_2  | Speaker1_feep_2       | alternation on 12

# Output
One can get output by running `zepdbextract` in the experiment directory.
This generates the following tables.

*Every look*
* hvf-01-pre_test_attention-1.csv
* hvf-01-habituation-1.csv
* hvf-01-test-1.csv
* hvf-01-post_test_attention-1.csv

*Every look collapsed/summised per trial*
* hvf-01-summary_habituation-1.csv
* hvf-01-summary_post_test_attention-1.csv
* hvf-01-summary_pre_test_attention-1.csv
* hvf-01-summary_test-1.csv

*Inclusion criteria per particpant (minimum habituation and minimum attention)*
* hvf-01-inclusion_variable_summary-1.csv

# Requirements
- Zep installed. (From [here](http://beexy.org/zep/wiki/doku.php?id=download)
for instance)
- This experiment unarchived.
- A camera to record participant.
- A method to overlay the camera stream with the control window.
- A BeexyBox response box or for less accurate look-time measurements: a
keyboard.

# Running the experiment
- Navigate via the CLI/CMD to the folder or run `linux-terminal.sh` /
`windows-terminal.bat`.
- Run the command `zep hvf.zp`.
- Create a participant and enter participant data via the control window.
- Press `start`.
- Use the response box to continue and mark looking time onset/offset.
(Note: keyboard alternatives are `t` for looking time and `spacebar` for
continue)
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Identification-Yes-No-Experiment,"# ZEP Identification Yes No Experiment
Experiment:
        Auditory Identification with Yes-No Response

Description:
        Purpose of this experiment is to measure a participant's ability 
        to identify (speech) sounds. For each trial a sound is presented. 
        Participant's task is to jugdge whether a prespecified property is
        present in the stimulus or not. Self-paced. Output: chosen value.

Author:
        Theo Veenker (UiL-OTS) <T.J.G.Veenker@uu.nl>

Client:
        -

Supervisor:
        -

References:
        -


You can start up the experiment by typing:

        zep ident_yn

For more information on running the experiment and extracting the experiment
results please go the the Zep website at http://www.hum.uu.nl/uilots/lab/zep
and check out the documentation section. There you'll also find explanations
and instructions that help you understand and modify a Zep experiment.


DISCLAIMER

This experiment script is released under the terms of the GNU General Public
License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in
the hope that it will be useful, but with absolutely no warranty. It is your
responsibility to carefully study and test the script before using it with 
real participants.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Inference-Test-hannah-mulder,"
# ZEP experiment: Inference
Five images and text is shown. The participant is to make a choice and select one of the corner images.

Trials start by pressing SPACE. Selection of the images can be done via either a 4-buttoned response box or the Numpad keys 4, 5, 1 and 2 for top left, top right, botton left and bottom right respectively.

The image of choice will highlight for a short while.

Eyetracking is possible but turned off by default. To turn on make sure the correct booleans are set in the test_items and defs: USE_EYETRACKING.

# AUTHOR(s)
* Chris van Run (C.P.A.vanrun@uu.nl)
* Rianne Vlaar (M.A.J.Vlaar@students.uu.nl)

# Groups
There are four possible groups (GRP1..GRP4). These have no effect other than to support participant categorization.

# Audio
Original script allowed the playback of audio. This has been indirectly disabled by leaving the audio filename input for the stimuli empty.

# Customisation
You can change the content of the stimuli by editing the .csv files in `stimuli` directory. For ease of editing the stimuli files are specified in different files for each of the types: A, B or C.

Images size, location and scaling can be adjuste via the following values in `modules/defs.zm`:
    TEST_CANVAS_SIZES_WIDTH = 400;
    TEST_CANVAS_SIZES_HEIGHT = 400;
    DISTANCE_OUTER_PICTURES = 300;
    IMAGE_SCALING = 0.5; //Liefst op 1.0

Texts (instructions and buttons) can be edited by changing the line `import text_nl` to `import text_en` at the header in `infer.zm`. One can edit the content of the text variables in `modules/texts_nl.zm` or `texts_en.zm`.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-magic-variables,"# ZEP-magic-variables

The code belonging to the Magic variables tutorial of the http://uilots-labs.wp.hum.uu.nl/ website
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Magnitude-Estimation-of-Linguistic-Acceptability,"Experiment:
        Magnitude Estimation of Linguistic Acceptability

Description:
        Purpose of this experiment is to measure the acceptability of a stimulus 
        (a sentence) a participant is perceiving relative to a standard stimulus
        (also known as the modulus). In every block, first, the modulus will be displayed. 
        For the modulus, the participant will enter a numerical value stating the acceptability 
        of the sentence. Then for each test trial the participant will be presented a sentence 
        to be judged. His task is to assign a score to the sentence relative to the score 
        entered for the modulus. 

        The sentences are displayed in a self-paced reading format, using a cumulative window 
        paradigm: the participant starts off with a single segment and he reveals next segment by hitting
        a button. The reaction times are stored, including one for a ""special"" word in the sentence, 
        a word that is most likely to cause a longer reading/reaction time. The resulting reaction times can 
        be used to cross-validate the magnitude estimations. 

        Note: the instructions and button texts are in Spanish.

Author:
        Martijn van der Klis (UiL-OTS) <M.H.vanderKlis@uu.nl>
        based on an earlier scripts by Theo Veenker for Auditory Magnitude Estimation and Self-Paced Reading: Continuous Window

Created for:
        Sergio Salvador Gutiérrez

References: 
        - Bard, E.G., Robertson, D., & Sorace, A. (1996). 
            Magnitude Estimation of Linguistic Acceptability. Language, 72(1), 32-68. 
            Retrieved from http://lingo.stanford.edu/sag/L227/BRS_1996.pdf.
        - http://www.stanford.edu/~tylers/notes/empirical/Mag_estimation_Bard_et_al_1996.pdf
        - http://www.powershow.com/view/1b513-MTRmZ/
            Magnitude_estimation_of_linguistic_acceptability_applications_to_research_on_developing_grammars_powerpoint_ppt_presentation


You can start up the experiment by typing:

        zep lexmagest

For more information on running the experiment and extracting the experiment
results please go the the Zep website at http://www.hum.uu.nl/uilots/lab/zep
and check out the documentation section. There you'll also find explanations
and instructions that help you understand and modify a Zep experiment.


DISCLAIMER

This experiment script is released under the terms of the GNU General Public
License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in
the hope that it will be useful, but with absolutely no warranty. It is your
responsibility to carefully study and test the script before using it with 
real participants.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Markers-Module,"# ZEP-Markers-Module

This [Zep](https://www.beexy.nl/zep/wiki/doku.php) module provides a way to send markers (i.e. triggers) from your Zep experiment using a parallel port. For instance you can send triggers from an EEG experiment to the BioSemi USB receiver.

Markers are setup sometime before their required onset. The timers can hold one marker at a time. Hence, if you set up markers in sequence the scheduler will plan to transfer them to the timers milliseconds before their onset. In this way the scheduler limits the frequency you can send markers at. This limit (using default settings) is ~133 Hz (or at 7.5ms intervals).

## Requirements for this module
*   [Zep version 2.0.9 or later](https://beexy.nl/zep2/wiki/doku.php?id=get_zep)
*   A parallel port (either via a PCIe peripheral or a direct bus from the motherboard).

## How to test this module
Simply run `zep-2.0 example.zp` and investigate the `example.zp` file to review some functionality.

## How to use this module
1.  Copy the files `zep_markers.zm` and `zep_markers_settings.zm` from this repository to a location found by your experiment (_e.g._ your experiment's `/modules` directory).
2.  Within your experiment import the module by adding `import zep_markers;` to
the top of your `.zp` file.
3.  Within your experiment script, after setting up the presentation of a
stimulus add the following function call:

    `setup_marker_at(<int marker>, <time tref>);`

    With _marker_ being the integer you want to send and _tref_ set to the _expected_start_time_ of the stimulus that has been set up.
4.  _Alternatively_, you might want to send a marker as quickly as possible. Use the following function call for that:

    `send_marker(<int marker>);`

    This will set up a marker to be sent as quickly as possible. Because of internal logistics there is a minimum setup time for the marker. This setup is determined by the sum of `SCHEDULER_PRE_EMPT` and `SCHEDULER_PRE_EMPT_ERROR_TOLERANCE` settings. Using default settings this time is approx. 7.5ms.
5.   _Alternatively_, if you want to incorporate a marker that is based on a response of the participant the best way to do this is to add a fixed delay between the response of the participant and the onset of the marker. When receiving the response at time `X` (e.g. `event_time` in Zep) use `setup_marker_at(RESPONSE, X+20ms)`. Using `send_marker(RESPONSE)` would introduce an _erroneous timing_ because there is a small but relevant latency jitter between the response-time registration and Zep handling the actual response event.


## Temporal Performance of Module
There is a very short delay between the requested marker onset and the actual marker onset. On internal lab systems this delay was around 15 and 18 microseconds.

Using audio the most crucial timing statistic is the variability of delay between marker onset and audio onset. Below we compared Presentation(c) audio-trigger delay with that of Zep using this marker module.

The results have an accuracy of 10 microseconds (0.010 milliseconds). Zep 2.0 was run under Kubuntu 16.04 and Presentation under Windows 10. Both used the front channels on the creative sound blaster Audigy 5/Rx and using a PCIe-based parallel port. The test were furthermore done using a Teensy with custom firmware to monitor both the audio and trigger line. Statistics are based on a 1000 sampled epochs per sample rate and software implementation. The sound file was a 16bit WAV at 44.1kHz with a 10ms 2kHz sine wave. 


| Software     | Soundcard Mode (Hz) | Mean (ms) | Std. Deviation (ms) | Min (ms) | Max (ms) | Range (ms) |
|--------------|-----------------|-----------|---------------------|----------|----------|------------|
| Presentation(c) | 44100           | 1.04      | 0.01                | 0.98     | 1.06     | 0.08       |
| Zep 2.0          | 44100           | 1.61      | 0.01                | 1.58     | 1.65     | 0.07       |
| Presentation(c) | 48000           | 0.81      | 0.04                | 0.73     | 0.90     | 0.17       |
| Zep 2.0          | 48000           | 1.53      | 0.01                | 1.50     | 1.55     | 0.05       |

Zep performed best when using the soundcard in the 48kHz mode; the Range and Std. Deviation were the lowest in this mode.

## Development mode
When you don't have a parallel port available you can still test and integrate this module by setting `DEVELOP_MODE` to true in `zep_markers_settings.zm`.

`const bool DEVELOP_MODE = true;`

This makes the module work without a parallel port but only simulates sending output. It might be a good idea to swith off of this mode when doing the actual experiment.

## Logs
The module logs the status of markers in a date-formatted file in the `./logs` directory. It logs both the failed (timing / validation) and successful markers.

## Troubleshooting
Below are some common problems and their solutions. If these do not work please ask your technician for help. Make sure you run the experiment in such a way you can see the error output of Zep. This module outputs _WARNINGS_ and _ERRORS_ that might explain trouble.

### Module cannot find the device or connects to wrong device

#### Check the `PORT_NUMBER`
Sometimes the parallel port you want is not the first one your system has found.
By default the port number is set to `0`. You can change the port number setting the `PORT_NUMBER` variable at the top of the `zep_markers_settings.zm` file.


### Warnings when sending more than one marker simultaneously
    `!! WARNING - Marker 5's onset conflicts with a marker (4) that has already been scheduled !!`
Sending two markers simultaneously is not possible. A marker consists of a pulse-up and a pulse-down phase. The interval between up and down is the pulse length or duration. During this pulse we are sending a marker and another cannot start. The pulse _down_ needs to finish before a pulse _up_ can start. Hence there is a minimal pause between two marker onsets. By default this pause is the pulse length of the first marker plus some scheduling time. The module uses a scheduler that requires extra time for planning and setup the marker data. This extends the actual required time for a marker.

For instance, if the marker' `pulse_length` is 20ms and the `SCHEDULER_PRE_EMPT` setting is 5ms (default) the marker requires a minimum time-window of 25ms. Within this 25ms period no other markers can be set.

The solution is to redesign your experiment so sending two or more markers at nearly the same time does not happen.

### Overloading the scheduler
    `!! ERROR Overloading - Marker 9 failed to be pre-empted on time (1287.166ms too late) !!`
    `!! ERROR Overloading - Marker 1 failed to be scheduled to pre-empt on time (294.923ms too late) !!`

The scheduler fires slightly before the onset timing of an marker. When the scheduler fires (i.e. _expires_) it needs some CPU time to pre-emptively transfer the marker to the timers. If the transfer does not occur on time the scheduler is considered _being overloaded_. This overload can result in a cascade of failed markers.

The solution is to lighten the load on the CPU during crucial and marked parts of your experiment. For instance, try to shuffle at the start of the experiment instead of at the start of a trial.

### The timing of the markers in the recording varies too much
Make sure you avoid using `send_marker()`. Use `send_marker()` if you want to insert a marker and care nothing for the accuracy of timing.

Try to use `setup_marker_at()` with the _expected_start_time_ of your stimulus object.
If using this function creates variation creates jitter something might be wrong with the way you set up the stimulus objects. Make sure you check for discrepancies between the _expected_start_time_ and the actual _start_time_ of you stimulus objects.

Note that the scheduler has some tolerance for scheduling-timing variance when sending the marker to the marker-sending device. The actual marker time is unaffected when the variance falls within this tolerance.

### EEG recordings with ActiView (Biosemi Software) suddenly pauses for no good reason
ActiView can be [configured](https://www.biosemi.com/faq/trigger_signals.htm) to start a pause or stop a pause on specific markers.
Check the .cfg you feed ActiView for the following:

     Example of .cfg text:
     PauseOff=""254 //-1 is disabled, 0-255 is enabled""
     PauseOn=""255 //-1 is disabled, 0-255 is enabled""
",2022-08-12
https://github.com/UiL-OTS-labs/zep-maye2017,"# zep-baby-habituation
Baby experiment using the visual habituation paradigm

# This task is used to run a habituation paradigm.
During each trial the gaze of the participant is lured by a attention grabber.
The attention grabber is a series of pictures that mimics a movie of a
laughing baby. When a participant is looking to the grabber the experimenter
Presses a button to start the main part of the trial. In each trial the
baby is looking to a bulls eye stimulus and hears repeated words.
If the baby stops looking the experimenter presses a button and if the
experimenter doesn't cancel this action the trial will finish in 2 seconds.
During a trial the cumulative looking and not looking time is recorded.


## Habituation
In the habituation phase trials are presented as described above. After
the sixth trial on we examine whether the infant meets the habituation
criterion. This criterion is met once the mean cumulative looking time of the
last three trial is less then 65% of the first threesome. Once the criterion
is met the test phase is started.

## Test phase
Two trials are presented.

## Attention phase
One trial is presented with a completely deviant stimulus. Generally it is
expected that a attentive baby will exhibit an increase in looking time,
since a novel stimulus is interesting.

#finish
The experiment finishes after a happy song is played along with some infant
friendly pictures.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Molletjes-Josje-Verhagen,"# ZEP-Molletjes-Josje-Verhagen
Experiment that shows visual stimuli and plays soundfiles in triplets. At the onset of each audio simulus a mole is shown. 
At the third and final audiostimulus the visual stimulus changes positively if the participant correctly presses a button.
No negative feedback is provided. The experiment also registers the participants reaction time when they press a button during when they use a button-box.

Triplets are generated according to a rule set and randomised.

# Documentation
A basis explanation (in Dutch) can be found in `documentation/instructies.pdf`.

# Zep Version
Writen for `Zep version 1.12` and the BeexyBox response button for optimal measurement of reaction times.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-namepic-margriet-wijngaarden,"# Name Pictures Experiment
Shows pictures in sequence.

## Output
Audio recording which includes a channel with a beep which ONSET equals the picture showing.

## Pseudorandomisation
Completely random.

## Targeted Language
Dutch 

## DISCLAIMER
This experiment script is released under the terms of the GNU General Public License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in the hope that it will be useful, but with absolutely no warranty. It is your responsibility to carefully study and test the script before using it with real participants.

## Request details
# Author
C. van Run
# Client
Margriet Wijngaarden
# Supervisor
Frank Wijnen
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-namepic-marjon-jessurun,"# Name Pictures Experiment
Shows pictures in sequence.

## Output
Audio recording which includes a channel with a beep which ONSET equals the picture showing.

## Pseudorandomisation
Completely random.

## Targeted Language
Dutch 

## DISCLAIMER
This experiment script is released under the terms of the GNU General Public License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in the hope that it will be useful, but with absolutely no warranty. It is your responsibility to carefully study and test the script before using it with real participants.

## Request details
# Author
C. van Run

# Client
Marjon Jessurun

# Supervisor(s)
prof. dr. Frank Wijnen and prof. dr. Ellen Gerrits
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Niloofar-Hashemzadeh-Auditory-Identification,"# ZEP; Spoken Sentence Identification
## Description:

Purpose of this ZEP experiment is to measure a participant's ability
to identify (speech) sounds. For each trial a sound is presented.
Participant's task is to jugdge whether a prespecified property is
present in the stimulus or not. In this case it is if a sentence
is negative or positive. Experiment is self-paced, has visual
appealing features, and auditory instruction (Dutch).

Reaction time is measured from the `critical_word_onset` of the `critical_word`.
This onset time needs to be defined per audio file in the input lists.

## Output: chosen value, observation time and reaction time.

Author:
Chris van Run (UiL-OTS) <labman.gw@uu.nl>

Client:
Niloofar Hashemzadeh

Supervisors:
* Prof. dr. Frank Wijnen
* Prof. dr. Hugo Quené
* Anne van Leeuwen MA

## Pseudo randomisation
Stimuli are build up out of pairs. Each pair either carries a negative
or a positive type of critical word. Randomisation is such that there are never
more than 3 pairs in a row with the same type of critical word.

## Installing and starting
* Download and install zep (1.12) [here](http://beexy.org/zep/wiki) for either Windows OS or a Debian based Linux OS.
* Download and unpack this repository
* Run either `windows-terminal.bat` or `linux-terminal.sh` to start up a _command-line interface_ within the unpacked repository
* You can then start up the experiment by typing: `zep ident_pn` or `zep-1.12 ident_pn`

## Acknowledgements
* Speaker of the audio: Dr. Frans Adriaans..
* https://www.vecteezy.com/ for the original owl artwork that was modified by Chris.

## A note about samplerates
The original stimuli files where in 12,000 Hz because of a manipulation. They are upsampled (using ffmpeg) to 48,000 Hz in order for the audio hardware to support playback.

## About Zep
Zep is a system for implementing and running (psycholinguistic) experiments. Zep delivers auditory, visual and cross modal stimuli, and provides interfacing with external hardware such as eye-trackers and button boxes. It is based on a multipurpose programming language (also called Zep) which enables you to implement not only experiments but other applications as well.

DISCLAIMER

This experiment script is released under the terms of the GNU General Public
License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in
the hope that it will be useful, but with absolutely no warranty. It is your
responsibility to carefully study and test the script before using it with
real participants.

## To do
* [x] Update output and input lists
* [x] Create visual appealing test-page
* [x] Create visual appealing tutorial page
* [x] Introduce pseudo-randomisation for stimuli
* [x] Credit speaker of audio
* [x] Incorporate stimuli files
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Non-adjacent-Dependency-Learning--HPP-,"Experiment:
        Non-adjacent Dependency Learning (HPP)

Description:
        Purpose of this headturn preference experiment is to see whether an 
        infant participant can detect a difference between two types of 
        auditory stimuli. The infant sits on the caregiver's lap facing a 
        wall on which a green light, an invisible speaker and a camera is 
        mounted. On each side wall a red light and an invisible speaker are 
        mounted. In this implementation there is a familiarization phase and
        a test phase. For each trial in the test phase the infant's attention 
        is drawn to one the side lights (blinking). When the infant looks at 
        the blinking light a sequence of sound stimuli starts and a timer runs
        as long as the infant keeps looking at the blinking light. The trial 
        ends when the infant looks away too long (or when a particular number
        of stimuli have been presented). In the familiarization phase that 
        precedes the test phase a similar contingency procedure is used but 
        only for the lights; the sound stimuli once started continue until 
        all have been presented. Output test phase: looking time. Output 
        familiarization phase: total looking time.

        In this implementation two participant groups are defined. Each group
        will be familiarized with a different set of stimuli (language). In
        the test phase half of the stimuli will be taken from one set and the
        other half will be taken from the other set. The stimuli in a test
        trial are sequences of different versions of the same type (language)
        rather than sequences of the same stimulus.

Author:
        Theo Veenker <theo.veenker@beexy.nl>

Client:
        -

Supervisor:
        -

References:
        Kerkhoff, A.O., Bree, E.H. de, Klerk, M. de & Wijnen, F.N.K. (2012).
          Non-adjacent dependency learning in infants at familial risk of 
          dyslexia.
          Journal of child language, to appear.


For information on running the experiment and extracting the experiment
results please go the the Zep website at http://www.beexy.nl/zep and check 
out the documentation section. There you'll also find explanations and 
instructions that help you understand and modify a Zep experiment.


DISCLAIMER

This experiment script is released under the terms of the GNU General Public
License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in
the hope that it will be useful, but with absolutely no warranty. It is your
responsibility to carefully study and test the script before using it with 
real participants.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-Non-adjacent-Dependency-Learning--HPP---alt1-,"Experiment:
        Non-adjacent Dependency Learning (HPP)

Description:
        Purpose of this headturn preference experiment is to see whether an 
        infant participant can detect a difference between two types of 
        auditory stimuli. The infant sits on the caregiver's lap facing a 
        wall on which a green light, an invisible speaker and a camera is 
        mounted. On each side wall a red light and an invisible speaker are 
        mounted. In this implementation there is a familiarization phase and
        a test phase. For each trial in the test phase the infant's attention 
        is drawn to one the side lights (blinking). When the infant looks at 
        the blinking light a sequence of sound stimuli starts and a timer runs
        as long as the infant keeps looking at the blinking light. The trial 
        ends when the infant looks away too long (or when a particular number
        of stimuli have been presented). In the familiarization phase that 
        precedes the test phase a similar contingency procedure is used but 
        only for the lights; the sound stimuli once started continue until 
        all have been presented. Output test phase: looking time. Output 
        familiarization phase: total looking time.

        In this implementation two participant groups are defined. Each group
        will be familiarized with a different set of stimuli (language). In
        the test phase half of the stimuli will be taken from one set and the
        other half will be taken from the other set. The stimuli in a test
        trial are sequences of different versions of the same type (language)
        rather than sequences of the same stimulus.

        The main difference between this implementation and the regular nadl
        is that here the familiarization phase also includes some 
        'ungrammatical' items.

Author:
        Theo Veenker <theo.veenker@beexy.nl>

Client:
        -

Supervisor:
        -

References:
        Kerkhoff, A.O., Bree, E.H. de, Klerk, M. de & Wijnen, F.N.K. (2012).
          Non-adjacent dependency learning in infants at familial risk of 
          dyslexia.
          Journal of child language, to appear.


For information on running the experiment and extracting the experiment
results please go the the Zep website at http://www.beexy.nl/zep and check 
out the documentation section. There you'll also find explanations and 
instructions that help you understand and modify a Zep experiment.


DISCLAIMER

This experiment script is released under the terms of the GNU General Public
License (see http://www.gnu.org/licenses/gpl-2.0.html). It is distributed in
the hope that it will be useful, but with absolutely no warranty. It is your
responsibility to carefully study and test the script before using it with 
real participants.
",2022-08-12
https://github.com/UiL-OTS-labs/ZEP-operationspan,"# ZEP-operationspan
This project is a small modification to a digitspan task
",2022-08-12
https://github.com/UiL-OTS-labs-backoffice/AD-Group-Checker,"# UiL OTS AD group checker
This script queries the Solis-AD to check which UiL OTS Lab groups a certain user is a member of.
It also displays an extra message to say if the user is a member of the AllUsers group.

# Requirements (Kerberos Auth)
- Python 3
- python3-ldap3
- python3-kerberos
- python3-gssapi
- A linux machine with a configured Kerberos for soliscom
(Should already be installed on the lab PC's)

# Requirements (Simple Auth)
- Python 3
- python3-ldap3

Note: the script will automatically fall back to simple auth if python3-gssapi isn't installed. You can force simple
auth by using the -s flag.

# Authentication
By default, authentication on the LDAP is done through a kerberos ticket. If kerberos can't be imported, it will fall
back to username and password (simple auth). When running the application as root, you must use the -s flag to force 
simple authentication, as root cannot log in with kerberos.",2022-08-12
https://github.com/UiL-OTS-labs-backoffice/Compare-CodeIgniter-Language-Folders,"# Compare-CodeIgniter-Language-Folders
Compares files in the different language folders that follow the code igniter markup. Gives, per language and then per file, an indication of which keys are missing in any of the other languages.

The script is not flawless, but it makes checking the differences easier than having to go by the entries one by one. 

# How to use
* Go to the project folder of your CodeIgniter website. Go to `applications` --> `language`. 
* Copy all the folders in that directory that you want to compare (for example: `english` and `dutch`), with all the files they contain, into the folder where you placed your `compare.py` script, or place the `compare.py` script into the language folder in your project.
* Run `python compare.py` in a terminal from the directory where your `compare.py` script is located.

# Read the output
The program makes a list of all the languages that were found (equal to the name of the folders). It then creates a list of files, for each of the languages. If any of the files that is present in any of the language folders is not present in any of the other folders, this will be printed to the terminal. 
For each of the files that are present in more than one folder, each of the language keys present in any of the files is checked. If any of the files with the same name in a different language folder lacks that key, the key, the language string, the containing file and all languages which lack the key are printed to the console. 
This is quite a litteral process, so comments that are spelled differently in all the language files will be printed as well.
",2022-08-12
https://github.com/UiL-OTS-labs-backoffice/extract-hvf,"# extract-hvf
Extracts data from the output of Hybrid Visual Fixation experiments. 

## Instructions
Copy the script to your directory with hvf output files. Run `perl extract-hvf.pl`. This will create a .csv-file called `out.csv` that you can then read in with your favourite spreadsheet application.

",2022-08-12
https://github.com/UiL-OTS-labs-backoffice/extract-maye,"# extract-maye
Extracts data from the output of the Maye replication study

## Instructions
Copy the script to your directory with maye output files. Run `perl extract-maye.pl`. This will create a .csv-file called `out.csv` that you can then read in with your favourite spreadsheet application.

## References

Maye, J., Werker, J. F., & Gerken, L. (2002). Infant sensitivity to distributional information can affect phonetic discrimination. *Cognition, 82*(3), B101-B111.

",2022-08-12
https://github.com/UiL-OTS-labs-backoffice/eyetracker-scripts,"# eyetracker-scripts
Helper scripts that are part of the chain to analyze Reading and VWP experiments
These scripts are generally started from the data directory of your zep
experiment.

## Purpose of the scripts
The scripts in this repository are ment to support researchers at the UiL OTS
labs that are doing eytracking and want to analyze the data with Fixation.

These scripts are meant to become a replacement for the older perl scripts

## dependencies
- python3.5 or greater
- PILLOW in order to convert .png's to bitmaps.
- edf2asc from SR-Research (necessary to convert edf to ascii files)

## note
Previously the eyetracker scripts depended on the presence of the
Image Magic utilities in order to convert the png's to bmp's. Than the
perlscripts started an external program (convert) to convert the .png's
to bitmaps.

That dependency is now changed to Pillow, since that is a python library
that avoids the dependency on external programs, however, it is still
a dependency so it must be installed.
",2022-08-12
https://github.com/UiL-OTS-labs-backoffice/holliday-planner-page-generator,"# holliday-planner-page-generator
Python script to generate the wiki code for a year calendar in table format. 

See https://lab.wiki.hum.uu.nl/wiki/index.php?title=Template:Holiday_Roster
",2022-08-12
https://github.com/UiL-OTS-labs-backoffice/lightdm-theme,"UiL OTS LightDM Theme



## Installation

```
1. download latest release in project's `download` folder
2. copy uncompressed `lightdm-theme` folder contents to /usr/share/lightdm-webkit/themes/uilots
3. update `webkit_theme` option in /etc/lightdm/lightdm-webkit2-greeter to `uilots`
```

## Change background image

location: `assets/background.jpg`

## Development

Recommended to use `yarn` over `npm` (`yarnpkg` on Ubuntu)

- `yarn` to install dependencies
- `yarn dev` runs webpack-dev-server
- `yarn build` creates bundle and assets over `/dist`

## License

This software is distributed under the MIT license, with the exception of any Utrecht University trademarks.

Consider using [enkel](https://github.com/vndre/enkel-greeter), the project this theme is based upon. (See below)


## Credits
Forked from [``enkel`` by Andre Aldana](https://github.com/vndre/enkel-greeter)

```
MIT License

Copyright (c) 2019 Andre Aldana

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```
",2022-08-12
https://github.com/UiL-OTS-labs-backoffice/Linux-Shell-webcam-recorder,"# Linux-Shell-webcam-recorder
A shell script to easily capture webcam video on the lab PC's of the UiL OTS Labs

There are two flags available with which the script can be called:

-r When the script is called with the -r flag, an extra window will appear before they select the save location, asking them which resolution they want to record in. They can choose SD quality (640 * 480px, 4:3), HQ (800 * 600px, 4:3, which is also the default option), HD (1280*720, 16:9 widescreen) or FHD (1920 * 1080, 16:9 widescreen). The preview window will have the same proportions as the selected resolution

-p When the script is called with the -p flag, an extra window pops up, asking the user if they want to show a live preview during recording as well as before starting recording. This can be useful if they want to moniter their subjects live. It also shows a warning that this will use a lot of CPU processing power and might therefor interfere with the experiment. That is why, by default, no live preview is shown during recording. 
",2022-08-12
https://github.com/UiL-OTS-labs-backoffice/SimpleSAML-Example-And-Documentation,"# SimpleSAML-Example-And-Documentation
This how to tries to describe the process of setting up SimpleSaml to talk to the Identity Provider of University Utrechts ITS, and subsequently setting up a PHP project to make use of the features offered by SimpleSaml. The example code is present in this repository

The how-to is published online at [https://uil-ots-labs-backoffice.github.io/SimpleSAML-Example-And-Documentation/](https://uil-ots-labs-backoffice.github.io/SimpleSAML-Example-And-Documentation/) 

The intention of this project is to use the community to add documentation and example implementations for many more langauges, which will be an ongoing project.

Do you see room for improvement in the existing documentation? Or have you used this how-to to use a library for another language? **Then join the cause!**. Clone this repository and add documentation and minimal sample code for the library you used. Or just suggest changes that you think make the existing work better. Then send in a pull request with your changes.
",2022-08-12
https://github.com/UiL-OTS-labs-backoffice/sound_eeg,"# sound_eeg A small testing utility to compare sounds with triggers from the parallel port

## Rationale
In EEG experiments you can measure with a relatively high temporal precision.
In order to achieve this A experimental program needs to synchronize the
stimuli with the EEG recording equipment, if there is no synchronization,
you will lose your signal in the noise.

## Goal: measure latency and jitter between trigger and output of the sound wave
This is a small utility to generate brief triggers with an Standard Parallel
Port (SPP) and generate tones with with a sound card.

## Requirements
- Zep2
- parallel port
- audio card with lineout cable
- oscilloscope

## Design

A large number of stimuli are presented. The stimuli are 20 ms of 1Khz. In
synchronous fashion, a large number of pulses/triggers are send.
The triggers are setting data line 0-7 of the SPP high for a 8ms as well. 8 ms
is chosen, because in the biosemidocumentation the usb -> parallel cable
sends pulses of 8 ms as well. The data lines are set high by writing 0xFF (255)
to the data register. Since we assume the triggers are presented at the same
time as the sound stimuli we can have a look at the signals on the
oscilloscope. Then we should have a reasonable idea of the offset and the
latency.

### Optimal results
So in the image below, we can see the desired results, where the trigger (yellow
line) starts simultaneous with the onset of the sound (purple line). This
should happen all the time ideally. Every square at the horizonal axis represents
5 ms. So you can see that the trigger duration is precisely 8 ms and the sound
duration is 4 squares, hence 20ms.

![alt text][simultaneous]

### Delayed results
In the scenario below one can see that the stimuli are always presented a little
to late. However, there is not any jitter, that is, the stimuli are alway quite
precisely 3.4ms to late. That is suboptimal, but something one can work with.
Using Zep on Linux, one can tell the SoundOutputDevice that there is a
hardware_delay of 3.4 ms and then zep will schedule the stimuli 3.4 ms earlier
to correct for the delay. With these settig the image above is made.

![alt text][delayed]

### Jittered results
Below is the situation of jittered pulse and signals, this is the worst case
scenario. Then there is a unpredictable delay for which cannot be controlled.
Hence than you should hope that the jitter is < 1ms, than it is reasonably alright.
but if it is much more, you will lose quite some temporal precision. The jitter
was generated using a jitter of 3ms, but this can be much more with some toolkits.
In the picture below you also see a ""shadow"" of all the traces of last second,
this enables to see the jitter in a picture.

![alt text][jitter]


## Notes
1. Parallel Port. Zep is not able to trigger properly when the previous trigger
hasn't been properly handled. With EEG experiments, this typically isn't a
problem. Since we trigger for roughly 5-10 ms, the BioSemi USB trigger triggers
with pulses with a duration of 8ms, So I'm thinking that should be used for
optimal results.
2. SoundPlayback. Make sure the audio is not at 100% either. Zep (its backends)
do not like it or the Xonar D2 devices do not like it. The waveforms are
distored.
3. Xonar has a hardware latency of roughly 3.4 ms. (on Linux).

[simultaneous]: ./images/oscilloscope.png
[delayed]: ./images/delayed.png
[jitter]: ./images/jitter.png

",2022-08-12
https://github.com/UiL-OTS-labs-backoffice/UiL-OTS-Video-Coding-System,"# UiL OTS Video Coding System
The UiL OTS Video Coding System is a piece of software for coding videos created in the UiL OTS Baby Lab, or in experiments with a similar setup. The software can be used to code looks of participants for each of the trials in the experiment. For each trial, the total look time is calculated, which can then be exported to a csv file. 

## Type of experiment
This software is specifically made for researchers of the UiL OTS using a setup like in the [babylab of the UiL OTS labs](http://uilots-labs.wp.hum.uu.nl/facilities/baby-lab/). This software does not allow for much more than is required by those experimenters, but exactly this makes it simple to use for the purposes for which it was made. 
In the babylab of the UiL OTS Lab, participants are usually presented with 8 trials in which stimuli have the form of sound or light from either the left or the right, or right in front of the participant. The participant is recorded and this recording gets an overlay, indicating the start and end times and relative location to the participant of the presented stimuli.
With this software, every time the participant looks at the source of a stimulus, a new look can be registered within that trial. The total look time for each trial is then calculated automatically and can later be exported to a csv.
The amount of trials and looks is not set, so any amount can be used, but before registering a look, a trial has to be registered first.

## Features
* Supports multiple video formats
* Add any number of trials
* Add any number of looks in any trial
* Automatically check for timeouts and indicate when timeouts occur
* Add comments to trials and looks
* Add research meta information about researcher, participant, etc
* Use quickeys for easy navigation
* Frame-by-frame step-through, both forward and backward
* Exports to csv
* Projects can be saved for later continuation
* Automatic file recovery option on inexpected shutdown

### Version 2
Version 2 of the UiL OTS Video Coding System has been officially released in January 2016. In this version, the user interface received a mayor overhaul. A time line like interface was added, to provided an even clearer overview of where trials and looks have been added inside the project. This time line also provides quick access to all sorts of actions that can be performed on trials and looks in the project.

An installer and a (portable) launcher for Windows are now provided as well. See the releases tab to download.

## Requires:
 * [VLC](http://www.videolan.org/vlc/) 3 or later
 * [Java Runtime Environment (JRE)](https://www.java.com/en/download/) 11 or later (although it is adviced to always use the latest version of Java).

Version 2.1 was the last version to support VLC 2 and Java 1.6. 

## Support
The UiL OTS Video Coding System is developed by the UiL OTS Labs of the Utrecht University. A [how-to for using the UiL OTS Video Coding System](http://uilots-labs.wp.hum.uu.nl/how-to/how-to-recode-videos-offline-in-the-babylab/) is provided on the lab's website, as well as an [FAQ](http://uilots-labs.wp.hum.uu.nl/how-to/faq-troubleshooting-uil-ots-video-coding-system/).
Please use the issues tracker if you encounter any problems.

## License
Copyright (C) 2015 UiL OTS Labs

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; version 2 of the License.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License along
with this program; if not, write to the Free Software Foundation, Inc.,
51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
",2022-08-12
https://github.com/UiL-OTS-labs-backoffice/Video-Conversion-Script,"# Video-Conversion-Script
With this script, video's can be converted to progressive h264 MP4. This can be useful to save space, or to use the video's with software that can only read MP4 extensions, H624 codecs or progressive video's.

Works on Linux only.
Requires VLC
",2022-08-12
https://github.com/UiL-OTS-labs-backoffice/zep-syntax-highlighter,"# zep-syntax-highlighter
ZEP Syntax Highlighting for GtkSourceView (to use e.g. in gedit)

Commands to install after downloading `zep.lang`: 

```bash
cd ~/.local/share
mkdir gtksourceview-3.0
cd gtksourceview-3.0
mkdir language-specs
cd language-specs
cp ~/zep.lang .
``` 

For Geany, you can download the syntax highlighter [here](http://beexy.org/zep/wiki/doku.php?id=experiment:faq#how_can_i_setup_syntax_highlig).
 
",2022-08-12
https://github.com/UtrechtCoastalGroup/Psamathe,"# Psamathe: Modelling wind-driven sand transport from the beach using the fetch concept

### Description

The Psamathe repository contains Matlab-scripts to predict meso-scale (seasonal to annual) aeolian sand supply from the beach to the foredune. The Psamathe model, named after the Greek goddess of sand beaches, was motivated by two seminal papers:
- Bauer, B.O. and R.G.D. Davidson-Arnott, 2003. A general framework for modeling sediment supply to coastal dunes including wind angle, beach geometry, and fetch effects. Geomorphology, 49, 89-108. https://doi.org/10.1016/S0169-555X(02)00165-4
- Delgado-Fernandez, I., 2011. Meso-scale modelling of aeolian sediment input to coastal dunes. Geomorphology, 2011, 230-243. https://doi.org/10.1016/j.geomorph.2011.04.001

The Psamathe model has three main modules:
- A beach groundwater module, based on the non-linear Boussinesq equation including the effect of wave runup;
- A surface moisture module that uses the Van Genunchten soil water retention curve to compute the spatio-temporal groundwater depth values from the first module into surface moisture;
- A fetch-based aeolian sand transport equation, in which the downwind increase in aeolian transport is computed based on the fetch concept; the critical fetch depends on the wind speed and the spatially varying surface moisture.

### Groundwater and surface moisture

Modules 1 and 2 are posted in the directory Groundwater. Its subdirectory ""example"" contains an example run file with corrosponding input data. 

The combined groundwater and surface-moisture modules have been described in:
- Brakenhoff, L.B., Y. Smit, J.J.A. Donker and G. Ruessink, 2019. Tide-induced variability in beach surface moisture: observations and modelling. Earth Surface Processes and Landforms, 44, 317-330. https://doi.org/10.1002/esp.4493 (Open Access)

### Fetch

The third module is posted in the directory Fetch. There is an example subdirectory; another one will be added fairly soon.

The fetch module has seen various versions, as witnessed in the following papers: 
- Hage, P., G. Ruessink, Z. van Aartrijk and J. Donker, 2020. Using video monitoring to test a fetch-based aeolian sand transport model. Journal of Marine Science and Engineering. https://doi.org/10.3390/jmse8020110 (Open Access)
- Tuijnman, J., J.J.A. Donker, C.S. Schwarz and G. Ruessink, 2020. Consequences of a storm surge for aeolian sand transport on a low-gradient beach. Journal of Marine Science and Engineering. https://doi.org/10.3390/jmse8080584 (Open Access)

In both papers the model was called Aeolus. But there appeared to be other Aeolus models, so Aeolus became Psamathe.

The full model, including the most recent modifications to the fetch module, can be found in:

- Ruessink, G., G. Sterk, Y. Smit, W. de Winter, P. Hage, J.J.A. Donker and B. Arens, under review. Predicting monthly to multi-annual foredune growth at a narrow beach. Earth Surface Processes and Landforms. (Will be updated once accepted for publication)

### Final words

Note that the code is research code. With comments, but no manual. If you have trouble finding out how it works, think you found a bug, or intend to add a feature, please let me know - b.g.ruessink@uu.nl.

Funded by the Dutch Technology Foundation (STW) of the Netherlands Organisation for Scientific Research (NWO), Vici project #13709.

Gerben Ruessink - December 6, 2021
",2022-08-12
https://github.com/UtrechtCoastalGroup/Self-organization-of-a-biogeomorphic-landscape-controlled-by-plant-life-history-traits,"# Self-organization-of-a-biogeomorhic-landscape-contorlled-by-plant-life-history-traits
This repositroy provides the vegetation-model code used for the Nature Geoscience publication ""Self-organization of a biogeomorphic landscape controlled by plant life-history traits"". The code is based on the TELEMAC model environment, for fruther information please refer to http://www.opentelemac.org/.

*****************************
        veg_model.f 
*****************************
The fortran-user-file contains altered fortran subroutines facilitating the incorporation of vegetation-growth in TELEMAC2D.
The modified telemac subroutiunes can be compiled and linked to the exisiting TELEMAC2d model environment by adding it to the keyword FORTRAN FILE, see ""cas-example.txt"".
For further information please refer to http://opentelemac.com/.

The vegetation module was tested for the release V7P2R0, downloadable on http://opentelemac.com/index.php/download.
The model code is freely available but will not be further maintained, for questions regarding the TELEMAC model environemnt please refer to http://opentelemac.com.
The vegetation model was developed together with Nicolas Claude, Veerle verschoren.


Please find below an overview of the altered subroutines:
Areas altered in the original subroutines are indicated by ""!###>CS""
----------------------------
1. Module Vegetation
----------------------------
Defines vegetation parameters used in subroutines below.
The area where vegetation can grow within the model domain is given as the Variable: VEG defined in the Geometry file.
The variable VEG in the Geometry file was defined using freely available blue-kenue-sofware package https://www.nrc-cnrc.gc.ca/eng/solutions/advisory/blue_kenue_index.html.

----------------------------
2. Subroutine DIFSOU
----------------------------
Uses the existing subroutine, which prepares the source terms in the diffusion equation for the trace to model lateral diffusion of vegetation biomass.

Vegetation is modeled as exisitng Tracer (T1), set in the steering-file(see below).

- calculates vegetation growth using a logistic growth-function(Line 280)
- calculates random settlement(Line 289) as expliciat source terms in the diffusion equation (no advection)
- calculates die-off related to velocity and inundation =(Line 356)

----------------------------
3. Subroutine DRAGFO
----------------------------
Uses the existing subroutine, which adds the drag force of vertical strutures in the momentum equation.

The vegetation biomass calculated in DIFSOU is now recalculated in a dragforce created by vegetation, which is then added when solving the momentum equation.

- drag-force calucalution follows the approach of Baptist(2007), using different formulations for emerged and submerged scenarios(line 747)
- drag-force re-calculation is based on chosing the Manning-equation as the law for bottom friction(if a different law is chosen re-calculation needs to be adapted)

----------------------------
3. Subroutine TELEMAC2D
----------------------------

Calculates averaged inundation and max. velocity over the defined coupling period (line 3199, 3325)


*****************************
       cas_example.txt
*****************************
Shows the additional keywords needed in the steering file to activate the vegetation development functions;
The example-cas file does not contain additional model settings, since these need to be set by the user.

VERTICAL STRUCTURES: YES 				--> activates the use of the DRAGFO-subroutine
FORTRAN FILE       :'veg_model.f'			--> activates the Veg_model.f and all including subroutines
VARIABLES FOR GRAPHIC PRINTOUTS   	     : 'T1'	--> saves output of Tracer T1(used as 'Vegetation') in the resuls files

NUMBER OF TRACERS                            : 1	--> adds tracer which will be used for diffusion of vegetation in DIFSOU-subroutine
NAMES OF TRACERS                             :'VEGETATION'
DIFFUSION OF TRACERS                         : YES
ADVECTION OF TRACERS                         : NO
MASS-LUMPING ON H                            : 1.
INITIAL VALUES OF TRACERS                    : 0
COEFFICIENT FOR DIFFUSION OF TRACERS         : user defined


-----------------------------
	References
-----------------------------
Baptist, M. J., et al. ""On inducing equations for vegetation resistance."" Journal of Hydraulic Research 45.4 (2007): 435-450.
",2022-08-12
https://github.com/UtrechtCoastalGroup/Sonar3D,"# Sonar3D
Code to process raw RW2 files produced by a ""3D Profiling Sonar 2001"" (Marine Electronics - http://www.marine-electronics.co.uk/3D_2001.html)

The UtrechtCoastalGroup owns a 3D Profiling Sonar 2001, manufactured by Marine Electronics. Although Marine Electronics provides code to process its RW2 files, we thought we could do a better job in detecting the bed. We developed this (Matlab) code within the EU-funded Hydralab IV Bardex II project. Results of the subsequent analyses can be found in Ruessink et al. (2015), https://doi.org/10.3390/jmse3041568. Later on, we were commissioned by Rijkswaterstaat to extend and apply the code to data collected as part of the Kustgenese 2 project. As part of this work, we made the resulting code OpenSource through the present GitHub. The code is now free for everyone to use.

This repository contains code in which bedlevel data are low-passed filtered with a quadratic loess filter. This code was written by Nathaniel Plant and has been documented in a paper in Marine Geology, please see https://doi.org/10.1016/S0025-3227(02)00497-8.

If you use the code in this repository, we would kindly ask you to refer to both Ruessink et al. (2015) - as the initial source of the code - and to Plant et al. (2002) - as the source for the loess code.

Please start with Process_sonar_example.m and have a look at the Word document (in Dutch).


",2022-08-12
https://github.com/UtrechtRiversEstuaries/riverbifurcation1D,"# riverbifurcation1D
One-dimensional steady flow morphodynamic model for river bifurcations
written in Matlab
Reference:
Kleinhans, M.G., K.M. Cohen, J. Hoekstra and J.M. IJmker (2011), Evolution of a bifurcation in a meandering river with adjustable channel widths, Rhine delta apex, The Netherlands, Earth Surf. Process. Landforms 36, 2011-2027, http://dx.doi.org/10.1002/esp.2222

Used in several publications, for example:
* Kleinhans, M.G., T. de Haas, E. Lavooi and B. Makaske (2012), Evaluating competing hypotheses for the origin and dynamics of river anastomosis, Earth Surf. Process. Landforms 37, 1337-1351, http://dx.doi.org/10.1002/esp.3282
* van Dijk, W.M., F. Schuurman, W.I. van de Lageweg and M.G. Kleinhans (2014). Bifurcation instability and chute cutoff development in meandering gravel-bed rivers, Geomorphology 213, 277-291, http://dx.doi.org/10.1016/j.geomorph.2014.01.018
* Gupta, N., M.G. Kleinhans, E.A. Addink, P.M. Atkinson and P.A. Carling (2014). One-dimensional modeling of a recent Ganga avulsion: assessing the potential effect of tectonic subsidence on a large river. Geomorphology 213, 24-37, http://dx.doi.org/10.1016/j.geomorph.2013.12.038
* van Denderen, R.P., R.M.J. Schielen, A. Blom, S.J.M.H. Hulscher and M.G. Kleinhans (2017). Morphodynamic assessment of side channel systems using a simple one-dimensional bifurcation model and a comparison with aerial images, Earth Surf. Process. Landforms 43, 1169–1182, https://doi.org/10.1002/esp.4267
",2022-08-12
https://github.com/UtrechtRiversEstuaries/tidalnetwork_1D,"# tidalnetwork_1D
This is the code of one-dimensional morphodynamic model for tide-influenced channel networks written in Matlab.

Reference:

Iwantoro, AP, van der Vegt, M, Kleinhans, MG. Effects of sediment grain size and channel slope on the stability of river bifurcations. Earth Surf. Process. Landforms. 2021; 46: 2004– 2018. https://doi.org/10.1002/esp.5141


",2022-08-12
https://github.com/UtrechtUniversity/.github,"# Organization repository of UtrechtUniversity

This repo defines the look and feel of the organization's account. 

See https://github.com/UtrechtUniversity/.github/tree/main/profile for the content of the landing page. 
",2022-08-12
https://github.com/UtrechtUniversity/animal-sounds,"# animal-sounds

<!-- Include Github badges here (optional) -->
<!-- e.g. Github Actions workflow status -->

The aim of this software is to classify Chimpanze vocalizations in audio recordings from the tropical rainforests of Africa. The software can be used for processing raw audio data, extracting features, and apply and compare Support Vector Machines and Deep learning methods for classification. The pipeline is reusable for other settings and species or vocalization types as long as a certain amount of labeled data has been collected. The best performing models will be available here for general usage.

<!-- TABLE OF CONTENTS -->
## Table of Contents

- [Animal Sounds](#animal-sounds)
  - [Table of Contents](#table-of-contents)
  - [About the Project](#about-the-project)
    - [Dataset description](#dataset-description)
    - [Processing](#processing)
    - [Feature extraction](#feature-extraction)
    - [Classification](#classification)
    - [Built with](#built-with)
    - [License](#license)
    - [Relevant Publications](#relevant-publications)
  - [Getting Started](#getting-started)
    - [Project structure](#project-structure)
  - [Contributing](#contributing)
  - [Contact](#contact)

<!-- ABOUT THE PROJECT -->
## About the Project

**Date**: June 2022

**Researchers**:

- Joeri Zwerts (j.a.zwerts@uu.nl)
- Heysem Kaya (h.kaya@uu.nl)

**Research Software Engineers**:

- Parisa Zahedi (p.zahedi@uu.nl)
- Casper Kaandorp (c.s.kaandorp@uu.nl)
- Jelle Treep (h.j.treep@uu.nl)

### Dataset description
The dataset for this project contains recordings in `.wav` format at 1 minute length and at a sample rate of 48000 samples/second. The recordings are taken at three locations:
- Chimpanze sanctuary
- Natural forest
- Semi-natural Chimanze enclosures  
The Chimpanze sanctuary and Natural forest are used for training and optimizing the classifiers. The Semi-natural Chimpanze recordings are used as an independent evaulation of the classifiers.

|Dataset| # Chimpanze samples | # Background samples |
| --- | --- | --- |
| Sanctuary | 17.921 | 74.163 | 
| Synthetic | 68.757 | 97.149 | 

### Preprocessing 
The datasets are labeled into 2 classes (Chimpanze & background) using [Raven Pro](https://ravensoundsoftware.com/software/) annotation software, and extracted from the original recordings. Find scripts [here](./bioacoustics/1_wav_processing/raven_to_wav).

To speed up the labeling process we developed an energy-change based algorithm to filter out irrelevant parts of the recordings, see [Condensation](./bioacoustics/1_wav_processing/condensation).

To increase and diversify our training set we have created synthetic samples by embedding the sanctuary vocalizations into the recorded jungle audio that is labeled as 'background', see [Synthetic data](./bioacoustics/1_wav_processing/syntetic_data).

### Feature extraction
We trained the models on frames of 0.5 seconds.  
Before calculating features we apply a Butterworth bandpass filter with low cutoff at 100 Hz and a high cutoff at 2000 Hz.  
For classification using SVM we extract statistical features from different representations of the audio signal.  
For classification using Deep learning we use a mel spectrogram representation as input. 

| <img src=""/img/melspectrogram.png"" width=""400"" /> | 
|:--:| 
| *Chimpanze vocalization in mel spectrogram representation* |

### Classification
**SVM**  
From the 1140 statistical features from the previous step we select a normalized feature set of 50 features. The selection is based on feature importances computed with an Extra Trees Classifier. We train and optimize the SVM model on those 50 features using 'macro average recall' as evaluation criterion.
On the independent test set the SVM model establishes a 'macro average recall' of **0.87**.
| <img src=""/img/A6_matrix.png"" width=""400"" /> | 
|:--:| 
| *SVM prediction results for A6 recorder* |

**Deep learning**  
We trained several architectures of Convolutional Neural Networks (CNN) and a Residual network model (Resnet). CNN10 is the best performing model.

| Trained on| SVM | CNN | CNN10 | 
| --- | --- | --- | --- |
| Sanctuary | 0.86 | 0.81 | 0.83 |
| Synthetic | 0.65 | 0.82 | 0.85 |
| Sanctuary + Synthetic | 0.87 | 0.83 | 0.87 | 

### Built with

- [Python 3.8](https://www.python.org/)
- [librosa](https://librosa.org/)
- [scikit-learn](https://scikit-learn.org/stable/index.html)
- [tensorflow](https://www.tensorflow.org/)

<!-- Do not forget to also include the license in a separate file(LICENSE[.txt/.md]) and link it properly. -->
### License

The code that is developed in this project is released under [Apache 2.0](LICENSE.md). Some of the scripts for [feature extraction](/bioacoustics/2_feature_extraction) that we use in this project are available under [CeCILL 1.1](https://github.com/malfante/AAA/blob/master/LICENSE_EN.txt) license. The scripts where this is the case contain license information at the header lines of the scripst. The original versions of these scripts are created by Marielle Malfante and are available via [GitHub](https://github.com/malfante/AAA).

### Relevant publications

- Introducing a central african primate vocalisation dataset for automated species classification.\ 
Zwerts, J. A., Treep, J., Kaandorp, C. S., Meewis, F., Koot, A. C., & Kaya, H. (2021).\ 
[arXiv preprint](https://arxiv.org/pdf/2101.10390.pdf)
- The INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19 cough, COVID-19 speech, escalation & primates.\
Schuller, B. W., Batliner, A., Bergler, C., Mascolo, C., Han, J., Lefter, I., ... & Kaandorp, C. (2021).\
[arXiv preprint](https://arxiv.org/pdf/2102.13468.pdf)
- Automatic Analysis Architecture, M. MALFANTE, J. MARS, M. DALLA MURA 
DOI: [10.5281/zenodo.126028](https://doi.org/10.5281/zenodo.1216028)


<!-- GETTING STARTED -->
## Getting Started
There are two situations in which you can directly apply the scripts in this repository and we tailored the documentation towards these situations:
1. You have audio data and a set of manual annotations (in e.g. txt or csv format) and want to use the whole pipeline (processing, augmentation, feature extraction and machine learning). 
2. You have a highly similar dataset and want to use one of our models to help find Chimpanze vocalizations.

If 1 applies to you, take a look at the project structure below and find getting started instructions for each step in the respective folders: [1_wav_processing](./bioacoustics/1_wav_processing), [2_feature_extraction](./bioacoustics/2_feature_extraction) and [3_classifier](./bioacoustics/3_classifier).

If 2 applies to you, go to step [3_classifier](./bioacoustics/3_classifier/README.md) and read the specific instructions for applying our models on your data.


### Project structure

```
.
├── .gitignore
├── CITATION.md
├── LICENSE.md
├── README.md
├── requirements.txt
├── bioacoustics              <- main folder for all source code
│   ├── 1_wav_processing 
│   ├── 2_feature_extraction
│   └── 3_classifier        
├── data               <- All project data, ignored by git
│   ├── original_wav_files
│   ├── processed_wav_files            
│   └── txt_annotations           
└── output
    ├── features        <- Figures for the manuscript or reports, ignored by git
    ├── models          <- Models and relevant training outputs
    ├── notebooks       <- Notebooks for analysing results
    └── results         <- Graphs and tables

```

<!-- CONTRIBUTING -->
## Contributing

Contributions are what make the open source community an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

To contribute:

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request


<!-- CONTACT -->
## Contact

[Joeri Zwerts](https://www.uu.nl/medewerkers/JAZwerts) - j.a.zwerts@uu.nl

[Research Engineering team](https://utrechtuniversity.github.io/research-engineering/) - research.engineering@uu.nl

Project Link: [https://github.com/UtrechtUniversity/animal-sounds](https://github.com/UtrechtUniversity/animal-sounds)
",2022-08-12
https://github.com/UtrechtUniversity/anonymize-ddp,"# Anonymize-DDP

Pseudonimizing software for data download packages (DDP), specifically focussed on Instagram.

## Table of Contents
* [About Anonymize-DDP](#about-anonymize-ddp)
  * [Built with](#built-with)
  * [License](#license)
  * [Attribution and academic use](#attribution-and-academic-use)
* [Getting Started](#getting-started)
  * [Prerequisites](#prerequisites)
  * [Preparatory steps](#preparatory-steps)
    * [Clone repository](#clone-repository)
    * [Download DDP](#download-ddp)
    * [Create additional files](#create-additional-files)
  * [Run software](#run-software)
  * [Validation](#validation)
  
## About Anonymize-DDP
**Date**: December 2020

**Researchers**:
* Laura Boeschoten (l.boeschoten@uu.nl)

**Research Software Engineers**:
* Martine de Vos (m.g.devos@uu.nl)
* Roos Voorvaart (r.voorvaart@uu.nl)

### Built With

The blurring of text in images and videos is based on a pre-trained version of the [EAST model](https://github.com/argman/EAST). Replacing the extracted sensitive info with the pseudonimized substitutes in the data download package is done using the [AnonymoUUs](https://github.com/UtrechtUniversity/anonymouus) package.

### License

The code in this project is licensed with [MIT](LICENSE.md).

### Attribution and academic use
The scientific paper detailing the first release of anonymize-ddp is available [here](https://doi.org/10.3233/DS-210035).

A data set consisting of 11 personal Instagram archives, or Data-Download Packages, was created to [validate](/anonymize/validation) the anonymization procedure.
This data set is publicly available at [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4472606.svg)](https://doi.org/10.5281/zenodo.4472606)


# Getting Started

## Prerequisites
This project makes use of Python 3.8 and [Poetry](https://python-poetry.org/) for managing dependencies. 
To clone this repository, you'll need *Git installed* on your computer.

## Preparatory steps

Before running the software, the following steps need to be taken:

1. **[Clone repository](#clone-repository)**
2. **[Download DDP](#download-ddp)**
3. **[Create additional files](#create-additional-files)**

### Clone repository

Run the following code in the command line:

```
# Clone this repository
$ git clone https://github.com/UtrechtUniversity/anonymize-ddp

# Go into the repository
$ cd anonymize-ddp/anonymize-ddp

# Install dependencies
poetry install 

```

### Download DDP

To download your Instagram data package:

1. Go to www.instagram.com and log in
2. Click on your profile picture, go to *Settings* and *Privacy and Security*
3. Scroll to *Data download* and click *Request download*
4. Enter your email adress and click *Next*
5. Enter your password and click *Request download*

Instagram will deliver your data in a compressed zip folder with format **username_YYYYMMDD.zip** (i.e., Instagram handle and date of download). For Mac users this might be different, so make sure to check that all provided files are zipped into one folder with the name **username_YYYYMMDD.zip**. Save the DDP(s) in the [data folder](/data).

### Create additional files

Before you can run the software, you need to make sure that the [src folder](/src) contains the following items:
* **Facial blurring software**: The *frozen_east_text_detection.pb* software, necessary for the facial blurring of images and videos, can be downloaded from [GitHub](https://github.com/oyyd/frozen_east_text_detection.pb) 
* **Participant file**\*: An overview of all participants' usernames and participant IDs (e.g., participants.csv). We recommend placing this file in the `anonymize` folder. However, you can save this file anywhere you like, as long as you refer to the path correctly while running the software.

**\*** N.B. Only relevant for participant based studies with *predefined* participant IDs. This file can have whatever name you prefer, as long as it is saved as .csv and contains 2 columns; the first being the original instagram handles (e.g., janjansen) and the second the participant IDs (e.g., PP001).

## Run software

When all preceding steps are taken, the data download packages can be pseudonimized. 
Note that the `poetry run` command executes the given command inside the project’s virtual environment.
Run the program with (atleast) the arguments `-i` for data input folder (i.e., [data](\data)) and ` -o` data output folder (i.e., [results/output](/results/output)):

```
$ poetry run python anonymize/anonymizing_instagram_uu.py [OPTIONS]

Options:
  -i  path to folder containing zipfiles (i.e., -i data/raw)
  -o  path to folder where files will be unpacked and pseudonimized (i.e., -o data/processed)
  -l  path to log file
  -p  path to participants list to use corresponding participant IDs (e.g., -p src/participants.csv)
  -c  replace capitalized names only (when not entering this option, the default = False; not case sensitive) (e.g., -c)

```

An overview of the program's workflow is shown below:
![flowanonymize.png](flowanonymize.png)

The output of the program will be a copy of the zipped data download package with all names, usernames, email addresses, and phone numbers pseudonimized, and all pictures and videos blurred. This pseudonimized data download package is saved in the output folder.

## Validation

The validation procedure determines the performance of anonymization code _concerning deidentification of text_.
It compares results of the automated anonymization with the ideal expected result, i.e., a manually created ground-truth.

For this validation an example data set is used which includes:
* A set of 11 DDPs with nonsense content
* A groundtruth file with results of manually labeling the PII in these DDPs

The example data set is available at [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4472606.svg)](https://doi.org/10.5281/zenodo.4472606).

### Prerequisites

scikit-learn

## Preparatory steps

Before running the software, the following steps need to be taken:

1. **Collect data: both Instagram DDPs and corresponding ground truth data**
2. **[Anonymize Instagram DDPs](#anonymize-ddps)**

### Anonymize DDPs
Run the automated anonymization as described in the [main Readme](/../../)
After the anonymization, make sure you have separate folders with the following data:
* original DDPs
* anonymized DDPs
* key files


## Perform evaluation

When all preceding steps are taken, the evaluation can be performed. 

```
$ cd anonymize/validation
$ poetry run python validation_script.py [OPTIONS]

Options:
  -r  path to file with results of manual labeling
  -p  path to folder with anonymized datapackages; output of anonymization
  -k  path to folder with key files; output of anonymization

```
## Output
Evaluation metrics:
* table with recall, precision en f1
* four folders with specific occurences of FP, FN, TP and special hashes

## Testing
Run tests with available test data to check the consistency of the evaluation procedure 
From the root folder:

```
# Go to the main folder of the poetry project
$ cd anonymize-ddp/anonymize-ddp

# Run the test 
$ poetry run pytest

```
",2022-08-12
https://github.com/UtrechtUniversity/anonymouus,"# anonymoUUs

anonymoUUs is a Python package for replacing identifiable strings in multiple files and folders at once. It can be used to pseudonymise data files and therefore contributes to protecting personal data. 

The goal of anonymoUUs is to substitute multiple identifying strings with pseudo-IDs to avoid tracable relationships between data batches. A single data batch typically consists of multiple nested folders that contain multiple files in multiple formats. AnonymoUUs runs through the **entire** file tree, looking for keywords to replace them with the provided substitute, including in:  
- the file contents
- file names
- folder names
- zipped folders 

**Note**: Whereas replacing personal details with non-personal details can make data less identifiable, it does not guarantee anonymous data!

## Supported file formats

AnonymoUUs can work with multiple text-based file types, like `.txt`, `.html`, `.json` and `.csv`. UTF-8 encoding is assumed. Users have several options to provide keyword-replacement mappings and to customize the behaviour of the software, visit the [usage section](#usage) for more information.  

## Table of Contents
  * [Getting Started](#getting-started)
    + [Prerequisites](#prerequisites)
    + [Installation](#installation)
    + [Example workflow](#example-workflow)
  * [Usage](#usage)
    + [1. Input Data](#1-input-data)
    + [2. Mapping](#2-mapping)
    + [3. Create an Anonymize object](#3-create-an-anonymize-object)
    + [4. Substitute data](#4-substitute-data)
  * [Validation](#validation)  
    + [Prepare](#prepare)
    + [Validate](#validate)
  * [Attribution and academic use](#attribution-and-academic-use)
  * [Contributing](#contributing)
  * [Contact](#contact)

## Getting Started

### Prerequisites

To install and run anonymoUUs, you need:
- an active [Python](https://www.python.org/) installation;
- a folder containing the data to be pseudonymised;
- a keyword-replacement mapping file.

### Installation

To install anonymoUUs, in your terminal run:

```sh
$ pip install anonymoUUs
```

### Example workflow
To get started with a simple example, you can go through this [Jupyter notebook](/anonymouus/examples/try_testing.ipynb), which runs through a minimal example of anonymoUUs.

Prerequisites:
- download the testdata from the [test_data folder](/anonymouus/tests/test_data)
- make sure you have [jupyter notebook](https://jupyter.readthedocs.io/en/latest/install/notebook-classic.html) installed

## Usage
To run the software, you need to take the following steps:
1. Provide the path to the [data](#1-input-data) to be substituted
2. Provide the [keyword-replacement mapping](#2-mapping)
3. Create and customize the [Anonymize object](#3-create-an-anonymize-object)
4. Perform the [substitutions](#4-substitute-data)

### 1. Input Data
Provide the path to the folder where the data resides, for example:
```python
from pathlib import Path
test_data = Path('../test_data/')
```

Details:
- Files are opened depending on their extension. Extensions that are not recognised will be skipped. 
- Errors will be ignored.
- The standard version of this package assumes 'UTF-8' encoding. Since reading file-contents is done with a single function, it will be easy to adjust it, for example to also read other encodings. You can do so by overloading it in an extension:

```python
# standard reading function
def _read_file(self, source: Path):
    f = open(source, 'r', encoding='utf-8', errors='ignore')
    contents = list(f)
    f.close()
    return contents
```

### 2. Mapping
In order to replace words or patterns, you need a replacement-mapping. AnonymoUUs allows mappings in the form of a dictionary, a csv file or a function. 
- In all cases, the keys will be replaced by the provided values.
- It is also possible to provide string *patterns* to replace, using regular expressions (regex) in the keys. AnonymoUUs will replace every matching pattern with the provided replacement string.

#### Dictionary mapping
To use a dictionary-type mapping, simply provide the (path to the) dictionary (file) and apply the `Anonymize` function. Note that you can provide a regular expression using `re.compile('regex')` to look for string patterns.

```python
from anonymoUUs import Anonymize

# Using a dictionary and regular expression for subject 02:
my_dict = {
    'Bob': 'subject-01',
    re.compile('ca.*?er'): 'subject-02',
}

anonymize_dict = Anonymize(my_dict)
```

#### CSV file mapping
To use a CSV for mapping, simply provide the path to the file. AnonymoUUs converts the provided csv file into a dictionary.

Requirements:
- The csv file needs to contain column headers (any format)
- The csv file needs to have the keys (which need to be replaced, e.g., names) in the first column and the values (the replacements, e.g., numbers) in the second column.
- The path can be a String, Path or PosixPath. 

It is possible to add a regular expression as keyword in the csv-file. Make sure they start with the prefix `r#`:

| key | value |
| ---| --- |
| `r#ca.*?er` | `replacement string` |

```python
# Using a csv file
key_csv = test_data/'keys.csv'

anonymize_csv = Anonymize(key_csv)
```

#### Function mapping
If you are replacing strings with a pattern, you can also use a function to 'calculate' the replacement string. The function takes a found match and should return its replacement. The function must have at least one input argument.

```python
# Define function
def replace(match, **kwargs):
    result = 'default-replacement'
    match = int(match)
    threshold = kwargs.get(""threshold"", 4000)
    if match < threshold:
        result = 'special-replacement'
    return result

# Subsitute using the defined replace function
anon = Anonymize(replace, pattern=r'\d{4}', threshold=1000)
anon.substitute(
    '/Users/casperkaandorp/Desktop/test.json', 
    '/Users/casperkaandorp/Desktop/result-folder'
)
```
Note the possibility to provide additional arguments when you initialize an Anonymize object that will be passed to the replacement function (in this example, the `threshold` is passed to the `replace` function).

### 3. Create an Anonymize object
By default, the Anonymize function is case sensitive. Basic use:
```python
from anonymoUUs import Anonymize

anonymize_object = Anonymize(keys)
```

Performance is probably best when your keywords can be generalized into a single regular expression. AnonymoUUs will search these patterns and replace them instead of matching the entire dictionary/csv-file against file contents or file/folder-paths. Example:

```python
anonymize_regex = Anonymize(my_dict, pattern=r'[A-B]\d{4}')
```

#### Arguments
The regular expressions that take care of the replacements can be modified by using the `flag` parameter. It takes one or more variables [which can be found here](https://docs.python.org/3/library/re.html). Multiple variables are combined by a bitwise OR (the | operator). Example for a case-insensitive substitution:

```
anonymize_regex = Anonymize(my_dict, flags=re.IGNORECASE)
```

By using the `use_word_boundaries` argument (defaults to False), the algorithm ignores substring matches. If 'ted' is a key in your dictionary, without `use_word_boundaries` the algorithm will replace the 'ted' part in f.i. 'created_at'. You can overcome this problem by setting `use_word_boundaries` to True. It will put the `\b`-anchor around your regex pattern or dictionary keys. The beauty of the boundary anchors is that '@' is considered a boundary as well, and thus names in email addresses can be replaced. Example:

```
anonymize_regex = Anonymize(my_dict, use_word_boundaries=True)
```

It is also possible to specify how to re-zip unzipped folders:

```python
# specifying a zip-format to zip unpacked archives after processing (.zip is default)
anonymize_zip = Anonymize('/Users/casper/Desktop/keys.csv', zip_format='gztar')
```

### 4. Substitute data

The `substitute` method is the step where the specified keys will be replaced by the replacements. It will replace all occurrences of the specified words with the substutions, in all files in the provided source folder.

Basic use:
```python
anonymize_object.substitute(source_path, target_path)
```

Arguments:
- `source_path` (required) path to the original file, folder or zip-archive to perform the substitutions on, either a string or a [Path](https://docs.python.org/3/library/pathlib.html#basic-use) object
- `target_path` (optional): a string or [Path](https://docs.python.org/3/library/pathlib.html#basic-use) object indicating whre the results need to be written. The path will be created if it does not yet exist.

If `target_path` is provided, anonymoUUs will create a processed copy of the source into the target folder. If the source is a single file, and the file path does not contain elements that will be replaced, and the target folder is identical to the source folder, then the processed result will get a 'copy' extension to prevent overwriting.

When `target_path` is omitted, the source will be overwritten by a processed version of it.

```python
# process the datadownload.zip file, replace all patterns and write a copy to the 'bucket' folder.
anonymize_regex.substitute(
    '/Users/casper/Desktop/datadownload.zip', 
    '/Users/casper/Desktop/bucket'
)

# process the 'download' folder and replace the original by its processed version
anonymize_regex.substitute('/Users/casper/Desktop/download')

# process a single file, and replace it
anonymize_regex.substitute('/Users/casper/Desktop/my_file.json')
```

## Validation
The validation procedure determines the performance of anonymization software. 
It compares results of the automated anonymization with a manually labeled ground-truth. 
The validation procedure checks whether all occurrences of personal identifiable information, 
as detected in the manually labeled ground-truth, are correctly substituted.

### Prepare
Clone this repository to run the validation. 

Make sure you have these data present:
* anonymized files
* key file
* manually labeled ground truth 
Example data can be found in the [test_data](test_data/) in this folder

Create your manually labeled file with dedicated software like [Label Studio](https://labelstud.io/).
In the graphical user interface you can easily add custom labels to the sensitive information in your text files


### Validate
Run from the commandline
```
$ cd tests
$ python validation.py [OPTIONS]

Options:
  --anymdir  path to folder with anonymized data
  --gtfile  path to labeled groundtruth file (json)
  --keyfile  path to key file (csv)

```

## Attribution and academic use
The code in this project is licensed with [MIT](LICENSE.md).
This software is archived at Zenodo [![DOI](https://zenodo.org/badge/281087099.svg)](https://zenodo.org/badge/latestdoi/281087099)
Please cite this software using the metadata in the [citation file](CITATION.cff)

## Contributing
Contributions are what make the open source community an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

You can contribute by:
1. Opening an Issue
2. Suggesting edits to the code
3. Suggesting edits to the documentation
4. If you are unfamiliar with GitHub, feel free to [contact](#contact) us.

To contribute to content directly:

1. Fork the project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## Contact
You can contact the Utrecht University Research Engineering team [by email](mailto:research.engineering@uu.nl).

Project Link: [https://github.com/UtrechtUniversity/anonymouus](https://github.com/UtrechtUniversity/anonymouus).
",2022-08-12
https://github.com/UtrechtUniversity/awesome-utrecht-university,"![banner.jpg](docs/img/banner.jpg)

# Awesome Utrecht University

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated list of awesome research code, software, manuals, and more on Git, developed by [**Utrecht University**](https://uu.nl) researchers, students, and employees. The list can be your starting point to find interesting UU projects, and get inpired and learn from other projects. Is your project also ""awesome""? Add it to this list (see [Contributing](#contributing)).

> ""The purpose of this list is to have a collection of projects using [Git](https://git-scm.com/) version control, that score high on openness, reusability, and transparency in order to showcase good examples of open practices. In the context of this, *awesome* refers to projects that showcase the [FAIR (findability, accessibility, interoperability, and reusability)](https://www.uu.nl/en/research/open-science/tracks/fair-data-and-software) and Open Science spirit. This is for example shown in repositories that have a license that permits reuse and a README file with clear documentation.
>
> In order to promote open science, Utrecht University has introduced the [Open Science Programme](https://www.uu.nl/en/research/open-science). Beside topics like *Open access*, *Public engagement*, and *Recognition and rewards*, there is a strong focus on *FAIR Data and Software*. This awesome list was created by efforts of the track of *FAIR Data and Software* to help researchers to find good examples. We believe that learning by example is very useful in the field of Open Science and FAIR Data and Software.""
>
> [FAIR Data and Software team](https://www.uu.nl/en/research/open-science/tracks/fair-data-and-software)

❣️ We are looking for Utrecht University researchers that are interested in helping to maintain this list. Please reach out if you would like to assist (see [Contact](#contact))!

- [Awesome Utrecht University](#awesome-utrecht-university)
  - [Projects](#projects)
    - [Research code](#research-code)
    - [Research software](#research-software)
    - [Research data](#research-data)
    - [Research project management](#research-project-management)
    - [Education and workshops](#education-and-workshops)
    - [Collaboration groups](#collaboration-groups)
  - [Add project to this list](#add-project-to-this-list)
  - [Background](#background)
    - [What is an Awesome list?](#what-is-an-awesome-list)
    - [Initial project collection](#initial-project-collection)
    - [Implementing Awesome lists for your university](#implementing-awesome-lists-for-your-university)
  - [Contact](#contact)

---

## Projects

<!-- START PROJECTS -->

### Research code

*Research projects with supplementing code stored on online Git repositories.*

- [Aragonite_clumped](https://github.com/nielsjdewinter/Aragonite_clumped) - R code for processing and plotting of clumped isotope data from aragonite samples in ""Temperature dependence of clumped isotopes (∆47) in aragonite""
- [ContrastiveExplanation](https://github.com/MarcelRobeer/ContrastiveExplanation) - Contrastive Explanation (Foil Trees), developed at TNO/Utrecht University
- [DA_model](https://github.com/nielsjdewinter/DA_model) - R code for simulating diffusion-advection models for leaching of trace elements into tooth enamel during burial and diagenesis of archeological and paleontological specimens in ""High-resolution trace element distributions and models of trace element diffusion in enamel of Late Neolithic/Early Chalcolithic human molars from the Rioja Alavesa region (north-central Spain) help to separate biogenic from diagenetic trends""
- [GeoNewsMiner](https://github.com/lorellav/GeoNewsMiner) The GeoNewsMiner (GNM): An interactive spatial humanities tool to visualize geographical references in historical newspapers
- [PCR-GLOBWB_model](https://github.com/UU-Hydro/PCR-GLOBWB_model) - PCR-GLOBWB (PCRaster Global Water Balance) is a large-scale hydrological model intended for global to regional studies and developed at the Department of Physical Geography.
- [PuReGoMe](https://github.com/puregome/notebooks) - Notebooks of the PuReGoMe Project of the Netherlands eScience Center and Utrecht University.
- [Saliency-Tubes-Visual-Explanations-for-Spatio-Temporal-Convolutions](https://github.com/alexandrosstergiou/Saliency-Tubes-Visual-Explanations-for-Spatio-Temporal-Convolutions) - Implementation of Saliency Tubes for 3D Convolutions in Pytoch and Keras to localise the focus spatio-temporal regions of 3D CNNs.
- [seasonalclumped](https://github.com/nielsjdewinter/seasonalclumped) - R package for generating virtual stable isotope data to model sampling strategies for seasonality reconstructions in ""Optimizing sampling strategies in high-resolution paleoclimate records""
- [SoftPool](https://github.com/alexandrosstergiou/SoftPool) - Code for approximated exponential maximum pooling.
- [Squeeze-and-Recursion-Temporal-Gates](https://github.com/alexandrosstergiou/Squeeze-and-Recursion-Temporal-Gates) - Code for : [Pattern Recognit. Lett. 2020] ""Learn to cycle: Time-consistent feature discovery for action recognition"" and [arXiv] ""Right on Time: Multi-Temporal Convolutions for Human Action Recognition in Videos"".
- [stdstats](https://github.com/japhir/stdstats) - Simulation and plotting code for ""Optimizing the use of carbonate standards to minimize uncertainties in clumped isotope data""

### Research software

*Software developed by researchers and employees of Utrecht University. The software in this list is installable and can be used in new or existing research projects or courses.*

- [asreview](https://github.com/asreview/asreview) - Active learning for systematic reviews
- [bain](https://github.com/cjvanlissa/bain) - Bayes Factors for Informative Hypotheses
- [mice](https://github.com/amices/mice) - Multivariate Imputation by Chained Equations
   * [ggmice](https://github.com/amices/ggmice) - Visualize incomplete and imputed data with the R package `ggmice`
- [oceanexplorer](https://github.com/UtrechtUniversity/oceanexplorer) - An R interface to the [NOAA World Ocean Atlas](https://www.ncei.noaa.gov/products/world-ocean-atlas)
- [osmenrich](https://github.com/sodascience/osmenrich) - Enrich sf data with geographic features from OpenStreetMaps.
- [parcels](https://github.com/OceanParcels/parcels) - Main code for Parcels (Probably A Really Computationally Efficient Lagrangian Simulator)
- [pcraster](https://github.com/pcraster/pcraster) - Environmental modeling software
- [pdb-tools](https://github.com/haddocking/pdb-tools) - A dependency-free cross-platform swiss army knife for PDB files.
- [recordlinkage](https://github.com/J535D165/recordlinkage) - A toolkit for record linkage and duplicate detection in Python
- [ShellChron](https://github.com/nielsjdewinter/ShellChron) - R package for constructing age models based on stable oxygen isotope records from accretionary carbonate archives
- [Stitch](https://github.com/snijderlab/stitch) - A program for de novo sequencing of antibodies/proteins based on massspectrometry data.
- [text_explainability](https://git.science.uu.nl/m.j.robeer/text_explainability) - A generic explainability architecture for explaining text machine learning models.
- [text_sensitivity](https://git.science.uu.nl/m.j.robeer/text_sensitivity) - Extension of text_explainability for sensitivity testing (robustness, fairness).

### Research data

*Research data stored in git repositories.*

- [childdevdata](https://github.com/D-score/childdevdata) - R package with *Child Development Data* from ten studies, containing 1,116,061 assessments made on 10,831 unique children during 28,465 visits, covering 21 different instruments.
- [CoronaWatchNL](https://github.com/J535D165/CoronaWatchNL) - Numbers concerning COVID-19 disease cases in The Netherlands by RIVM, LCPS, NICE, ECML, and Rijksoverheid.

### Research project management

*Tools for research project management, data management, software management, and lab tools.*

- [labphew](https://github.com/SanliFaez/labphew) - a minimalist functioning code module and folder structure, built to teach and exercise with computer-controlled measurements using Python.
- [worcs](https://github.com/cjvanlissa/worcs) - Rstudio project template and convenience functions for the Workflow for Open Reproducible Code in Science (WORCS)

### Education and workshops

*Open teaching materials are guidelines, tutorials or any other educational material. Where to discover further resources relevant for UU research like books, podcasts, additional websites and newsletters.*

- [Textbook on Quantitative Methods and Statistics](https://github.com/hugoquene/QMS-EN) Textbook on Quantitative Methods and Statistics aimed at humanities researchers and students [(English version, EN)](https://hugoquene.github.io/QMS-EN/) [(Dutch version, NL)](https://hugoquene.github.io/KMS-NL/)
- [workshop-introduction-to-R-and-data](https://github.com/UtrechtUniversity/workshop-introduction-to-R-and-data) - Material for the workshop 'Introduction to R & data' by [RDM Support](https://www.uu.nl/en/research/research-data-management)

### Collaboration groups

*Collaboration Groups are organizations with many involved parties.*

- [CLARIAH](https://github.com/CLARIAH) - CLARIAH offers humanities scholars a Common Lab providing access to large collections of digital resources and innovative tools for research
- [stan](https://github.com/stan-dev) - Stan is a state-of-the-art platform for statistical modeling and high-performance statistical computation. Thousands of users rely on Stan for statistical modeling, data analysis, and prediction in the social, biological, and physical sciences, engineering, and business.

<!-- END PROJECTS -->

## Add project to this list

Do you know about a project that should be in the Awesome Utrecht University list? This could be your own project or a project of one of your colleagues. We would love to hear about that project! The [contribution guidelines](https://github.com/UtrechtUniversity/awesome-UU/blob/main/CONTRIBUTING.md) help you to propose a new project to the list.

## Background

### What is an Awesome list?

""Awesome lists"" are curated lists of awesome stuff. The lists are very popular in the field of open source development (see https://github.com/sindresorhus/awesome). Read [""The awesome manifesto""](https://github.com/sindresorhus/awesome/blob/main/awesome.md) for more information about awesome lists.

### Initial project collection

The initial collection of projects was made based on the collected repositories from the SWORDS-UU
project (more information follows soon). Repositories were considered when they have a license and 25 stars or fulfill 4/5 FAIR criteria.

### Implementing Awesome lists for your university

We encourage other universities to also implement awesome lists for their research. You can fork this repository as a starting point. Having such a list is a useful resource to showcase good projects that have been conducted or are still ongoing and helps promoting the open science approach. If you need help getting started please don't hesitate to reach out to us. We will gladly assist you.

## Contact

This awesome list was created by efforts of the [FAIR Data and Software team](https://www.uu.nl/en/research/open-science/tracks/fair-data-and-software) of Utrecht University. If you have any question or remark about this list, do not hesitate to contact any of the **current maintainers** via mail:

- [Jelle Treep](mailto:h.j.treep@uu.nl?subject=[GitHub]%20Awesome-UU)
- [Jonathan de Bruin](mailto:j.debruin1@uu.nl?subject=[GitHub]%20Awesome-UU)
- [Keven Quach](mailto:k.quach@uu.nl?subject=[GitHub]%20Awesome-UU)

 or open an issue on GitHub.

Are you a Utrecht University researcher looking for support on making your research code and data open and FAIR? Or do you want to brainstorm about these topics? Feel free to [contact RDM Support](https://www.uu.nl/en/research/research-data-management/contact-us).

![Utrecht University Open Science](https://www.uu.nl/sites/default/files/styles/original_image/public/Utrecht-University-towards-open-science.jpg)
",2022-08-12
https://github.com/UtrechtUniversity/best-practices,"![banner.jpg](images/banner.jpg)

# Best practices

Welcome at the best practices for git, research software development, and research data management. This repository was created for and by researchers and support staff of Utrecht University. The community is strongly encouraged to share best practices in this repository. 

## Topics

### Using Git and Github

Git is free and open-source software for version management. It was created in the 2005 and is nowadays the facto-standard version control software for software development. GitHub is an online service based on Git that makes it easy to work together and share your work openly. It can also be used as a backup of your local work. Learning Git and GitHub can be steep in the beginning, but will be worth all the effort. 

**Quick start**

1. [Create GitHub Account](https://github.com/join)
2. [Download and install GitHub Desktop](https://desktop.github.com/)
3. [Authenticate GitHub for Utrecht University](https://github.com/UtrechtUniversity/getting-started) [OPTIONAL]
4. [Create your first repository with Github Desktop](https://docs.github.com/en/desktop/installing-and-configuring-github-desktop/overview)

**Read more**

- [Git vs Github – Demystifying The Differences](https://www.edureka.co/blog/git-vs-github/)

### FAIR Software

The [FAIR principles](https://www.nature.com/articles/sdata201618) are a framework that is widely used in research data management. The acronym stands for Findable, Accessible, Interoperable and Reusable. Recently, the FAIR principles are applied to [research software](https://content.iospress.com/articles/data-science/ds190026) by Anna-Lena Lamprecht et al (2020). 

**Quick start**

- [Five Recommendations for FAIR Software](https://fair-software.eu/)

**Tools**

- [Python package to analyze a GitHub or GitLab repository's compliance with the fair-software.eu recommendations](https://github.com/fair-software/howfairis)

**Read more**

- Paper on FAIR software principles: [Towards FAIR principles for research software (Lamprecht et al, 2020)](https://content.iospress.com/articles/data-science/ds190026)
- Paper on FAIR data principles: [The FAIR Guiding Principles for scientific data management and stewardship](https://www.nature.com/articles/sdata201618)
- Working group *FAIR for Research Software (FAIR4RS)* https://www.rd-alliance.org/groups/fair-research-software-fair4rs-wg

### Visibility

To facilitate reuse of your wonderful work, people should be able to find you and your work. GitHub offers a lot of options to make your research software more visible. 

**Quick start**

- Add a [bio and profile README to your GitHub account](https://docs.github.com/en/github/setting-up-and-managing-your-github-profile/customizing-your-profile/about-your-profile) 
- Add GitHub profile to [your staff page](https://www.uu.nl/staff/)
- Add [topics](https://docs.github.com/en/github/administering-a-repository/managing-repository-settings/classifying-your-repository-with-topics) to your repository to increase findability https://github.com/topics. There is also topic for [`#utrecht-university`](https://github.com/topics/utrecht-university).

**Read more**

- [Utrecht University Visibility To-Do List](https://www.uu.nl/en/university-library/advice-support-to/researchers/visibility/research-visibility-check/visibility-to-do-list)
- Use [Github Pages](https://pages.github.com/) to publish a project overview and link to all relevant repositories and collaborators


## Contributing

Contributions to this repository are very welcome. The main idea of this repository is to have a collection of good practices tailored to researchers at Utrecht University. 

The following contribution guidelines apply to this repository: 

- Open a Pull Request or file an Issue if you have anything to add, change, or delete. 
- Browse the [issue with label Help Wanted](https://github.com/UtrechtUniversity/best-practices/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22) to see what you can contribute 
- Create a new topic if necessary. 
- Use an editorial style guide for writing, preferably the [Google developer documentation style guide](https://developers.google.com/style). 

## License

The content in this repository is licensed [CC0-1.0](/LICENSE) (release to the public domain).

## Contact

This repository is currently maintained by the [Utrecht University Open Science Programme](https://www.uu.nl/en/research/open-science), and the ITS department. Are you an Utrecht University researcher or support staff and willing to help maintaining this repository? Reach out to its.ris@uu.nl. 
",2022-08-12
https://github.com/UtrechtUniversity/cd2-ansible,"Ansible playbook for the deployment of the CD² metadata catalogue.

# Requirements
- [Vagrant](https://www.vagrantup.com) (installation instructions [here](https://www.vagrantup.com/downloads))
- [Ansible](https://ansible.com) (installation instructions [here](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html#installing-the-ansible-community-package))
- [community.postgresql Ansible Galaxy repository](https://galaxy.ansible.com/community/postgresql) (install by running `ansible-galaxy collection install community.postgresql`)

# Confirmed to work on the following host operating systems
- Debian 10
- Ubuntu 21.04


# Usage
- Navigate to the ypp-ansible directory
- Copy an SSH keypair with access to git.science.uu.nl/epos-msl/msl\_ckan\_util to `provisioning/files`
- Run `vagrant up`
- Add the following line to your hostfile (e.g. /etc/hosts on Unix-based systems): `192.168.56.30 cd2.test`
- You can now access the CKAN environment at http://cd2.test (the credentials for the default administrative user are: `admin:password`)
- To log into the virtual machine using SSH, run `vagrant ssh`
",2022-08-12
https://github.com/UtrechtUniversity/cd2-ckan,"# ckanext-msl_ckan

This extension contains configurations and templates for the EPOS MSL CKAN portal.
The use of this extension is depended on several other extensions as described in the requirements section. Reusable 
functionality has been placed within the msl_ckan_util extension.

## Requirements

This extension has been developed and tested with CKAN version 2.9.*

This extension requires the following other extension to be installed and activated:

| CKAN extension        | Plugin   |
| ---------------       | ------------- |
| ckanext-scheming      | scheming_datasets |
| ckanext-scheming      | scheming_groups |
| ckanext-scheming      | scheming_organizations |
| ckanext-msl_ckan_util | msl_ckan |
| ckanext-msl_ckan_util | msl_custom_facets |
| ckanext-msl_ckan_util | msl_repeating_fields |

**TODO:** Add links to extension repos

## Installation

**TODO:** Add any additional install steps to the list below.
   For example installing any non-Python dependencies or adding any required
   config settings.

To install ckanext-msl_ckan:

1. Activate your CKAN virtual environment, for example:

     . /usr/lib/ckan/default/bin/activate

2. Clone the source and install it on the virtualenv

            git clone https://git.science.uu.nl/epos-msl/epos-msl.git
            cd ckanext-msl_ckan
            pip install -e .
            pip install -r requirements.txt

3. Add `msl_ckan` to the `ckan.plugins` setting in your CKAN
   config file (by default the config file is located at
   `/etc/ckan/default/ckan.ini`).

4. Restart CKAN.

## SOLR changes

Depending on how SOLR was installed combined with CKAN a schema.xml supplied with the CKAN installation has 
been used. These changes assume the CKAN supplied schema.xml have been used. The following additions should be 
made to the schema.xml.

Add to `<fields>` definitions:

      <!-- MSL custom fields for indexing and web services -->
      <field name=""msl_hidden_text"" type=""text"" indexed=""true"" stored=""false"" multiValued=""true""/>
    
      <!-- coming from IPackageController msl_search.MslIndexRepeatedFieldsPlugin::before(index) -->
      <field name=""msl_material"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_rock_measured_property"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_rock_apparatus"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_rock_ancillary_equipment"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_rock_pore_fluid"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_rock_inferred_deformation_behavior"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_author_name"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_author_name_text"" type=""text"" indexed=""true"" stored=""false"" multiValued=""true""/>
      <field name=""msl_lab_name"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_lab_name_text"" type=""text"" indexed=""true"" stored=""false"" multiValued=""true""/>

And to the bottom list with `copyField` definitions add:

      <!-- customizations MSL-->
      <copyField source=""msl_material"" dest=""text""/>
      <copyField source=""msl_hidden_text"" dest=""text""/>
      <copyField source=""msl_rock_measured_property"" dest=""text""/>
      <copyField source=""msl_rock_apparatus"" dest=""text""/>
      <copyField source=""msl_rock_ancillary_equipment"" dest=""text""/>
      <copyField source=""msl_rock_pore_fluid"" dest=""text""/>
      <copyField source=""msl_rock_inferred_deformation_behavior"" dest=""text""/>
      <copyField source=""msl_author_name"" dest=""text""/>
      <copyField source=""msl_author_name"" dest=""msl_author_name_text""/>
      <copyField source=""msl_lab_name"" dest=""text""/>
      <copyField source=""msl_lab_name"" dest=""msl_lab_name_text""/>

Within the `solrconfig.xml` make sure that the `<str name=""q.op"">` setting is set to AND for the select request handler:

      <requestHandler name=""/select"" class=""solr.SearchHandler"">
    <!-- default values for query parameters can be specified, these
         will be overridden by parameters in the request
      -->
    <lst name=""defaults"">
      <str name=""echoParams"">explicit</str>
      <int name=""rows"">10</int>
      <str name=""mm"">1</str>
      <str name=""q.op"">AND</str> <----
      ...

## Config settings

This extension includes several configuration files that are used by other extension required by this project. To 
make the correct links to the other extensions/plugins the following lines should be added to the `ckan.ini`.

### Load plugins

`ckan.plugins` in the `ckan.ini` should contain the following plugins:

      msl_ckan
      scheming_datasets
      scheming_groups
      scheming_organizations 
      msl_custom_facets
      msl_repeating_fields

Make sure to keep the above order of plugin declaration in the `ckan.ini`. The order of plugin loading determines the 
order of execution of hooks and usage of templates.

### plugin specific settings

To use the schemas as included within this extension by the scheming plugin the following lines should be added to the 
`ckan.ini` file:

      scheming.dataset_schemas = ckanext.msl_ckan:schemas/datasets/ckan_dataset.yaml ckanext.msl_ckan:schemas/datasets/rock_physics.yml ckanext.msl_ckan:schemas/datasets/paleomagnetic.yml ckanext.msl_ckan:schemas/datasets/analogue_modelling.yml ckanext.msl_ckan:schemas/datasets/labs.json
      scheming.group_schemas = ckanext.msl_ckan:schemas/groups/custom_group_msl_subdomain.json
      scheming.organization_schemas = ckanext.msl_ckan:schemas/organizations/custom_org_institute.json

To use the included facet configuration:

      mslfacets.dataset_config = ckanext.msl_ckan:config/facets.json

To use the included index fields configuration:

      mslindexfields.field_config = ckanext.msl_ckan:config/msl_index_fields.json

## Adjusting settings within CKAN
Some texts and settings have to be adjusted by signing in as admin within the portal. The default username and password 
depend on the installation type. It is recommended to change the default credentials after installation.

### Generating an API key
Go to your profile in the top bar and click on `manage`. Within this form click on the `Regenerate API key` button to 
make an API key that can be used for access. This API key will be used in other steps of this guide.

### Add organizations
Some organizations have to be added to use the current import process. Currently these have to be added manually. Add 
organizations with the names: `EPOS Multi-scale Laboratories Thematic Core Service` and `yoda repository`.

### Import data

### Set texts
Go to the `Sysadmin settings` section which is linked to in the top right menu. Click on the `Config` tab and set the 
following values using the form.

About:
```
![EPOS](https://www.epos-eu.org/themes/epos/logo.svg)

[Check](https://www.epos-eu.org) the new EPOS ERIC website
***
Click [here](https://epos-no.uib.no/epos-tna/facilities) to go to the EPOS TNA-/Infrastructure portal for an overview of labs currently involved within the MSL network.
##Background

In a world that demands increasing interoperability and collaboration inside the scientific community, solid Earth science laboratories are challenged with finding each other to exchange best practices and re-usable standardized data. Data produced by the various laboratory centres and networks are crucial to serving society’s needs for geo-resources exploration and for protection against geo-hazards. Indeed, to model resource formation and system behaviour during exploitation, we need an understanding from the molecular to the continental scale, based on experimental and analytical data. Therefore, coordination and communication inside the European solid Earth science laboratories, complemented with services to increase curation and access for re-use of laboratory data is needed to effectively contribute to solve the grand challenges facing society.

The **EPOS Mult-scale Laboratories community** (https://epos-ip.org/tcs/multi-scale-laboratories) aims at the collection and harmonization of available and emerging laboratory data on the properties and processes controlling rock system behaviour at multiple scales. As a result, it generates products uniformly accessible and interoperable through services for supporting research activities into Geo-resources and Geo-storage, Geo-hazards and Earth System Evolution.

The EPOS Multi-Scale Laboratories community includes a wide range of world-class laboratory infrastructures. The length scales addressed by these infrastructures cover the nano- and micrometre levels (electron microscopy and micro-beam analysis) to the scale of experiments on centimetre and decimetre sized samples, to analogue model experiments simulating the reservoir scale, the basin scale and the plate scale. The MSL community includes at the moment over sixty laboratories, affiliated to eleven institutes in eight European countries. The research infrastructures are grouped into four major sub-domains:

* Analogue modelling of geologic processes
* Rock and melt physical properties
* Paleomagnetic and magnetic data
* Geochemical data (rock geochemistry)

Data emerging from the community can be categorized as

_analytical and properties data on_

* volcanic ash from explosive eruptions
* magmas in the context of eruption and lava-flow hazard evaluation
* rock systems of key importance in mineral exploration and mining operations

_experimental data describing_

* rock and fault properties of importance for modelling and forecasting natural and induced subsidence, seismicity and associated hazards
* rock and fault properties relevant for modelling the containment capacity of rock

_systems for CO2 energy sources and wastes_

* crustal and upper mantle rheology as needed for modelling sedimentary basin formation and crustal stress distributions
* the composition, porosity, permeability and frackability of reservoir rocks of interest in relation to unconventional resources and geothermal energy

_repository of analogue models on tectonic processes, from the plate to the reservoir scale, relevant to the understanding of Earth dynamics, geo-hazards, and geo-energy_

_paleomagnetic data, that are crucial for_

* understanding the evolution of sedimentary basins and associated resources
* charting geo-hazard frequency
```

Intro Text:
```
![EPOS](https://www.epos-eu.org/themes/epos/logo.svg)

[Check](https://www.epos-eu.org) the new EPOS ERIC website
***
Click https://epos-no.uib.no/epos-tna/facilities to go to the EPOS TNA-/Infrastructure portal for an overview of labs currently involved within the MSL network.
***
This is the central data catalog of the EPOS Multi-scale laboratories community. Here you can find openly published data coming from a wide range of world-class experimental laboratory infrastructures: from high pressure-temperature rock and fault mechanics and rock physics facilities, to electron microscopy, micro-beam analysis, analogue modelling and paleomagnetic laboratories. More information about the Multi-scale laboratories community is to be found at https://epos-ip.org/tcs/multi-scale-laboratories.
```

## Developer installation

To install ckanext-msl_ckan for development, activate your CKAN virtualenv and
do:

    git clone https://git.science.uu.nl/epos-msl/epos-msl.git
    cd ckanext-msl_ckan
    python setup.py develop
    pip install -r dev-requirements.txt


## Tests

To run the tests, do:

    pytest --ckan-ini=test.ini


## Releasing a new version of ckanext-msl_ckan

If ckanext-msl_ckan should be available on PyPI you can follow these steps to publish a new version:

1. Update the version number in the `setup.py` file. See [PEP 440](http://legacy.python.org/dev/peps/pep-0440/#public-version-identifiers) for how to choose version numbers.

2. Make sure you have the latest version of necessary packages:

    pip install --upgrade setuptools wheel twine

3. Create a source and binary distributions of the new version:

       python setup.py sdist bdist_wheel && twine check dist/*

   Fix any errors you get.

4. Upload the source distribution to PyPI:

       twine upload dist/*

5. Commit any outstanding changes:

       git commit -a
       git push

6. Tag the new release of the project on GitHub with the version number from
   the `setup.py` file. For example if the version number in `setup.py` is
   0.0.1 then do:

       git tag 0.0.1
       git push --tags

## License

[AGPL](https://www.gnu.org/licenses/agpl-3.0.en.html)
",2022-08-12
https://github.com/UtrechtUniversity/clumped-processing,"#+title: clumped-processing
[[https://zenodo.org/badge/latestdoi/400205023][https://zenodo.org/badge/400205023.svg]]

This is processing code for the clumped isotope (\Delta_{47}) measurements made with the Thermo Fisher MAT 253 plus isotope ratio mass spectrometer with the Kiel IV automatic line, located in the Vening-Meineszbuilding B, for the Stratigraphy and Paleontology group at Earth Sciences, UU.

* ~targets~ explanation

Using the R-package [[https://wlandau.github.io/targets-manual/][targets]], we can run only those parts of the code that need to be updated. The basic structure is we first load the necessary libraries, then we define functions that do most of the work, and then a workflow of targets with function calls that ~targets~ analyses for inter-dependencies.

* Deployment

The workflow is deployed by [[https://www.uu.nl/en/organisation/faculty-of-geosciences/about-the-faculty/organisation/faculty-office][GEO-ICT]] and runs on a Windows Virtual Machine that is located close to the rawdata drive of the Geolab.

This VM can read in the rawdata directly and synchronizes the metadata file and final output to the Clumped Isotope Teams OneDrive.

To run the code, log in to the VM, start RStudio and run ~targets::tar_make()~ from the command line.

* Dependencies

We use the [[https://www.r-project.org/][R programming language]] >4.1.0 (because we use the native pipe ~|>~ operator every now and then).

The required R packages for this workflow are:

- [[https://github.com/isoverse/clumpedr/][clumpedr]] :: install with ~devtools::install_github(""isoverse/clumpedr"")~ to do the generic clumped isotope calculations
- tidyverse :: It's probably easiest to just install them all...
  - dplyr :: data wrangling
  - readr :: reading/writing csv files, changing column types
  - purrr :: functional programming, never loop again
  - tibble :: better data.frames
  - lubridate :: work with dates and times
  - tidyr :: i.e. extract character column, nesting, pivoting
- readxl :: to read in our excel files with metadata changes
- qs :: for optimized cache of lists
- fst :: for optimized cache of data.frames and tibbles
- isoreader :: to read in the raw files
- slider :: for the rolling empirical transfer function (currently not preferred)
- targets :: for the cached reproducible pipeline workflow
- tidylog :: for neat log messages about dplyr verbs

* Copying

The code was created by Ilja J. Kocken https://orcid.org/0000-0003-2196-8718
Copyright 2022 © Ilja Kocken

This program is free software: you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free Software
Foundation, either version 3 of the License, or (at your option) any later
version.

This program is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with
this program. If not, see <https://www.gnu.org/licenses/>.

* Contributing

Contributions in the form of issues and pull requests are most welcome. However, the goal of this repository is to proccess clumped isotope data from the Utrecht University laboratory.

* Citation

If you use this software in your work, please cite it using the following metadata.

Kocken, I. J. (2022). clumped-processing: R scripts to process clumped isotopes from raw data to final values at Utrecht Univerity (Version 1.0.0) [Computer software]. https://doi.org/10.5281/zenodo.6421836",2022-08-12
https://github.com/UtrechtUniversity/cTORS,"# Treinonderhoud- en -rangeersimulator (TORS)
This implementation of TORS consists of a backend written in C++ (cTORS), and a front-end written in python (TORS).

## Project setup
The basic project setup uses the structure provided by cmake. The subfolders are subprojects:
* cTORS: The c++ implementation of TORS
* cTORSTest: The tests for cTORS
* pyTORS: The python interface for cTORS
* TORS: The challenge environment, in python

## Installation
The following section explains how to compile this source code. Alternatively one can run the code in a docker container.
The Dockerfile also shows how this project can be compiled and run step-by-step.
### Install Cmake and Python development libraries
To compile cTORS, cmake 3.11 (or higher) is required and the python development libraries:
```
apt-get install cmake
apt-get install python3-dev
```

For windows:
 * download and install cmake: https://cmake.org/download/
 * python header files already installed with python, see the include folder in your python folder.

### Install Google Protocol Buffers
This project uses Google Protocol Buffers to read the data files. Installation is required to compile the C++ source:
 * Download the source from the [github page](https://github.com/protocolbuffers/protobuf) (version 3.15.6 is used)
 * Follow the [instructions](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md) to install

### Build with setuptools
You can build cTORS and the pyTORS library with the following command.
```sh
mkdir build
python setup.py build
python setup.py install
```

### Compile cTORS from C++ source
In the source directory execute the following commands:
```sh
mkdir build
cd build
cmake ..
cmake --build .
```
This has been tested with gcc 9.3. Older versions may not support the c++17 standard. 

### Alternative: build with docker
You can also run TORS in a docker container. To build and run the container, run:
```sh
docker build -t tors-base .
docker run --network=""host"" --rm -it tors-base /bin/bash
```

# Basic usage

## Run the challenge environment
To run challenge environment, run the following code

```sh
cd TORS
python run.py
```

Optionally you can change the episode or agent data by changing the parameters
```sh
python run.py --agent agent.json --episode episode.json
```
The `--agent` option sets the file that configures the agent.
The `--episode` option sets the file that configures the episode.

You can also run the file with the `--train` flag to train the agent instead of eveluating its performance.

## Usage in Python
To use cTORS in python, you need to import they `pyTORS` library. E.g.

```python
from pyTORS import Engine

engine = Engine(""data/Demo"")
scenario = engine.get_scenario(""data/Demo/scenario.json"")
state = engine.start_session(scenario)

actions = engine.step(state)
engine.apply_action(actions[0])

engine.end_session(state)
```

## Running the visualizer

The visualizer runs as a flask server. Install the dependencies in `TORS/requirements-visualizer` first.
```sh
pip install -r TORS/requirements-visualizer
```
Now flask can be run by running the commands:
```sh
cd TORS/visualizer
export FLASK_APP=main.py
export FLASK_ENV=development
export FLASK_RUN_PORT=5000
python -m flask run
```

## Running the example RL-agent with gym
The repository also includes example code that wraps cTORS in a gym-environment and uses an RL implementation from stable-baselines3 to learn a policy. To run this example, first install the requirements:
```sh
pip install -r TORS/requirements-gym
```
Then run:
```sh
cd TORS
python run_gym.py
```
You can check the learning progress using tensorboard:
```sh
tensorboard --logdir ./log_tensorboard/
```

## Configuration
TORS can be configured through configuration files. Seperate configuration exists for
1. The location
2. The scenario
3. The simulator
3. The episode
4. The agent

### Configuring the location
A location is described by the `location.json` file in the data folder.
It describes the shunting yard: how all tracks are connected, what kind of tracks they are, and distances among tracks.

In order to use the visualizer for that location, you need to provided another file `vis_config.json`. See the folder `data/Demo` and `data/KleineBinckhorstVisualizer` for examples.

### Configuring the scenario
A scenario is described by the `scenario.json` file in the data folder.
It describes the scenario: which employees are available, shunting units' arrivals and departures, and possible disturbances.

### Configuring the simulator
The simulator can be configured by the `config.json` file in the data folder.
It describes which business rules need to be checked and the parameters for the actions

### Configuring the episode
You can provide an episode configuration and pass it to `TORS/run.py` with the `--episode` parameter.
This file describes the evaluation/training episode.
It contains the path to the data folder, the number of runs, RL parameters and parameters for scenario generation.

### Configuring the agent
You can provide an agent configuration and pass it to `TORS/run.py` with the `--agent` parameter.
This file prescribes which agent to use, and passes parameters to the agent.

## Tests
### Run the cTORS tests
To run the cTORS tests, execute the commands
```sh
cd build
ctest
```

## Documentation
The documentation in the C++ code is written in the Doxygen format. Install doxygen (optional) to generate the documentation, or check the full documentation online at [algtudelft.github.io/cTORS](https://algtudelft.github.io/cTORS/).

### Dependencies installation
To generate the documentation, install the following programs:
```sh
apt-get install -y doxygen graphviz fonts-freefont-ttf
apt-get install -y libclang-dev
python -m pip install git+git://github.com/pybind/pybind11_mkdoc.git@master
python -m pip install pybind11_stubgen
```

### Generate the documentation
With the dependencies installed, cmake automatically generates the documentation. It can also be generated manually by running
```sh
cd cTORS
doxygen Doxyfile
cd ..
python -m pybind11_mkdoc -o pyTORS/docstrings.h cTORS/include/*.h -I build/cTORS
```
This produces as output the `cTORS/doc` folder and the `pyTORS/docstrings.h` source file. This last file is used in `pyTORS/module.cpp` to generate the python docs.

## Contributors
* Mathijs M. de Weerdt: Conceptualization, Supervision, Project administration, Funding acquisition, Writing - review & editing
* Bob Huisman: Conceptualization
* Koos van der Linden: Software, Writing - Original draft
* Jesse Mulderij: Writing - Original draft
* Marjan van den Akker: Supervision of the bachelor team
* Han Hoogeveen: Supervision of the bachelor team
* Joris den Ouden: Conceptualization, Supervision of the bachelor team
* Demian de Ruijter: Conceptualization, Supervision of the bachelor team
* Bachelor-team, consisting of Dennis Arets, Sjoerd Crooijmans, Richard Dirven, Luuk Glorie, Jonathan den Herder, Jens Heuseveldt, Thijs van der Horst, Hanno Ottens, Loriana Pascual, Marco van de Weerthof, Kasper Zwijsen: Software, Visualization

",2022-08-12
https://github.com/UtrechtUniversity/dataprivacyhandbook,"# Data Privacy Handbook

[![read the book](https://img.shields.io/badge/read-the%20book-yellow)](https://utrechtuniversity.github.io/dataprivacyhandbook/)
[![i want to contribute!](https://img.shields.io/badge/i%20want%20to-contribute!-brightgreen)](https://github.com/UtrechtUniversity/dataprivacyhandbook/blob/main/CONTRIBUTING.md)
[![All Contributors](https://img.shields.io/badge/all_contributors-2-orange.svg?style=flat-square)](#contributors-)
![issues](https://img.shields.io/github/issues/utrechtuniversity/dataprivacyhandbook?color=red)  
[![start/join a discussion](https://img.shields.io/badge/start%2Fjoin%20a-GitHub%20Discussion-blue)](https://github.com/UtrechtUniversity/dataprivacyhandbook/discussions)
[![chat on Slack](https://img.shields.io/badge/chat%20on-Slack-blueviolet)](https://join.slack.com/t/dtl-dsig/shared_invite/zt-krwq991u-6PczI7~fxokJuLOpnI3u0A)
<a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by/4.0/80x15.png"" /></a>

## ✨Welcome!✨

The Data Privacy Handbook is a guide to handling personal data in scientific research, in line with European data protection and privacy regulations.  

You can find the Handbook here: https://utrechtuniversity.github.io/dataprivacyhandbook/. You are currently viewing the GitHub repository that underlies the Handbook - this is where the Handbook is built and maintained. 

Feel free to take a look around the repository! This README will give you an overview of the project. You can jump straight to one of the sections below, or just scroll down to find out more.

## Table of Contents
- [About the Handbook](#about-the-handbook)
- [A Little More Background](#a-little-more-background)
- [Contribute](#contribute)
- [Get or stay in touch](#get-or-stay-in-touch)
- [Citing the Handbook](#citing-the-handbook)
- [License](#license)
- [Wiki](#wiki)
- [History and Acknowledgements](#history-and-acknowledgements)
- [Contributors ✨](#contributors-)

## About the Handbook

### This is an open-source, community-driven, _living_ Handbook.  

- 🤝 As an _open-source_, _community-driven_ Handbook, anyone can get involved and contribute! You'd help us build something that is hopefully useful to many.  
- 🌱 As a _living_ Handbook, the content can be continually edited and updated. 

### Content

The Data Privacy Handbook consists of: a knowledge base, an overview of privacy-enhancing techniques & tools, and use cases.  

- 🧠 The _knowledge base_ serves to introduce readers to the EU General Data Protection Regulation (GDPR), explain how it applies to scientific research, and describe how scientific research can be carried out in a GDPR-compliant manner.
- 🛠️ The overview of _privacy-enchancing techniques & tools_ will introduce readers to privacy-enchancing techniques (PETs) such as anonymization and pseudonymization, and illustrate how these techniques can be implemented with available developed tools.
- 👨🏽‍🤝‍👨🏻 The _use cases_ will be featured as practical examples to draw inspiration from. 

### Style / Approach

While we call it a 'Handbook' - it's not meant to be written or read like a textbook. Our goal is to make knowledge and solutions surrounding this topic FAIR (Findable, Accessible, Interoperable, Reusable) and present them in a practical and actionable format. 

- As far as possible, we'll keep the text short and sweet. We'd be delighted if we end up with entire 'chapters' presented in the form of an infographic or flowchart.
- The 'chapters' will follow a linear structure, but they need not be read one by one. Readers can simply navigate to the chapter they need at a given moment.   

### Audience & Authors

Our primary audience is **scientific researchers**. You might catch us saying the Handbook needs to be 'researcher-readable' 🙃
That being said, **privacy professionals** and **research data management (RDM) staff** might find the Handbook useful for reference or recommendation.

The Handbook is developed at Utrecht University (UU) in the Netherlands 🌷. In the interest of open science and FAIR principles, we'll be keeping the content insitution-agnostic as far as possible. Nonetheless, some sections or chapters might be opinionated towards UU policies and preferences.  

## A Little More Background

The Data Privacy Handbook is part of a larger project within UU: the _Data Privacy Project_. The aim of this project is to develop knowledge, tools, and experience of how researchers can and should deal with personal data, with an emphasis on pseudonymization and anonymization techniques and tools. Specifically, the project goals are as follows:  

1. 🧠 Create a **knowledge base** and an overview of **available tools** for researchers to learn about handling personal data. The _Data Privacy Handbook_ forms the basis for this this deliverable.  
2. 🛠️ Realize or expand a number of **tools** to solve concrete problems of research projects in this area. For this, we will take on **use cases** that are typical of various privacy-related questions.
3. 🔍 Perform **qualitative and quantitative investigations** into the current practices and needs of UU research staff concerning the handling of privacy-sensitive data.
4. 💪 Develop various **training materials** for researchers and support staff about dealing with personal data and the use of the available tools.  

You can read more about the project on the <a href=""https://utrechtuniversity.github.io/dataprivacyproject/"" target=""_blank"">project website</a>.

## Contribute

Want to get involved in this project? Awesome! You can find how to contribute in the [Contributing Guidelines](https://github.com/UtrechtUniversity/dataprivacyhandbook/blob/main/CONTRIBUTING.md).

In short, you can:
- Open or comment on [Issues](https://github.com/UtrechtUniversity/dataprivacyhandbook/issues)
- Start or join a [Discussion](https://github.com/UtrechtUniversity/dataprivacyhandbook/discussions)
- Contribute direct Handbook content

When participating in this project, we expect you to abide by our [Code of Conduct](https://github.com/UtrechtUniversity/dataprivacyhandbook/blob/main/CODE_OF_CONDUCT.md).

## Get or stay in touch

- **GitHub Discussions**: You can use [GitHub Discussions](https://github.com/UtrechtUniversity/dataprivacyhandbook/discussions) to ask and answer questions, share ideas/updates/information, and conduct/participate in (open-ended or otherwise) conversations about the project.
- **MS Teams**: Staff at UU can join the project's MS Team: _Data Privacy Project_. You can contact the project coordinator to add you to the Team.
- **Slack**: For our collaborators outside UU, we have a dedicated `#sig-privacy channel` on the [DTL Data Stewards Interest Group](https://www.dtls.nl/community/interest-groups/data-stewards-interest-group/)'s [Slack community](https://join.slack.com/t/dtl-dsig/shared_invite/zt-krwq991u-6PczI7~fxokJuLOpnI3u0A)!
- **Email**: You can always contact the project coordinators - [Neha Moopen](https://www.uu.nl/medewerkers/NMoopen) & [Dorien Huijser](https://www.uu.nl/medewerkers/DCHuijser) by email.
- **Newsletter**: You can subscribe to UU's [Research Data Management Support newsletter](https://mailings.uu.nl/hp/gKQRpokhPxN6bw8NlPTr8g/subscribe_newsletter_research_data_management_support) for periodic updates on our project.

## Citing the Handbook

To cite the Data Privacy Handbook, click `Cite this repository` at the [repository homepage](https://github.com/UtrechtUniversity/dataprivacyhandbook), under the header `About` (at top right of the screen).

## License 

Non-software content in this project is licensed under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/). You can view the license [here](https://github.com/UtrechtUniversity/dataprivacyhandbook/blob/main/LICENSE.md).

## Wiki

The repository's [Wiki](https://github.com/UtrechtUniversity/dataprivacyhandbook/wiki) contains details about the tech stack for the Handbook. This documentation is relevant for current and future maintainers of the repository and/or Handbook. You're more than welcome to check it out if you're curious about the technical set-up of the Handbook or, for example, you want to reuse the Handbook - but adapt it to your own/insitution's needs. 

## History and Acknowledgements

This project was initiated by Menno Rasch in early 2020, along with colleagues from UU RDM Support. It is currently supported with funding from UU's Research IT program and a grant from the NWO-DCC funding instrument.

## Contributors ✨

Thanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tr>
    <td align=""center""><a href=""https://github.com/nehamoopen""><img src=""https://avatars.githubusercontent.com/u/37183829?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>Neha Moopen</b></sub></a><br /><a href=""https://github.com/UtrechtUniversity/dataprivacyhandbook/commits?author=nehamoopen"" title=""Tests"">⚠️</a> <a href=""#projectManagement-nehamoopen"" title=""Project Management"">📆</a> <a href=""https://github.com/UtrechtUniversity/dataprivacyhandbook/commits?author=nehamoopen"" title=""Documentation"">📖</a> <a href=""#infra-nehamoopen"" title=""Infrastructure (Hosting, Build-Tools, etc)"">🚇</a> <a href=""#maintenance-nehamoopen"" title=""Maintenance"">🚧</a></td>
    <td align=""center""><a href=""http://www.dorienhuijser.com""><img src=""https://avatars.githubusercontent.com/u/58177697?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>Dorien Huijser</b></sub></a><br /><a href=""#projectManagement-DorienHuijser"" title=""Project Management"">📆</a> <a href=""https://github.com/UtrechtUniversity/dataprivacyhandbook/commits?author=DorienHuijser"" title=""Documentation"">📖</a> <a href=""#infra-DorienHuijser"" title=""Infrastructure (Hosting, Build-Tools, etc)"">🚇</a> <a href=""#maintenance-DorienHuijser"" title=""Maintenance"">🚧</a></td>
    <td align=""center""><a href=""https://github.com/Danny-dK""><img src=""https://avatars.githubusercontent.com/u/45395070?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>Danny-dK</b></sub></a><br /><a href=""https://github.com/UtrechtUniversity/dataprivacyhandbook/pulls?q=is%3Apr+reviewed-by%3ADanny-dK"" title=""Reviewed Pull Requests"">👀</a></td>
    <td align=""center""><a href=""http://garrettspeed.com""><img src=""https://avatars.githubusercontent.com/u/6378547?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>Garrett Speed</b></sub></a><br /><a href=""https://github.com/UtrechtUniversity/dataprivacyhandbook/pulls?q=is%3Apr+reviewed-by%3Agspeed0689"" title=""Reviewed Pull Requests"">👀</a></td>
    <td align=""center""><a href=""https://www.uu.nl/staff/MGdeVos""><img src=""https://avatars.githubusercontent.com/u/1482239?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>Martine de Vos</b></sub></a><br /><a href=""https://github.com/UtrechtUniversity/dataprivacyhandbook/pulls?q=is%3Apr+reviewed-by%3AMartineDeVos"" title=""Reviewed Pull Requests"">👀</a></td>
    <td align=""center""><a href=""https://github.com/Mish-JPFD""><img src=""https://avatars.githubusercontent.com/u/22911522?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>Jacques Flores</b></sub></a><br /><a href=""#ideas-Mish-JPFD"" title=""Ideas, Planning, & Feedback"">🤔</a> <a href=""#content-Mish-JPFD"" title=""Content"">🖋</a> <a href=""https://github.com/UtrechtUniversity/dataprivacyhandbook/pulls?q=is%3Apr+reviewed-by%3AMish-JPFD"" title=""Reviewed Pull Requests"">👀</a></td>
    <td align=""center""><a href=""https://github.com/RonScholten""><img src=""https://avatars.githubusercontent.com/u/54577946?v=4?s=100"" width=""100px;"" alt=""""/><br /><sub><b>Ron Scholten</b></sub></a><br /><a href=""#content-RonScholten"" title=""Content"">🖋</a> <a href=""https://github.com/UtrechtUniversity/dataprivacyhandbook/pulls?q=is%3Apr+reviewed-by%3ARonScholten"" title=""Reviewed Pull Requests"">👀</a></td>
  </tr>
</table>

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->

<!-- ALL-CONTRIBUTORS-LIST:END -->

This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!
",2022-08-12
https://github.com/UtrechtUniversity/dataprivacyproject,"## Data Privacy Project - project landing page

This is the repository underlying the landing page of the Data Privacy Project at Utrecht University. We use this space to update the content and looks of the website.

**You can access the website with information about the project here: https://utrechtuniversity.github.io/dataprivacyproject/**

Other repositories related to this project:

- The <a href = ""https://utrechtuniversity.github.io/dataprivacyhandbook/"" target = ""_blank"">Data Privacy Handbook</a>, a guide on handling personal data in scientific research (<a href = ""https://github.com/UtrechtUniversity/dataprivacyhandbook#readme"" target = ""_blank"">link to repository</a>).
- The <a href = ""https://utrechtuniversity.github.io/dataprivacysurvey/"" target = ""_blank"">Data Privacy Survey</a>, a university-wide survey about the challenges and needs of Utrecht University researchers surrounding the handling of personal data (<a href = ""https://github.com/UtrechtUniversity/dataprivacysurvey"" target = ""_blank"">link to repository</a>).

## Technical information

In this repository, we are using:

- The jekyll theme ""Alembic"", forked from [this repository](https://github.com/daviddarnes/alembic)
- Dependabot, to check for security issues and run automatic updates

Here's a quick summary of the folder contents in this repository:

- `.github`: contains dependabot settings
- `.jekyll-cache`: cached files, no need to do anything with this
- `_data`: data files from which the menu and people page is built
- `_drafts`: draft news posts (not published)
- `_includes`: html-files that make up parts of the webpages
- `_layouts`: the skeleton layout of the webpages
- `_posts`: markdown files that will become listed as newsposts, filename format should be ""yyyy-mm-dd-title-of-post.md""
- `_sass`: styling of the website (e,g., colors, size, alignment, etc.)
- `assets`: fixed files like pdf documents (docs), images, fonts and javascripts
- `news`: contains the main page displayed on the news webpage. This is a collector-page which shows the collection of news posts
- `pages`: all web pages in editable .md format (except the homepage)
- Separate files:
  - `.gitignore`: which files or folder should not be tracked by git?
  - `Gemfile`: which Ruby gems are used in this theme?
  - `README.md`: this readme file
  - `_config.yml`: the site settings
  - `alembic-jekyll-theme.gemspec`: specifications of the theme gem
  - `index.md`: the homepage, should always be in the root of the repository

## Contact and contribution

For questions about this repository, please contact Utrecht University's <a href = ""https://www.uu.nl/en/research/research-data-management/contact-us"" target = ""_blank"">Research Data Management Support</a>, or open an Issue or Pull request in this repository.

## License

This repository is licensed under an MIT license. You can view the <a href=""https://github.com/UtrechtUniversity/dataprivacyproject/blob/master/LICENSE"">license text here</a>.",2022-08-12
https://github.com/UtrechtUniversity/dataprivacysurvey,"# Data Privacy Survey

In the second quarter of 2022, Utrecht University (UU) Research Data Management Support (RDM Support) sent out a survey among all scientific personnel at Utrecht University: the Data Privacy Survey. The aim of this survey was to investigate 1) How UU researchers currently deal with personal data in research, 2) What challenges they run into when handling personal data in research, and 3) How RDM Support and collaborators can improve their services and support concerning personal data in research.

The survey is part of the <a href=""https://utrechtuniversity.github.io/dataprivacyproject"" target=""_blank"">Data Privacy Project</a>, an RDM Support  project to improve information, tools and services surrounding personal data in research. More information about this survey can be found in the following files:

- The <a href=""assets/survey-privacy-statement.pdf"" target=""_blank"">survey's privacy statement</a>
- The <a href=""assets/survey-questions-qualtrics.pdf"" target=""_blank"">full survey</a>.

## This repository

This repository contains the code for analysing the data from the Data Privacy Survey. The code can be found in the current GitHub repository (`src` folder) as an R markdown file. As the raw dataset contains personal data in the form of demographic information, free text, and email addresses, it is not shared in this repository. The code for pseudonymising the dataset is shared however (see `src` folder), for full transparency on how the survey data were processed.

- Here you can find <a href=""docs/survey-first-glance.html"" target=""_blank"">a first glance of the survey results</a>.
- Here, you can find <a href = ""docs/data-privacy-survey-report-v0.1.html"" target = ""_blank"">a first (semi-)full report of the findings</a>.
- A complete first version of the report, including reports per UU faculty, is currently being written.

## Contact and contribution

For questions about this repository, please contact Utrecht University's <a href = ""https://www.uu.nl/en/research/research-data-management/contact-us"" target = ""_blank"">Research Data Management Support</a>, or open an Issue or Pull request in this repository.

## License and citation

This repository is licensed under a GPL 3.0 license. You can view the <a href= ""https://github.com/UtrechtUniversity/dataprivacysurvey/blob/main/LICENSE"" target = ""_blank"">license text here</a>.

Citation will be made possible once the report is published in Zenodo or a similar archive.",2022-08-12
https://github.com/UtrechtUniversity/datascienceday-2018,"# Data Handling in R and Python

Jupyter notebooks and datasets for the Utrecht University Data Science Day workshop Data Handling.
",2022-08-12
https://github.com/UtrechtUniversity/datascienceday-hpc,"# Data Science Day 2018 Workshop - HPC: Speed up your data science problems!

*Authors: [Roel Brouwer](https://www.uu.nl/medewerkers/RJJBrouwer) and [Kees van Eijden](https://www.uu.nl/medewerkers/CJvanEijden/0) and [Jonathan de Bruin](https://www.uu.nl/medewerkers/JdeBruin1)*

Is your desktop occupied for a couple of days because of a long-running data
science problem? Or doesn’t your program even start because your computer does
not have enough memory? It’s time to run your problem on a computer cluster! A
computer cluster consists of many interconnected computers. The trick to speed
up your computations is to use these computers in parallel.

In this hands-on Breakout Session, we show you how to split up a data science
problem and run it on several computers at the same time. You will learn how
to access the cluster, upload files and schedule programs. We will provide you
with an account to a cluster. You will be working on your laptop. We strongly
advise Windows users to install MobaXterm and Mac users to install Cyberduck
in advance.

## Manual

- [Mac/Linux](/manual_mac.md)
- [Windows](/manual_windows.md)

## Material 

- [Github](https://github.com/UtrechtUniversity/datascienceday-hpc)
- [Data](/data/)
- [Presentation](/presentation/)
- [License: CC BY 4.0](http://creativecommons.org/licenses/by/4.0/)
",2022-08-12
https://github.com/UtrechtUniversity/davrods,"Davrods - An Apache WebDAV interface to iRODS
=============================================

Davrods provides access to iRODS servers using the WebDAV protocol.
It is a bridge between the WebDAV protocol and the iRODS API,
implemented as an Apache HTTPD module.

Davrods leverages the Apache server implementation of the WebDAV
protocol, `mod_dav`, for compliance with the WebDAV Class 2 standard.

Notable features include:

- WebDAV Class 2 support, with locks local to the Apache server
- Connects to iRODS server versions 4+
- PAM and Native (a.k.a. STANDARD) iRODS authentication
- Optional negotiated SSL encryption for the entire iRODS connection
- Configurable using Apache configuration directives
- Optional anonymous access mode for password-less public access
- Themeable directory listings for read-only web browser access.
- Partial file up- and downloads and resumes (HTTP byte-ranges)
- iRODS ticket-based access

Themeable listings and anonymous access were inspired by Simon Tyrrell's
[work](https://github.com/billyfish/eirods-dav) at Earlham Institute.

## Download ##

Please choose the right version for your platform:

1. If you run Davrods on the same server as your iRODS service, you need a
   Davrods version built against the same version iRODS *runtime*.
2. If you run Davrods separately, on its own server, then the iRODS runtime
   version does not matter - just pick the newest Davrods you can get.
   All Davrods packages below should be compatible with any iRODS 4.x
   server version.

| Date        | Davrods ver. | iRODS runtime ver. | Packages                                                                          | Remarks
| ----------- | ------------ | ------------------ | --------------------------------------------------------------------------------- | ---------------- |
| 2021-12-20  | 1.5.0        | 4.2.11             | [RPM, DEB](https://github.com/UtrechtUniversity/davrods/releases/tag/4.2.11_1.5.0)| Pre-release      |
| 2021-07-28  | 1.5.0        | 4.2.10             | [RPM, DEB](https://github.com/UtrechtUniversity/davrods/releases/tag/4.2.10_1.5.0)|                  |
| 2021-06-16  | 1.5.0        | 4.2.9              | [RPM, DEB](https://github.com/UtrechtUniversity/davrods/releases/tag/4.2.9_1.5.0) |                  |
| 2020-06-02  | 1.5.0        | 4.2.8              | [RPM, DEB](https://github.com/UtrechtUniversity/davrods/releases/tag/4.2.8_1.5.0) |                  |
| 2019-12-20  | 1.4.2        | 4.2.7              | [RPM, DEB](https://github.com/UtrechtUniversity/davrods/releases/tag/4.2.7_1.4.2) |                  |
| 2019-06-20  | 1.4.2        | 4.2.6              | [RPM, DEB](https://github.com/UtrechtUniversity/davrods/releases/tag/4.2.6_1.4.2) |                  |
| 2019-04-03  | 1.4.2        | 4.2.5              | [RPM, DEB](https://github.com/UtrechtUniversity/davrods/releases/tag/4.2.5_1.4.2) |                  |
| 2018-12-17  | 1.4.2        | 4.2.4              | [RPM, DEB](https://github.com/UtrechtUniversity/davrods/releases/tag/4.2.4_1.4.2) |                  |
| 2018-07-15  | 1.4.1        | 4.2.3              | [RPM, DEB](https://github.com/UtrechtUniversity/davrods/releases/tag/4.2.3_1.4.1) |                  |
| 2018-07-15  | 1.4.1        | 4.1.x              | [RPM](https://github.com/UtrechtUniversity/davrods/releases/tag/4.1_1.4.1)        |                  |

(older versions are still available on the
[releases page](https://github.com/UtrechtUniversity/davrods/releases))

If you require a certain Davrods/iRODS runtime version combination that
is not listed above, you can most likely still build it yourself (see
""Building from source"").

A log describing which features were added and which bugs were fixed in
each version can be found in [changelog.txt](changelog.txt).

We currently distribute RPM packages for CentOS 7 & RHEL systems and
DEB packages for Debian & Ubuntu systems.
We test our packages on CentOS 7 and (as of Davrods 1.4.0) Ubuntu 16.04.

## Installation ##

This section describes the installation steps for iRODS 4.2+ based
Davrods releases.

To view instructions for iRODS 4.1-based Davrods releases, switch to the
[`irods-4.1-libs`](https://github.com/UtrechtUniversity/davrods/tree/irods-4.1-libs)
branch.

### Using the binary distribution ###

Davrods depends on certain iRODS packages, which as of iRODS 4.2 are
distributed at https://packages.irods.org/

After following the instructions for adding the iRODS repository to your
package manager at the link above, Davrods can be installed as a binary
package using the RPM or DEB file from the
[releases page](https://github.com/UtrechtUniversity/davrods/releases)
(use the table near the top of this README to select the right version).

Download the Davrods package for your platform and install it using your
package manager, for example:

    yum install davrods-4.2.8_1.5.0-1.rpm
    --or--
    apt install davrods-4.2.8_1.5.0.deb

Now see the __Configuration__ section for instructions on how to
configure Davrods once it has been installed.

### Davrods and SELinux ##

If the machine on which you install Davrods is protected by SELinux,
you may need to make changes to your policies to allow Davrods to run:

- Apache HTTPD must be allowed to connect to TCP port 1247

For example, the following command can be used to resolve this
requirement:

    setsebool -P httpd_can_network_connect true

## Configuration ##

Davrods is configured in two locations: In a HTTPD vhost configuration
file and in an iRODS environment file. The vhost config is the main
configuration file, the iRODS environment file is used for iRODS
client library configuration, similar to the configuration of
icommands.

### HTTPD vhost configuration ###

The Davrods RPM distribution installs two vhost template files:

1. `/etc/httpd/conf.d/davrods-vhost.conf`
2. `/etc/httpd/conf.d/davrods-anonymous-vhost.conf`

(for Ubuntu, replace `/etc/httpd/conf.d` with
`/etc/apache2/sites-available`)

These files are provided completely commented out. To enable either
configuration, simply remove the first column of `#` signs, and then
tune the settings to your needs.

Note that on Ubuntu, you will additionally need to enable the Davrods
module and vhosts, like so:

    a2enmod dav
    a2enmod davrods
    a2ensite davrods-vhost
    a2ensite davrods-anonymous-vhost

The normal vhost configuration (1) provides sane defaults for
authenticated access.

The anonymous vhost configuration (2) allows password-less public
access using the `anonymous` iRODS account.

You can enable both configurations simultaneously, as long as their
ServerName values are unique (for example, you might use
`dav.example.com` for authenticated access and
`public.dav.example.com` for anonymous access).

### The iRODS environment file ###

The binary distribution installs the `irods_environment.json` file in
`/etc/httpd/irods`. In most iRODS setups, this file can be used as
is.

Importantly, options such as `irods_host` and `irods_zone_name` are
**not** read from this file (and are omitted for that reason).
These settings are taken from their equivalent Davrods configuration
directives in the vhost config file instead.

Options in `irods_environment.json` that are known to affect Davrods
behavior are the negotiation, ssl and encryption settings.

See the official documentation for more information on these settings:
https://docs.irods.org/4.2.7/system_overview/configuration/#irodsirods_environmentjson

### Ticket-based access ###

Ticket access is disabled by default, and requires special configuration
to enable, depending on your use case. Please see
[README.advanced.md](./README.advanced.md) for more information.

## Building from source ##

This repository includes a Vagrant configuration for building Davrods from source
on either CentOS 7 (for the RPM package) or Ubuntu 18.04 LTS (for the DEB package).
It can be found in `vagrant/build`. In order to build a package using Vagrant, edit
the .env file in the Vagrant build directory. Adjust the BOXNAME and IRODS_VERSION vars
as needed. Then run `vagrant up` to provision the VM. The VM has all dependencies
pre-installed, as well as a clone of the Davrods repository. Log in on the VM
using `vagrant ssh`, then generate the build directory and create the package (see below).

To build from source without using the Vagrant configuration, the following build-time
dependencies must be installed (package names may differ on your platform):

- `cmake`
- `make`
- `gcc`
- `httpd-devel >= 2.4`
- `irods-devel >= 4.2.0`
- `openssl-devel`
- `irods-runtime >= 4.2.0`
- `rpmdevtools` (if you are creating an RPM)

Additionally, the following runtime dependencies must be installed:

- `irods-runtime >= 4.2.0`
- `openssl-libs`
- `httpd >= 2.4`

Follow these instructions to build from source:

- First, browse to the directory where you have unpacked the Davrods
  source distribution.

- Check whether your umask is set to a sane value. If the output of
  `umask` is not `0022`, run `umask 0022` to fix it. This is important
  for avoiding conflicts in created packages later on.

- Create and generate a build directory.

```bash
mkdir build
cd build
cmake ..
```

- Compile the project

```bash
make
```

Now you can either build an RPM/DEB or install the project without a
package manager.

**To create a package:**

```
make package
```

That's it, you should now have an RPM or DEB in your build directory
which you can install using yum or apt.

**To install without a package manager on CentOS:**

Run the following as user root:

```
make install
chown apache:apache /var/lib/davrods
chmod 700 /var/lib/davrods
```

**To install without a package manager on Debian:**

Run the following as user root:

```
make install
chown www-data:www-data /var/lib/davrods
chmod 700 /var/lib/davrods
```

**To install without a package manager on other distros:**

Linux distributions other than RHEL, Debian and their derivatives may
have different HTTPD configuration and directory layouts, which are not
currently supported by the build system.
For this reason you will need to install the files manually on such
Linux distributions:

- Copy `mod_davrods.so` to your Apache module directory.
- Copy `davrods.conf` to your Apache module configuration/load directory.
- Copy `davrods-vhost.conf` and `davrods-anonymous-vhost.conf` to your
  Apache vhost configuration directory.
- Create an `irods` directory in a location where Apache HTTPD has read
  access.
- Copy `irods_environment.json` to the `irods` directory.
- Create directory `/var/lib/davrods`, and give apache exclusive access
  to it.

## Bugs and feature requests ##

Please report any issues you encounter on the
[issues page](https://github.com/UtrechtUniversity/davrods/issues).

## Contact information ##

For questions or support, please contact the Yoda team at yoda@uu.nl.

## Authors ##

- [Chris Smeele](https://github.com/cjsmeele)

## License ##

Copyright (c) 2016 - 2021, Utrecht University.

Davrods is licensed under the GNU Lesser General Public License version
3 or higher (LGPLv3+). See the COPYING.LESSER file for details.

The `lock_local.c` and `byterange.c` files were adapted from components
of Apache HTTPD, and are used with permission granted by the Apache
License. See the copyright and license notices in these files for
details.
",2022-08-12
https://github.com/UtrechtUniversity/DCC2020-Yoda-ResearchCloud,"# DCC2020-Yoda-ResearchCloud
(for English see below)

## Dutch:
Deze repository vormt een presentatie van en index naar de resultaten
van het project ""Veilig verbinden van een virtuele werkomgeving met een
datamanagementomgeving: de verbinding tussen Research Cloud
en iRODS en Yoda"". 
Dit project is in 2021 uitgevoerd door Universiteit Utrecht.

Open [project index](https://utrechtuniversity.github.io/DCC2020-Yoda-ResearchCloud/) 
voor verdere informatie.

## English:
This repository is a presentation of and index to the output of a project
""Safely interconnect cloud research data analysis workspaces with
data management services: connecting SURF Research Cloud with Yoda"". 
This project is realized in 2021 by Utrecht University.

Please navigate to our 
[project index](https://utrechtuniversity.github.io/DCC2020-Yoda-ResearchCloud/)
for further information. 
While the ruling language for the project has been Dutch, some
documentation has been prepared in English,
often to facilitate discussions with international staff.
 

",2022-08-12
https://github.com/UtrechtUniversity/DCC2021-FAIRYODA4Communities,"# FAIR YODA 4 Communities
Project Repository for DCC2021 funded project FAIR Yoda for Communities (FY4C)
",2022-08-12
https://github.com/UtrechtUniversity/dokuwiki-ansible,"# dokuwiki-ansible
Ansible-module for Dokuwiki

This is an Ansible playbook for configuring Dokuwiki on a CentOS 7 server.

It is partially based on the [EPOS-MSL playbook](https://github.com/UtrechtUniversity/epos-msl).

## Testing locally

The repository contains a Vagrantfile to deploy a local VM running Dokuwiki. It requires Vagrant 2.x,
as well as a VirtualBox version that is compatible with your Vagrant version. Deploy the VM with
Vagrant: _vagrant up_

## Adding accounts and changing admin credentials

When deploying the Dokuwiki server, you'll want to change the default admin credentials (admin/admin)

In order to do this, define the dokuwiki\_users variable. This file supports bcrypt hashed passwords. You can
calculate bcrypt hashes using Python 3. For example:

```
pip3 install bcrypt
python3 -c 'import bcrypt; print(bcrypt.hashpw(b""mypassword"", bcrypt.gensalt(rounds=15)).decode(""ascii""))' | sed 's/\$2b/\$2a/'
```

The dokuwiki\_users variable should contain a list of user names, passwords, and groups, like so:

```
dokuwiki_users:
  admin:
    passwordhash: $2a$15$tavJPYxA3TkWow9biRifBuOzqLEOsSxSpswgejPATuYRawcRTluZO
    fullname: Admin User
    email: example@example.org
    groups: users
```
",2022-08-12
https://github.com/UtrechtUniversity/emissions,"# Emissions in The Netherlands in 2030

Emissions of CO2 and NOx play a key role in global warming. In this project,
we aim to predict the CO2 emission and NOx emission in The Netherlands in
2030. This project is one of the demonstration projects of the workshop
[Computational Reproducibility](https://github.com/UtrechtUniversity/workshop-computational-reproducibility) at [Utrecht
University](https://www.uu.nl). The workshop is organised by [Research Data
Management Support](https://www.uu.nl/en/research/research-data-management).


## Getting Started

This section contains instructions on running the analyses for this project. 

### Prerequisites

The code in this repository is written in R. If you do not have R installed,
download it from the [R project website](https://www.r-project.org/) and follow the
installation instructions. RStudio can be used as an interface for intuitive and fast
development. Check out the [Rstudio website](https://www.rstudio.com/) to
download and install the program.

Using RStudio:

- Open `install.R`
- Run the script (CTRL+SHIFT+ENTER)

Using the command line:

```
Rscript install.R
```

### Data

The data [data/Emissions_to_air__ro_190918144657.csv](data/Emissions_to_air__ro_190918144657.csv) 
was downloaded on September 19, 2018 from the Dutch Bureau of Statistics at [statline.cbs.nl](http://statline.cbs.nl/Statweb/selection/VW=T&DM=SLEN&PA=7063ENG&D1=a&D2=a&D3=a&LA=EN&HDR=T&STB=G1,G2). In 'topics', we selected greenhouse
gases only (CO2, N2O, and CH4), but selected all sources and periods
(1980-2017). The data was downloaded as a csv.

**Note** that Statline has been renewed and the above link may no longer function. In
this case, visit [opendata.cbs.nl](https://opendata.cbs.nl/statline/#/CBS/en/dataset/7063eng/table?ts=1537862508225) and perform the described selection. We
recommend comparing the header and footer of the resulting document with the
input csv file in the data folder, to ensure they match.


### Run the analysis


- Run `scripts/data_cleaning.R`
- Run `scripts/analysis.R`

The results, datasets and figures are stored in the `/output` folder.


## Running the tests

This project contains no tests at this moment.

## Contributing

Any remarks and contributions can be proposed in the issue tracker
https://github.com/UtrechtUniversity/emissions/issues. 

## Versioning

This project makes use of versions control through Git and is hosted on
Github. For the versions available, see the [tags on this repository](https://github.com/UtrechtUniversity/emissions/tags).

## Citation 

The content of this repository is open source. If you use this project for one of
your research articles, please cite our work:

```
@misc{foo2010,
  author = ""Vreede, B and De Bruin, J"",
  journal = ""N/A"",
  year = 2019,
  title = ""N/A"",
  doi = {10.1.1.not.really.a.doi},
  url = {http://dx.doi.org/10.1.1.not.really.a.doi}
}
```

## Authors

* **Barbara Vreede** - [bvreede](https://github.com/bvreede)
* **Jonathan de Bruin** - [J535D165](https://github.com/J535D165)

See also the list of
[contributors](https://github.com/your/project/contributors) who participated
in this project.

## License

This project is licensed under the MIT - see the [LICENSE.md](LICENSE.md) file for details

## Acknowledgments

* [Utrecht University - Research Data Management Support](https://www.uu.nl/en/research/research-data-management)
* [Big list of people and their links](http://example.org/)

## Disclaimer

This project is one of the demonstration projects of the workshop
[Computational Reproducibility](https://github.com/UtrechtUniversity/workshop-computational-reproducibility) at [Utrecht
University](https://www.uu.nl). The workshop is organised by [Research Data
Management Support](https://www.uu.nl/en/research/research-data-management).
The content of the project is for illustration purposes only. 

Research projects and computational reproducible research can be debated. This
project provides one of the solutions to setting up, managing and publishing a
research project. We know that there is much more to share on this topic.
Unfortunately, it is not possible to cover everything. Nevertheless, we
appreciate all feedback on this demonstration project and the workshop.












",2022-08-12
https://github.com/UtrechtUniversity/energize,"# energize
Reliable data transfer to and from iRODS

# Use cases

Energize is an abstraction layer that implements reliable data transfer
to and from iRODS. The main use cases are:
- Working around software bugs, for example by partitioning
  transfers of large number of files in multiple parts to limit the
  impact of memory leaks.
- Restarting transfers after network issues and keeping track of what
  data has been transferred previously.
- Facilitating transfer of data in batches, for example if a researcher
  intends to upload data from a laptop at night, and temporarily halt
  the transfer during the daytime.
- Generating reports of data transfers for provenance tracking
",2022-08-12
https://github.com/UtrechtUniversity/epos-msl,"# epos-msl
[Ansible](https://docs.ansible.com) scripts for automatic deployment of EPOS-MSL.

## Requirements
### Control machine requirements
* [Ansible](https://docs.ansible.com/ansible/intro_installation.html) (>= 2.9)
* [VirtualBox](https://www.virtualbox.org/manual/ch02.html) (>= 5.1)
* [Vagrant](https://www.vagrantup.com/docs/installation/) (2.x)

### Managed node requirements
* [Ubuntu](https://www.ubuntu.com/) 20.04 LTS

## Deploying EPOS-MSL development instance

Configure the virtual machine for development:
```bash
vagrant up
```

On a Windows host first SSH into the Ansible controller virtual machine (skip this step on GNU/Linux or macOS):
```bash
vagrant ssh epos-msl-controller
cd ~/epos-msl
```

Deploy EPOS-MSL to development virtual machine:
```bash
ansible-playbook playbook.yml
```

Add following host to /etc/hosts (GNU/Linux or macOS) or %SystemRoot%\System32\drivers\etc\hosts (Windows):
```
192.168.60.10 epos-msl.ckan.test
```

## Upgrading EPOS-MSL instance
Upgrading the EPOS-MSL development instance to the latest version can be done by running the Ansible playbooks again.

On a Windows host first SSH into the Ansible controller virtual machine (skip this step on GNU/Linux or macOS):
```bash
vagrant ssh controller
cd ~/epos-msl
```

Upgrade Ansible scripts:
```bash
git pull
```

Upgrade EPOS-MSL instance:
```bash
ansible-playbook playbook.yml
```

## Database creation/seeding for the MSL API

You currently need to manually trigger creation and seeding of the MSL API database, as well as linking its storage
space.

Run the following commands in /var/www/msl\_api after deploying the application using Ansible:

```bash
/usr/bin/php8.0 artisan migrate
/usr/bin/php8.0 artisan db:seed
/usr/bin/php8.0 artisan storage:link
```

# Configuration

The main configuration settings are:

|Settingi       | Meaning       |
| ------------- |:-------------:|
|epos_msl_fqdn             | fully qualified domain name (FQDN) of the catalog, e.g. epos-catalog.mydomain.nl |
|ckan_database_password    | password for the CKAN database (can be randomly generated with e.g. pwgen -n 16) |
|ckan_admin_password       | password for the ckanadmin account (can be randomly generated with e.g. pwgen -n 16) |
|msl_api_database_password | password for the MSL API database (can be randomly generated with e.g. pwgen -n 16) |
|msl_api_app_url           | application URL for the MSL API web service, e.g. https://epos-catalog.mydomain.nl/webservice |
|msl_api_asset_url         | asset URL for the MSL API web service, e.g. https://epos-catalog.mydomain.nl/webservice |
|ckan_api_token            | the MSL API uses this value to authenticate to the CKAN API. this should currently be the API key (not API token!) of the ckanadmin account. The current way to use this field is: deploy the catalog using a dummy value for this parameter, log in on CKAN using the ckanadmin account, generate an API key, replace the dummy value in the host\_vars file with the real API key, and run the playbook a second time.
|msl_api_app_key           | the MSL API application key. The current way to configure this is to deploy the application, generate the app key by running `/usr/bin/php8.0 artisan key:generate` in /var/www/msl\_api. Finally copy the generated key in /var/www/msl\_api/.env to the host\_vars file.

# CKAN catalog

EPOS-MSL is based on [CKAN](https://www.ckan.org). It uses several modules / extensions to customize CKAN for the EPOS catalog.

## MSL CKAN extension

The [MSL CKAN core extension](https://github.com/UtrechtUniversity/msl_ckan_core) contains specific settings and configuration
for the EPOS MSL catalog.

## MSL CKAN Util extension

The [MSL CKAN util extension](https://github.com/UtrechtUniversity/msl_ckan_util) contains functionality used in the EPOS catalog
that can be reused in other catalogs, specifically custom facets and repeating fields.

## License
This project is licensed under the GPL-v3 license.
The full license can be found in [LICENSE](LICENSE).
",2022-08-12
https://github.com/UtrechtUniversity/epos-msl-theme,".. You should enable this project on travis-ci.org and coveralls.io to make
   these badges work. The necessary Travis and Coverage config files have been
   generated for you.

.. image:: https://travis-ci.org/jcid/ckanext-custom_theme.svg?branch=master
    :target: https://travis-ci.org/jcid/ckanext-custom_theme

.. image:: https://coveralls.io/repos/jcid/ckanext-custom_theme/badge.svg
  :target: https://coveralls.io/r/jcid/ckanext-custom_theme

.. image:: https://pypip.in/download/ckanext-custom_theme/badge.svg
    :target: https://pypi.python.org/pypi//ckanext-custom_theme/
    :alt: Downloads

.. image:: https://pypip.in/version/ckanext-custom_theme/badge.svg
    :target: https://pypi.python.org/pypi/ckanext-custom_theme/
    :alt: Latest Version

.. image:: https://pypip.in/py_versions/ckanext-custom_theme/badge.svg
    :target: https://pypi.python.org/pypi/ckanext-custom_theme/
    :alt: Supported Python versions

.. image:: https://pypip.in/status/ckanext-custom_theme/badge.svg
    :target: https://pypi.python.org/pypi/ckanext-custom_theme/
    :alt: Development Status

.. image:: https://pypip.in/license/ckanext-custom_theme/badge.svg
    :target: https://pypi.python.org/pypi/ckanext-custom_theme/
    :alt: License

=============
ckanext-custom_theme
=============

.. Put a description of your extension here:
   What does it do? What features does it have?
   Consider including some screenshots or embedding a video!


------------
Requirements
------------

For example, you might want to mention here which versions of CKAN this
extension works with.


------------
Installation
------------

.. Add any additional install steps to the list below.
   For example installing any non-Python dependencies or adding any required
   config settings.

To install ckanext-custom_theme:

1. Activate your CKAN virtual environment, for example::

     . /usr/lib/ckan/default/bin/activate

2. Install the ckanext-custom_theme Python package into your virtual environment::

     pip install ckanext-custom_theme

3. Add ``custom_theme`` to the ``ckan.plugins`` setting in your CKAN
   config file (by default the config file is located at
   ``/etc/ckan/default/production.ini``).

4. Restart CKAN. For example if you've deployed CKAN with Apache on Ubuntu::

     sudo service apache2 reload


---------------
Config Settings
---------------

Document any optional config settings here. For example::

    # The minimum number of hours to wait before re-checking a resource
    # (optional, default: 24).
    ckanext.custom_theme.some_setting = some_default_value


------------------------
Development Installation
------------------------

To install ckanext-custom_theme for development, activate your CKAN virtualenv and
do::

    git clone https://vcs.jcid.nl/open-data/ckan-universiteit-utrecht-epos-theme.git
    cd ckanext-custom_theme
    python setup.py develop
    pip install -r dev-requirements.txt


-----------------
Running the Tests
-----------------

To run the tests, do::

    nosetests --nologcapture --with-pylons=test.ini

To run the tests and produce a coverage report, first make sure you have
coverage installed in your virtualenv (``pip install coverage``) then run::

    nosetests --nologcapture --with-pylons=test.ini --with-coverage --cover-package=ckanext.custom_theme --cover-inclusive --cover-erase --cover-tests


---------------------------------
Registering ckanext-custom_theme on PyPI
---------------------------------

ckanext-custom_theme should be availabe on PyPI as
https://pypi.python.org/pypi/ckanext-custom_theme. If that link doesn't work, then
you can register the project on PyPI for the first time by following these
steps:

1. Create a source distribution of the project::

     python setup.py sdist

2. Register the project::

     python setup.py register

3. Upload the source distribution to PyPI::

     python setup.py sdist upload

4. Tag the first release of the project on GitHub with the version number from
   the ``setup.py`` file. For example if the version number in ``setup.py`` is
   0.0.1 then do::

       git tag 0.0.1
       git push --tags


----------------------------------------
Releasing a New Version of ckanext-custom_theme
----------------------------------------

ckanext-custom_theme is availabe on PyPI as https://pypi.python.org/pypi/ckanext-custom_theme.
To publish a new version to PyPI follow these steps:

1. Update the version number in the ``setup.py`` file.
   See `PEP 440 <http://legacy.python.org/dev/peps/pep-0440/#public-version-identifiers>`_
   for how to choose version numbers.

2. Create a source distribution of the new version::

     python setup.py sdist

3. Upload the source distribution to PyPI::

     python setup.py sdist upload

4. Tag the new release of the project on GitHub with the version number from
   the ``setup.py`` file. For example if the version number in ``setup.py`` is
   0.0.2 then do::

       git tag 0.0.2
       git push --tags
",2022-08-12
https://github.com/UtrechtUniversity/getOldTweets-Airflow,"# Collect historical tweets with Airflow

This repository contains an [Airflow](https://airflow.apache.org/) workflow
for the collection of historical tweets based on a search query. The method
combines the package [getOldTweets3](https://github.com/Mottl/GetOldTweets3)
with the [Twitter status lookup API](https://developer.twitter.com/en/docs/tweets/post-and-engage/api-reference/get-statuses-lookup.html).
Airflow is used to programmatically author, schedule and monitor the 
collection process.

The [Twitter Standard Search API](https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html)
searches against a sampling of recent Tweets published in the past 7 days.
Therefore, this API is not very useful to collect historical tweets.
The package getOldTweets3 is a webscraping package for Twitter that was 
developed to solve this problem. The package can ce used to collect
variables like ""id"", ""permalink"", ""username"", ""to"",
""text"", ""date"" in UTC, ""retweets"", ""favorites"", ""mentions"", ""hashtags"" and
""geo"". Unfortunately, not all relevant variables are returned and data can be
a bit messy (broken urls). To collect the full set of variables, we can use 
the [Twitter status lookup API](https://developer.twitter.com/en/docs/tweets/post-and-engage/api-reference/get-statuses-lookup.html).
This API is less restrictive compared to the [Twitter Standard Search API](https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html), but requires a list of status identifiers as input. 
These identifiers are collected with getOldTweets and passed to the lookup API.
The result is a complete set of information on every tweet collected. 

![DAG Twitter](img/dag.png)

The workflow contains steps (Operators in Airflow). The first step is the
collection of tweets with getOldTweets3 `get_old_tweets_*`. The result of
getOldTweets is not always complete. Therefore, we run this step 3 times (See
DAG file to change the number of runs.) The next step, `merge_get_old_tweets`
, is used to find the unique status identifiers of the 3 runs. In the
`lookkup_tweets`, each status identifier is passed to Twitter status lookup
API, and results are stored in the folder `lookup/`. In the last step, the
completeness of the lookup process is evaluated. The `validate_get_old_tweets`
task compares the identifiers with the identifiers collected in the
getOldTweets runs and reports the completeness.

## Installation and preparation

This project runs on Python 3.6+ and depends on tools like `Airflow`, `tweepy`
and `getOldTweets3`. Install all the dependencies from the `requirements.txt`
file.

```
pip install -r requirements.txt
```

Create a json file with your Twitter credentials (e.g.
`~/Credentials/twitter_cred.json`). Read more about Twitter access tokens on
the [Twitter developer documentation](https://developer.twitter.com/en/docs/basics/authentication/guides/access-tokens.html).

```
{
    ""consumer_key"":""XXXXXXXXXXXXXXXXXXXXXXXXX"",
    ""consumer_secret"":""XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"",
    ""access_token"":""00000000-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"",
    ""access_token_secret"":""XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX""
}
```

Initialise and start Airflow. Please read the documentation of Airflow if you
are not familiar with setting up Airflow.

```
export AIRFLOW_HOME=/PATH/TO/YOUR/PROJECT/getOldTweets_airflow

# initialize the database
airflow initdb

# start the web server, default port is 8080
airflow webserver -p 8080

# start the scheduler
airflow scheduler
```

Open another terminal and add the Twitter credentials to the enviroment
variables.

```
export TWITTER_CREDENTIALS=~/Credentials/twitter_cred.json
```

## Usage 

Airflow can be used to schedule jobs, but also do a backfill operation. This
backfill operation is very useful for collecting historical tweets. By
default, pipelines are split into montly intervals.

Edit the search query in the file `dag/dag_tweet_search.py`. Adjust the query
by adjusting `QUERY_SEARCH` and/or `QUERY_LANG`.  It is recommended to save
the file with another file name and `dag_id`.

The following backfill operation collects all tweets from 2018. The results
are stored in 12 different files, one for each month.

```
airflow backfill tweet_collector -s 2018-01-01 -e 2018-12-31
```

Navigate to http://localhost:8080/ to monitor the collection process. 

![Tree example](img/airflow_tree.png)

The format of this query is: `airflow backfill dag_id -s start_date -e end_date`

Results can be found in the output folder.

## License

[BSD-3](/LICENSE)

## Contact 

This project is a project by Parisa Zahedi (p.zahedi@uu.nl, [@parisa-zahedi](https://github.com/parisa-zahedi))
and Jonathan de Bruin (j.debruin1@uu.nl, [@J535D165](https://github.com/J535D165)).
",2022-08-12
https://github.com/UtrechtUniversity/getting-started,"![banner.jpg](images/banner.jpg)

# Getting started

Welcome to the getting started with [Utrecht University GitHub organization page](https://github.com/UtrechtUniversity). A GitHub organization is a shared account where members can manage their projects using their personal GitHub account. The Utrecht University GitHub organization is meant for storing, managing and publishing research projects. Below you will find instructions for connecting to the organization and general usage instructions for the organization. See the bottom of the page for resources to get started with using Git for version control.

For best practices, see [Best Practices for Git @UtrechtUniversity](https://github.com/UtrechtUniversity/best-practices).

## First time connecting to the GitHub organization

Membership of the Utrecht University GitHub organization, including access to internal repositories, is restricted to employees of Utrecht University (UU). Become a member using your existing personal GitHub account; if you do not have an account yet, please create one. During this process, you will have to authenticate for this GitHub organization using your Solis-id. Your Solis-id is merely used to verify your employee status; all commits and actions performed in GitHub will be attributed to your personal GitHub account. By using one GitHub account, your GitHub profile page (which can be used as your curriculum vitae) will not only show your contributions in repositories belonging to this GitHub organization, but contributions in different GitHub organizations or personal repositories as well.

### Quick start

1. [Create a personal GitHub account](https://github.com/join) (not necessary if you already have a GitHub account)
2. Login to your personal GitHub account
3. [Configure two-factor authentication for your GitHub account](https://docs.github.com/en/authentication/securing-your-account-with-two-factor-authentication-2fa/configuring-two-factor-authentication)
4. **[Connect to the organization](https://github.com/orgs/UtrechtUniversity/sso) (one-time only)**
5. Authenticate using your Solis-id. This will make you a member of the organization and grants you permission to create repositories and teams.
6. Go to [https://github.com/UtrechtUniversity](https://github.com/UtrechtUniversity) and start working!

### Command line and RStudio access
To get initial access to the organization, follow the steps above.

> :warning: After turning on two-factor authentication for your personal GitHub account, you cannot connect to your remote repositories using HTTPS URLs in combination with your GitHub password anymore. You will need to configure an 'SSH key' or a 'Personal Access Token' instead. This is a one-time action. After that your interaction with GitHub will be as before.

Use an SSH key or a Personal Access Token to access your resources from the command line or from RStudio, see [Using two-factor authentication with the command line](https://docs.github.com/en/enterprise-server@3.0/authentication/securing-your-account-with-two-factor-authentication-2fa/accessing-github-using-two-factor-authentication#using-two-factor-authentication-with-the-command-line). These keys have to be authorized to be used for [the GitHub organization UtrechtUniversity](https://github.com/UtrechtUniversity). See the following instructions:

- SSH access  
  [Creating an SSH key](https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh)  
  [Authorizing SSH key for usage in the UU organization](https://docs.github.com/en/github/authenticating-to-github/authorizing-an-ssh-key-for-use-with-saml-single-sign-on)
- PAT access  
  [Authorizing Personal Access Token for usage in the UU organization](https://docs.github.com/en/github/authenticating-to-github/authorizing-a-personal-access-token-for-use-with-saml-single-sign-on)  
  [Personal Access Token for RStudio and GitHub](https://happygitwithr.com/https-pat.html)

## Usage

### Platform for research and UU projects 
Use this GitHub organization for research projects or other UU-related work. Do not use this organization for exercises or personal activities.

### Code and non-sensitive data
This GitHub organization is meant for managing source code. Using this GitHub organization is not recommended for storing datasets with personal or sensitive data (feel free to [contact Research Data Management Support](mailto:info.rdm@uu.nl) for advice). However, it may be used for version control of documents or small-sized non-sensitive datasets. For managing research data, make sure that you create a data management plan first and check the [Storage finder](https://www.uu.nl/en/research/research-data-management/tools-services/tools-for-storing-and-managing-data/storage-solutions) and [Tooladvisor](https://tools.uu.nl/tooladvisor/) for appropriate solutions. 

### Work responsibly
Be aware of the general university's [user regulations](https://intranet.uu.nl/en/security/information-security-policy-and-regulations) for ICT platforms, and [security best practices for GitHub](./docs/security-best-practices.md).
If you are a Git novice, make sure to learn the [basics of Git version control](#learning-git) first in order to manage your projects properly. Typical dangers include **publishing data that should not be published or publishing passwords**. Make sure you learn how to tell Git which files should and should not be tracked, especially if you work with any kind of sensitive data. Finally, be aware that if you (or any of your collaborators) have authorized any third-party applications, these applications may be able to view data in your private repositories. 
- Read the [security best practices](./docs/security-best-practices.md) for more tips.    
- The security (suitability) level for this GitHub organization can be found in the Service Description on Intranet.  
- More about GitHub and GDPR compliance: [GitHub Data Protection Agreement](https://docs.github.com/en/github/site-policy/github-data-protection-agreement#attachment3) and [GitHub Privacy Statement](https://docs.github.com/en/github/site-policy/github-privacy-statement).

### GitHub Pages
You can use GitHub Pages to promote research projects, for example, by publishing a project website. For design purposes, it is OK to use a UU logo. However, do not use any other design formats that relate to the university's corporate website ([uu.nl](https://www.uu.nl))

### Repository naming conventions
Use repository names that are descriptive for the project. Don't use names that are in some way ambiguous, especially when they relate to law or policy (e.g. repository names containing ""policy"" or ""terms""). We reserve the right to rename repositories with ambiguous names. When in doubt, [contact us](https://github.com/UtrechtUniversity/getting-started#contact).

### Licensing and Copyrighted materials
Be sure to add a license to your work, since the license defines the rules for people who want to use your code/software. If you reuse licensed software, make sure that the license you choose is compatible with the license of the reused software. [Choosing a license](https://choosealicense.com/).

- Contact [Research Data Management Support](mailto:info.rdm@uu.nl) for questions regarding code and software licensing.
- Contact the [Copyright Information Office](https://www.uu.nl/en/organisation/copyright-information-office) if you have other questions regarding working with copyrighted contents.

## Resources

### Creating repositories
As soon as you have authenticated with your Solis-id using the steps outlined above, you are granted permission to create repositories in the [Utrecht University GitHub organization](https://github.com/UtrechtUniversity). View [GitHub documentation](https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-new-repository) for instructions on how to create a repository.

### Creating teams
Create teams to organize your team or project repositories. As soon as you have authenticated with your Solis-id using the steps outlined above, you will be able to create teams in the [Utrecht University GitHub organization](https://github.com/UtrechtUniversity). View [GitHub documentation](https://docs.github.com/en/organizations/organizing-members-into-teams) for instructions on how to create a team. Invite your team as a collaborator on a repository to make the repository appear on the Team overview page. 

### Inviting colleagues
When you create a repository or team, you will automatically have permission to invite collaborators. When you [invite](https://docs.github.com/en/organizations/organizing-members-into-teams/adding-organization-members-to-a-team) a colleague to a team, they will automatically receive an invitation to join the Utrecht University GitHub organization. When you want to invite a colleague to a repository without using GitHub teams, the colleague should first become a member of the GitHub organization via the Getting Started steps above. If your colleague is a member, you can invite your colleague to collaborate on repositories. 

### Inviting external collaborators
You can invite non-UU collaborators or students by adding them as external collaborator to repositories: [see GitHub documentation](https://docs.github.com/en/organizations/managing-access-to-your-organizations-repositories/adding-outside-collaborators-to-repositories-in-your-organization). Be responsible on inviting outside collaborators; only invite them to repositories they need access to. 

### Transferring an existing repository to this GitHub organization
Migrate a repository to the Utrecht University GitHub organization by [mirroring the repository](https://docs.github.com/en/repositories/creating-and-managing-repositories/duplicating-a-repository). 
> :warning: Do **not** use the ""Transfer ownership"" option in your repository settings to transfer a repository to this GitHub organization. You will lose admin rights for the repository. 

### GitHub Actions limits
GitHub Actions minutes and storage are unlimited for public repositories. Whenever possible, use __public__ repositories if you are using GitHub Actions. There are monthly limits for using GitHub actions in private repos on an organization level. **As soon as this limit has been reached, GitHub action minutes will be disabled for private repositories for the remaining part of the month**.

### GitHub applications and third-party access
The following applications are approved for usage in this GitHub organization:  

- [Zenodo](https://zenodo.org/)
- Codecov
- [Microsoft Teams](https://teams.github.com/)
- Slack
- Travis CI
- GitHub Desktop
- AllContributors

Activation of these applications for your repositories differs per application: [view instructions](docs/third-party-applications.md).

Submit an [issue](https://github.com/UtrechtUniversity/getting-started/issues/new) if you would like to use a new application.

## Learning Git

Using Git version control is key in the Open Science paradigm and helps managing versions of files, collaboration and publication.
A Git novice should invest some time to get familiar with the way of working. A one-day course will get you started.

Resources:
- [Software carpentries course documentation](http://swcarpentry.github.io/git-novice/)  

Courses:  
- [Best practices for writing reproducible code](https://www.uu.nl/en/research/research-data-management/training-workshops/best-practices-for-writing-reproducible-code)

## Contributing
We are very happy with any suggestions or contributions to improve the contents. The aim of this Repository is to help UU employees getting started with the Utrecht University GitHub organization. Read the contributing [guidelines](/CONTRIBUTING.md).

## License
The content in this repository is licensed under the [Creative Commons Zero 1.0](/LICENSE) (release to the public domain).

## Contact
Do you need help or have any other requests? Submit an [issue](https://github.com/UtrechtUniversity/getting-started/issues/new) or send an email: its.ris@uu.nl
",2022-08-12
https://github.com/UtrechtUniversity/GitMiller,"
# GitMiller

This description can be found [on GitHub here](https://github.com/UtrechtUniversity/GitMiller)

GitMiller is a tool for running Jupyter Notebooks from a (partial) Github repository. It downloads the repository in your temp folder, runs a designated notebook within it, and 
removes all downloaded files afterwards.

## Installation

`$ pip install gitmiller`

After installation you can use the `gitmiller` command-line tool to run your github notebooks.

## Usage

The command-line interface takes the following input parameters:

<table>
<tr>
    <th>parameter</th>
    <th>description</th>
</tr>
<tr>
    <td nowrap>-u, --username</td>
    <td>Username to gain access to GitHub repository</td>
</tr>
<tr>
    <td nowrap>-p, --password</td>
    <td>Password Github repository</td>
</tr>
<tr>
    <td nowrap>-t, --token</td>
    <td>Or use a Github token</td>
</tr>
<tr>
    <td nowrap>-r, --repository</td>
    <td>URL of GitHub repository (can be a subfolder)</td>
</tr>
<tr>
    <td nowrap>-n, --notebook</td>
    <td>filename of notebook you wish to execute, this file must exist in the root folder of the (partially) downloaded folder structure.
    </td>
</tr>
<tr>
    <td nowrap>-o, --output</td>
    <td>local path where GitMiller will put the executed version of your notebook</td>
</tr>
<tr>
    <td nowrap>-c, --config</td>
    <td>local path of YAML file in which you can add all mentioned paramaters</td>
</tr>
</table>

GitMiller uses [Papermill](https://github.com/nteract/papermill) to run your remote notebook. Besides executing, Papermill also enables you to parameterize notebooks. If you would like to override certain variables in your notebook, add the variables and values in the config YAML file under the `papermill` key. See example below.

This repository contains a folder `example` in which a notebook `test.ipynb` exists. If you would like to run this notebook with GitMiller, use either:

```
$ gitmiller -u <GITHUB USERNAME> -p <GITHUB PASSWORD>, -r https://github.com/UtrechtUniversity/GitMiller/tree/master/example, -n test.ipynb, -o <LOCAL OUTPUT-PATH>
```
or create the following YAML file:
```
repository: https://github.com/UtrechtUniversity/GitMiller/tree/master/example
username: <GITHUB USERNAME>
password: <GITHUB PASSWORD>
notebook: test.ipynb
output: <LOCAL OUTPUT-PATH>

papermill:
  a: 10
  b: 60
```
and run with:
```
$ gitmiller -c <PATH TO YAML-FILE>
```

## Note for Windows users
The script isn't able to remove the temp-folder after the Notebook has been processed on *Windows* machines. I have tried a lot of things, but couldn't find a solution. Manually removing also turned out to be impossible when using a terminal. I give up. At the end a message is printed in which the path of the folder is mentioned so you can remove it yourself.



",2022-08-12
https://github.com/UtrechtUniversity/global-goals,"# Global Goals

This repository describes an approach to obtain historical hyperlinks among a given set of International Organizations (IOs).
The hyperlinks are retrieved from the oranizations' websites from 2012 up to 2019 via the Internet Archive.

This approach is further developed in the Crunchbase project.
For an improved and more generic version of the IA webscraping pipeline, check out the [ia-webscraping repository](https://github.com/UtrechtUniversity/ia-webscraping).


<!-- TABLE OF CONTENTS -->
## Table of Contents

- [Project Title](#global-goals)
  - [Table of Contents](#table-of-contents)
  - [About the Project](#about-the-project)
    - [License](#license)
    - [Attribution and academic use](#attribution-and-academic-use)
  - [Getting Started](#getting-started)
    - [Prerequisites](#prerequisites)
    - [Installation](#installation)
  - [Usage](#local-usage)
  - [Ansible workflow](#ansible-workflow)
  - [Contact](#contact)

<!-- ABOUT THE PROJECT -->
## About the Project

**Date**: June 2022

**Researcher(s)**:

- Maya Bogers (m.j.bogers@uu.nl)

**Research Software Engineer(s)**:

- Jelle Treep (h.j.treep@uu.nl)
- Martine de Vos (m.g.devos@uu.nl)

<!-- Do not forget to also include the license in a separate file(LICENSE[.txt/.md]) and link it properly. -->
### License

The code in this project is released under [LICENSE.md](LICENSE).

### Attribution and academic use

The hyperlinks collected with the pipeline from this repository have been used in the following scientific publication:
Bogers, M et al. [_The impact of the Sustainable Development Goals on a network of 276 international organizations_](https://www.sciencedirect.com/science/article/pii/S0959378022001054)(2022)


<!-- GETTING STARTED -->
## Getting Started

Guide to get a local copy of this project up and running.
```
git clone https://github.com/UtrechtUniversity/global-goals.git
```


### Prerequisites

To install and run this project you need to have the following prerequisites installed.

- Ansible
- Python


<!-- USAGE -->
## Local Usage 

The project consists of four stages as listed below. In short: 
- get list of cdx records
- download html pages
- extract all hyperlinks 
- extract network hyperlinks.

If a fairly large amount of url's needs to be obtained, multiple servers should be used. 
These servers can be populated using Ansible, see [Ansible workflow](#ansible-workflow). In that case the workflow looks like:
> obtain cdx records -> split records -> setup AWS -> deploy to servers -> fetch logs -> obtain status (-> redo)

### Stage 1: Fetching CDX Records (cdx_record_fetcher)
A CDX Record is a record from the [CDX Api](https://github.com/internetarchive/wayback/tree/master/wayback-cdx-server) of the Internet Archive. Such a record consists of an url_key and a timestamp. An example would be:
```
timestamp = 20180806145630
url_key = https://www.uu.nl/en/research/research-data-management
```
This would result in the following url: [https://web.archive.org/web/`20180806145630`/`https://www.uu.nl/en/research/research-data-management`](https://web.archive.org/web/20180806145630/https://www.uu.nl/en/research/research-data-management).

Using the script `cdx_record_fetcher.py` and a list of organisations called `organisations.txt`, it is possible to fetch all CDX Records for each organisation. The output is a `csv`-file per organisation.

### Stage 2: Fetching html pages (page_fetcher)
The next step is to combine the timestamp and url_key to a working url, and fetch the corresponding HTML file. This can be done using the script `page_fetcher.py`, which takes the following arguments:
| Argument      | Description                                     |
|---------------|-------------------------------------------------|
| `data`        | Path to csv data formatted as (timestamp, url). |
| `log`         | Path to log file.                               |
| `bucket_name` | Amazon S3 Bucket name to store HTML files in.   |

If there are many files to be fetched, multiple servers and Ansible should be used. This is described in section Ansible.

### Stage 3: Fetching links from html pages (link_lyxer)
The third step is to convert html files to a list of hyperlinks. This is done using the command line based web browser `lynx` in a Python script called `python/link_lyxer.py`. The input is a directory in the following structure:
```
root
|
organisation domains
|
html_page
```
An example of this could be: `html/uu.nl/20180806145630_uu.nl_en_research_research-data-management`. With this input, the `link_lyxer` will obtain all links and write them per organisation to a single file. The output file has the name `[domain]_links`, i.e.: `uu.nl_links`. Output files will contain the source and destination of a link. An example row can be:
```
uu.nl/20180806145630_uu.nl_en_research_research-data-management∞https://web.archive.org/web/20180806145630/https://www.uu.nl/en/research/research-data-management/guides
```
The source and destination are separated by a delimiter (`∞`), which was picked on it's low likelihood of occurring in an url. All slashes in the source have been replaced with `_`, as `/` can be used in a file name.

### Stage 4: Filtering links (ext_link_lister)
The Python regular expressions library is used in `python/ext_link_lister.py` to filter hyperlinks to organizations in the 'target' list from the list with all hyperlinks in from the previous step. 

## Ansible Workflow
If a fairly large amount of url's needs to be obtained, multiple servers should be used. These servers can be populated using Ansible.

### Stage 1: Obtain the data to fetch
Using the Project Workflow it should be possible to start with a list of organisation domains and end up with a lot of csv's containing CDX Records. These csv files can be combined in the following manner:
```
cd [the correct directory!]
for file in *.csv
do
  # Remove header line
  sed -i '1d' $file
done

# Combine all csv files
cat *.csv > data.csv
```

### Stage 2: Split the data in multiple files
If Stage 1 is completed succesfully, a combined `data.csv` file is obtained which contains all CDX records. Each server will take a slice of this file, and fetch the corresponding records. Splitting this file can be done using `splitter.sh`. It expects the `data.csv` to be in the working directory. It's input argument is the amount of files to create.

Example:
- We have 64 servers, and a file called all_data.csv
- We rename the file to data.csv and open a terminal in that directory
- We run `./splitter.sh 64`

The output of this stage is a folder named `splitting` containing `data0.csv` to `dataX.csv`. All of these files combined would make the original `data.csv`

### Stage 3: Setup AWS
We are using AWS as cloud provider in this project. You will need the following services: S3 & EC2. What is required:
- A created or reused IAM user.
- A bucket with an accessible bucket policy to said IAM user.
- Credentials for said IAM user (resulting in an AWS_Key and AWS_Secret).
- All EC2 instances must be SSH accessible, thus the inbound firewall should allow port 22. This can be accomplished using a custom Security Group.
- The required amount of EC2 instances.
- All EC2 instances should be accessible via *one* ssh key.

### Stage 4: Deploy using Ansible
Ansible is an automation tool that is used to deploy the `page_fetcher` with dependencies in a consistent manner to multiple servers.

The following must be set up in the `hosts` file:
- The hosts files contains an updated list of IP addresses. Using `ansible/get_ip_addresses.sh`, you can obtain this list.
- Variable `ansible_ssh_private_key_file` refers to the correct ssh keyfile in order to access all EC2 instances
- Variable `bucket_name` refers to the correct S3 bucket
- Variable `aws_access_key` refers to AWS Access Key to access the S3 bucket (combo with `aws_secret_key`)
- Variable `aws_secret_key` refers to AWS Secret Key to access the S3 bucket (combo with `aws_access_key`)

After configuring `hosts`, it is possible to run the Ansible playbook using:
```
./run_playbook.sh
```

All variables can be encrypted. In this repo, the AWS credentials have been encrypted and require a vault secret to decrypt. If you run `run_playbook.sh`, a `VAULT_SECRET` is asked by default. If no variables are encrypted, you should remove `--ask-vault-pass` from the bash script.

### Stage 6: Monitoring
There are several ways of monitoring the progress.

#### Checking /tmp/status
```
# Get IP Addresss
aws2 ec2 describe-instances --query 'Reservations[*].Instances[*].[PublicIpAddress]' --filters Name=instance-state-name,Values=running --output text | sort > instances.txt

# Get /tmp/status versus len(data.csv)
cat instances.txt | while read line; do ssh ubuntu@$line 'cat /tmp/status;echo "";"";cat /home/aztec/`(ls /home/aztec | grep data)` | wc -l' < /dev/null; echo "";$line""; done
```
Example output is shown below. It means that 1538/3035 records have been fetched.
```
1538;           <-- /tmp/status
3035            <-- length of data.csv
;xx.xxx.xx.xxx  <-- ip
```

#### Checking journalctl
```
ssh -i [path_to_ssh_key] ubuntu@[ip_of_ec2_instance]
journalctl -efu globalgoals.service
```
This will output all logs from the service (the python file wrapped in systemd service).

#### Checking systemd status
```
ssh -i [path_to_ssh_key] ubuntu@[ip_of_ec2_instance]
sudo systemctl status globalgoals.service
```
This will output the systemd status. If the script is running, its status should be on `active (running)`. The script could still be hanging but not crashing, so do check the logs.

#### Checking S3 before and after
```
aws2 s3 ls s3://975435234474-global-goals --recursive | wc -l
```
This will output all files in the S3 bucket. By running this command before deployment and after, it is possible to see the amount of records that have been fetched.

### Stage 5: Fetch logs and terminate servers
Before terminating servers, you should fetch all logs. Logs may explain any errors or mishaps that have occurred while the scripts were running. Save the logs per session per ip.

```
# Create folder
mkdir sess1_64servers
cd sess1_64servers

# Get IP Addresss
aws2 ec2 describe-instances --query 'Reservations[*].Instances[*].[PublicIpAddress]' --filters Name=instance-state-name,Values=running --output text | sort > instances.txt

# Get logs from journalctl
get_journalctl_logs.sh instances.txt
```
This will copy all logs from journalctl to your working directory. Now you can terminate the relevant servers in AWS.

### Stage 6: Parse logs to obtain status
There are two types of logs to parse: journalctl logs and S3 ls logs. The first are the output of the services that run the python scripts, the latter are the result of an list files command of the S3 bucket.

#### Parsing server logs
If Stage 5 was run successfully, you have a folder with logs with filenames such as log123.456.789.txt. The first step is to combine all those log files.
```
cat log*.txt > ../all_logs.txt
```
This will output a file called `all_logs.txt` in the parent directory. The next step is to run `log_parser` over these log files.
```
./log_parser.sh all_logs.txt
```
This will output `all_logs.txt_notfound.csv` and `all_logs.txt_success.csv`. The first file contains all CDX Records that resulted in a 404 error, the latter contains all CDX Records that have been uploaded to S3. Both files are formatted as `timestamp∞url_key`. The columns have been delimited with a `∞` as urls may contain many typical delimiter characters but an infinity sign is very unlikely.

**To get rid of the infinity delimiter:**
1) (Optional) Split the file in multiple csv files when it is too large
   ```
   split -d -n 2 [path_to_csv_file] --additional-suffix=.csv
   ```
1) Open each csv file with excel software (i.e. LibreOffice or Excel)
1) (Optional) If the csv file has been splitted, the last line of the first file may be incomplete and should be combined with the first line of the second file.
1) Save each csv file in another extension (i.e. ods or xlsx)
1) Save each file as csv. This prompt the delimiter choice rather than defaulting to what was already used.
1) (Optional) Combine the split files
   ```
   cat x*.csv > [path_to_output_csv_file]
   ```

#### Parsing AWS S3 ls
To get all the contents of the S3 bucket without actually fetching the files, run `aws2 s3 ls`. Then use `sed` to convert the format to timestamp `timestamp∞url_key`. These steps are shown in the code below:
```
aws2 s3 ls s3://975435234474-global-goals --recursive > downloaded.txt
sed -E 's/.*\/([0-9]{14})_(.*)/\1∞\2/g' downloaded.txt > B.csv
```
This outputs a csv file that is not yet compatible for matching with other csv files. All urls have their slashes(`/`) replaced with underscores(`_`). The original url_keys don't have this. Therefore we have to run a Python file called `reduceAWS.py`. It expects two files: A.csv (all data) and B.csv (S3 ls data).
```
python3 reduceAWS.py
```
This outputs three files:
- matches = A union B
- Aonly   = A - B
- Bonly   = B - A

#### Subtracting fetched and not found from all data
To keep an overview of all subtractions, we can recommend to have aliases for certain types of data, as shown in the table below. If multiple sessions exists, a filename could be C2.csv for the second sessions that produced a C.csv file.
| Filename | Description                                       |
|----------|---------------------------------------------------|
| A        | All data                                          |
| B        | All data in S3 Bucket                             |
| C        | All data from logs that should have been uploaded |
| D        | All data from logs that resulted in a 404         |

To subtract data, `comm` can be used. The files should be sorted beforehand using `sort A.csv > A_sorted.csv`.
```
comm -23 A.csv C.csv > A-C.csv
comm -13 A.csv C.csv > C-A.csv
comm -12 A.csv C.csv > AC.csv
```
These commands above have been incoroporated in the script `calc_unions.sh`. When `./calc_unions.sh A.csv C.csv` is executed, the file counts of each subtractions or union is reported. If the output doesn't make sense, it could be that the format is not the same. `comm` matches per character, so even line-endings could mess the results up. This can be investigated by outputting the head and tail from both files:
```
head A.csv
head C.csv
tail A.csv
tail C.csv
```
If you suspect a line-ending error, run `dos2unix` for both files. Please be aware that csv's have to escape the delimiter if it occurs in the data (i.e.: `x, ""y,z""`).

### Stage 7: Redo
When computing A -B -C -D using `comm`, it could be that not all data has been fetched. Start from Stage2 using the subtracted data. Be sure to separate the session's logs and data for proper bookkeeping. For example, split both C.csv files by using C1.csv and C2.csv as filenames.


<!-- CONTACT -->
## Contact

Contact Name - research.engineering@uu.nl

Project Link: [https://github.com/UtrechtUniversity/global-goals](https://github.com/UtrechtUniversity/global-goals)
",2022-08-12
https://github.com/UtrechtUniversity/google-semantic-location-history,"# Google Semantic Location History Faker

<!-- TABLE OF CONTENTS -->
## Table of Contents
- [About the Project](#about-the-project)
- [Getting Started](#getting-started)
   - [Prerequisites](#prerequisites)
   - [Installation](#installation)
   - [Testing](#testing)
- [Usage](#usage)
- [Contributing](#contributing)
- [Contact](#contact)

## About the Project
Generate fake Google Semantic Location History (GSLH) data using the python libraries [GenSON](https://pypi.org/project/genson/), [Faker](https://github.com/joke2k/faker), and [faker-schema](https://pypi.org/project/faker-schema/).

First, we generate a JSON schema from a JSON object using `GenSON's SchemaBuilder` class. The JSON object is derived from an example GSLH data package, downloaded from Google Takeout. The GSLH data package consists of monthly JSON files with information on e.g. geolocations, addresses and time spend in places and in activity. The JSON schema describes the format of the JSON files, and can be used to generate fake data with the same format. This is done by converting the JSON schema to a custom schema expected by `faker-schema` in the form of a dictionary, where the keys are field names and the values are the types of the fields. The values represent available data types in `Faker`, packed in so-called providers. `Faker` provides a wide variety of data types via providers, for example for names, addresses, and geographical data. This allows us to easily customize the faked data to our specifications.

## Getting Started

### Prerequisites
This project makes use of Python 3.9.2 and [Poetry](https://python-poetry.org/) for managing dependencies. 

### Installation
You can simply install the dependencies with 
`poetry install` in the projects root folder.

### Testing
To run the unit tests:

`poetry run pytest`

Note that the `poetry run` command executes the given command inside the project’s virtual environment.

## Usage

`poetry run python google_semantic_location_history/simulation_gslh.py`

This creates a zipfile with the simulated Semantic Location History data in `Location History.zip`.

<!-- CONTRIBUTING -->
## Contributing

Contributions are what make the open source community an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

To contribute:

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request


<!-- CONTACT -->
## Contact
[UU Research Engineering team](https://github.com/orgs/UtrechtUniversity/teams/research-engineering) - research.engineering@uu.nl
",2022-08-12
https://github.com/UtrechtUniversity/hist-aware,"[![MIT License][license-shield]][license-url]

<!-- PROJECT LOGO -->
<br />
  <h3 align=""center"">hist-aware</h3>

  <p align=""center"">
    Mining Historical Trajectories of Awareness: A machine learning approach to historicized sentiment mining.
    <br />
    <a href=""https://github.com/UtrechtUniversity/hist-aware"">View Demo</a>
    ·
    <a href=""https://github.com/UtrechtUniversity/hist-aware/issues"">Report Bug</a>
    ·
    <a href=""https://github.com/UtrechtUniversity/hist-aware/issues"">Request Feature</a>
  </p>
</p>

<!-- TABLE OF CONTENTS -->
## Table of Contents

- [Table of Contents](#table-of-contents)
- [About The Project](#about-the-project)
  - [Project description](#project-description)
  - [Built With](#built-with)
- [Getting Started](#getting-started)
  - [Installation](#installation)
- [Usage](#usage)
- [Roadmap](#roadmap)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)
- [Acknowledgements](#acknowledgements)

<!-- ABOUT THE PROJECT -->
## About The Project

This project makes use of the Delpher archive (delpher.nl/kranten), which is the largest public collection of digitized pages from Dutch historical newspapers. The research team is mining articles’ sentiments, as expressed by the author of the articles, extracting all the relevant Delpher articles around specific topics (i.e. energy) and is currently training natural language processing (NLP) models called Transformers to extract a sufficiently accurate representation of the sentiment of each article. Currently, the team is making use of the period 1960-1995 consisting of around 250.000 articles around the topics chosen.

<p align=""center"">
  <a href=""https://github.com/UtrechtUniversity/hist-aware"">
    <img src=""img/trend-example.png"" alt=""Logo"" width=""500"" height=""250"">
</a>

### Project description

In recent years, the Royal Library of the Netherlands (KB) has been collecting and digitizing the majority of available Dutch newspapers, books and magazines. In this context, a research team from Utrecht University set out to create a partnership with the KB aimed at enabling the possibility of running more complex natural language processing analyses on this dataset.

The first research project stemming out of this partnership, deals with the analysis of sentiment within the majority of newspaper articles contained in the Delpher dataset - the database that archives the digitized text of newspapers, books and magazines. This project uses a sentiment analysis pipeline to trace historical shifts in awareness. Many debates - whether about climate change, genetically modified foodstuffs, or #metoo - hint at a high form of awareness in our current global society. It is, however, far less evident where these sentiments are rooted in and how they have evolved over time. 

This project investigates this for the Dutch case by developing a historical sentiment analysis pipeline based on machine learning. This project will make sentiment analysis more historically dynamic, context-specific and easily repeatable on different topics at a very low cost. The output of the models creates a sentiment variable which is topic- and context-specific and ranges from  -2 (very negative/pessimistic) to +2 (very positive/optimistic). Instead of relying on an historical thesauri, the sentiment analysis models trained on opinionated data from historical newspapers will have a considerable positive impact on the investments necessary in future research, will create new lines of research that were not possible before 
### Built With

- [Dutch Royal Library](https://www.kb.nl/en)
- [ResearchCloud - Surf Sara](https://portal.rsc-pilot2.surfresearchcloud.nl/)
- [Jupyter Notebooks](https://jupyter.org/)
- [Huggingface Transformers](https://huggingface.co/)


<!-- GETTING STARTED -->
## Getting Started

To get a local copy up and running follow these simple steps.

### Installation

1. Clone the repo
```sh
git clone https://github.com/github_username/hist-aware.git
```
2. Install the package dependencies
   1. If you have poetry 
    ```sh
    poetry install 
    ```
   2. If you have only pip
   ```sh
   pip install -r requirement.txt
   ```

<!-- USAGE EXAMPLES -->
## Usage

To carry out the main extraction of Delpher articles you will make use of the file [src/histaware.py](src/histaware.py).

This is a temporary file used to route the complete extraction and first selection of articles. The selection and extraction will later be changed to be easily operable by non-experts.

Currently, there are some variables that have to be changed for the extraction of each batch. All the variables that need to be changed are found in the file [`src/core/config.py`](`src/core/config.py`):

### Core variables

#### Directory variables

All these directories need to be first created manually. The configuration parameters are only used to tell the processing scripts where to find the data.

* `DATA_DIR`: the name of the main directory in the base path
* `DATA_DIR_SAVE`: the name of the directory where the processed data will be saved
* `DATA_DIR_RAW`: the name of the directory that contains the raw files within `DATA_DIR`
* `DATA_DIR_DELPHER`: the name of the directory containing the delpher raw data

The folder structure should look like this:
<p align=""center"">
    <img src=""img/folder-structure.png"" alt=""Logo"" height=""250"">
</a>

* Within each folder, there should be the same structure of the decades of the data available. For example:
    * file_info:
      * 1960s
        * All the years of 1960s 
      * 1970s 
       * All the years of 1960s 


#### Variables specific for the extraction of each decade

We retrieved data from KB downloading one decade at the time. Each decade we downloaded was composed by 10 files, each representing one year (1980, 1981, ...). These files were put into a folder called `1980s` for the years 1980-1989, `1990s` for 1990-1999 and so on.

We then processed each decade at the time configuring the following variable for each decade and the ""topic"" of articles we were searching for.

* `DECADE`: string indicating the decade we are interested in using in the processing.

Given the slow processing of these files, we recommend to set the next values to `True` only once per decade (meaning that if you have mulitple topics you want to extract, after the first topic these values should be `False`)
  * `UNGIZP`: boolean indicating whether to ungizp the metadata files into .xml. G
  * `DATAFILE`: is a dictionary that allows us to decide whether to process and save articles and metadata data. **This is a long process and should be also done only once per decade!**.
    * `start`: should be `True` to start the process
    * `metadata`: should be `True` process metadata into .csv files
    * `articles`: should be `True` process articles into .csv files
  * `MERGE`: is a boolean value to merge articles and metadata. Please take in consideration that this is also a slow process (2/3 hours per decade with 8 cores)

From this point onward, the variables should be configured for each topic one wants to extract within a decade.

* `TOPIC`: name of the topic
* `SEARCH_WORDS`: should be True when searching for keywords before running the tf-idf within the found articles
* `LIST_INCL_WORDS`: we use a word list to include articles that contains words. Please add your list to [`src.utils.keywords`](src.utils.keywords)
* `LIST_EXCL_WORDS`: we use a word list to exclude articles that contains words. Please add your list to [`src.utils.keywords`](src.utils.keywords)
* `PREPROCESS`: we want to preprocess articles to clean them.
* `CLASSIFY`: if there is a labeler available that can label N examples of articles that belong to the topic one wants to extract for this decade, then we suggest to put this setting to `True` and 

<!-- ROADMAP -->
## Roadmap

See the [open issues](https://github.com/UtrechtUniversity/hist-aware/issues) for a list of proposed features (and known issues).


<!-- CONTRIBUTING -->
## Contributing

Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are **greatly appreciated**.

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request



<!-- LICENSE -->
## License

Distributed under the MIT License. See `LICENSE` for more information.


<!-- CONTACT -->
## Contact

Leonardo Vida - [@leonardojvida](https://twitter.com/leonardojvida) - l.j.vida@uu.nl

Project Link: [https://github.com/UtrechtUniversity/hist-aware](https://github.com/UtrechtUniversity/hist-aware)



<!-- ACKNOWLEDGEMENTS -->
## Acknowledgements

* [Martin Brand - SurfSara]()
* [The ResearchCloud team]()

<!-- MARKDOWN LINKS & IMAGES -->
<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->
[contributors-shield]: https://img.shields.io/github/contributors/UtrechtUniversity/repo.svg?style=flat-square
[contributors-url]: https://github.com/UtrechtUniversity/repo/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/UtrechtUniversity/repo.svg?style=flat-square
[forks-url]: https://github.com/UtrechtUniversity/repo/network/members
[stars-shield]: https://img.shields.io/github/stars/UtrechtUniversity/repo.svg?style=flat-square
[stars-url]: https://github.com/UtrechtUniversity/repo/stargazers
[issues-shield]: https://img.shields.io/github/issues/UtrechtUniversity/repo.svg?style=flat-square
[issues-url]: https://github.com/UtrechtUniversity/repo/issues
[license-shield]: https://img.shields.io/github/license/UtrechtUniversity/repo.svg?style=flat-square
[license-url]: https://github.com/UtrechtUniversity/repo/blob/master/LICENSE.txt
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=flat-square&logo=linkedin&colorB=555
[linkedin-url]: https://linkedin.com/in/UtrechtUniversity
[product-screenshot]: images/screenshot.png
",2022-08-12
https://github.com/UtrechtUniversity/hpc-batch-gen,"# Batchgen
Package for generating simple batch scripts in HPC environments. Currently available: GNU Parallel, SLURM backend. 

In its current implementation, it will only **create** the batch files for submission, and **not** actually submit them. This might change in future versions.

### Requirements:

##### Software

- GNU Parallel
- Python 2.7+

### Installation:

The easiest way to install the batchgen package is to use the following command in a terminal:

``` bash
pip install git+https://github.com/UtrechtUniversity/hpc-batch-gen.git
```

Use the following in case you have no administrator access:

``` bash
pip install --user git+https://github.com/UtrechtUniversity/hpc-batch-gen.git
```

### Usage

There are two ways to use the *batchgen* package. 

##### [Command line interface (CLI)](docs/cli.md)

The first is using the command line interface, which does need Python and GNU Parallel to be installed, but no programming in Python is required to use it. The basic command is the following (`bash batchgen -h` for a help file):

```bash
batchgen command_file config_file [-pre pre_file] [-post post_file] [-pp pre_post_file] [-f]
```

If this returns a ""command not found"" error, set your PATH to include the installation directory, or use the following:

```bash
python -m batchgen command_file config_file [-pre pre_file] [-post post_file] [-f]
```

For a more detailed description see [here](docs/cli.md).

##### [Application Programming Interface (API)](docs/api.md)

The second option is to use the package directly within Python. There are two main ways to access the package. The first is similar to access provided through the CLI:

```python
batch_from_files(command_file, config_file, pre_post_file=None,
					    pre_com_file=None, post_com_file=None, force_clear=False)
```

The second method bypasses the need to create as many files by supplying the package with strings (except the configuration file):

```python
batch_from_strings(command_string, config_file, pre_com_string="""",
                       post_com_string="""", force_clear=False)
```

A more detailed description is available [here](docs/api.md)

### Example

First go to the samples directory:

```bash
cd samples
```

Then run the batch generator with the parallel backend (assuming the GNU parallel is installed):

```bash
batchgen command_list.sh parallel.ini
```

There should be a directory called ""batch.parallel/my\_test/"" in which a batch script called ""batch.sh"" is present.

Run the batch script:

```bash
batch.parallel/my_test/batch.sh
```

The output of this will be stored in the directory sum\_output. See the files sum.sh and command\_list.sh for what is computed.

### Contributors

- Raoul Schram (r.d.schram@uu.nl, [@qubixes](github.com/qubixes))


",2022-08-12
https://github.com/UtrechtUniversity/HPC-data-synchronization,"# HPC-data-synchronization
This guide provides instructions for data synchronization (or data staging) in workflows involving High Performance Computing platforms such as the UBC cluster, Cartesius or Lisa. 

## Issue
When you take the step of performing data-analyses (or simulations) from a PC to High Performance Computing (HPC) platforms, you are faced with new challenges of managing data, for example because you need to have your data available on multiple locations for running analyses on different platforms (e.g. on both your PC and an HPC platform). Besides, file sizes may be too large to efficiently store locally on your PC, and HPC clusters are typically not meant for long-term storage of your data. Within this new infrastructure it is easy to lose track of the most recent versions of your datasets and scripts. Therefore, it is necessary to think of a workflow that allows efficient management of your data and scripts. 
Also for experienced users it is good to think about HPC workflows from time to time. Software tools and technology for data management and transfer are rapidly evolving and tools that were available a few years ago are replaced by new tools leading to new opportunities for saving time and keeping your workflow efficient and clear.

## Objective
This guide aims to provide instructions on how to manage and transfer your data efficiently when working on HPC platforms. It describes several solutions for different use cases and evaluates the performance of the different tools.
For questions and support contact Research IT via `info.rdm@uu.nl`


## Outline
A typical workflow for HPC calculations is outlined in the [Workflow](./docs/workflow.md) section. The different tools that are available are evaluated and discussed in the section [Evaluation of storage and transfer tools](./docs/Evaluation.md). Instructions and automated scripts for data synchronization and transfer are available for the storage platforms that are explored in this test: I) [Yoda staging](./docs/Yoda.md), II) [Surfdrive staging](./docs/surfdrive.md), III) [Archive staging Lisa](./docs/Archive.md) or [Archive staging UBC](./docs/ArchiveUBC.md), IV) [Job scripts](./docs/jobs.md).


Next: [Typical workflow](./docs/workflow.md)

",2022-08-12
https://github.com/UtrechtUniversity/ia-webscraping,"# ia-webscraping

This repository provides code to set up an AWS workflow for collecting and analyzing webpages from the Internet Archive.
It is developed for the Crunchbase project to assess the sustainability of European startup-companies by analyzing their websites. 

This software is designed for users with prior knowledge of python, aws and infrastructure.

## Table of Contents

- [Project Title](#ia-webscraping)
  - [Table of Contents](#table-of-contents)
  - [About the Project](#about-the-project)
    - [Built with](#built-with)
    - [License](#license)
    - [Architecture](#architecture)
  - [Getting Started](#getting-started)
    - [Prerequisites](#prerequisites)
    - [Installation](#installation)
  - [Usage](#usage)
    - [Deploy AWS resources](#deploy-aws-resources)
    - [Fill SQS queue ](#fill-sqs-queue)
    - [Test Function](#test-function)
    - [Clean Up](#clean-up)


## About the Project

**Date**: Jan-Jun 2021

**Researcher**:

- Jip Leendertse (j.leendertse@uu.nl)

**Research Software Engineer**:

- Casper Kaandorp (c.s.kaandorp@uu.nl)
- Martine de Vos (m.g.devos@uu.nl)
- Robert Jan Bood (robert-jan.bood@surf.nl)

This project is part of the Public Cloud call of [SURF](https://www.surf.nl/en/) 

### Built with

- [Terraform](https://www.terraform.io/)
- [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
- [asyncio](https://docs.aiohttp.org/en/stable/glossary.html#term-asyncio)

### License

The code in this project is released under [MIT](LICENSE).

### Architecture
The ia-webscraping repository utilizes the following AWS services:
- **Simple Queueing System**: manage distribution of tasks among Lambda functions and give insight in results
    - queue with initial urls 
    - queue with scraping tasks
- **AWS Lambda**: run code without the need for provisioning or managing servers
    - lambda to retrieve cdx records for initial urls, filter these and send tasks to scraping queue 
    - lambda to retrieve webpages for cdx records and send these to s3 bucket
- **S3**: for storage of the HTML pages
- **CloudWatch**: monitor and manage AWS services
   - CloudWatch to monitor the metrics of the SQS queue and Lambda functions
   - CloudWatch to trigger the Lambda function on a timely basis, the interval can be changed to throttle the process

Deploying this solution will result in the following scrape pipeline in the AWS Cloud.

![Alt text](docs/architecture_overview.png?raw=true ""Architecture Overview"")

## Getting started

  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
  - [Build Lambda function](#build-lambda-functions)
  - [Update Terraform](#update-terraform)

### Prerequisites
To run this project you need to take the following steps:
- (optional) install package manager. 
	- windows: [chocolatey](https://chocolatey.org/install), 
	- mac: [brew](https://brew.sh)
	- With a package manager it becomes easy to install/update the below prerequisites. 
    For example:  ```choco install awscli' ```
- [install aws cli](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
- [configure aws cli credentials](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)
    create a profile with the name 'crunch' 
- install python3 , pip3 , pandas, boto3
- install [terraform](https://www.terraform.io/downloads.html)
- create a personal S3 bucket in AWS (region: eu-central-1)
- if you are going to use an IAM account for the pipeline, make sure it has the proper permissions to create buckets, queues and policies. A very crude set of permissions would be: AmazonSQSFullAccess, AmazonS3FullAccess, IAMFullAccess, CloudWatchEventsFullAccess and AWSLambda_FullAccess.

### Installation
Create your own copy of the 'cdx-records' directory on your local client and change your working folder to it.
```
# Go to terraform folder
$ cd cdx-records
```

### Build lambda functions
The 'build.sh' script will for each of the lambda functions:
- install all requirements from the 'requirements.txt' file in the lambda folder
- create a zip file 
- calculate a hash of this zipfile and write this to 'example_lambda.zip.sha256'
- upload the zip file to the s3 bucket

First update the variable values in the build script:
- `CODE_BUCKET`: replace the default value with your own AWS bucket name (see [Prerequisites](#prerequisites)). The name of the zip files containing the lambda code matters. Lambda references the code on S3 when it sets up the function.
- `LAMBDA_NAME`: replace the default value with [YOUR_LAMBDA_NAME] which is listed in the [variables file](#update_terraform).

Then run the build script
```
# Build zip files
$ ./build.sh 
```

You might run into some errors, but usually these errors are automatically fixed.

### Update Terraform

In the [terraform folder](/cdx-records/terraform) create a file ```terraform.tfvars``` that lists the terraform variables with their corresponding values.
See [variables file](/cdx-records/terraform/variables.tf) for more information on the variables. The file should have the following format:

```
code_bucket = ""[CODE_BUCKET_NAME]"" # must be a string, subsitute the [CODE_BUCKET_NAME]-part

result_bucket = ""[RESULT_BUCKET_NAME]"" # can't be the same as [CODE_BUCKET_NAME]

lambda_name = ""[YOUR_LAMBDA_NAME]"" # must match the prefix of the lambda-zip files 

# ------- Optional: when not specified the default applies, 
# uncomment if you would like to use these parameters -----------

# use a subfolder in the result bucket
# result_bucket_folder = ""[SUBFOLDER_NAME]"" # default=""""

# Internet Archive: search in years from-to
# ia_payload_year_from = ""[START_YEAR]"" # default=""2018""
# ia_payload_year_to = ""[END_YEAR]"" # default=""2019""

# cdx_logging_level = [CDX_DEBUG_LEVEL; DEFAULT=error]

# scraper_logging_level = [SCRAPER_DEBUG_LEVEL; DEFAULT=error]

# sqs_fetch_limit = [MAX_MESSAGES_FETCH_QUEUE; DEFAULT=1000]

# sqs_message_delay_increase = [DELAY_INCREASE; DEFAULT=10 sec]

# sqs_cdx_max_messages = [MAX_CDX_MESSAGES_RECEIVED_PER_ITERATION; DEFAULT=10]

# cdx_lambda_n_iterations = [NUMBER_ITERATIONS_CDX_FUNCTION=2]

# cdx_run_id = [CDX_RUN_METRICS_IDENTIFIER; DEFAULT=1]

```
Keep the square brackets intact. Substitute `[YOUR_BUCKET_NAME]` with the name of the bucket you have created. The value for `[RESULT_BUCKET_NAME]` is yours to choose. In this example workflow, the value for [YOUR_LAMBDA_NAME] must be identical to the prefix of the zip discussed earlier.

This file is automatically loaded by terraform and the values are assigned values to the variables in ```main.tf``` and ```provider.tf``` 

NB: The file ```backend.tf``` should be modified directly in the code :
- line 5: provide your AWS bucket name (see [Prerequisites](#prerequisites))
- line 10: change the key with a key of your own, e.g. 'terraform/state/[your-lambda function]/terraform.tfstate' 

## Usage
- [Deploy AWS resources](#deploy-aws-resources)	
- [Fill SQS queue ](#fill-sqs-queue)
- [Test Function](#test-function)
- [Monitor Process](#monitor-process)
- [Collect Results](#collect-results)
- [Clean Up](#clean-up)

### Deploy AWS resources

#### init
The terraform init command is used to initialize a working directory containing Terraform configuration files. This is the first command that should be executed after writing a new Terraform configuration or cloning an existing one from version control. It is safe to run this command multiple times.
```
# Go to terraform folder
$ cd terraform

# Initialize terraform
$ terraform init
```

#### plan
The terraform plan command is used to create an execution plan. Terraform performs a refresh, unless explicitly disabled, and then determines what actions are necessary to achieve the desired state specified in the configuration files. The optional -out argument can be used to save the generated plan to a file for later execution with terraform apply, which can be useful when running Terraform in automation.
```
$ terraform plan -out './plan'
```

#### apply
The terraform apply command is used to apply the changes required to reach the desired state of the configuration, or the pre-determined set of actions generated by a terraform plan execution plan.
By using the “plan” command before “apply,” you’ll be aware of any unforeseen errors or unexpected resource creation/modification!
```
$ terraform apply ""./plan""
```

### Fill SQS queue
The 'fill_sqs_queue.py' script adds messages to the initial SQS queue.
These messages each contain a set of urls. The lambda function takes messages from the SQS queue and -for the given urls- requests CDX records from the Internet Archive.

Before running the script, set the following environment variable in your cmd prompt:
- 'AWS_PROFILE'=<'AWS profile'>

Execute the script:
```
# Go to cdx folder
$ cd ..

# Fill sqs queue
$ python fill_sqs_queue.py [ARGUMENTS]

Arguments:
  -f  path to the file containing urls 
  -q  SQS ID: human readable name of sqs cdx queue (check the terraform output)

Example:
$ python fill_sqs_queue.py -f example.csv -q lambda-cdx-queue
```

### Test Function
Look up your newly create Lambda function in the AWS console (note, make sure that the console is set to the correct region 'eu-central-1').
Open the function and create a test event for your function. You can use the ""hello world"" event template.
The content of the test event is not used by the python code.
Run you're newly created test event and check the Lambda logs to see the result.	

### Monitor Process
Each AWS service in the workflow can be monitored in the AWS console. 	
The cloudwatch logs provide additional information on the lambda functions.
Set the logging level to 'info' to get verbose information on the progress.	
	
The 'get_cdx_scrape_logs.py' script can be used to query the cloudwatch logs, to retrieve the metrics of the CDX Lambda funtion.
The output of this script is a csv file containing: domain, run id, number of urls fetched from internet archive, number of filtered urls.
Because there is a maximum on the number of results that cloudwatch can return (10.000), the script is currently configured to query the result of 1 single day (24 hours).
The date is hardcoded and should be updated before running the script. The output filename contains the date for which the results were collected.

### Collect Results
Scraping results are collected in your s3 bucket

### Clean up
Run the following [command](https://www.terraform.io/docs/commands/destroy.html), to cleanup the AWS resources that were deployed by terraform:
```
# Go to terraform folder
$ cd terraform

# Clean up AWS resources
$ terraform destroy
```
",2022-08-12
https://github.com/UtrechtUniversity/icat-database-checker,"# icat-database-checker

This script checks the iRODS ICAT database for unexpected issues, specifically:
- Referential integrity issues
- Timestamp order issues (creation timestamp is later than modification timestamp)
- Creation or modification timestamps that refer to the future
- Object names that contain characters which are not handled correctly on XML-based clients,
  such as the python-irods-client (See https://github.com/irods/irods/issues/4132 for details).
- Data objects with empty names
- Collection and data object names with trailing slashes (See https://github.com/irods/irods/issues/3892)
- Files in vaults that have a directory name which is inconsistent with the collection name
- Hard links: multiple data objects refer to the same physical file
- Duplicate replica: multiple replica entries for the same file
- Data objects with too few replicas (the default minimum is one replica)
- Missing indexes

The present version of the script is suitable for Postgresql databases. It is compatible with iRODS 4.2.x.

# Requirements

It is recommended to use Python 3.6 or higher. Older Python versions are not supported.

You'll also need tools to build the psycopg2 package. Example command for CentOS 7:

```
sudo yum -y install python3 python3-devel python-virtualenv gcc git postgresql-devel postgresql-libs
```

# Installation

The script can be installed virtual environment, typically in the irods account on an iRODS server.

If the system doesn't have the virtualenv module yet, install it first: _sudo python3 -m pip install virtualenv_

Now create a virtual environment and install the tool:
- _python3 -m virtualenv venv_
- _source venv/bin/activate_
- _pip3 install ./icat-database-checker_

# Usage

```
usage: icat-database-checker [-h] [--config-file CONFIG_FILE] [-m {human,csv}]
                             [-v]
                             [--run-test {ref_integrity,timestamps,names,hardlinks,minreplicas,path_consistency,indexes,all}]
                             [--min-replicas MIN_REPLICAS]

Performs a number of sanity checks on the iRODS ICAT database

optional arguments:
  -h, --help            show this help message and exit
  --config-file CONFIG_FILE
                        Location of the irods server_config file (default:
                        etc/irods/server_config.json )
  -m {human,csv}        Type of output
  -v                    Verbose mode
  --run-test {ref_integrity,timestamps,names,hardlinks,minreplicas,path_consistency,all}
                        Test to run (default: all)
  --min-replicas MIN_REPLICAS
                        Minimum number of replicas that a dataobject must have
                        (default: 1).
  --data-object-prefix DATA_OBJECT_PREFIX
                        Only check data objects with a particular prefix. The
                        referential integrity and hard links tests do not
                        support this option yet, and will ignore it.

```

By default, the script retrieves the database connection parameters from the iRODS server configuration file.
It is possible to override the server config file location using the --config-file parameter, like so:
_./icat-database-checker --config-file my-server-config.json_ . 

By default, the script only displays (potential) issues.  Use the -v (verbose mode) switch to print additional
information about which checks are performed.
",2022-08-12
https://github.com/UtrechtUniversity/ii,"# ii
ii - Command line utilities for iRODS

## Summary

ii is a collection of commandline utilities for iRODS.
The current version is a functional prototype, in the sense
that the interface is still subject to change and that it
hasn't been tested extensively yet.

## Requirements and installation

ii supports Python 3.6 and higher. It has currently been
tested with iRODS 4.2.7. The eventual goal is to support all
recent iRODS versions.

If you want to try it out, you can install it in a virtualenv:

```
virtualenv --no-site-packages --python /usr/bin/python3 venv
source venv/bin/activate
pip3 install --upgrade git+https://github.com/UtrechtUniversity/ii.git
```

## Commands

### ii cd

Equivalent to the icd command in the iCommands. Changes the current
working directory. If no argument is given, it goes back to the
home directory.

```
usage: ii cd [-h] [--verbose] [directory]

positional arguments:
  directory      Directory to change to

optional arguments:
  -h, --help     show this help message and exit
  --verbose, -v  Print verbose information for troubleshooting
```

### ii find

Finds data objects, and returns names of data objects matching
the specified properties. The basic idea is similar to the Unix
find command.

```
usage: ii find [-h] [--verbose] [--print0] [--dname DNAME]
               [--owner-name OWNER_NAME] [--owner-zone OWNER_ZONE]
               [--resc-name RESC_NAME] [--minsize MINSIZE] [--maxsize MAXSIZE]
               [--size SIZE]
               [queries [queries ...]]

positional arguments:
  queries               Collection, data object or data object wildcard

optional arguments:
  -h, --help            show this help message and exit
  --verbose, -v         Print verbose information for troubleshooting
  --print0, -0          Use 0 byte delimiters between results
  --dname DNAME         Wildcard filter for data object name
  --owner-name OWNER_NAME
                        Filter for data object owner name (excluding zone)
  --owner-zone OWNER_ZONE
                        Filter for data object owner zone
  --resc-name RESC_NAME
                        Filter for data object resource
  --minsize MINSIZE     Filter for minimum data object size (you can
                        optionally use human-readable sizes, like ""2g"" for 2
                        gigabytes)
  --maxsize MAXSIZE     Filter for maximum data object size (you can
                        optionally use human-readable sizes, like ""2g"" for 2
                        gigabytes)
  --size SIZE           Filter for (exact) data object size (you can
                        optionally use human-readable sizes, like ""2g"" for 2
                        gigabytes)
```

### ii ls

Equivalent to the ils command in the iCommands. Lists data objects
or collections.

```
usage: ii ls [-h] [--verbose] [-m {plain,json,csv,yaml}]
             [-s {name,ext,size,date,unsorted}] [-H {default,yes,no}]
             [--recursive] [-l] [-L]
             [queries [queries ...]]

positional arguments:
  queries               Collection, data object or data object wildcard

optional arguments:
  -h, --help            show this help message and exit
  --verbose, -v         Print verbose information for troubleshooting
  -m {plain,json,csv,yaml}, --format {plain,json,csv,yaml}
                        Output format
  -s {name,ext,size,date,unsorted}, --sort {name,ext,size,date,unsorted}
                        Propery to use for sorting
  -H {default,yes,no}, --hr-size {default,yes,no}
                        Whether to print human-readable sizes
                        [yes,no,default].By default, enable human-readable for
                        text output, disable for other formats.
  --recursive, -r       Include contents of subcollections
  -l                    Display replicas with size, resource, owner, date
  -L                    like -l, but also display checksum and physical path
```

### ii pwd

Equivalent to the ipwd command in the iCommands. Prints the current
working directory.

```
usage: ii pwd [-h] [--verbose]

optional arguments:
  -h, --help     show this help message and exit
  --verbose, -v  Print verbose information for troubleshooting
```

## Known limitations

- ls command: sorting only has effect for the contents of collection
  arguments (e.g. `ii ls .`), but not for other arguments
  (e.g. `ii ls ""*""` or `ii ls foo.dat bar.dat baz.dat`).
- ls command: human-readable output is not yet supported for JSON
  and YAML output.
- ls command: this command currently supports dataobject-only wildcards
  (e.g. `*.dat`), but not yet wildcards with collection names (e.g.
  `foo/*.dat` or `*/foo.dat`).
",2022-08-12
https://github.com/UtrechtUniversity/ilab-catalog,"# ilab-catalog
[Ansible](https://docs.ansible.com) scripts for automatic deployment of I-lab Catalog.

## Requirements
### Control machine requirements
* [Ansible](https://docs.ansible.com/ansible/intro_installation.html) (>= 2.4)
* [VirtualBox](https://www.virtualbox.org/manual/ch02.html) (>= 5.1)
* [Vagrant](https://www.vagrantup.com/docs/installation/) (>= 1.9)

### Managed node requirements
* [CentOS](https://www.centos.org/) (>= 7.3)

## Deploying I-lab Catalog development instance

Configure the virtual machine for development:
```bash
vagrant up
```

On a Windows host first SSH into the Ansible controller virtual machine (skip this step on GNU/Linux or macOS):
```bash
vagrant ssh ilab-catalog-controller
cd ~/ilab-catalog
```

Deploy I-lab Catalog to development virtual machine:
```bash
ansible-playbook playbook.yml
```

Add following host to /etc/hosts (GNU/Linux or macOS) or %SystemRoot%\System32\drivers\etc\hosts (Windows):
```
192.168.70.10 ilab-catalog.ckan.test
```

## Upgrading I-lab Catalog instance
Upgrading the I-lab Catalog development instance to the latest version can be done by running the Ansible playbooks again.

On a Windows host first SSH into the Ansible controller virtual machine (skip this step on GNU/Linux or macOS):
```bash
vagrant ssh controller
cd ~/ilab-catalog
```

Upgrade Ansible scripts:
```bash
git pull
```

Upgrade I-lab Catalog instance:
```bash
ansible-playbook playbook.yml
```

## License
This project is licensed under the GPL-v3 license.
The full license can be found in [LICENSE](LICENSE).
",2022-08-12
https://github.com/UtrechtUniversity/ilab-catalog-theme,".. You should enable this project on travis-ci.org and coveralls.io to make
   these badges work. The necessary Travis and Coverage config files have been
   generated for you.

.. image:: https://travis-ci.org/jcid/ckanext-custom_theme.svg?branch=master
    :target: https://travis-ci.org/jcid/ckanext-custom_theme

.. image:: https://coveralls.io/repos/jcid/ckanext-custom_theme/badge.svg
  :target: https://coveralls.io/r/jcid/ckanext-custom_theme

.. image:: https://pypip.in/download/ckanext-custom_theme/badge.svg
    :target: https://pypi.python.org/pypi//ckanext-custom_theme/
    :alt: Downloads

.. image:: https://pypip.in/version/ckanext-custom_theme/badge.svg
    :target: https://pypi.python.org/pypi/ckanext-custom_theme/
    :alt: Latest Version

.. image:: https://pypip.in/py_versions/ckanext-custom_theme/badge.svg
    :target: https://pypi.python.org/pypi/ckanext-custom_theme/
    :alt: Supported Python versions

.. image:: https://pypip.in/status/ckanext-custom_theme/badge.svg
    :target: https://pypi.python.org/pypi/ckanext-custom_theme/
    :alt: Development Status

.. image:: https://pypip.in/license/ckanext-custom_theme/badge.svg
    :target: https://pypi.python.org/pypi/ckanext-custom_theme/
    :alt: License

=============
ckanext-custom_theme
=============

.. Put a description of your extension here:
   What does it do? What features does it have?
   Consider including some screenshots or embedding a video!


------------
Requirements
------------

For example, you might want to mention here which versions of CKAN this
extension works with.


------------
Installation
------------

.. Add any additional install steps to the list below.
   For example installing any non-Python dependencies or adding any required
   config settings.

To install ckanext-custom_theme:

1. Activate your CKAN virtual environment, for example::

     . /usr/lib/ckan/default/bin/activate

2. Install the ckanext-custom_theme Python package into your virtual environment::

     pip install ckanext-custom_theme

3. Add ``custom_theme`` to the ``ckan.plugins`` setting in your CKAN
   config file (by default the config file is located at
   ``/etc/ckan/default/production.ini``).

4. Restart CKAN. For example if you've deployed CKAN with Apache on Ubuntu::

     sudo service apache2 reload


---------------
Config Settings
---------------

Document any optional config settings here. For example::

    # The minimum number of hours to wait before re-checking a resource
    # (optional, default: 24).
    ckanext.custom_theme.some_setting = some_default_value


------------------------
Development Installation
------------------------

To install ckanext-custom_theme for development, activate your CKAN virtualenv and
do::

    git clone https://vcs.jcid.nl/open-data/ckan-universiteit-utrecht-epos-theme.git
    cd ckanext-custom_theme
    python setup.py develop
    pip install -r dev-requirements.txt


-----------------
Running the Tests
-----------------

To run the tests, do::

    nosetests --nologcapture --with-pylons=test.ini

To run the tests and produce a coverage report, first make sure you have
coverage installed in your virtualenv (``pip install coverage``) then run::

    nosetests --nologcapture --with-pylons=test.ini --with-coverage --cover-package=ckanext.custom_theme --cover-inclusive --cover-erase --cover-tests


---------------------------------
Registering ckanext-custom_theme on PyPI
---------------------------------

ckanext-custom_theme should be availabe on PyPI as
https://pypi.python.org/pypi/ckanext-custom_theme. If that link doesn't work, then
you can register the project on PyPI for the first time by following these
steps:

1. Create a source distribution of the project::

     python setup.py sdist

2. Register the project::

     python setup.py register

3. Upload the source distribution to PyPI::

     python setup.py sdist upload

4. Tag the first release of the project on GitHub with the version number from
   the ``setup.py`` file. For example if the version number in ``setup.py`` is
   0.0.1 then do::

       git tag 0.0.1
       git push --tags


----------------------------------------
Releasing a New Version of ckanext-custom_theme
----------------------------------------

ckanext-custom_theme is availabe on PyPI as https://pypi.python.org/pypi/ckanext-custom_theme.
To publish a new version to PyPI follow these steps:

1. Update the version number in the ``setup.py`` file.
   See `PEP 440 <http://legacy.python.org/dev/peps/pep-0440/#public-version-identifiers>`_
   for how to choose version numbers.

2. Create a source distribution of the new version::

     python setup.py sdist

3. Upload the source distribution to PyPI::

     python setup.py sdist upload

4. Tag the new release of the project on GitHub with the version number from
   the ``setup.py`` file. For example if the version number in ``setup.py`` is
   0.0.2 then do::

       git tag 0.0.2
       git push --tags
",2022-08-12
https://github.com/UtrechtUniversity/irods-consistency-check,"# irods-consistency-check
ichk (iRODS consistency check) performs a consistency check between data object information in
the iCAT catalog and files in unixfilesystem vaults on an iRODS server. It must be installed locally on the
server.

It can run in three modes:
- In resource mode, a consistency check is performed for every registered data object on a local resource.
- In vault mode, a local unixfilesystem vault is scanned. A consistency check is performed for every file in the vault.
  This mode will detect files that are present in the vault, but not registered in the iCAT database, whereas such files
  are ignored in resource mode.
- In object list mode, a list of data objects is read from a file. A consistency check is performed for all local
  replicas of these data objects. This mode can be used to check whether an iRODS server has valid replicas
  of a particular set of data objects.

Ichk can use either a human-readable output format, or comma-separated values (CSV).

## Requirements
- iRODS >= v4.2.x
- Python 3.6+

## Installation
This project contains a setup.py file which supports Python 3.6+ environments.
Installation is easiest with pip. Just run the following commands:

```bash
virtualenv --no-site-packages default
. default/bin/activate
pip3 install git+https://github.com/UtrechtUniversity/irods-consistency-check.git@v2.0.0
```

When using a virtual environment, make sure that the iRODS system user has access to this environment.

## Usage
When the installation was successful, the ichk command will be available.
It extracts the credentials and iRODS settings from the iRODS environment file of the current user.
This environment file can be (re-)created with the iinit command.
This user should also have access to the files in the vault path directly.

The command line switches are displayed below:
```
usage: ichk [-h] [-f FQDN]
            (-r RESOURCE | -v VAULT | -l DATA_OBJECT_LIST_FILE | --all-local-resources | --all-local-vaults)
            [-o OUTPUT] [-m {human,csv}] [-t TRUNCATE] [-T TIMEOUT]
            [-s ROOT_COLLECTION]

Check consistency between iRODS data objects and files in vaults.

optional arguments:
  -h, --help            show this help message and exit
  -f FQDN, --fqdn FQDN  FQDN of resource
  -r RESOURCE, --resource RESOURCE
                        iRODS path of resource
  -v VAULT, --vault VAULT
                        Physical path of the resource vault
  -l DATA_OBJECT_LIST_FILE, --data-object-list DATA_OBJECT_LIST_FILE
                        Check replicas of a list of data objects on this
                        server.
  --all-local-resources
                        Scan all unixfilesystem resources on this server
  --all-local-vaults    Scan all vaults of unixfilesystem resources on this
                        server
  -o OUTPUT, --output OUTPUT
                        Write output to file
  -m {human,csv}, --format {human,csv}
                        Output format
  -t TRUNCATE, --truncate TRUNCATE
                        Truncate the output to the width of the console
  -T TIMEOUT, --timeout TIMEOUT
                        Sets the maximum amount of seconds to wait for server
                        responses, default 600. Increase this to account for
                        longer-running queries.
  -s ROOT_COLLECTION, --root-collection ROOT_COLLECTION
                        Only check a particular collection and its
                        subcollections.

```

You need to supply either a resource, a vault path, a data object list, the --all-local-resources option
or the --all-local-vaults option.

The FQDN (fully qualified domain name) defaults to the FQDN of the current machine.

When composable resources are used, the ichk command will scan for leaf resources starting from the given resource.

## Output

The objects that are checked are categorized as follows:
* COLLECTION
* DATAOBJECT
* DIRECTORY
* FILE

These status codes can be used to represent the result of the check:
* `OK`
* `NOT_EXISTING`:  This object is found in the iRODS catalog, but is missing in the vault path.
* `NOT_REGISTERED`:  This object is found on the disk, but is missing from the iRODS catalog.
* `FILE_SIZE_MISMATCH`:  The object has another file size than registered in the iRODS catalog.
* `CHECKSUM_MISMATCH`:  This object does not have the same checksum as registered in the iRODS catalog.
* `ACCESS_DENIED`:  The current user has no access to this object in the vault path.
* `NO_CHECKSUM`:  There is no checksum registered in the iRODS catalog. This implies that file sizes do match.
* `NO_LOCAL_REPLICA`: No replica of data object present on server (only used for object list check)
* `NOT_FOUND`: Object name not found in iRODS (only used for object list check)
* `REPLICA_IS_STALE` : Replica is stale (i.e. out of date)


The meaning of the fields in CSV output is:
1. Object type
2. Status code
3. Logical path
4. Vault path
5. Observed checksum value (field is empty for collections / directories, as well as for files / data objects with a size mismatch)
6. Expected checksum value (field is empty for collections / directories, as well as for files / data objects with a size mismatch)
7. Observed file size (field is empty for collections / directories)
8. Expected file size (field is empty for collections / directories)
9. Resource name
",2022-08-12
https://github.com/UtrechtUniversity/irods-Csharp,"# irods-Csharp

## Installation
1. Download the repository .
2. Build the solution (using visualstudio).
3. Add a reference to irods-Csharp.dll in your solution.
4. Done.

## Usage

### Session startup

For any action that the user wishes to perform on the IRODS server, a session object is required.\
The session contains the following objects that contain operations corresponding to that category:

* IrodsSession.Collections
* IrodsSession.DataObjects
* IrodsSession.Queries
* IrodsSession.Meta

#### Authentication

When starting a new session, authentication parameters are required to authenticate with the server.\
The parameters required are:
* host : string - Domain name of server host
* port : int - Port on which server can be reached
* home : string - Collection name of home directory
* user : string - User name of user
* zone : string - Zone name of server
* scheme : string - Authentication scheme to be used (Currently only “pam” is supported)

When a session has been created, the session needs to be set up using the users password. The setup method will return a hashed password.
This hashed password can then be used to start the session after which the session is ready to be used.

```csharp
// Setting up a session using login parameters
IrodsSession session = new IrodsSession(""host.rods.nl"", 9999, ""/nlex1/home"", ""exampleuser@rods.com"", ""nlex1"",""pam"", 24);
// Getting hashed password
string hashedPassword = session.Setup(""secretPassword"");
// Starting session
session.Start(hashedPassword);
```

### Using Collections

Methods regarding collections are located within the collection manager of the session : IrodsSession.Collections\
It contains the following methods:
* Rename - Rename a collection
* Open - Returns a collection object for the specified path
* Create - Creates new collection at specified path
* Remove - Removes collection at specified path

From the session, a new collection can be created, after which it can be opened.\
When opening a collection, a Collection object is returned which can be used to perform actions inside the corresponding collection

```csharp
// Creating a new collection inside a directory which already exists
session.Collections.Create(""exampleDir/newCollection"");

// Opening this collection
Collection newCollection = session.Collections.Open(""exampleDir/newCollection"");

// Creating collection within this collection
newCollection.CreateCollection(""deeperCollection"");

// This collection can be renamed
newCollection.RenameCollection(""deeperCollection"", ""deeperCollectionV2"");

// Removing this new collection, this time from session
session.Collections.Remove(""exampleDir/newCollection/deeperCollectionV2"");
```

#### The Collection class

Collection objects house almost every method available within the iRods_Csharp library. The advantage of using a collection object is that the methods that it contains will be executed using the collection’s path as location for executing the method.
For example, when creating a new collection using session.Collections, one would need to pass two parameters, the path of the collection where the new collection should be created and the name of the new collection.
When using a collection object, only the name of the new collection is needed, the collection is created inside of the collection object’s path.

The collection class also houses some methods not available in session.Collections:
* Collection.ChangeDirectory - Changes collection directory, as one would do using cd
* Collection.Rename - Changes the current collection’s name
* Collection.PrintDirectory - Returns string of the currents collections’ path
* Collection.Meta - Returns array of metadatas attached to this collection

From a Collection object, the following methods are available which perform methods from IrodsSession.DataObjects using its own path as starting point:
* Collection.OpenCollection
* Collection.RenameCollection
* Collection.CreateCollection
* Collection.RemoveCollection

### Using Data Objects

Methods regarding data objects are found within the DataObject manager of the session : IrodsSession.DataObjects\
It contains the following methods:
* Open - Returns DataObject object for the specified path and name (requires filemode Read, Write or ReadWrite from Options.FileMode enum)
* Create - Creates new data object in specified path
* Write - Writes byte array to specified file
* Read - Returns byte array with content of specified file
* Remove - Removes specified file

#### The DataObj Class

When opening a data object with IrodsSession.DataObjects.Open, a DataObj Object is returned. This object contains the following methods:
* DataObj.Write - Write byte array to this file
* DataObj.Insert - Write byte array at the beginning of this file
* DataObj.Seek - Offset the file pointer from start, current offset or end (seekmode)
* DataObj.Read - Returns byte array with contents of this file
* DataObj.Remove - Removes this file
* DataObj.Meta - Returns an array of metadatas attached to this data object

#### Access from Collection Object

From a collection object, the following methods are available which perform methods from IrodsSession.DataObjects using its own path as starting point:
* Collection.OpenDataObj
* Collection.RenameDataObj
* Collection.CreateDataObj
* Collection.WriteDataObj
* Collection.ReadDataObj
* Collection.RemoveDataObj

Below is an example of the Data Object Functionality

```csharp
// Creating a new Data object inside a pre-existing directory
session.DataObjects.Create(""exampleDir/newObject.txt"");

// Opening this new data object
DataObj newObject = session.DataObjects.Open(""exampleDir/newObject.txt"", Options.FileMode.ReadWrite);

// Writing data to this new file
newObject.Write(File.ReadAllBytes(""myPc/exampleFile.txt""));

// Writing file contents to console directly from session this time
Console.Write(Encoding.UTF32.GetString(session.DataObjects.Read(""exampleDir/newObject.txt"")));
```

### Adding Metadata

Metadata can be added to Collections or Data Objects using the MetaManager in IrodsSession: IrodsSession.Meta\
It contains the following methods:
* AddMeta - Adds meta Name, Value and (optionally) Units triple
* RemoveMeta - Removes meta Name, Value and (optionally) Units triple

Units are optional. However, they won’t stack.\
Adding triple <name,value,1> to an object which already has a tag <name,value,2> will result in the object having both tags attached to it as unique tags.
To use these methods, the user needs to supply either a DataObj or Collection Object and the meta values, or call the methods directly from these objects.

Below is a small example of adding metadata to collections and data objects

```csharp
// Adding metadata to newCollection from before, without units
session.Meta.AddMeta(newCollection, ""metaName"", ""metaValue"");

// Adding metadata directly, this time also with some units
newCollection.AddMeta(""metaName"", ""metaValue"", 8);

// The following method will return an array with two meta objects: <metaName, metaValue> and <metaName, metaValue, 8>
Meta[] metadata = newCollection.Meta();

// Metadata tags can also be removed, but the values need to be an exact match, even the units, as these do not aggregate
newCollection.RemoveMeta(""metaName"", ""metaValue"", 8);
```

### Performing queries

Methods to query data are found in the query manager of the IrodsSession : IrodsSession.Queries\
It contains the following methods:
* QueryCollection - query collections based on name
* MQueryCollection - query collections based on metadata
* QueryObj - query objects based on name
* MQueryObj - query objects based on metadata
* QueryMeta - query meta tags for collection or data objects

When using the QueryCollection and QueryObj methods, the a name and a path need to be provided. The query will search for all collections or data objects within that path or deeper that contain the provided name in their path.
For example, consider the path “home/collection” and name “example”

Executing the method
```csharp
session.Queries.QueryCollection(“home/collection”, “example”);
```
Might return an array of collections with the following paths:
* “home/collection/example”
* “home/collection/abcexample12”
* “home/collection/example/nestedCollection”

#### Queries based on metadata

MQueryCollection and MQueryObj will perform a query to find collections or data objects which are tagged with metadata with the specified values.
It is not required to specify all three parts of the metadata triple.

For example, one could query all objects with a specific metadata triple
```csharp
session.Queries.MQueryObj(“path”, “name”,”value”,1);
```
But if the user would like to query all objects which have metadata that matches a certain metadata value, then the user would need to specify it wants to query based on metadata value:
```csharp
session.Queries.MQueryObj(“path”, metaValue : “value”);
```

#### Querying metadata

Querying metadata attached to a collection or data object can be done either by calling IrodsSession.Queries.QueryMeta using the path, name and type of the object (e.g. -c or -d), or by calling the .Meta() function of a Collection or Data Object object:
Collection.Meta() and DataObj.Meta()

#### Access from Collection object

As with most methods that have a path as paramenter, the query methods can also be called directly from a collection object. The following methods are supported:
* Collection.QueryCollection
* Collection.MQueryCollection
* Collection.QueryObj
* Collection.MQueryObj

Below is a code snippet showing the query functionality

```csharp
// Query all collections within /example/dir with a name that contains the word ""apple""
Collection[] collections = session.Queries.QueryCollection(""/example/dir"", ""apple"");

// A new query could be performed on the first of this collections, this time querying a data object
// This particular query will find all .txt files within the collection
DataObj[] objects = collections[0].QueryObj("".txt"");

// It is also possible to query based on meta data
DataObj[] objects2 = session.Queries.MQueryObj(""/example/dir"", ""color"", ""red"", 5);

// Not all meta data triple values need to be specified however,
// this wil query all collections within newCollection
Collection[] collections2 = newCollection.MQueryCollection(metaValue: ""red"");

// Then it is also possible to get all the meta triples attached to a collection
Meta[] collectionMeta = collections2[0].Meta();
```

### Logging

There are two TextWriters in the Utility class which are set to null by default:
Utility.Log and Utility.Text
Utility.Log is used by the library to write debugging data such as messages that are sent to and received from the server, Utility.Text can be used to write a line with a console color by using the Utility.WriteText method.\
For example, to make sure the library writes debug data to the console, simply set the logger:
```csharp
Utility.Log = Console.Out;
```

## To-do
* Multiple parallel connections
* Other authentication schemes next to PAM
* Query parser to allow more customized queries

## Known bugs
* No bugs detrimental to normal use are known at the moment.
* If any bugs are found, please report this using the contact information below

## Contact info
Any question or problems regarding this library can be mailed to h.fidder2@students.uu.nl",2022-08-12
https://github.com/UtrechtUniversity/irods-offline-install,"# irods-offline-install

This is a set of scripts for creating a local repository of iRODS packages and their dependencies.
It can be used for installing iRODS on offline computers, for example using a USB stick.

The basic idea is that you first run a script to create an archive file with
a local repository of iRODS packages on an Internet-connected system.
The archived local repository can then be copied to one or more offline systems in order
to install the iRODS packages without access to Internet.

This assumes that the Internet-connected system is in the same state as the offline system(s).
Differences in installed packages between the online system and the offline system(s) might
result in an incomplete local repository, or other issues with installing the packages on the
offline systems.

The scripts have only been tested on Ubuntu 18.04 LTS.

# Prerequisites

In order to test the scripts locally using Vagrant, you will need [VirtualBox](https://www.virtualbox.org/wiki/Downloads), as well as [Vagrant 2.x](https://www.vagrantup.com/downloads.html) with these plugins:
- SCP plugin: _vagrant plugin install vagrant-scp_
- Env plugin: _vagrant plugin install vagrant-env_

# Usage

The default configuration installs the latest version of iRODS, as well as the Postgresql database plugin, Python rule engine, and the
Postgresql database. If you'd like to customize the list of packages to be installed, adjust the APT_PACKAGES variable in _local-repo.env_

First create the archive of the local repository on an Internet-connected system:
- Copy the _local-repo.env_ file and the _create-local-repo.sh_ script to the online system.
- If the system does not have a fresh OS install, you might want to check for any irrelevant .deb files in /var/cache/apt/archives that have been downloaded previously. Remove them, so they don't end up in the local repository.
- Run the _create-local.repo.sh_ script with the location of the _local-repo.env_ file as an argument, e.g. _./create-local-repo.sh /home/user/local-repo.env_. The argument can be omitted, if the _local-repo.env_ file is in the current working directory.
- The script should create an _offline_repo.tar.gz_ file in the root directory.

Now install the packages on an offline system:
- Copy the _offline_repo.tar.gz_ file from the online system to the offline system, as well as the _local-repo.env_ file.
- Copy the _offline-install/install-offline.sh_ script from the repository to the offline system.
- Now install the packages from the local repository: _./install-offline.sh offline_repo.tar.gz local-repo.env_. Adjust the location of the repository tarball and the environment file if they aren't in the working directory.
- In order to use iRODS, it is necessary to initialize the database and run the setup script. Please consult the _Installing iRODS_ section of the [iRODS beginner training](https://github.com/irods/irods_training/blob/master/beginner/irods_beginner_training_2019.pdf) for details.

# Testing the scripts using Vagrant

- If you'd like to customize the packages to be installed, adjust the the APT_PACKAGES variable in _local-repo.env_.
- Run _./prepare-offline-repo.sh_ . This script creates a VM and downloads the required iRODS packages, as well as their dependencies. It copies a tarball containing a local apt repository with the packages to the _offline-install_ directory.
- Start the test VM: _cd test-offline-install && vagrant destroy -f && vagrant up_. Vagrant automatically configures the local iRODS repository and installs the iRODS packages.
- You can log in on the test VM using _vagrant ssh_ to check that the packages have been installed correctly. In order to use iRODS, it is necessary to initialize the database and run the setup script. Please consult the _Installing iRODS_ section of the [iRODS beginner training](https://github.com/irods/irods_training/blob/master/beginner/irods_beginner_training_2019.pdf) for details.

# License

MIT License - see LICENSE file

# Author

Sietse Snel, Utrecht University

# Acknowledgements

Thanks to Tony Anderson, Rob van Schip and Terrell Russell for their feedback on earlier
versions of these scripts.
",2022-08-12
https://github.com/UtrechtUniversity/irods-ruleset-research,"This repository is archived and not actively maintained.

Research module Rules and Policies for Yoda/iRODS
==========

NAME
----
irods-ruleset-research - Subset of rules and policies for iRODS based Yoda required for the yoda-portal-research module

DESCRIPTION
-----------
Yoda is configuration of an iRODS server for dataset management.
These rules are required on top of the rules-uu rules to use the Yoda Portal research module designed.

It consists of:
- Research module specific rules and policies

DEPENDENCIES
------------
- [irods-ruleset-uu](https://github.com/UtrechtUniversity/irods-ruleset-uu)
- [irods-uu-microservices](https://github.com/UtrechtUniversity/irods-uu-microservices)
- All dependencies of the above

INSTALLATION
-----------
1) On the iRODS server, become the iRODS user and navigate to ``/etc/irods``

2) Clone the repository.

3) Navigate to the directory this repository was cloned in

4) Checkout the target branch.

5) Use the make file to compile and install the rules: ``make install``

6) Add the generated `rules-research.re` (as well as the requisite `rules-uu.re`) to the `server_config.json` the _re_rulebase_set_, above `core` in `/etc/irods/server_config.json`:

```javascript
""re_rulebase_set"": [
    {  ""filename"":  ""rules-research"" },
    {  ""filename"":  ""rules-uu"" },
    { ""filename"": ""core""}
]
```

7) Install the default schema and formelements for the metadata form. The user needs to be a rodsadmin. If the default target resource ""irodsResc"" does not exist, add a *resc parameter.
```
$ irule -F ./tools/install-default-xml-for-metadata.r
# or
$ irule -F ./tools/install-default-xml-for-metadata.r '*resc=""demoResc""'
```

8) Configure a cronjob under a rodsadmin account to copy datapackages to the vault. Example line for crontab -e:

```
*/2 * * * * /bin/irule -F /etc/irods/irods-ruleset-research/tools/copy-accepted-folders-to-vault.r >>$HOME/iRODS/server/log/job_copy-accepted-folder-to-vault.r 2>&1
```

LICENSE
-------
This project is licensed under the GPLv3 license.

The full license can be found in [LICENSE](LICENSE).

AUTHORS
-------
- [Paul Frederiks](https://github.com/pfrederiks)
- [Jan de Mooij](https://github.com/ajdemooij)
- [Lazlo Westerhof](https://github.com/lwesterhof)
",2022-08-12
https://github.com/UtrechtUniversity/irods-ruleset-youth-cohort,"This repository is archived and not actively maintained.

Moved to: https://github.com/UtrechtUniversity/irods-ruleset-uu
",2022-08-12
https://github.com/UtrechtUniversity/irods-sudo-microservices,"iRODS Sudo Microservices
========================

The Sudo microservices empower users to execute operations typically
reserved for iRODS administrators. Combined with application of the
iRODS programmable policies they facilitate fine-grained delegation of
authority.  The supported operations cover managing users and groups,
metadata and ACLs.

See
[Documentation for individual microservices](#documentation-for-individual-microservices)
for the list of microservices and their parameter specification.


## Download ##

We distribute RPM packages for iRODS 4.2.2, 4.2.1 and iRODS 4.1.8:

- [`irods-sudo-microservices-4.2.2_1.0.0`](https://github.com/UtrechtUniversity/irods-sudo-microservices/releases/tag/4.2.2_1.0.0)
- [`irods-sudo-microservices-4.2.1_1.0.0`](https://github.com/UtrechtUniversity/irods-sudo-microservices/releases/tag/4.2.1_1.0.0)
- [`irods-sudo-microservices-4.1.8_0.2.0`](https://github.com/UtrechtUniversity/irods-sudo-microservices/releases/tag/0.2)

The left side (4.2.2) of the version number indicates the compatible
iRODS server version. The right side (1.0.0) is the major/minor/patch
version of the microservices themselves.

We do not currently package for Debian/Ubuntu based systems. Please let
us know if you need a .DEB package by
[creating an issue](https://github.com/UtrechtUniversity/irods-sudo-microservices/issues/new).

## Installation ##

Sudo microservices can be installed using the RPMs provided on the
[releases page](https://github.com/UtrechtUniversity/irods-sudo-microservices/releases/).

You can also build the microservices yourself, see
[Building from source](#building-from-source).

## Security Considerations ##

Sudo Microservices provide a way for normal iRODS users to perform
normally restricted operations. The administrator-defined policy rules
are the only barrier that can perform authorization and check the
validity of each msi call. Misconfiguration can result in security
breaches.

With that in mind, we have a few recommendations for policy
implementors:

- Keep policy rules concise and readable
- Document each condition extensively
- Use a whitelist approach instead of a blacklist approach

## Configuration / Policy implementation ##

By default, when no policies have been defined, access to the sudo
microservices is denied to all users.
By implementing pre- and postproc policy rules, you can selectively
grant access.

An example policy ruleset is provided in `policies.re`, which is
installed in `/etc/irods/sudo-default-policies.re`. This file can be
added to the ruleset list in the iRODS server config json.
You can use this file as a template for your policy
implementations.

Every microservice has its own pre- and postproc policy rule. The naming
scheme follows this example:

    msiSudoUserAdd() has policy rules acPreSudoUserAdd() and acPostSudoUserAdd()

The pre and post rule receive the same set of parameters that are
passed to the microservice itself. Additionally, they can make
decisions based on session variables like `$userNameClient` and
`$rodsZoneClient`.

As an example, the following preproc rule for msiSudoGroupAdd
restricts the operation to a single user with a specific name:

```
acPreSudoGroupAdd(*groupName, *initialAttr, *initialValue, *initialUnit, *policyKv) {
    if (""$userNameClient#$rodsZoneClient"" == ""piet#tempZone"") {
        succeed;
    } else {
        fail;
    }
}
```

### Logging ###

You can log msi execution similar to how you can log other iRODS
operations that use policy enforcement points:
Create a pre- rule for each microservice, and make it log and fail:

```
acPreSudoGroupAdd(*groupName, *initialAttr, *initialValue, *initialUnit, *policyKv) {
    writeLine(""serverLog"", ""In acPreSudoGroupAdd, group is <*groupName>, actor is <$userNameClient#$rodsZoneClient>"");
    fail;
}
```

Make sure this pre- rule is executed before any other implementations
of the same policy, for example by adding it to a ruleset that is
loaded before the ruleset in which authorization takes place.

If you do not want to have multiple implementations of the pre rule,
simply insert the logging calls into the authorizing rule.

## Calling Sudo Microservices ##

A microservice call will succeed (return status 0) only if its 'pre-'
rule, the operation itself *and* the post rule succeed.
If any of these three parts fail the microservice is failed
immediately and an error code is returned.

## Documentation for individual microservices ##

Every microservice has a `*policyKv` as the last parameter. This
parameter can be used to pass extra information to the policy pre- and
post- rules. The microservices themselves do not use this parameter.

If you do not make use of this feature, you can pass an empty kv list
or an empty string `""""` in its place.

### User and group management ###

####  `msiSudoUserAdd(*userName, *initialAttr, *initialValue, *initialUnit, *policyKv)` ####

Creates a new iRODS user of type `rodsuser`. The `*initialAttr`,
`*initialValue` and optionally `*initialUnit` are applied as metadata
to the user if they are not empty.

#### `msiSudoUserRemove(*userName, *policyKv)` ####

Removes the given iRODS user.

#### `msiSudoGroupAdd(*groupName, *initialAttr, *initialValue, *initialUnit, *policyKv)` ####

Creates a new iRODS group of type `rodsgroup`. The `*initialAttr`,
`*initialValue` and optionally `*initialUnit` are applied as metadata
to the user if they are not empty.

#### `msiSudoGroupRemove(*groupName, *policyKv)` ####

Removes the given iRODS group.

#### `msiSudoGroupMemberAdd(*groupName, *userName, *policyKv)` ####

Adds a user to a group.

#### `msiSudoGroupMemberRemove(*groupName, *userName, *policyKv)` ####

Removes a user from a group.

### ACL operations ###

#### `msiSudoObjAclSet(*recursive, *accessLevel, *otherName, *objPath, *policyKv)` ####

Modifies ACLs on data objects and collections.

`*recursive` can be either the string `""recursive""` to apply the change
recursively, or an empty string `""""` to modify only the given object.

`*accessLevel` can be one of `null`, `read`, `write`, `own`, `inherit`
or `noinherit` similar to the parameters for `ichmod`.

When `*accessLevel` is not `inherit` or `noinherit`, the `*otherName`
must be filled with the user or group name whose access to the given
object will be changed.

### Metadata operations ###

In all metadata operations `*objType` indicates the type of object. Its possible values are the same as for `imeta`:

- `-d` = data object
- `-C` = collection
- `-u` = user or group
- `-R` = resource

#### `msiSudoObjMetaSet(*objName, *objType, *attribute, *value, *unit, *policyKv)` ####

Similar to `imeta set`:
Set an AVU on an object.

If the given `*attribute` already exists, it is overwritten with the
new `*value` and `*unit`.

#### `msiSudoObjMetaAdd(*objName, *objType, *attribute, *value, *unit, *policyKv)` ####

Similar to `imeta add`:
Add an AVU to an object. The given AVU combination must not already
exist for the given object.

#### `msiSudoObjMetaRemove(*objName, *objType, *wildcards, *attribute, *value, *unit, *policyKv)` ####

Similar to `imeta rm(w)`:
Remove metadata from an object.

`*wildcards` indicates whether `%` characters in AVU parameters should
be interpreted as wildcards. The value of this parameter can be either
the string `""wildcards""` or an empty string `""""`.

## Building from source ##

To build from source, the following build-time dependencies must be
installed:

- `cmake`
- `make`
- `irods-devel`
- `irods-externals-clang3.8-0`
- `irods-externals-clang-runtime3.8-0`
- `rpmdevtools` (if you are creating an RPM)

Follow these instructions to build from source:

- First, browse to the directory where you have unpacked the source
  distribution.

- Check whether your umask is set to a sane value. If the output of
  `umask` is not `0022`, run `umask 0022` to fix it. This is important
  for avoiding conflicts in created packages later on.

- Create and generate a build directory:

```bash
mkdir build
cd build
cmake ..
```

- Compile the project

```bash
make
```

Now you can either build an RPM or install the project without a package
manager.

**To create a package:**

```bash
make package
```

That's it, you should now have an RPM in your build directory which you
can install using yum.

**To install without creating a package**

```bash
make install
```

This will install the `.so` files into the microservice plugin
directory.

## Bugs and ToDos ##

Please report any issues you encounter on the
[issues page](https://github.com/UtrechtUniversity/irods-sudo-microservices/issues/).

## Authors ##

- [Chris Smeele](https://github.com/cjsmeele)

## Contact information ##

For questions or support, contact Chris Smeele or Ton Smeele either
directly or via the
[Utrecht University RDM](http://www.uu.nl/en/research/research-data-management/contact-us)
page.

## License ##

Copyright (c) 2016, 2017, Utrecht University.

Sudo Microservices is licensed under the GNU Lesser General Public
License version 3 or higher (LGPLv3+). See the COPYING.LESSER file for
details.
",2022-08-12
https://github.com/UtrechtUniversity/irods-uu-microservices,"# iRODS UU Microservices
Miscellaneous iRODS microservices developed or modified by Utrecht University.

## Included microservices
Developed at Utrecht University:
  * msiStrToUpper: Returns an uppercase string
  * msiSetUpperCaseWhereQuery: Set the UPPERCASE flag on a irods query.

Developed at Donders Institute:
  * msi\_json\_objops: get, add and set values in a json object
  * msi\_json\_arrayops: get, add and set values in a json array. modified to handle arrays of arrays

  The msi\_json\_arrayops and msi\_json\_objops microservices are derived from
  ork from the Donders Institute. The license in DONDERS-LICENSE applies

Developed at Maastricht University:
  * msi\_add\_avu: Microservice to add AVU
  * msi\_rmw\_avu: Microservice to remove AVU

## Installation
iRODS UU microservices can be installed using the packages provided on the
[releases page](https://github.com/UtrechtUniversity/irods-uu-microservices/releases).

You can also build the microservices yourself, see [Building from source](#building-from-source).

## Building from source
To build from source, the following build-time dependencies must be installed:

- `make`
- `gcc-c++`
- `irods-devel`
- `irods-externals-cmake3.5.2-0`
- `irods-externals-clang6.0-0`
- `boost-devel`
- `boost-locale`
- `openssl-devel`
- `libcurl-devel`
- `libuuid-devel`
- `jansson-devel`
- `rpmdevtools` (if you are creating an RPM)

```
sudo yum install make gcc-c++ irods-devel irods-externals-cmake3.5.2-0 irods-externals-clang6.0-0 boost-devel boost-locale openssl-devel libcurl-devel jansson-devel libuuid-devel rpmdevtools
```

Follow these instructions to build from source:

- First, browse to the directory where you have unpacked the source
  distribution.

- Check whether your umask is set to a sane value. If the output of
  `umask` is not `0022`, run `umask 0022` to fix it. This is important
  for avoiding conflicts in created packages later on.

- Compile the project
```bash
export PATH=/opt/irods-externals/cmake3.5.2-0/bin:$PATH
cmake .
make
```

Now you can either build an RPM or install the project without a package manager.

**To create a package:**
```bash
make package
```

That's it, you should now have an RPM in your build directory which you can install using yum.

**To install without creating a package**
```bash
make install
```

This will install the `.so` files into the microservice plugin directory.
",2022-08-12
https://github.com/UtrechtUniversity/iswitch,"# iswitch
easy switching between iRODS iCommands configurations

iswitch is a script for managing iRODS iCommands configurations. It is
intended for situations when you need to be able to switch easily between
multiple configurations, for example if you need to work with multiple
nonfederated zones or with multiple user accounts. The advantage of using
iswitch is that you can switch between configurations with a single short
command (e.g. `iswitch to production`), rather than having to adjust
environment variables and log in again after every configuration switch.

## Installation

iswitch requires Python 3. It has been tested with Python 3.6.x and higher.
If your Linux distribution doesn't include Python 3, you'll need to install it
first. Example command for CentOS 7:

```
sudo yum install python3
```

Put the iswitch script somewhere in your path. For example:

```
sudo install -m 0755 -g root -o root iswitch /usr/local/bin/iswitch
```

Then initialize iswitch with the current iCommands configuration:

```
iswitch init
```

### Customizing the shell prompt

If you'd like your shell prompt to show the name of your present iRODS configuration,
you can add a call to iswitch in your prompt definition in ~/.bashrc or ~/.bash\_profile
(depending on your Linux distribution). For example:

```
PS1=""[\u@\h:\w \$(/usr/local/bin/iswitch which)] $ ""
```

The new prompt will become active when you open a new shell/terminal.

### Command-line completion

If you'd like to have command-line completion for iswitch, first ensure that your
distribution has command line completion installed. Example for CentOS 7:

```
sudo yum -y install epel-release
sudo yum -y install bash-completion
```

Then install the completion script for iswitch:

```
install -m 0644 -o root -g root iswitch-complete.sh /etc/bash_completion.d/iswitch
```

## Usage

Use `iswitch list` to print a list of available configurations, and `iswitch which` to see just the current
configuration.

use `iswitch to CONFIGNAME` to switch to a different configuration. For example: `iswitch to testenvironment`

Use `iswitch add CONFIGNAME FILENAME` to add a configuration. For example:
`iswitch add production irods_environment.production.json`.

Use `iswitch update CONFIGNAME FILENAME` to update a configuration. For example: `iswitch update production updated_config.json`.

Use `iswitch rm CONFIGNAME` to remove a configuration. For example: `iswitch rm testenvironment`.

Use `iswitch clone NEWCONFIGNAME` to clone the current configuration to a new configuration. For example:
`iswitch clone copyofproduction`

Use `iswitch edit` to open the current configuration in an editor. If a default editor has been defined in the EDITOR
environment variable, that one is used. Otherwise vi/vim is used.

Use `iswitch mv SOURCECONFIG DESTINATIONCONFIG` to move or rename a configuration. If you intend to move the current configuration
or overwrite the current configuration, please switch to another config first.
",2022-08-12
https://github.com/UtrechtUniversity/KI1V14005-Tutoraat-KI-Basis,"# KI1V14005 - Tutoraat KI, Basis

This repository hosts the source files for the guideline for tutors and mentors in our first-year tutoring program for artificial intelligence `KI1V14005 - Tutoraat KI, Basis`.

A compiled PDF can be found under `Releases`.

Contributions (PRs, issues, etc.) very welcome, especially from current mentors, tutors, or students who happen to find this.

# Dependencies

To compile, you need `GNU make`, `pandoc`, some standard latex install (`latex-core` should do), and my `uureport`-class (<https://github.com/jkorb/uureport>). 
`uureport` is added as a git submodule for compilation/convenience, so just clone with `git clone --recursive` to get it copied into `src/tex/uureport`.

# License

<a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by/4.0/88x31.png"" /></a><br />This work is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/"">Creative Commons Attribution 4.0 International License</a>.
",2022-08-12
https://github.com/UtrechtUniversity/labdatasync,"![icon](https://raw.githubusercontent.com/UtrechtUniversity/labdatasync/RC1/icons/win_incon.ico) 

# labdatasync (package *labsync*)

Python code to synchronise/clean up lab data after check that all data resides on server.

------------------------------------------------------------------------------------------------------
# Upload data unknown in vault

Get (new) data from lab computers to server using WebDAV protocol.

# Delete data known in vault

Using checksums, figure out which data is already present at remote location and then delete data from workstation.
See the packages, especially *sync.py* for more specifics, like time delta's for reuploads and deletions.

Safekeeping of the algorithm/flow suggested by Ton Smeele:

# Basic algorithm for workstation labsync code
1. Walk tree of directories
  * for file in tree:
     * calculate checksum of file
  * list CHECKSUM_FILEPATH
  * sort list on checksum
  
2. Download checksum file from Remote location
  * sort list in the same way
  
3. Compare lists --> find items that can be deleted)
4. (Prompt user to) Delete lab data files. 
5. Prompt user to warn when there were any errors (Wally ascii art.)

# Implementation conventions for names/files
Hashtype: MD5 ('md5') or SHA256 ('sha2') [checksums](https://en.wikipedia.org/wiki/Hash-based_message_authentication_code)  

Format becomes: HASHTYPE HASHKEY (optional filename) FILESIZE

Thus, for example a line looks like this:

> 'md5 xf7bc83f430538424b13298e6aa6fb143ef4d59a14946175997479dbc2d1a3cd8 IM_optional.file 42424242'

See also: Yoda_indexfile_format.txt

# Dependencies

- Python3 (Py > 3.3) Tested with 3.5.1 (OSX 10.9.2) and older parts with 3.4.3 (Ubuntu 14.04 LTS) 
- numpy (for numpy record array database results)
- easywebdav (use [Julia's Fork](https://github.com/JuliaPython/easywebdav))
- sqlite3 (Not always correctly installed by default python installation, check it!)

# Soft dependencies
These modules are recommended, but not very important for main functionality 
- pdoc (for nice documentation generation) 
- ipython (for nice interactive shell)

## Setting up a Mac in the lab 

For Mac lab computers (AKA Workstations) there are some things to take into account:
- Some sort of python is by default installed and used on OSX, whatever you do, don't mess with that python version. It's best to setup a completely isolated Python environment. I chose to install Homebrew (http://brew.sh) and set up Python 3 from there, there are of course alternatives.  

For setting up homebrew I followed this instruction (http://coolestguidesontheplanet.com/setting-up-os-x-mavericks-and-homebrew/)

Other requirements then are:

- Xcode + command line tools (App store, annoying by the way)
- Homebrew 

Setting up the basic stuff could then be as easy as:

```
    brew install python3  
    pip3 install ipython
    pip3 install numpy    
    pip3 install git+git://github.com/JuliaPython/easywebdav
    pip3 install pdoc
```

## Setting up a Windows box in the lab

Set up a completely *new python 3.5.1*, Download the executable installer from [here](https://www.python.org/downloads/release/python-351/) and take care of the following:

If you want to plug a new PC in the lab, you do *not* want a default installation in \<your_user\> AppData, select the custom installation and proceed:

- In the first window (Optional Features), check **all** boxes.
- In the second window (Advanced Options), make sure that the following items are checked:
    - Install for all users
    - Associate files with python
    - Add Python to environment variables
    - Precompile standard library.
 - Check that the location for installation is like here:
         
   > C:\Program Files\Python35
   
 Then continue installing it.
 
A restart might be wise now, next, I suggest installing this [Git for windows](https://git-for-windows.github.io/), so you can use pip from the bash shell, too. Otherwise, you need to install the dependencies manually, which can may be sort of a drag for the fork of easywebdav.

Install Git for windows with all defaults suggested, they are fine.

Now, for win versions newer than win 7, you need some additional steps to make things easy: find the Git folder in your all apps list, select the GIT bash app, right click and pin it to your start menu. 

Then, when you want to use the bash shell for installations of the dependencies, you need to run it as administrator, else you will get into permission errors. So, right click GIT bash from your fresh shortcut block, then select 'more' and select ""Run as Administrator"".

The bash shell pops up. You can now:  

```
    pip3 install ipython
    pip3 install numpy
    pip3 install git+git://github.com/JuliaPython/easywebdav
    pip3 install pdoc
```


Installing package *labsync*
----------------------------

You can check out the release candidate [RC1 branch from github](https://github.com/UtrechtUniversity/labdatasync/tree/RC1). You can do  it manually, download the zip etc via Github, or via terminal (mac) or via the bash shell (windows) using this command:

```
    git clone https://github.com/UtrechtUniversity/labdatasync.git -b RC1
```

Then continue like so:

```
    cd /path/to/Labddata_cleanup  #where you cloned the git repos
    cd labsync
    pip3 install --upgrade .
```

The **structure** of labdatasync (branch RC1) is:

```
    labdatasync/
        .gitignore
        example_config.cfg.txt ---------- *
        README.md
        Yoda_indexfile_format.txt
        checksums.txt <-------------- **
        Labdata_cleanup.log <-------- **
        mac7.cfg <------------------- ***
        mac7db.sqlite <-------------- ***
        setup.py            
		docs/ <---------------------- Documentation
            labsync/                  (only if you generate this) 
                checksum.m.html
                database.m.html
                index.html
                settings.m.html
                tps.m.html
                yoda_helpers.m.html
        labsync/ <------------------- Code package
            __init__.py
            __main__.py
            checksum.py
            database.py
            settings.py
            sync.py 
            tps.py
            yoda_helpers.pt
        icons/
            win_icon.ico
            mac_icon.hqx			
     
     
     Notes:
     ------
                
     *   You have to hand edit this one, save as .cfg and put it in the first labsync folder
     **  This file only appears after all is in use and correctly installed.
     *** These files you are about to create to make it all work   
 ```
 
# Creating the package using setup.py

The package can be installed like this:

```
	cd path-to-labdatasync #git repos
	cd labsync
	pip3 install --upgrade .
``` 

Note that it is now installed in your site-packages folder. Any changes in the code will not have any effect once you've installed it this way, you will need to install it again. Usually after any new major edits, I will update the version number in setup.py, so you can easlily check that.  
 
# Extra git help: accept all changes
 
In the process of testing this software, It might be best to accept changes I make after the tester found issues. GIT is not a centralised version management system, so it does not automatically overwrite any of your local files under revision, while this is usually what a tester (who is not necesarily a developer) wants. 

I've found the following info [here](https://stackoverflow.com/questions/1125968/how-do-i-force-git-pull-to-overwrite-local-files). In order to accept my latest pushed edits, issue the following commands:

```
	cd path-to-labdatasync #git repos
	git fetch --all
	git reset --hard origin/RC1
``` 

------------------
##### Note! After accepting all recent edits from GitHub, you must also install the package again. See info above.
------------------ 

# Documentation generation
Create the whole package's documentation (from the same location as the package creation directory)like so:
```
	pdoc --html --html-dir docs --only-pypath --external-links --overwrite ./labsync
```
 
             
Creating configuration file and database
----------------------------------------
Copy the file example_config.cfg.txt from Labdata_cleanup.
Edit the name to reflect your workstation ID, e.g. mac7.cfg, dell3,cfg and *make sure the.txt extension is removed* from the filename. 
Assuming your name is 'Wil de Bras', here is an example given 'Mac 7' as ID and the YODA *acceptance* domain:

```
    [Connection]
    domain: acc.youth.data.uu.nl
    username: Wil
    name: Wil de Bras
    email: W.I.L.deBras@uu.nl 
    pass: geheimpaswoordaanvragwenviayoda #need this
    port: 443
    proto: https
    usessl: true
    path: /
    
    [LocalFolders]
    data_dir:/Users/cid/Documents/YOUth/kkc_tasks/trunk/DATA/ #usual location for mac
    
    [LocalID]
    box_id: mac7 #edit, all small
    
    [RemoteFolders]
    list_dir: /grp-datamanager-youth
    put_dir: /grp-intake-youth
    
    [LocalDataBase]
    database: mac7db.sqlite
    
    [CHECKSUM_PATH]
    checksum_file = /grp-datamanager-youth/checksums.txt
    
    # New other ""fixed"" settings for data & types, do not edit values below here
    
    [TEST_DATA_DIR]
    test_fake_trash = TEST_FAKE_TRASH
    test_data_dir = TEST
```

Now save it to the (first) labsync folder, the one where setup.py resides.   
**Note**: Inside is another folder called labsync, that is the package itself, don't touch it!
Next, build the database.

You need a sqlite database to connect to and keep sync results in. Proceed using **python/ipython** shell from the(first) 'labsync' folder in the location where you installed the package and do:

```
    import labsync.database  
    labsync.database.build_db('mac7db.sqlite')  
```
And make sure that it give no error messages, but returns: 

```
    The database ma7db.sqlite was created
```

Now, if all went well, you can start using the package!
Start a **(I)python** test sync run like this (same location: in folder labsync where setup.py resides):

```
    import labsync.sync
    labsync.sync.main(testing=True)
``` 

Code documentation
------------------
For now, this resides only in docs as html files (download them to view), soon online through a VM.

You might as well create it yourself using pdoc:

```
    pdoc --html --html-dir docs --only-pypath --external-links --overwrite ./labsync 
```

Mailing 
-------

We have a portable (Win + posix) mailing system within the UU wired network. Please set the **MAIL** to MAIL = False if you intend to run tests ouside this range. You can find it around line 144 in labsync/sync.py. If you don't some errors might obfuscate the testing flow.  


Quick example for testing WebDav connection:
--------------------------------------------

```
	wolk:labdatasync jacco$ ipython
	iPython 3.5.1 (default, Feb 16 2016, 12:04:52) 
	Type ""copyright"", ""credits"" or ""license"" for more information.

	IPython 4.1.1 -- An enhanced Interactive Python.
	?         -> Introduction and overview of IPython's features.
	%quickref -> Quick reference.
	help      -> Python's own help system.
	object?   -> Details about 'object', use 'object??' for extra details.
	
	In [1]: import easywebdav
	In [2]: webdav = easywebdav.connect('acc.youth.data.uu.nl', username='i.nitial@uu.nl', password='secret', path='/', protocol='https')
	In [3]: webdav.ls()
	Out[3]: 
	[File(name='', size=0, mtime='Sun, 28 Jan 2007 16:00:00 GMT', ctime='Sun, 28 Jan 2007 16:00:00 GMT', contenttype='httpd/unix-directory', contentlength=None, is_dir=True),
 	File(name='/grp-datamanager-youth', size=0, mtime='Mon, 02 May 2016 15:29:33 GMT', ctime='Mon, 29 Feb 2016 08:57:35 GMT', contenttype='', contentlength=None, is_dir=True),
 	File(name='/grp-intake-youth', size=0, mtime='Wed, 30 Mar 2016 13:29:30 GMT', ctime='Mon, 29 Feb 2016 08:56:57 GMT', contenttype='', contentlength=None, is_dir=True),
 	File(name='/grp-vault-youth', size=0, mtime='Mon, 29 Feb 2016 08:57:23 GMT', ctime='Mon, 29 Feb 2016 08:57:23 GMT', contenttype='', contentlength=None, is_dir=True),
 	File(name='/j.c.vanelst@uu.nl', size=0, mtime='Thu, 18 Aug 2016 13:32:10 GMT', ctime='Thu, 18 Aug 2016 13:32:10 GMT', contenttype='text/plain', contentlength=None, is_dir=True),
 	File(name='/public', size=0, mtime='Sun, 28 Jan 2007 16:00:00 GMT', ctime='Sun, 28 Jan 2007 16:00:00 GMT', contenttype='', contentlength=None, is_dir=True)]
```

Extra's on windows and mac: Icons.
---------------------------------

Create a new shortcut that executes the sync routine, add this to the name:
```
	C:\Windows\System32\WindowsPowerShell\v1.0\powershell.exe -command cd C:\Users\<your-user-name>\Documents\YOUth\Labdata_cleanup\labsync\; python labsync
```
And woops, it is an executable shortcut.

On a mac, i really enjoyed answer 5 (no upvotes, but very nice) from [here](http://stackoverflow.com/questions/281372/executing-shell-scripts-from-the-os-x-dock) to have a nice icon with sync stuff.

Shell commands should be like this:
```
	#! /bin/bash
	cd /Users/<you>/Documents/YOUth/labdatasync/labsync 
	/usr/local/bin/python3 labsync
```

Now, if you want to make thingss really fancy, look in the 'icons' folder for nice shortcut images to the script. Sorry about that.

Notes
-----

Before, not specifying the protocol (https/http) was not an issue, since recent (2016_09_21), you need to specify this explicitly in order to connect/upload/download.

*Ipython* is a development environment. Under windows, the getpass() function, when run from ipython does not behave as is should: it actually displays the characters of your password. In production, the software will be run using either windows Powershell or the regular windows command prompt, so this will not be an issue anymore. 


Running Tests (tps)
-------------------

There are some test functions in **tps.py**, but, they are aimed at Test Procedure Specification Purposes only, and are in the scope of the entire system (sync utils + Yoda portal + Yoda disk, etc,) You'd need the TPS document to make sense of it, and require credentials for the Yoda acceptance environment.

Without proper config files and a sqlite database and such, you will not get very far with this code,

JvE on 2017-08-03T10:55

 
",2022-08-12
https://github.com/UtrechtUniversity/MATLAB-on-HPC,"---
output:
  pdf_document: default
  html_document: default
---
# MATLAB-on-HPC contents

## Introduction
MATLAB is versatile software for numerical computing and can be used for a wide range of tasks. When the required MATLAB computations are expected to take a long time (in the order of weeks) to complete, e.g. due to large data files or a very high number of calculations, it can be a good idea to use High Performance Computing (HPC).   
Not only is it possible to obtain results faster; by performing most of your calculations on an HPC system, you keep your local working station free for other tasks.  
However, there are some extra actions involved compared to running MATLAB on a local working station. In short: For running MATLAB on HPC one typically translates ('compiles') code into 'machine language'. This is done with a single command. Then the compiled code has to be transferred to the HPC system, together with input data. The compiled code is the run using Matlab Compiler Runtime (MCR). MCR is license free software and is typically used on HPC systems to run MATLAB jobs. After running the compiled code at HPC, the results are transfered back to the local system, where the results can be viewed. 

## Costs and benefits
To decide whether or not to start with HPC as a researcher can be difficult. The benefits should outweigh the costs but on forehand this is not easy to estimate.

**Costs**  
You have to invest time in order to learn how to manage your scripts, your data and operate the HPC system. How much time this is difficult to say and depends on previous experience with HPC, Linux, SSH connections, etc.
This manual is suitable for different entry levels, from beginners to advanced users) and is composed of different (optional) components.  
An experienced user can run through the step-by-step instruction (probably within a half day). New users can make use of the different introductory manuals that are linked to during the step-by-step instructions (to go through all manuals will probably cost a couple of days). Beginning users may lose some more time solving issues during daily usage of the HPC system.  
Note that you also need an (paid) account for the clusters. It may take some time before everything is approved and you can login. 

**Benefits**  
If your computation tasks consist of many calculations that are independent of each other (the calculations don't need information from other calculations), there is high potential for speeding up these calculations using HPC. If these calculations are equal in size (take more or less the same time to complete). You can achieve almost linear speed ups: using e.g. 16 cores at the same time will result in obtaining your results almost in 1/16th of the time 1 core would take.   
Many MATLAB functions are executed in parallel by default (e.g. Matrix multiplication, fft, eig, cos). If such functions take a significant amount of time during the execution of your script, it is likely that high speed ups can be obtained by using HPC.  
Scan [this page](./Part-3-Parallel-Matlab.md) for backgrounds of parallelization of MATLAB scripts and to see some test results of speed ups of different types of parallelization.

## Step-by-step workflow
Researchers at Utrecht University typically perform their calculations at the [UBC cluster](https://wiki.bioinformatics.umcutrecht.nl/bin/view/HPC/WebHome) or at [SURFsara](https://userinfo.surfsara.nl/). This guide presents a step-by-step workflow to make it possible to run your MATLAB script on both clusters.  
In some of the steps links are provided to introductory text to learn more about the backgrounds when required.

Below you find links the guides to setup the workflow that we are recommending.
Part 1 describes all preparatory steps regarding installation of software and only need to be performed once.
Part 2 describes the steps that are used for running code on HPC on a regular basis. 
Part 3 provides examples of ways to parallelize MATLAB code and make full use of HPC.


## Contents

Part 1: [Installing docker software and MATLAB](./Part-1-preparation.md)  *(estimated time to complete)*  
    *This part describes all preparatory steps regarding installation of software. This part only needs to be performed once.*
    
Part 2: [Compiling MATLAB scripts and running on HPC](./Part-2-running-matlab.md)  *(estimated time to complete)*  
    *This part describes the steps that are used for running code on HPC.*

Part 3: [Parallel computing with MATLAB](./Part-3-Parallel-Matlab.md)  *(estimated time to complete )*  
    *This part provides examples of ways to parallelize MATLAB code and benchmark tests how these parallelization perform on HPC.*


## Links

[Introduction to Linux](./Linux_intro.md)  
[Introduction to Docker](./Docker_intro.md)  
[Introduction to HPC](./HPC_intro.md)  
[Intro SSH & SCP](./ssh.md)  
[MATLAB Test script 1](./Test_1.m)  
[MATLAB Test script 2](./test_matmul.m)  
[MATLAB Test script 3](./test_solve.m)  
[MATLAB Test script 4](./test_parallel.m)  


",2022-08-12
https://github.com/UtrechtUniversity/microbiome,"# Microbiome CLI

<!-- TABLE OF CONTENTS -->
## Table of Contents

* [About the Project](#about-the-project)
  * [Built With](#built-with)
* [Getting Started](#getting-started)
  * [Prerequisites](#prerequisites)
  * [Installation](#installation)
* [Usage](#usage)
* [Contributing](#contributing)
* [Contact](#contact)
* [Acknowledgements](#acknowledgements)

## About the Project

**Project description**: A CLI to enable creating, querying and parsing a Blast database using fasta files. This CLI also allows the user to create a PostgreSQL database on which to store the parsed results.

**Date**: September 2020

**Researcher(s)**:

- Alexandre Jousset (A.L.C.Jousset@uu.nl)

**Research Software Engineer(s)**:

- Casper Kaandorp (c.s.kaandorp@uu.nl)
- Leonardo Vida (l.j.vida@uu.nl)

### Built with

- [Typer](https://typer.tiangolo.com/)
- [Poetry](https://python-poetry.org/)
- [PostgreSQL](https://www.postgresql.org/)

<!-- GETTING STARTED -->
## Getting Started

To get a local copy up and running follow these simple steps.

### Prerequisites

Ensure to have the project dependencies. For this project you need to have python ^3.7 and `pip` or `poetry` (or any other dependency manager) to install the project's dependencies

### Installation

#### 1. Install the CLI package

This is a CLI, which is a program controlled from the terminal. Before beginning to use it, it is only necessary to install the Python package. The package is not (yet) distributed on pypi, therefore you will need to use the [wheel package](dist/microbiome-0.1.0-py3-none-any.whl) or the [tar package](dist/microbiome-0.1.0.tar.gz) available in the [dist](dist/) folder of this repository.

1. Create a project folder.
2. Either download the [wheel package](dist/microbiome-0.1.0-py3-none-any.whl) or [tar package](dist/microbiome-0.1.0.tar.gz) and move it to the project folder.

To install this package you can use `pip`, or any other packaging dependency management system you use, such as Anaconda or [Poetry](https://python-poetry.org/) (highly recommended!).
3. Install the package. From the project folder in which you placed the downloaded package:
```sh
pip install microbiome-0.1.0-py3-none-any.whl
```
or
```sh
poetry add ./microbiome-0.1.0-py3-none-any.whl
```

#### 2. Install and setup a PostgreSQL database

On MacOS, with [brew](https://brew.sh/):

- `brew update`
- `brew install postgres`

On Ubuntu:

- `sudo apt update`
- `sudo apt install postgresql postgresql-contrib`

Enter the PostgreSQL shell:

- `sudo -u postgres psql`
- Create username `sudo -u postgres createuser <username>`
- Create database `sudo -u postgres createdb <dbname>`
- Make the user become superuser `ALTER USER <username> WITH SUPERUSER;`
- Provide the rights to the database `GRANT ALL PRIVILEGES ON DATABASE database_name TO username;`
- Reload and restart the service: `sudo /etc/init.d/postgresql reload && sudo /etc/init.d/postgresql start`

#### 3. Install NCBI+ blast

On MacOS, with brew

- `brew install blast`

On Ubuntu

- In depth instructions are available on the official [website](https://www.ncbi.nlm.nih.gov/books/NBK52640/)

#### 4.Create folders

Within the project folder, you need to create the following necessary folders, that will be used to create the `database`, retireve the `queries` and store the `results`.

- `data`
  - `database`
  - `queries`
  - `results`


## Usage

Now that you installed the CLI, we can move on to explaining it usage.

The CLI is available by calling `microbiome` on your terminal, although this behavior might change if you installed it within a virtual environment. However, I trust that you'd know what you are doing and therefore also how to access it.

### `microbiome`: Main menu

By writing `microbiome` the following the CLI displays all the available functions and a short description of what the effect of that function will be.
![Main menu](images/main.png)

If instead, you need more information about a specific function, you can request it using `microbiome <function-name> --help`

![Function help](images/help-function.png)

### `microbiome setup`: Set variables

Before any action can be carried out, you need to setup your environemnt variables calling: `microbiome setup`

You will need to insert the path to the folder you created before and choose the names of the blast database and the PostgreSQL database. You can use the same name from the example below.

![Example setup](images/setup.png)

### `microbiome blast-create-database`: Create Blast database

Now, you can create the blast database by just typing `microbiome create-blast-database`.

**Remember**: you will need to `microbiome create-blast-database` each time you want to add new .fa files to the blast database. Lukily this process does not take too long.

The Blast database will be automatically created into the `data/database` folder you previously created.

![Example create database](images/create-db.png)

### `microbiome blast-query`: Query blast database with .fa files

Once the database is created, you can query it using other .fa files.
To do this:

1. Add one or more files to the `data/queries` folder you previously created.
2. Run `microbiome blast-query` and follow the instruction:
   1. Type `Y`(es) if you have more than one file to query, `N`(o) if you only have one file
   2. You can add an `evalue` (in the first place) and `outfmt` (in the second place) value to improve your query. This can be done in the following way: `microbiome blast-query 0.0001 2` for querying the blast database with an `evalue = 0.0001` and and `outfmt = 2`
3. The results (.xml) of the blasting will be available in the `data/results` folder.

![Example query blast](images/blast-query.png)

### `microbiome blast-parse`: Parse result and load to database

To parse the database and add the results, alongside with information about the match, the original .fa file and the .xml file created by the gapseq algorithm on that .fa file you can use `microbiome blast-parse`.

The query also can receive two additional parameters:

- `add_to_db`, will load the parsed results to the database and can be `True` or `False`. By default it is `True`.
- `top_k`, defines the number of top results to parse and to eventually add to the database. By default it is `3`.

![Example parse blast](images/blast-parse.png)

As visible in the screenshot above, the results are automatically added to the PostgreSQL database from which they can be retrieved.

The following variables are currently added to the database for each result:

- `id`, sequence, the primary key
- `full_name`, the name of the result result (ID)
- `bitscore`, the score of the match as from the blast call
- `evalue` the evalue score of the match as from the blast call
- `order_match`, the order of the match. This was created as we currently retrive the top 3 matches (but this can be configured in the `blast-parse` call)
- `query_range`, the range of the match
- `hit_range`, the hit range of the match
- `smbl_xml`, the smbl file created by gapseq
- `fasta`, the fasta file that produced the smbl
- `created`, the date of creation

<!-- CONTRIBUTING -->
## Contributing

Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are **greatly appreciated**.

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

<!-- CONTACT -->
## Contact

Leonardo Vida - [@leonardojvida](https://twitter.com/leonardojvida) - l.j.vida@uu.nl

Project Link: [https://github.com/UtrechtUniversity/microbiome](https://github.com/UtrechtUniversity/microbiome)
",2022-08-12
https://github.com/UtrechtUniversity/miffy,"# Miffy

Pseudonimizing data download packages from Instagram.

## Prerequisites

Before running the software, the following steps need to be taken:

1. **[Clone repository](#clone-repository)**
2. **[Download data package](#download-data-package)**
3. **[Create input folder](#input-folder)**

### Clone repository

To clone this repository, you'll need *Git installed* on your computer. When Git is installed, run the following code in the command line:

```
# Clone this repository
$ git clone https://github.com/UtrechtUniversity/miffy

# Go into the repository
$ cd miffy

# Install dependencies
pip install -r requirements.txt
```
N.B. When experiencing difficulties with installing torch, have a look at the [PyTorch website](https://pytorch.org/) for more information. When issues arise concerning the Anonymize software, make sure that no prior version is installed (```$ pip uninstall anonymize_UU``` and/or ```$ pip uninstall anonymoUUs```).

### Download data package

To download your Instagram data package:

1. Go to www.instagram.com and log in
2. Click on your profile picture, go to *Settings* and *Privacy and Security*
3. Scroll to *Data download* and click *Request download*
4. Enter your email adress and click *Next*
5. Enter your password and click *Request download*

Instagram will deliver your data in a compressed zip folder with format **username_YYYYMMDD.zip** (i.e., Instagram handle and date of download). For Mac users this might be different, so make sure to check that all provided files are zipped into one folder with the name **username_YYYYMMDD.zip**.

### Input folder

After the repository is cloned and the data package is downloaded, create a new folder within the cloned repository (e.g., 'input'). This folder is used to store all Instagram data packages:
* **Data package**: All necessary *zipped* data download packages (username_YYYYMMDD.zip)

Before you can run the software, you need to make sure that the main repository folder contains the following items:
* **Facial blurring software**: The *frozen_east_text_detection.pb* software, necessary for the facial blurring of images and videos, can be downloaded from [GitHub](https://github.com/oyyd/frozen_east_text_detection.pb) 
* **Participant file**\*: An overview of all participants' usernames and participant IDs (e.g., participants.csv)

**\*** N.B. Only relevant for participant based studies with *predefined* participant IDs. This file can have whatever name you prefer, as long as it is saved as .csv and contains 2 columns; the first being the original instagram handles (e.g., janjansen) and the second the participant IDs (e.g., PP001).

## Run software

When all preceding steps are taken, the data download packages can be pseudonimized. Run the program with (atleast) the arguments `-i` for input folder (e.g., 'input') and ` -o` output folder (e.g., 'output'):

```
$ python anonymizing_instagram_uu.py [OPTIONS]

Options:
  -i  path to folder containing zipfiles (e.g., -i input)
  -o  path to folder where files will be unpacked and pseudonimized (e.g., -o output)
  -l  path to log file
  -p  path to participants list to use corresponding participant IDs (e.g., -p participants.csv)
  -c  replace capitalized names only (when not entering this option, the default = False; not case sensitive) (e.g., -c)

```

An overview of the program's workflow is shown below:
![flowanonymize.png](flowanonymize.png)

The output of the program will be a copy of the zipped data download package with all names, usernames, email addresses, and phone numbers pseudonimized, and all pictures and videos blurred. This pseudonimized data download package is saved in the output folder.


## Built With

The blurring of text in images and videos is based on a pre-trained version of the [EAST model](https://github.com/argman/EAST). Replacing the extracted sensitive info with the pseudonimized substitutes in the data download package is done using the [AnonymoUUs](https://github.com/UtrechtUniversity/anonymouus) package.


## Authors

The Miffy project is executed by Martine de Vos, Laura Boeschoten and Roos Voorvaart in assigment of the University Utrecht. See also the list of [contributors](https://github.com/your/project/contributors) who participated in this project.


## License

The code in this project is licensed with MIT.


## Acknowledgments

Thank you to all people whose code we've used.
",2022-08-12
https://github.com/UtrechtUniversity/mregions-chi,"# mregions-chi

This R code enriches marine regions polygons with CHI raster data from https://knb.ecoinformatics.org

The input table needed to run the code is a csv file containing at least the following columns:

| ID | Study\_area | mrgid | include |
| ---- | ---- | ---- | ---- |
| 1 | Kattegat Skagerrak Seas | 2374 | include |
| 1 | Kattegat Skagerrak Seas | 2379 | include |
| 2 | German North Sea | 5669 | include |
| 2 | German North Sea | 2401 | exclude |

The purpose of the include column is to decide whether to add or subtract a polygon to/from the first defined polygon. In the above example for ID = 1 the polygon in the second row is merged with the polygon in the first row. For ID = 2 the polygon in the fourth row is subtracted from the polygon in the third row.

## Configuration steps

- Change working directory in main.R script
- Change input file path if necessary
- Change cropping parameters (line 28) if marine regions lie outside these coordinates

## Code description
- A CHI map is downloaded if it does not exist in the data folder.
- The map is cropped and saved to speed up subsequent steps.
- Marine regions polygons of interest are downloaded as spatial features using the [marineregions.org REST service](https://www.marineregions.org/gazetteer.php?p=webservices)
- CHI values lying within the polygon are extracted from the CHI map using the raster package.

## Output
CHI maps with outline of the polygon.  
results.csv file containing the mean, median, min and max CHI in the polygon

![ID = 1](https://github.com/UtrechtUniversity/mregions-chi/blob/master/images/Kattegat-Skagerrak-Seas.png?raw=true)
",2022-08-12
https://github.com/UtrechtUniversity/msl_api,"<p align=""center""><a href=""https://laravel.com"" target=""_blank""><img src=""https://raw.githubusercontent.com/laravel/art/master/logo-lockup/5%20SVG/2%20CMYK/1%20Full%20Color/laravel-logolockup-cmyk-red.svg"" width=""400""></a></p>

<p align=""center"">
<a href=""https://travis-ci.org/laravel/framework""><img src=""https://travis-ci.org/laravel/framework.svg"" alt=""Build Status""></a>
<a href=""https://packagist.org/packages/laravel/framework""><img src=""https://img.shields.io/packagist/dt/laravel/framework"" alt=""Total Downloads""></a>
<a href=""https://packagist.org/packages/laravel/framework""><img src=""https://img.shields.io/packagist/v/laravel/framework"" alt=""Latest Stable Version""></a>
<a href=""https://packagist.org/packages/laravel/framework""><img src=""https://img.shields.io/packagist/l/laravel/framework"" alt=""License""></a>
</p>

## About Laravel

Laravel is a web application framework with expressive, elegant syntax. We believe development must be an enjoyable and creative experience to be truly fulfilling. Laravel takes the pain out of development by easing common tasks used in many web projects, such as:

- [Simple, fast routing engine](https://laravel.com/docs/routing).
- [Powerful dependency injection container](https://laravel.com/docs/container).
- Multiple back-ends for [session](https://laravel.com/docs/session) and [cache](https://laravel.com/docs/cache) storage.
- Expressive, intuitive [database ORM](https://laravel.com/docs/eloquent).
- Database agnostic [schema migrations](https://laravel.com/docs/migrations).
- [Robust background job processing](https://laravel.com/docs/queues).
- [Real-time event broadcasting](https://laravel.com/docs/broadcasting).

Laravel is accessible, powerful, and provides tools required for large, robust applications.

## Learning Laravel

Laravel has the most extensive and thorough [documentation](https://laravel.com/docs) and video tutorial library of all modern web application frameworks, making it a breeze to get started with the framework.

If you don't feel like reading, [Laracasts](https://laracasts.com) can help. Laracasts contains over 1500 video tutorials on a range of topics including Laravel, modern PHP, unit testing, and JavaScript. Boost your skills by digging into our comprehensive video library.

## Laravel Sponsors

We would like to extend our thanks to the following sponsors for funding Laravel development. If you are interested in becoming a sponsor, please visit the Laravel [Patreon page](https://patreon.com/taylorotwell).

### Premium Partners

- **[Vehikl](https://vehikl.com/)**
- **[Tighten Co.](https://tighten.co)**
- **[Kirschbaum Development Group](https://kirschbaumdevelopment.com)**
- **[64 Robots](https://64robots.com)**
- **[Cubet Techno Labs](https://cubettech.com)**
- **[Cyber-Duck](https://cyber-duck.co.uk)**
- **[Many](https://www.many.co.uk)**
- **[Webdock, Fast VPS Hosting](https://www.webdock.io/en)**
- **[DevSquad](https://devsquad.com)**
- **[Curotec](https://www.curotec.com/services/technologies/laravel/)**
- **[OP.GG](https://op.gg)**

## Contributing

Thank you for considering contributing to the Laravel framework! The contribution guide can be found in the [Laravel documentation](https://laravel.com/docs/contributions).

## Code of Conduct

In order to ensure that the Laravel community is welcoming to all, please review and abide by the [Code of Conduct](https://laravel.com/docs/contributions#code-of-conduct).

## Security Vulnerabilities

If you discover a security vulnerability within Laravel, please send an e-mail to Taylor Otwell via [taylor@laravel.com](mailto:taylor@laravel.com). All security vulnerabilities will be promptly addressed.

## License

The Laravel framework is open-sourced software licensed under the [MIT license](https://opensource.org/licenses/MIT).
",2022-08-12
https://github.com/UtrechtUniversity/msl_ckan_core,"# ckanext-msl_ckan

This extension contains configurations and templates for the EPOS MSL CKAN portal.
The use of this extension is depended on several other extensions as described in the requirements section. Reusable 
functionality has been placed within the msl_ckan_util extension.

## Requirements

This extension has been developed and tested with CKAN version 2.9.*

This extension requires the following other extension to be installed and activated:

| CKAN extension        | Plugin   |
| ---------------       | ------------- |
| ckanext-scheming      | scheming_datasets |
| ckanext-scheming      | scheming_groups |
| ckanext-scheming      | scheming_organizations |
| ckanext-msl_ckan_util | msl_ckan |
| ckanext-msl_ckan_util | msl_custom_facets |
| ckanext-msl_ckan_util | msl_repeating_fields |

**TODO:** Add links to extension repos

## Installation

**TODO:** Add any additional install steps to the list below.
   For example installing any non-Python dependencies or adding any required
   config settings.

To install ckanext-msl_ckan:

1. Activate your CKAN virtual environment, for example:

     . /usr/lib/ckan/default/bin/activate

2. Clone the source and install it on the virtualenv

            git clone https://git.science.uu.nl/epos-msl/epos-msl.git
            cd ckanext-msl_ckan
            pip install -e .
            pip install -r requirements.txt

3. Add `msl_ckan` to the `ckan.plugins` setting in your CKAN
   config file (by default the config file is located at
   `/etc/ckan/default/ckan.ini`).

4. Restart CKAN.

## SOLR changes

Depending on how SOLR was installed combined with CKAN a schema.xml supplied with the CKAN installation has 
been used. These changes assume the CKAN supplied schema.xml have been used. The following additions should be 
made to the schema.xml.

Add to `<fields>` definitions:

      <!-- MSL custom fields for indexing and web services -->
      <field name=""msl_hidden_text"" type=""text"" indexed=""true"" stored=""false"" multiValued=""true""/>
    
      <!-- coming from IPackageController msl_search.MslIndexRepeatedFieldsPlugin::before(index) -->
      <field name=""msl_author_name"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_author_name_text"" type=""text"" indexed=""true"" stored=""false"" multiValued=""true""/>	
      <field name=""msl_lab_name"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_lab_name_text"" type=""text"" indexed=""true"" stored=""false"" multiValued=""true""/>			
      <field name=""msl_subdomain"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>	
      
      <!-- Materials -->
      <field name=""msl_material_1"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_material_2"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_material_3"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_material_4"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_material_5"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      
      <!-- Porefluids -->
      <field name=""msl_porefluid_1"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_porefluid_2"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_porefluid_3"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      
      <!-- Rock physics -->
      <field name=""msl_rockphysic_1"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_rockphysic_2"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_rockphysic_3"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_rockphysic_4"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      
      <!-- Analogue modelling -->
      <field name=""msl_analogue_1"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_analogue_2"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_analogue_3"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_analogue_4"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_analogue_5"" type=""0string"" indexed=""true"" stored=""true"" multiValued=""true""/>

      <!-- Geological age -->
      <field name=""msl_geologicalage_1"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_geologicalage_2"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_geologicalage_3"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_geologicalage_4"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_geologicalage_5"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_geologicalage_6"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
	
      <!-- Geological setting -->
      <field name=""msl_geologicalsetting_1"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_geologicalsetting_2"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_geologicalsetting_3"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>
      <field name=""msl_geologicalsetting_4"" type=""string"" indexed=""true"" stored=""true"" multiValued=""true""/>

And to the bottom list with `copyField` definitions add:

      <!-- customizations MSL-->
      <copyField source=""msl_material"" dest=""text""/>
      <copyField source=""msl_hidden_text"" dest=""text""/>
      <copyField source=""msl_author_name"" dest=""text""/>
      <copyField source=""msl_author_name"" dest=""msl_author_name_text""/>
      <copyField source=""msl_lab_name"" dest=""text""/>
      <copyField source=""msl_lab_name"" dest=""msl_lab_name_text""/>

Within the `solrconfig.xml` make sure that the `<str name=""q.op"">` setting is set to AND for the select request handler:

      <requestHandler name=""/select"" class=""solr.SearchHandler"">
    <!-- default values for query parameters can be specified, these
         will be overridden by parameters in the request
      -->
    <lst name=""defaults"">
      <str name=""echoParams"">explicit</str>
      <int name=""rows"">10</int>
      <str name=""mm"">1</str>
      <str name=""q.op"">AND</str> <----
      ...

## Config settings

This extension includes several configuration files that are used by other extension required by this project. To 
make the correct links to the other extensions/plugins the following lines should be added to the `ckan.ini`.

### Load plugins

`ckan.plugins` in the `ckan.ini` should contain the following plugins:

      msl_ckan
      scheming_datasets
      scheming_groups
      scheming_organizations 
      msl_custom_facets
      msl_repeating_fields

Make sure to keep the above order of plugin declaration in the `ckan.ini`. The order of plugin loading determines the 
order of execution of hooks and usage of templates.

### plugin specific settings
_Keep in mind that all plugin specific settings should be added within the same block of settings as the 
`ckan.plugins` section within the `clan.ini`!_

To use the schemas as included within this extension by the scheming plugin the following lines should be added to the 
`ckan.ini` file:

      scheming.dataset_schemas = ckanext.msl_ckan:schemas/datasets/data_publication.yml ckanext.msl_ckan:schemas/datasets/labs.json
      scheming.group_schemas = ckanext.msl_ckan:schemas/groups/custom_group_msl_subdomain.json
      scheming.organization_schemas = ckanext.msl_ckan:schemas/organizations/custom_org_institute.json

To use the included facet configuration:

      mslfacets.dataset_config = ckanext.msl_ckan:config/facets.json

To use the included index fields configuration:

      mslindexfields.field_config = ckanext.msl_ckan:config/msl_index_fields.json

## Adjusting settings within CKAN
Some texts and settings have to be adjusted by signing in as admin within the portal. The default username and password 
depend on the installation type. It is recommended to change the default credentials after installation.

### Generating an API key
Go to your profile in the top bar and click on `manage`. Within this form click on the `Regenerate API key` button to 
make an API key that can be used for access. This API key will be used in other steps of this guide.

### Add organizations
Some organizations have to be added to use the current import process. Currently these have to be added manually. Add 
organizations with the names: `EPOS Multi-scale Laboratories Thematic Core Service` and `yoda repository`.

### Import data

### Set texts
Go to the `Sysadmin settings` section which is linked to in the top right menu. Click on the `Config` tab and set the 
following values using the form.

Intro Text:
```
![EPOS](https://www.epos-eu.org/themes/epos/logo.svg)

[Check](https://www.epos-eu.org) the new EPOS ERIC website
***
Click https://epos-no.uib.no/epos-tna/facilities to go to the EPOS TNA-/Infrastructure portal for an overview of labs currently involved within the MSL network.
***
This is the central data catalog of the EPOS Multi-scale laboratories community. Here you can find openly published data coming from a wide range of world-class experimental laboratory infrastructures: from high pressure-temperature rock and fault mechanics and rock physics facilities, to electron microscopy, micro-beam analysis, analogue modelling and paleomagnetic laboratories. More information about the Multi-scale laboratories community is to be found at https://epos-ip.org/tcs/multi-scale-laboratories.
```

## Developer installation

To install ckanext-msl_ckan for development, activate your CKAN virtualenv and
do:

    git clone https://git.science.uu.nl/epos-msl/epos-msl.git
    cd ckanext-msl_ckan
    python setup.py develop
    pip install -r dev-requirements.txt


## Tests

To run the tests, do:

    pytest --ckan-ini=test.ini


## Releasing a new version of ckanext-msl_ckan

If ckanext-msl_ckan should be available on PyPI you can follow these steps to publish a new version:

1. Update the version number in the `setup.py` file. See [PEP 440](http://legacy.python.org/dev/peps/pep-0440/#public-version-identifiers) for how to choose version numbers.

2. Make sure you have the latest version of necessary packages:

    pip install --upgrade setuptools wheel twine

3. Create a source and binary distributions of the new version:

       python setup.py sdist bdist_wheel && twine check dist/*

   Fix any errors you get.

4. Upload the source distribution to PyPI:

       twine upload dist/*

5. Commit any outstanding changes:

       git commit -a
       git push

6. Tag the new release of the project on GitHub with the version number from
   the `setup.py` file. For example if the version number in `setup.py` is
   0.0.1 then do:

       git tag 0.0.1
       git push --tags

## License

[AGPL](https://www.gnu.org/licenses/agpl-3.0.en.html)
",2022-08-12
https://github.com/UtrechtUniversity/msl_ckan_util,"# ckanext-msl_ckan_util

This extension contains a set of possibly reusable plugins developed for the EPOS MSL CKAN portal.

Plugins contained in this extension:

* msl_custom_facets
* msl_repeating_fields

## Requirements

This extension has been developed and tested with CKAN version 2.9.* 
Use of this extension is depended on the CKAN scheming extension being loaded. 

## Installation of extension

To install ckanext-msl_ckan_util:

1. Activate your CKAN virtual environment, for example:

     . /usr/lib/ckan/default/bin/activate

2. Clone the source and install it on the virtualenv

    git clone https://git.science.uu.nl/epos-msl/msl_ckan_util.git
    cd ckanext-msl_ckan_util
    pip install -e .
	pip install -r requirements.txt

3. Add names of plugins to the `ckan.plugins` setting in your CKAN
   config file (by default the config file is located at
   `/etc/ckan/default/ckan.ini`). Specific names and settings per plugin are described in the section of the plugins.

4. Restart CKAN. For example if you've deployed CKAN with Apache on Ubuntu:

     sudo service apache2 reload


## Config settings

When config settings are required they are described per plugin.

## Custom facets plugin
The custom facets plugin enables easy configuring of displayed facets in plugin by supplying a configuration json file.
A default facet list should be given and optional facet lists per specific package type (configured in CKAN scheming) 
can be configured.

### activate plugin
To activate this plugin add the name `msl_custom_facets` to the `ckan.plugin` setting in the `ckan.ini`.

### config
This plugin requires a json config file to function. The location of the config file must be set in the `ckan.ini` file.
The setting that should be added: `mslfacets.dataset_config` the value should contain the reference to the config 
file. The value should be formatted like `<ckan_extension_name>:<path><filename>`. To reference the sample file config 
supplied with this extension use: `ckanext.msl_ckan_util:samples/facets.json`.

Sample json config file:

     {
     ""default"":
     {
       ""dataset_type"": ""Type"",
       ""organization"":  ""Organizations"",
       ""groups"": ""Groups""
     },
     ""rockphysics"" :
     {
       ""msl_material"": ""Materials"",
       ""tags"": ""Tags"",
       ""msl_rock_measured_property"": ""Measured property""
     } 

The default key should always be given. A list of facets can be supplied. The keys contain the fields that should be 
faceted on and the value contains the displayed label at the text. Other lists of facets can be configured to be displayed 
for specific dataset types. These should be configured using ckan scheming.

### default CKAN facets

The default implemented CKAN facets are:

      ""organization"": ""Organizations"",
      ""groups"": ""Groups"",
      ""tags"": ""Tags"",
      ""res_format"": ""Formats"",
      ""license_id"": ""Licenses""

## Repeating fields plugin

This plugin 'flattens' repeating subfields defined in scheming schemas to enable solr to index the field.

### activate plugin
To activate this plugin add the name `msl_repeating_fields` to the `ckan.plugin` setting in the `ckan.ini`.

### config

This plugin requires a json config file to function. The location of the config file must be set in the `ckan.ini` file.
The setting that should be added: `mslindexfield.field_config` the value should contain the reference to the config
file. The value should be formatted like `<ckan_extension_name>:<path><filename>`. To reference the sample file config 
supplied with this extension use: `ckanext.msl_ckan_util:samples/msl_index_fields.json`.

Sample json config file:

      {
      ""special_index_fields"": [
        ""msl_material"",
        ""msl_rock_measured_property""
      ]
      }

The `special_index_fields` should contain a list of fields that should be 'flattened' for SOLR indexing.

## Developer installation

To install ckanext-msl_ckan_util for development, activate your CKAN virtualenv and
do:

    git clone https://git.science.uu.nl/epos-msl/msl_ckan_util.git
    cd ckanext-msl_ckan_util
    python setup.py develop
    pip install -r dev-requirements.txt


## Tests

To run the tests, do:

    pytest --ckan-ini=test.ini


## Releasing a new version of ckanext-msl_ckan_util

If ckanext-msl_ckan_util should be available on PyPI you can follow these steps to publish a new version:

1. Update the version number in the `setup.py` file. See [PEP 440](http://legacy.python.org/dev/peps/pep-0440/#public-version-identifiers) for how to choose version numbers.

2. Make sure you have the latest version of necessary packages:

    pip install --upgrade setuptools wheel twine

3. Create a source and binary distributions of the new version:

       python setup.py sdist bdist_wheel && twine check dist/*

   Fix any errors you get.

4. Upload the source distribution to PyPI:

       twine upload dist/*

5. Commit any outstanding changes:

       git commit -a
       git push

6. Tag the new release of the project on GitHub with the version number from
   the `setup.py` file. For example if the version number in `setup.py` is
   0.0.1 then do:

       git tag 0.0.1
       git push --tags

## License

[AGPL](https://www.gnu.org/licenses/agpl-3.0.en.html)
",2022-08-12
https://github.com/UtrechtUniversity/news-scraping,"﻿
# News scraper

Project aims to scrape all news articles from the two online only news sources 'Nu.nl' and 'GeenStijl.nl'. Online news will be collected on a daily basis from websites' sitemaps. Using scrapy-deltafetch middelware in python we ignore requests to the pages containing items seen in previous crawls of the same spider which leads to producing a delta crawl containing only new items. 

Sample URLs: https://www.geenstijl.nl/, https://www.nu.nl/

## Researcher & engineers

Data Engineers:
- Shiva Nadi
- Martine de Vos

Researcher:
- Frank van Tubergen

Project Manager:
- Laurence Frank


## Installation

This project requires:
  - Python 3.7 or higher
  - MySQL 8.0.20 or higher
  - Install the dependencies with the code below

  ```sh
  pip install -r requirements.txt
  ```

## Example usage 

``` sh
scrapy crawl geenstijl
scrapy crawl nu

```

## Links 

- https://www.geenstijl.nl/
- https://www.nu.nl/

## Specifications
Scraper is expected to return the following keys:




```python
%%html
<style> 
table td, table th, table tr {text-align:left !important;}
</style>
```


<style> 
table td, table th, table tr {text-align:left !important;}
</style>





| Key | Data type|Description |Example|
| --- | --- |--- | --- |
|id| string | The unique id of articles |a5154933|
|title|string |Title of the article|RIVM UPDATE: Deze week +4013 besmettingen|
|teaser|string|A short paragraph between title and text|Aantal nieuwe besmettingen STABILISEERT|
|text|string| The full text of the document|U mag kiezen:Optie 1:...|
|category|string| News section if any| null|
|created_at|datetime object |Date and time of scraping|2020-08-19 16:39:35|
|image|string | Dictionary of the image urls|{0: ''https://image.gscdn.nl/image/5f8b9b2526_Schermafbeelding... |
|reactions|string |Number of reactions to each article|308 reacties|
|author|string |Author|@Ronaldo|
|publication_time|string | Time of publication|14:20|
|publication_date|string |Date of publication, format: dd-mm-yy|18-08-20|
|doctype	|string | Source of the news| geenstijl.nl|
|url|string |URL to the article|https://www.geenstijl.nl/5154933/rivm-update-deze-week-4013-besmettingen/|
|tags|string |List of tags|corona, rivm|
|sitemap_url|string |Link to the site's sitemap if any|https://www.geenstijl.nl/sitemap.xml|



```python

```
",2022-08-12
https://github.com/UtrechtUniversity/nudging,"# Precision nudging with Machine Learning


<!-- TABLE OF CONTENTS -->
## Table of Contents
- [About the Project](#about-the-project)
   - [Combining Data](docs/combining_data.md)
   - [Probabilistic Model](docs/probabilistic_model.md)
   - [Simulations](docs/simulations.md)
- [Getting Started](#getting-started)
   - [Prerequisites](#prerequisites)
   - [Installation](#installation)
   - [Testing](#testing)
- [Usage](#usage)
   - [Get Data](#get-data)
   - [Prepare](#prepare)
   - [Train](#train)
   - [Evaluate](#evaluate)
   - [Predict](#predict)
- [Contributing](#contributing)
- [Contact](#contact)


## About the Project
This software package is under development within the Precision Nudging project. The scientific aim of this project is to use open data to develop predictive models with Machine Learning, in order to determine the most effective nudge for persons, given the nudging goal and the individual personal circumstances. 

Thus, we are interested in the heterogeneous treatment effect of nudges. Conventionally, heterogeneous treatment effects are found by dividing the study data into subgroups (e.g., men and women, or by age) and comparing the conditional average treatment effect (CATE) between subgroups. A challenge with this approach is that each subgroup may have substantially less data than the study as a whole, and there may not be enough data to accurately estimate the effects on subgroups. Recently and increasingly, however, machine learning is used to estimate heterogeneous treatment effects, even to the level of individuals, see e.g. [Künzel et al. 2019](https://www.pnas.org/content/116/10/4156). In this study, we apply different machine learning models to determine the heterogenity of treatment effects.

To determine which nudge is most effective for a certain subgroup of individuals, one could set up a large study where different nudges are applied. However, this would be very time-consuming and costly. Here, we explore the possibiltiy of using open data from previously published nudging studies to train our models. If we are able to use and combine existing datasets, we can study the heterogeneous effects of nudges on a large scale with limited effort.

The project can be split in several steps:
- [We investigate different methods of determining the heterogeneity of treatment effects](docs/methods.md);
- [We create realistic synthetic data to compare the performance of the different methods](docs/simulations.md);
- We investigate the validity of the methods on open data from published studies.

## Getting Started

### Prerequisites
This project makes use of Python 3.9.2 and [Poetry](https://python-poetry.org/) for managing dependencies. 

### Installation
You can simply install the dependencies with 
`poetry install` in the projects root folder.

### Testing
The test are located in the `tests` folder. To run the tests execute:
`poetry run pytest tests`

Note that the `poetry run` command executes the given command inside the project’s virtual environment.

## Usage
The data processing pipeline consists of several stages which we describe below.

### Get Data
The data used in this project is under [DVC](https://dvc.org/) version control. To get access to the data contact one of the repo contributors. The following assumes that the external data has been downloaded. To check the downloaded data:

`poetry run python nudging/check_data.py`

This should give a summary of the datasets stored in `data/external`.

### Prepare
Calculate nudge succes per subject with:

`poetry run python nudging/prepare.py`

This generates a csv file for each study in `data/interim` and a combined csv file `data/interim/combined.csv`. Each row represents a subject and contains personal characteristics (covariates) and nudge success calculated using propensity score matching. The covariates used for propensity score matching are defined per dataset/study separately.

### Train
First, choose the features to be used for training through the configuration file `config.yaml`. Only datasets containing all features will be used. By default, we use:
- nudge_domain
- nudge_type
- gender
- age

Train a probabilistic classifier on the combined dataset:

`poetry run python nudging/train.py`

By default logistic regression is used, but you can also select naive Bayes with:

`poetry run python nudging/train.py naive_bayes`

The trained model is written to `models/nudging.joblib`.

### Evaluate
Evaluate the trained model using the combined dataset:

`poetry run python nudging/evaluate.py`

 Note that we have not (yet) split off the dataset for training and evaluation. This means we apply the model on the same dataset we trained on. The computed probability is rounded to 0 or 1 and compared to the nudge success. TO DO: evaluate on unused data.

### Predict
Predict nudge effectiveness using the trained model:

`poetry run python nudging/predict.py`

The predicted nudge effectiveness per subgroup is written to `data/processed/nudge_probability.csv`. Also, plots of the nudge effectiveness are generated and stored in the `plots` folder.


<!-- CONTRIBUTING -->
## Contributing

Contributions are what make the open source community an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

To contribute:

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request


<!-- CONTACT -->
## Contact
[UU Research Engineering team](https://github.com/orgs/UtrechtUniversity/teams/research-engineering) - research.engineering@uu.nl
",2022-08-12
https://github.com/UtrechtUniversity/oceanexplorer,"
<!-- README.md is generated from README.Rmd. Please edit that file -->

<a href=""https://erc.europa.eu/""><img src=""man/figures/erc-logo.jpg"" align=""right"" height=""56"" /></a>
<a href=""https://www.uu.nl/en/news/erc-grant-for-peter-bijl-to-predict-future-sea-level""><img src=""man/figures/oceanice-logo.jpg"" align=""right"" height=""56"" /></a>

# The oceanexplorer

<!-- badges: start -->

[![Project Status: Active – The project has reached a stable, usable
state and is being actively
developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)
[![license](https://img.shields.io/github/license/mashape/apistatus.svg)](https://choosealicense.com/licenses/mit/)
[![Last-changedate](https://img.shields.io/badge/last%20change-2022--08--12-yellowgreen.svg)](/commits/master)
[![R-CMD-check](https://github.com/UtrechtUniversity/oceanexplorer/workflows/R-CMD-check/badge.svg)](https://github.com/UtrechtUniversity/oceanexplorer/actions)
[![Codecov test
coverage](https://codecov.io/gh/UtrechtUniversity/Oceanexplorer/branch/master/graph/badge.svg)](https://app.codecov.io/gh/UtrechtUniversity/Oceanexplorer?branch=master)
<!-- badges: end -->

The goal of oceanexplorer is to enable easy access and exploration of
the [WORLD OCEAN
ATLAS](https://www.nodc.noaa.gov/OC5/SELECT/woaselect/woaselect.html) of
the US agency [NOAA](https://www.ncei.noaa.gov/).

<figure>
<img src=""man/figures/oceanexplorer-demo.gif"" style=""width:95.0%""
alt=""Demo of the ocean explorer app"" />
<figcaption aria-hidden=""true"">Demo of the ocean explorer
app</figcaption>
</figure>

## Check the app

Check the beta-version of the app here:
<https://utrecht-university.shinyapps.io/oceanexplorer/>

## Funding

This project was funded by ERC Starting grant number 802835, OceaNice,
awarded to Peter Bijl.

## Credits

The construction of the R (R Core Team 2022) package *oceanexplorer* and
associated documentation was aided by the packages; *devtools* (Wickham
et al. 2021), *roxygen2* (Wickham, Danenberg, et al. 2022), *testthat*
(Wickham 2022), *shinytest* (Chang, Csárdi, and Wickham 2021), *vdiffr*
(Henry et al. 2022), *knitr* (Xie 2014 ; Xie 2015), *rmarkdown* (Xie,
Allaire, and Grolemund 2018; Xie, Dervieux, and Riederer 2020), and the
superb guidance in the book: *R packages: organize, test, document, and
share your code*, by Wickham (2015).

Data transformation, cleaning and visualization is performed with:
*dplyr* (Wickham, François, et al. 2022), *ggplot2* (Wickham, Chang, et
al. 2022), and *rlang* (Henry and Wickham 2022).

In addition, this package relies on a set of packages for spatial data
analysis: *sf* (Pebesma 2022a) and *stars* (Pebesma 2022b).

The app is build with *shiny* (Chang et al. 2022) and the guidance in
the book: *Mastering Shiny: Build Interactive Apps, Reports &
Dashboards* (Wickham 2020) was a great help in learning how to develop
such applications. Furthermore, the packages *shinyjs* (Attali 2021),
*waiter* (Coene 2022), *bslib* (Sievert and Cheng 2022) and *thematic*
(Sievert, Schloerke, and Cheng 2021) ensure user-friendliness of the
interface and visually pleasing graphics.

## Installation

You can install the latest version of oceanexplorer from CRAN

``` r
# Install oceanexplorer from CRAN: 
install.packages(""oceanexplorer"")
```

## Example

The package allows extraction of global databases of several physical
and chemical parameters of the ocean from the NOAA WORLD OCEAN ATLAS.

``` r
library(oceanexplorer)
# obtain the NOAA world ocean atlas for oxygen content
oxy_global <- get_NOAA(""oxygen"", 1, ""annual"")
```

Slice a specific interval from the array with `filter_NOAA()`, like so:

``` r
# filter a depth of 200 meters to show OMZs
(oxy_omz <- filter_NOAA(oxy_global, depth = 200))
#> stars object with 2 dimensions and 1 attribute
#> attribute(s):
#>            Min.  1st Qu.   Median     Mean  3rd Qu.     Max.  NA's
#> o_an  0.9701567 164.1833 218.6721 206.2584 266.9612 359.0279 26041
#> dimension(s):
#>     from  to offset delta refsys point values x/y
#> lon    1 360   -180     1 WGS 84    NA   NULL [x]
#> lat    1 180    -90     1 WGS 84    NA   NULL [y]
```

In addition, the sliced array can be plotted, like so:

``` r
# plot the NOAA world ocean atlas for oxygen content
plot_NOAA(oxy_omz, depth = NULL)
```

<img src=""man/figures/README-plot1-1.png"" width=""100%"" />

The same plot can be produced by taking the original data and supplying
a value to the `depth` argument and specifying the range of oxygen
content to `oxy_omz`.

``` r
# plot the NOAA world ocean atlas for oxygen content
plot_NOAA(oxy_global, depth = 200, rng = range(oxy_omz[[1]]))
```

<img src=""man/figures/README-plot2-1.png"" width=""100%"" />

# Interactive exploration

Lastly, the package can launch a Shiny app for interactive exploration
of the datasets.

``` r
# launch an interactive shiny session
NOAA_app()
```

Or, the RStudio addin can be launched within the RStudio viewer pain by
executing the following code, or by using the `Addins` drop down menu in
the taskbar.

``` r
# launch an interactive shiny session
NOAA_addin()
```

## Code of Conduct

Please note that the oceanexplorer project is released with a
[Contributor Code of
Conduct](https://utrechtuniversity.github.io/oceanexplorer/CODE_OF_CONDUCT.html).
By contributing to this project, you agree to abide by its terms.

# References

<div id=""refs"" class=""references csl-bib-body hanging-indent"">

<div id=""ref-shinyjs"" class=""csl-entry"">

Attali, Dean. 2021. *Shinyjs: Easily Improve the User Experience of Your
Shiny Apps in Seconds*. <https://deanattali.com/shinyjs/>.

</div>

<div id=""ref-shiny"" class=""csl-entry"">

Chang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke,
Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara
Borges. 2022. *Shiny: Web Application Framework for r*.
<https://shiny.rstudio.com/>.

</div>

<div id=""ref-shinytest"" class=""csl-entry"">

Chang, Winston, Gábor Csárdi, and Hadley Wickham. 2021. *Shinytest: Test
Shiny Apps*. <https://github.com/rstudio/shinytest>.

</div>

<div id=""ref-waiter"" class=""csl-entry"">

Coene, John. 2022. *Waiter: Loading Screen for Shiny*.
<https://CRAN.R-project.org/package=waiter>.

</div>

<div id=""ref-vdiffr"" class=""csl-entry"">

Henry, Lionel, Thomas Lin Pedersen, T Jake Luciani, Matthieu Decorde,
and Vaudor Lise. 2022. *Vdiffr: Visual Regression Testing and Graphical
Diffing*. <https://CRAN.R-project.org/package=vdiffr>.

</div>

<div id=""ref-rlang"" class=""csl-entry"">

Henry, Lionel, and Hadley Wickham. 2022. *Rlang: Functions for Base
Types and Core r and Tidyverse Features*.
<https://CRAN.R-project.org/package=rlang>.

</div>

<div id=""ref-sf"" class=""csl-entry"">

Pebesma, Edzer. 2022a. *Sf: Simple Features for r*.
<https://CRAN.R-project.org/package=sf>.

</div>

<div id=""ref-stars"" class=""csl-entry"">

———. 2022b. *Stars: Spatiotemporal Arrays, Raster and Vector Data
Cubes*. <https://CRAN.R-project.org/package=stars>.

</div>

<div id=""ref-rversion"" class=""csl-entry"">

R Core Team. 2022. *R: A Language and Environment for Statistical
Computing*. Vienna, Austria: R Foundation for Statistical Computing.
<https://www.R-project.org/>.

</div>

<div id=""ref-bslib"" class=""csl-entry"">

Sievert, Carson, and Joe Cheng. 2022. *Bslib: Custom Bootstrap ’Sass’
Themes for Shiny and Rmarkdown*.
<https://CRAN.R-project.org/package=bslib>.

</div>

<div id=""ref-thematic"" class=""csl-entry"">

Sievert, Carson, Barret Schloerke, and Joe Cheng. 2021. *Thematic:
Unified and Automatic Theming of Ggplot2, Lattice, and Base r Graphics*.
<https://CRAN.R-project.org/package=thematic>.

</div>

<div id=""ref-Wickham2015"" class=""csl-entry"">

Wickham, Hadley. 2015. *R Packages: Organize, Test, Document, and Share
Your Code*. O’Reilly Media, Inc. <https://r-pkgs.org/>.

</div>

<div id=""ref-Wickham2020"" class=""csl-entry"">

———. 2020. *Mastering Shiny: Build Interactive Apps, Reports &
Dashboards.* O’Reilly Media, Inc. <https://mastering-shiny.org/>.

</div>

<div id=""ref-testthat"" class=""csl-entry"">

———. 2022. *Testthat: Unit Testing for r*.
<https://CRAN.R-project.org/package=testthat>.

</div>

<div id=""ref-ggplot2"" class=""csl-entry"">

Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,
Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey
Dunnington. 2022. *Ggplot2: Create Elegant Data Visualisations Using the
Grammar of Graphics*. <https://CRAN.R-project.org/package=ggplot2>.

</div>

<div id=""ref-roxygen2"" class=""csl-entry"">

Wickham, Hadley, Peter Danenberg, Gábor Csárdi, and Manuel Eugster.
2022. *Roxygen2: In-Line Documentation for r*.
<https://CRAN.R-project.org/package=roxygen2>.

</div>

<div id=""ref-dplyr"" class=""csl-entry"">

Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022.
*Dplyr: A Grammar of Data Manipulation*.
<https://CRAN.R-project.org/package=dplyr>.

</div>

<div id=""ref-devtools"" class=""csl-entry"">

Wickham, Hadley, Jim Hester, Winston Chang, and Jennifer Bryan. 2021.
*Devtools: Tools to Make Developing r Packages Easier*.
<https://CRAN.R-project.org/package=devtools>.

</div>

<div id=""ref-knitr2014"" class=""csl-entry"">

Xie, Yihui. 2014. “Knitr: A Comprehensive Tool for Reproducible Research
in R.” In *Implementing Reproducible Computational Research*, edited by
Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman;
Hall/CRC. <http://www.crcpress.com/product/isbn/9781466561595>.

</div>

<div id=""ref-knitr2015"" class=""csl-entry"">

———. 2015. *Dynamic Documents with R and Knitr*. 2nd ed. Boca Raton,
Florida: Chapman; Hall/CRC. <https://yihui.org/knitr/>.

</div>

<div id=""ref-rmarkdown2018"" class=""csl-entry"">

Xie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. *R Markdown: The
Definitive Guide*. Boca Raton, Florida: Chapman; Hall/CRC.
<https://bookdown.org/yihui/rmarkdown>.

</div>

<div id=""ref-rmarkdown2020"" class=""csl-entry"">

Xie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. *R Markdown
Cookbook*. Boca Raton, Florida: Chapman; Hall/CRC.
<https://bookdown.org/yihui/rmarkdown-cookbook>.

</div>

</div>
",2022-08-12
https://github.com/UtrechtUniversity/protosc,"# protosc

Python library for automatic feature extraction. It is currently under active development. The overall pipeline looks as follows:

- preprocessing of the data
- feature extraction
- feature selection

Below we list from all these categories, what is currently developed or under development.

## IO

- Read image from file.

## Preprocessing

Currently only image processing preprocessing steps are included.

- convert to grey scale
- Apply Viola-Jones algorithm for face detection
- Cut a circle in the middle (face after VJ)


## Feature extraction

- Fourier transformation of image


## Feature selection

- Filter feature selection
- Combination of results from filter feature selection to final feature selection.

## Pipeline

The pipeline system allows for easy definition of pipelines:

```python
pipe1 = ReadImage()*GreyScale()*ViolaJones(add_perc=15)*FourierFeatures()
pipe2 = ReadImage()*ViolaJones()*FourierFeatures()
pipe_complex = pipe1 + pipe2
features = pipe_complex.execute(x)  # Here x can be a list of filenames
```",2022-08-12
https://github.com/UtrechtUniversity/Qualtrics-Peer-Reports,"Qualtrics: Peer report package

Qualtrics is a powerful online survey tool to create complex surveys that meet a variety of needs. Utrecht University has developed an extension to the standard Qualtrics package that makes it possible to use peer assessment techniques, both peer nominations (e.g., “Which children in your class help others?”) and peer ratings (e.g., “How much do you like ….. [group member]”).

In addition, there are several possibilities to export and analyse the data, including calculation of the indegree, the outdegree, the average received rating, and the adjacency matrix for social network analysis. The package contains a manual, a template to easily create your own peer reports survey, Excel files for importing group members and exporting data, as well as the functional specifications with a brief introduction to peer reports.

-----------------

Qualtrics: Peer report pakket

Qualtrics is een krachtige online-vragenlijsttool om complexe enquêtes te maken. De Universiteit Utrecht heeft een uitbreiding ontwikkeld op het standaard Qualtrics pakket die het mogelijk maakt om peer assessment technieken te gebruiken, zowel peer nominaties (bijv. ""Welke kinderen in je klas helpen anderen?"") als peer ratings (bijv. ""Hoe aardig vind je ..... [naam groepslid]""). 

Bovendien zijn er verschillende mogelijkheden om de gegevens te exporteren en te analyseren, waaronder berekening van de indegree, de outdegree, de gemiddelde ontvangen rating en de adjacency matrix voor sociale-netwerkanalyse. Het pakket bevat een handleiding, een template om gemakkelijk uw eigen peer reports vragenlijst te maken, Excel-bestanden voor het importeren van groepsleden en het exporteren van gegevens, evenals de functionele specificaties met een korte inleiding tot peer reports.

",2022-08-12
https://github.com/UtrechtUniversity/R-data-cafe,"# R Cafe


:exclamation::exclamation: No R Cafe due to COVID-19 outbreak :exclamation::exclamation: 

:question: Are you stuck on an R problem or do you want to share your R code? Feel free to drop a message at [j.debruin1@uu.nl](mailto:j.debruin1@uu.nl).

[![rcafe_logo.png](rcafe_logo.png)](https://github.com/UtrechtUniversity/R-data-cafe/)

The R Cafe is a community event for R users of Utrecht University, UMCU and HU 
(University of Applied Sciences Utrecht). The goal 
of the event is to share knowledge about R and help researchers with programming
problems. The meeting lasts 2 hours, is held every month, and is open without 
registration.

This repository contains additional material for the R Cafe. There are several 
exercises and tips in the [exercises folder](/exercises/). 

During each event, an R user demonstrates an interesting package or application of R. 
The additional material for the demonstrations can be found in the [themes folder](/themes/).


## Themes/showcases

Each R Cafe features a short showcase or
demonstration of (maximum) half an hour. These presentations are given by attendees of the
Cafe. We are always looking for people who want to present something. Please
send an email to Jonathan de Bruin ([j.debruin1@uu.nl](mailto:j.debruin1@uu.nl)) to request to give a
presentation.

## Scheduled meetings

:exclamation::exclamation: No R Cafe due to COVID-19 outbreak :exclamation::exclamation: 

:question: Are you stuck on an R problem or do you want to share your R code? Feel free to drop a message at [j.debruin1@uu.nl](mailto:j.debruin1@uu.nl).

| When | Where | Showcase |
|:---------------------------------------------------------------------------------:|:---------------------------------:|:--------------------------------------------------------------:|
| ~[23 March 2020 from 15:00 to 17:00](https://www.uu.nl/en/node/90435)~ | Bucheliusroom UU Library (Uithof) | CANCEL -> Coronavirus |
| [20 April 2020 from 15:00 to 17:00](https://www.uu.nl/en/node/90439) | Bucheliusroom UU Library (Uithof) | CANCEL -> Coronavirus |
| [18 May 2020 from 15:00 to 17:00](https://www.uu.nl/en/node/90443) | Bucheliusroom UU Library (Uithof) | CANCEL -> Coronavirus |
| [15 June 2020 from 15:00 to 17:00](https://www.uu.nl/en/node/90447) | Bucheliusroom UU Library (Uithof) | CANCEL -> Coronavirus |



## Previous meetings
| When | Where | Showcase |
|:---------------------------------------------------------------------------------:|:---------------------------------:|:--------------------------------------------------------------:|
| [19 February 2020 from 15:00 to 17:00](https://www.uu.nl/en/node/89771) :exclamation: Wednesday :exclamation: | Bucheliusroom UU Library (Uithof) | Deep Learning in R by [Ayoub Bagheri](https://www.uu.nl/staff/ABagheri)(Start at 15:30, 30 minutes) |
| [9 December 2019 from 15:00 to 17:00](https://www.uu.nl/en/node/82093) | Bucheliusroom UU Library (Uithof) | [Start with RMarkdown;A gentle slide (show) into R-package development](themes/start_with_rmd) by Marc Teunis |
| [25 November 2019 from 15:00 to 17:00](https://www.uu.nl/en/node/82091) | Bucheliusroom UU Library (Uithof) | [Text classification in R (part 2)](themes/TextClassification) by Jonathan de Bruin |
| [28 October 2019 from 15:00 to 17:00](https://www.uu.nl/en/node/82083) | Bucheliusroom UU Library (Uithof) | [Text mining in R (part 1)](themes/TextMining) by Jonathan de Bruin |
| [30 September 2019 from 15:00 to 17:00](https://www.uu.nl/en/node/82079) | Bucheliusroom UU Library (Uithof) | -  |
| [17 June 2019 from 15:00 to 17:00](https://www.uu.nl/en/node/73729) | Bucheliusroom UU Library (Uithof) | [Machine Learning by Wienand Omta](themes/machine_learning_omta) by Wienand Omta |
| [20 May 2019 from 15:00 to 17:00](https://www.uu.nl/en/node/73723) | Bucheliusroom UU Library (Uithof) | [Simple Features](themes/simple_features) by Jonathan de Bruin |
| [15 April 2019 from 15:00 to 17:00](https://www.uu.nl/en/node/73715) | Bucheliusroom UU Library (Uithof) |  |
| [25 March 2019 from 15:00 to 17:00](https://www.uu.nl/en/events/r-cafe-4) | Bucheliusroom UU Library (Uithof) | [Code snippets in RStudio](themes/snippets) by Jonathan de Bruin |
| 25 February 2019 | Bucheliusroom UU Library (Uithof) | [Heatmaps](themes/heatmaps) by Barbara Vreede |
| 28 January 2019 | Bucheliusroom UU Library (Uithof) | [How to keep it togetheR](https://github.com/UtrechtUniversity/R-data-cafe/tree/master/themes/list_columns), by Marc Teunis |
| 10 December 2018 | Bucheliusroom UU Library (Uithof) | [Long and wide data](themes/longwide) by Jonathan de Bruin |
| 12 November 2018 | Bucheliusroom UU Library (Uithof) | Shiny apps by Kimberley Lek |
| 15 October 2018 | Bucheliusroom UU Library (Uithof) | [git and github](https://github.com/UtrechtUniversity/R-data-cafe/blob/master/themes/20181015_github.pdf) by Barbara Vreede |
| 10 September 2018 | Bucheliusroom UU Library (Uithof) | [DESeq2 for RNASeq analysis](https://github.com/UtrechtUniversity/R-data-cafe/blob/master/themes/20180910_deseq2.html) by Barbara Vreede |
| 11 June 2018 | Bucheliusroom UU Library (Uithof) | Meta-analyses by Cathalijn Leenaars (UU) |
| 14 May 2018 | Bucheliusroom UU Library (Uithof) | [Joining data](themes/joining_data) by Jonathan de Bruin |
| 16 April 2018 | Bucheliusroom UU Library (Uithof) | JASP by Erik-Jan van Kesteren |
| 12 March 2018 | Bucheliusroom UU Library (Uithof) | [R Markdown by Jonathan de Bruin (UU)](themes/Rmarkdown) |
| 12 February 2018 | Bucheliusroom UU Library (Uithof) | [Reading and writing foreign dataformats by Wietze Pasma (UMCU)](themes/data_import_export) |
| 15 Januari 2018 | Bucheliusroom UU Library (Uithof) | [Regular Expressions by Tessa Pronk (UU)](themes/regular_expressions) |
| 11 December 2017 | Bucheliusroom UU Library (Uithof) | ~~Cancelled due to bad weather conditions~~ |
| 13 November 2017 | Bucheliusroom UU Library (Uithof) | [Hypothesis testing by Kees Mulder (UU)](themes/hypothesis_testing) |
| 16 October 2017 | Bucheliusroom UU Library (Uithof) | [ggplot by Jonathan de Bruin (UU)](themes/ggplot) |



## Resources

**Data Handling**
- http://r4ds.had.co.nz/transform.html
- [Dates and times cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/lubridate.pdf)
- [Work with strings cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/strings.pdf)
- [Data transformation cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf)

**Generalised linear models (GLM)**
- https://www.r-bloggers.com/generalised-linear-models-in-r/
- https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/
- https://www.r-bloggers.com/an-intro-to-models-and-generalized-linear-models-in-r/

**Visualisation**
- http://r4ds.had.co.nz/data-visualisation.html (ggplot)
- [Data visualisation cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf)

**Documenting**
- https://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf
- [R markdown reference](https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf)

**R Packages**
- https://emilyriederer.netlify.com/post/rmarkdown-driven-development/
- https://github.com/jennybc/pkg-dev-tutorial
- https://r4ds.had.co.nz/
- http://r-pkgs.had.co.nz/
- [Rtools](https://cran.r-project.org/bin/windows/Rtools/)
- [Functions](https://richmedia.lse.ac.uk/methodologyinstitute/20170814_ExpressYourselfUsingR.mp4)
- [""here! here!"" - github link J. Bryan](https://github.com/jennybc/here_here)

## Cheatsheets

There are useful cheatsheets available for R topics like visualisation, string handling and Rmarkdown. You can find the cheatsheets at the website of RStudio (https://www.rstudio.com/resources/cheatsheets/)

<img src=""https://www.rstudio.com/wp-content/uploads/2018/08/lubridate.png"" width=""400"">   <img src=""https://www.rstudio.com/wp-content/uploads/2018/08/data-transformation.png"" width=""400"">

## Organisation:

- [Jonathan de Bruin](https://github.com/J535D165)

## More:
RDM support hosts a monthly course with an introduction to R and data. [Check it out!](https://github.com/UtrechtUniversity/workshop-introduction-to-R-and-data)
",2022-08-12
https://github.com/UtrechtUniversity/rc-deeplearningtemplate,# rc-deeplearningtemplate,2022-08-12
https://github.com/UtrechtUniversity/RCrstudio,"---
title: rire
output:
  pdf_document: default
  html_document: default
---
## [![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)


## rire: A Robust Interactive R Environment

",2022-08-12
https://github.com/UtrechtUniversity/reclarify,"## reclarify | automated categorisation and processing of LDIR particle data

Last updated: 04-MAY-2022

### Purpose
The purpose of this code is to automate the grouping of polymer analysis data from the Agilent Clarity software into polymer classes and then bin the resultant data by defined size groups to produce an overview of size and type. This is achieved through the creation of several routines as well as the provision of sample code in this package.

### Author
This code is written by [Joey O'Dell](https://github.com/joey4247) primarily in R. The base methodology is derived from the LDIR Polymer Analysis spreadsheet produced by [Ole Klein](https://www.hereon.de/institutes/coastal_environmental_chemistry/inorganic_environmental_chemistry/team/098593/index.php.de) at the Helmholtz-Zentrum Hereon.

Further support has been provided by the [Department of Earth Sciences at Utrecht University](https://www.uu.nl/en/research/department-of-earth-sciences), [Department of Inorganic Environmental Chemistry at the Helmholtz-Zentrum Hereon GmbH](https://www.hereon.de/institutes/coastal_environmental_chemistry/inorganic_environmental_chemistry/index.php.en), the [Helhomtz Coastal Data Centre](https://www.hereon.de/central_units/hcdc/index.php.en), and the [International Marine Litter Unit of the University of Plymouth](https://www.plymouth.ac.uk/research/marine-litter).

### Help
README files are provided throughout the code bundle. Further help is available in the [Wiki](https://github.com/UtrechtUniversity/reclarify/wiki) and bug reports, suggestions, or faults can be reported [through the GitHub issue tracker](https://github.com/UtrechtUniversity/reclarify/issues).

### Copyright
No rights are reserverd by the author and the code is in the public domain [under the GPLv3 Licence](https://www.gnu.org/licenses/gpl-3.0.en.html).
",2022-08-12
https://github.com/UtrechtUniversity/remindo-api,"[![Documentation Status](https://readthedocs.org/projects/remindo-api/badge/?version=latest)](https://remindo-api.readthedocs.io/en/latest/?badge=latest)

<p align=""center"">
  <img src=""docs/img/remindo.png"" width=""250px""></img>
  </p>
<br/>

# Remindo API

This package provides a Python interface for the API endpoint offered by [Remindo](https://www.paragin.nl/remindotoets/), a product of [Paragin](https://paragin.nl). This API wrapper allows you to extract and read the majority of the exam-related data from Remindo.

This package was developed by Utrecht University and is meant to be used with a read-only access. Therefore, the API features to modify exam-related data are not currently available.

<!-- TABLE OF CONTENTS -->
## Table of Contents

- [Remindo API](#remindo-api)
  - [Table of Contents](#table-of-contents)
  - [About the Project](#about-the-project)
    - [Built with](#built-with)
  - [Dependencies](#dependencies)
  - [Installing](#installing)
    - [Poetry](#poetry)
    - [Pip](#pip)
  - [Getting Started](#getting-started)
  - [Documentation](#documentation)
  - [Examples](#examples)
    - [Creating a Client instance](#creating-a-client-instance)
    - [Check Connection](#check-connection)
    - [Retrieving Clusters](#retrieving-clusters)
    - [Retrieving Studies](#retrieving-studies)
    - [Retrieving Recipes](#retrieving-recipes)
    - [Retrieving Moments](#retrieving-moments)
    - [Retrieving Results](#retrieving-results)
    - [Retrieving Items](#retrieving-items)
    - [Retrieving Items' Statistics and Reliabilities](#retrieving-items-statistics-and-reliabilities)
  - [Contributing](#contributing)
  - [Contact](#contact)

<!-- ABOUT THE PROJECT -->
## About the Project

**Date**: Started January 2019

**Research Software Engineer**:

- Leonardo Jaya Vida (l.j.vida@uu.nl)

### Built with

- [Requests](https://requests.readthedocs.io/en/master/)

## Dependencies

The package contains the following dependencies:

- `requests`. To communicate with the API endpoint.
- `Cryptodome`. To encrypt and decrypt data.
- `json`. To read data.

The dependecies can be installed preferably using ``poetry`` but also using ``pip``. This can be done as explained in [Installing](#installing).

## Installing

### Poetry

On terminal, after navigating to the repository:

```bash
    poetry install
```

If you want to make changes to the code, please install the repository in ""developer mode"":

```bash
    poetry install --dev
```

### Pip

On terminal, after navigating to the repository, install the requirements:

```bash
    pip install remindo-api
```

If you want to make changes to the code, please install the requirements first:

```bash
    pip install -r requirements.txt
    pip install remindo-api
```

## Getting Started

The first thing is to request an API key from Utrecht University's (UU) ITS services.
One person you could contact, if you believe you will be granted access authorization
is Patrick Van der Veer, system architect of UU's ITS.

Once you have the API keys, you need to substitute your keys in the `config.cfg` file,
before creating any client instance to query the Remindo database.


## Documentation

Read more about this package
[here](http://remindo.readthedocs.org/en/latest/).


## Examples

This package provides a Python interface for many Remindo API methods.
Here are a few examples demonstrating how to access data on Remindo.

### Creating a Client instance

```python
    from remindo_api import client
    rc = client.RemindoClient(
        config[""REMINDOKEYS""][""UUID""],
        config[""REMINDOKEYS""][""SECRET""],
        config[""REMINDOKEYS""][""URL_BASE""],
    )
```

### Check Connection

Let's use the `helloworld()` function to test if the credentials to connect to the Remindo endpoint are correct:

```python
    hw = rc.helloworld()
    print(hw.message)
    >>> hello world
```

Once we determined that the credentials are correct, we can continue using the client to retrieve Remindo data.

### Retrieving Clusters

```python
    clusters = rc.list_cluster()
```

Once you have retrieved a list of `cluster`, you can
access data for the queried clusters.

Let's access the first cluster of the list of clusters retrieved.

```python
    >>> clusters[0].rid
    >>> '1111'
    clusters[0].name
    >>> 'cluster_name'
```

More details on the calls available for `clusters` are available [here](https://readthedocs.com).

### Retrieving Studies

You can retrieve information about studies as well.

```python
    studies = rc.list_studies()
    study = studies[0]
```

You can access information about a study as before:

```python
    study.rid
    >>> 'rid'
    study.name
    >>> 'name'
    study.code
    >>> 'code'
    ...
```

More details on the calls available for `studies` are available [here](https://readthedocs.com).

### Retrieving Recipes

Recipes can also be retrieved with a simple call. In the example below we save a list of recipes to be used in later examples.

```python
    list_recipes = []
    [list_recipes.append(recipe.rid) for recipe in rc.list_recipes(cln, full = True)]
```

The call uses the parameter `full = True` (by default is `False`) to indicate that we want to retrieve all the information available on Remindo concerning the Recipes.

More details on the calls available for `recipes` are available [here](https://readthedocs.com).

### Retrieving Moments

Using the list of recipes saved from the previous call, we can now retrieve information about moments contained in each recipe.

Let's access the rid of the first recipe

```python
    recipe_rid = list_recipes[0].rid
    moments = rc.list_moments(recipe_ids = recipe_rid)
```

Once we have retrieved all the moments information for one recipe, we can access these information as usual. We also save the moment id, to be used in a later call.

```python
    # To be used later in examples
    moment_rid = moments[0].rid
    moments[0].rid
    >>> 'moment_rid'
    moments[0].code
    >>> 'moment_code'
    moments[0].date_end
    >>> 'moment_date_end'
    ...
```

More details on the calls available for `moments` are available [here](https://readthedocs.com).

### Retrieving Results

Using the previously saved recipes information, we can use the general call `list_results` to retrive results' information about recipes.

```python
    for recipe in list_recipes[1:2]:
        result_recipe = rc.list_results(cln, recipe_ids= recipe)
        print([[r.i_correct,r.i_answered]  for r in result_recipe])
```

More details on the calls available for `recipes`' results are available [here](https://readthedocs.com).

Using the previously saved moment id, we now retrieve results' informations on individual moments using the `list_moment_results` call.

```python
    moment_results = rc.list_moments_results(id = moment_rid)
    moment_results[0].subscription_id
    >>> 'moment_subscription_id'
    moment_results[0].user_id
    >>> 'moment_user_id'
    moment_results[0].user_code
    >>> 'moment_user_code'
    ...
```

More details on the calls available for `moments`' results are available [here](https://readthedocs.com).

### Retrieving Items

```python
    for recipe in list_recipes[1:2]:
        print([item.duration for item in rc.list_itemresults(
            recipe_id = recipe,
            add_item_info = True)]
        )
        print(cln.list_itemresults(recipe_id = recipe, add_item_info = True))
```

More details on the calls available for `items`' results are available [here](https://readthedocs.com).

### Retrieving Items' Statistics and Reliabilities

```python
    for recipe in list_recipes:
        print(cln.list_reliability(recipe_id = recipe))
```

More details on the calls available for `items`' statistics and `reliabilities` are available [here](https://readthedocs.com).

<!-- CONTRIBUTING -->
## Contributing

Contributions are what make the open source community an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

To contribute:

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

<!-- CONTACT -->
## Contact

Leonardo Vida - [@leonardojvida](https://twitter.com/leonardojvida) - l.j.vida@uu.nl

Project Link: [https://github.com/UtrechtUniversity/remindo-api](https://github.com/UtrechtUniversity/remindo-api)
",2022-08-12
https://github.com/UtrechtUniversity/research-engineering,"|[Home](index.md)|[Projects](_pages/projects/index.md)|[Publications](_pages/publications/index.md)|[Manuals](_pages/manuals/index.md)|
|---|---|---|---|

## Research Engineering at Utrecht University

This is the repo of our [team website](https://utrechtuniversity.github.io/research-engineering/) build with [GitHub Pages](https://docs.github.com/en/pages) and [Jekyll](https://jekyllrb.com/).

- To contribute, please create a pull request and ask a team member to review. If you commit directly to the main branch, the changes will be immediately deployed to the live website.

- You can also build the website locally to preview and test changes. See instructions for [testing Github pages locally](https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll/testing-your-github-pages-site-locally-with-jekyll).

- If you like to customize the layout and style, please read the docs on the [Slate theme](https://github.com/pages-themes/slate).",2022-08-12
https://github.com/UtrechtUniversity/researchcloud-items,"# researchcloud-items
This repository contains [Ansible](ansible.com) installation scripts for use in conjunction with [SURF ResearchCloud](https://portal.live.surfresearchcloud.nl). ResearchCloud catalog maintainers can configure a playbook from this repo as a script source for a plugin-type catalog item.  
Alternatively feel free to clone this repository on a target host and locally run a playbook using the command  
`ansible-playbook <name-of-the-playbook>`. 

## Documentation
Script developers, please consult the [developer documentation](docs/index.md) before using a playbook 
to find out if the playbook meets your use case.  
For end-users, there is a [Primer SURF ResearchCloud](docs/primer-for-users.md).

## Applicable licences
Some of the code maintained in this repo is derived from other sources. As a consequence, unfortunately we are unable to provide the entire repo content under a single general license. We use the following licensing policy:
1) If their exists a license file in the root directory of a role then that license applies to all files belonging to that role. 
2) In all other cases the license specified in the top-level directory of this repository applies. 

## Contributing
We are very happy with any contributions in terms of new Ansible scripts for Research Plug-ins and/or documentation. Read the contributing [guidelines](/CONTRIBUTING.md).
",2022-08-12
https://github.com/UtrechtUniversity/S23,"# Utrecht Summer School S23
This repository contains the computer lab exercises of Utrecht Summer School S23 ""Advanced course on using Mplus"", days 1 to 4.
",2022-08-12
https://github.com/UtrechtUniversity/SCCA,"
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4665670.svg)](https://doi.org/10.5281/zenodo.4665670)

## **SCCA: Spectral Clustering Correspondence Analysis in R**

### Introduction

The SCCA package implements in R the methodological approach to CA as proposed in **Correspondence analysis, spectral clustering and graph embedding: applications to ecology and economic complexity** [van Dam et al; 2021](#article).

### Installation

The package can be installed directly from Github with the code below. Ensure the package `devtools` has been installed.

```
#install.packages(""devtools"")
library(devtools)
install_github(""UtrechtUniversity/scca"", build_vignettes = TRUE)
```

### Documentation of exported functions and data set

After loading the package a list of all exported functions and data sets can be retrieved by `?SCCA` and the documentation of an individual function by `?<function name>`; e.g. `?scca_compute`.

The methodology and the use of the functions and the data are explained in the included vignette. After installing package SCCA use `browseVignettes('SCCA')` in the R(Studio) console.

### License

The software code is licensed under [**MIT**](https://opensource.org/licenses/MIT). The next section (References) provides links to 
sources of the included datasets. See there for licences of those data sets.

### References

#### Software
<a name=""article""></a>van Dam, Alje, Dekker, Mark,  Morales-Castilla, Ignacio, Rodríguez, Miguel Á., Wichmann, David and Baudena, Mara (2021); Correspondence analysis, spectral clustering and graph embedding: applications to ecology and economic complexity; _Scientific Reports_; DOI: 10.1038/s41598-021-87971-9

#### Included data set
Faurby, Søren e.a; 2019; [HYLACINE 1.2: The Phylogenetic Atlas of Mammal Macroecology](https://datadryad.org/stash/dataset/doi:10.5061/dryad.bp26v20)


### The team

The team members are:

* Mathematical foundations  of the code
   - Alje van Dam, Copernicus Institute of Sustainable Development and Centre for Complex Systems Studies, Utrecht University, the Netherlands
   - Mark Dekker, Department of Information and Computing Sciences and Centre for Complex Systems Studies, Utrecht University, the Netherlands

* Programming and packaging
   - [Kees van Eijden](k.vaneijden@uu.nl) Research Engineering/ITS, Utrecht University, the Netherlands

* With contributions of
   - Ignacio Morales Castilla, Global Change Ecology and Evolution Group, Department of Life Sciences, University of Alcala´, Spain 
   - Jonathan de Bruin, Research Engineering/ITS, Utrecht University, the Netherlands
   - Raoul Schram, Research Engineering/ITS, Utrecht University, the Netherlands
   - Mara Baudena, National Research Council of Italy, Institute of Atmospheric Science and Climate (CNR-ISAC), Turin, Italy; Copernicus Institute of Sustainable Development and Centre for Complex Systems Studies, Utrecht University, the Netherlands


### How to cite SCCA

To cite the SCCA repository and R package, use `citation(""SCCA"")` to retrieve the BibTex entry. Otherwise use the following format:

van Eijden, Kees et al; 2021; SCCA: Spectral Clustering Correspondence Analysis in R; Utrecht University; DOI: 10.5281/zenodo.4665670.
Also available at [Utrecht University](https://github.com/UtrechtUniversity/SCCA). 

Please also cite the paper [van Dam et al, 2021](#article) when using the SCCA repository.

",2022-08-12
https://github.com/UtrechtUniversity/streetview-greenery,"# streetview-greenery

## Most of the package does not work anymore, because the municipality of amsterdam made their panorama data private.

Project to retrieve and process panoramic photo's and compute the greenery as perceived from street level.

## Description

The base idea of this package is to download street view panorama (or other) pictures and compute measures of greenness. The first step is to do a segmentation analysis: assigning object classes to individual pixels. The second step is to convert these segmented pictures into a measure of greenness. The last step is to create a map from individiual greenness measures.

## Example maps

Sampled map of Amsterdam/Almere: [link](https://qubixes.github.io/streetview-greenery/docs/adam_alm.html)

## Installation

There is at the moment no real installation script. First install the (non-python) GDAL library seperately. Then install the python bindings that has the same version (`pip install ""gdal==x""`), where x is the version of your GDAL library. Finally install the remaining requirements using `pip install -r requirements.txt`.

## Quickstart [Command Line Interface]

There are two ways to interact with this package: Command Line Interfaces (CLI) or directly using it as a python library (API). The quickest way to start using the package is to use the CLI. There are several runnable files, the main one being ""streetgreen.py"".



#### Creating a dataset [street_green.py]

The script ""streetgreen.py"" is the workhorse of the package, downloading and processing images, doing spatial kriging and creating maps. Currently a single data source is included, which is panoramas provided freely by the municipality of Amsterdam: [data.amsterdam.nl](https://data.amsterdam.nl).

By default the program will select some area in Amsterdam/Almere and it will sample panoramas in a grid. At the coarsest level it will obtain one data point for each 1km x 1km tile. Then from the greenery measure of each of these tiles, a map will be constructed using a Kriging procedure. 

Options for the ""streetgreen.py"" script can be obtained by navigating to the installation directory and typing:

```sh
./streetgreen.py --help
```

The most important option is to select the bounding box of the area to map, with `""-b/--bbox`. The bounding box ""almere_amsterdam"" includes nearly the whole coverage of the dataset.

The segmentation model can be selected through `-m/--model`. Currently, only pretrained models from [DeepLab](https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md) are available for selection. Choose ""deeplab-mobilenet"" for fast inference and ""deeplab-xception_71"" for higher quality inference.

Process based parallelization is available through a combination of the `-n` and `-i` options, which distributes the 1km x 1km tiles over different jobs. For example if you have two machines, then on one you would run `-n 2 -i 0`, while you would give the other machine the `-n 2 -i 1` option. Then merging the tile data, you would have the complete data that you would get running with default options.

The resolution can be adjusted by using the `-l/--grid` options. Every level higher increases the spatial resolution of the map by a factor of 2, with the lowest being 1km x 1km, using the option `-l 0`.

Instead of sampling only once per grid point, the option `-y/--historical-data` allows for sampling of each available year at each grid point.

By default the program uses cubic pictures. With the `--panorama` option, panorama images are used instead, which takes less time, since only one instead of four pictures are analysed. But accuracy is expected to be reduced.

#### Individual picture analysis [seg_analysis.py]

Segmentation analysis of individual/arbitrary pictures/panoramas is available through the `seg_analysis.py` script. It takes a single argument: the picture to analyse. It will show the picture + the segmentation overlay in a new window.

#### Basic statistical comparison [comp_green.py]

To compare different ""green"" measures against each other, the `comp_green.py` script can be used. It computes a linear regression between different variables or the same variable but panoramic vs cubic pictures.


#### Temporal comparison [ time_compare.py ]

Compare points at the same location but different points in time with each other to observe trends of variations with regards to season, year, time of day.

## Application Programming Interface

There are several layers of abstraction to solve the problem of getting data, organizing files, and doing segmentation analyses. The recommended way is to use the tile manager, which ensures that the panoramas are divided into 1km x 1km tiles. This gives the benefit of possible parallelization, but also ensures that not too many pictures are placed in the same folder, and the amount of memory used is limited.

#### TileManager

To create an instance of a tile managers, one does the following:

```python
tile_man = TileManager(seg_model=DeepLabModel,
					 seg_kwargs={""model_name"": ""xception_71""}
					 green_model=ClassPercentage,
					 grid_level=1,
					 bbox=select_area(""amsterdam""))
```

This creates a tile manager that organizes the data and pictures. By initializing the tile manager, the data is not automatically downloaded yet. There are two different ways to continue; the recommended way to obtain the greenery data is to use the `green_direct` method, which does all the steps in sequence. One advantage is that it will do each tile in sequence and clears the memory usage of temporary data:

```python
green_res = tile_man.green_direct()
```

One thing to keep in mind is that a lot of temporary data is stored/cached, so that it does not need to be recalculated every time. For example the segmentation of each picture is stored in a file such as `segments_cubic_deeplab_mobilenet.json`, which contains a gzip'ed segmentation array converted to base64 to make it portable/human readable, but also space efficient.

The functions will try to detect this file and not recompute it if it is already available. A next step would be to compute the vegetation percentages, which are also cached in a `green_res.json`, and include the WGS84 coordinates.

#### Visualization

The greenery can be plotted in a 2d contour plot by using either the ""plot\_greenery"" or the  or ""krige\_map"" method of the TileManager class.

```python
plot_greenery(green_res)
```

This procedure uses linear interpolation of the data points.

or create an HTML map (under data.amsterdam/maps/) overlayed on top of OpenStreetMap:

```python
tile_man.krige_map()
```

This procedure uses ordinary kriging, with currently an exponential variogram. For performance reasons, the kriging procedure is not applied to the data as a single block, but instead, stitched together from each tile. The variogram is computed from all points (but with sampling).

",2022-08-12
https://github.com/UtrechtUniversity/summer-fair,"# SUMMER_FAIR
![GitHub](https://img.shields.io/github/license/UtrechtUniversity/summer-fair)

This software integrates existing data sets on transmission experiments by using semantic web technologies

Data sets on transmission experiments may vary in format, structure, syntax and semantics.
It is difficult to combine these heterogeneous data sets, for example, to do reanalysis and meta-analysis.
For this reason we developed an [ontology](/src/create_ontology/map_ontology/infection_trans.owl).
This shared vocabulary describes the main concepts and relations in the domain of transmission.
By mapping existing data sets to the concepts in the ontology, the data sets can be combined.
The mapped data sets are represented as linked data triples. 
## WIKI 

Please refer to a [wiki page](https://github.com/UtrechtUniversity/summer-fair/wiki) for tutorials and background information on ontologies and SPARQL
## License

This project is licensed under the terms of the [MIT License](/LICENSE.md)

## Citation

Please [cite this project as described here](/CITATION.md).
",2022-08-12
https://github.com/UtrechtUniversity/SWORDS-template,"# Welcome to SWORDS

Welcome to the **S**can and revie**W** of **O**pen **R**esearch **D**ata and **S**oftware (**SWORDS**) project. The SWORDS project is a powerful framework to gain insights in the open source activities of your, but not limited to, university or research institute. This repository is the start of your own implementation for your organisation! Read this part carefully. For any questions, please use the [issue tracker](https://github.com/UtrechtUniversity/SWORDS-template/issues). 

This repository consists of 2 parts, the instruction and the template. The instruction is what you are reading right now, the template starts after `<<< The template starts here >>>` below. To implement the SWORDS framework to your organisation easily, we recommend to make use of the template function in GitHub. The instruction below helps you to implement SWORDS for your organisation in step-by-step manner. 

## Getting started with SWORDS

1. Define a name for your project. The recommended name is SWORDS@{INSERT YOUR ORGANISATION NAME ABBRAVATION}. 
2. [Create a repository from the SWORDS template](https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-repository-from-a-template) with the name as defined in step 1. 
3. Replace all fields in the README of your repostitory starting with `{INSERT ...}` by your details of your project and organisation. Use search `{INSERT`, because some can be harder to find. 
4. Delete in this README everything between `<!-- REMOVE EVERYTHING BEFORE THIS LINE IN STEP 4 -->`. 

## <<< The template starts here >>>
<!-- REMOVE EVERYTHING BEFORE THIS LINE IN STEP 4 -->

# SWORDS@{INSERT YOUR ORGANISATION NAME ABBRAVATION}

![banner](docs/banner.png)

This repository implements for the [**S**can and revie**W** of **O**pen **R**esearch **D**ata and **S**oftware (SWORDS)](https://github.com/UtrechtUniversity/SWORDS-template) framework. SWORDS is a powerful tool to gain insights in the open source activities of your, but not limited to, university or research institute. Studies show that open source contributions can be very benificial for organisations and society. SWORDS is divided into 3 stages that can be exectued and analyzed standalone: finding user profiles associated to your organisation, extract relevant repositories, and study the contents of the respositories. 

<p align=""center"">
  This repository is an implementation of SWORDS for <b>{INSERT YOUR ORGANISATION NAME}</b>.
   <!-- For example Utrecht University -->
</p>
<p align=""center"">
  <b>SWORDS@{INSERT YOUR ORGANISATION NAME ABBRAVATION}</b>
  <!-- For example SWORDS@UU -->
</p>

<p align=""center"">
  <img src=""docs/your_logo.svg"">
</p>

## Background

Conducting science in an open and collaborative way is very important for greater scientific and societal impact (https://www.nwo.nl/en/open-science). The Open Science movement is therefore actively promoting and advancing openness in academic publications, data, software, and other types of output. Major steps have been taken in the last years, however, there is still a lot to improve and win. A result of this movement towards more open research output is the increase of code and software published by researchers on online platforms. The use and reuse of open source (research) software and code is widespread nowadays. 

For (research) organisations, it can be interesting to have insights into the contributions and activities in open source software projects. Especially because activities of members, researchers, or employees can be divided over multiple user profiles and platforms (e.g. GitHub and GitLab). Having an overview of the activity, contents, and quality can be useful for various reasons like connecting initiatives, improve quality, and reward and recognize contributions. The SWORDS framework was introduced to help with collecting insights and assessing quality. 

<!-- Introduce FAIR -->

## The SWORDS framework


- Its goal is to analyze the FAIRness of GitHub repositories of {INSERT YOUR ORGANISATION NAME} researchers and see how they develop and manage software. The purpose of this research is to serve as a template for other researchers to scan and review repositories for their organisation as well.



The SWORDS framework consists of three mostly-independent steps. Each step is described below and detailed information and instructions can be found in the links. 

1. **[Find user profiles associated to organisation](collect_users/)**. In this first step of the framework, multiple strategies are available to find users on GitHub and GitLab that are members or employees of your organisation. By using multiple collection strategies, the recall is considered to be high. The findability of members or employees is one of the output variables of this step. A structured list of users is used as input for the next step.
2. **[Collect relevant repositories](collect_repositories/)**. In the second step, the repositories of the users collected in the previous step are collected and filtered. Not all repositories are research output. These repositories are filted out automatically and/or manually.
3. **[Study and analyze repositories](collect_variables/)**. In this step, further variables of research repositories are gathered. These are then examined and studied on various quantitiative and qualitative properties. One can think about quality assesments, documentation availablity, FAIRness scores. 

<p align=""center"">
  <img src=""docs/SWORDS_basic_flow.drawio.png"">
</p>

The SWORDS framework is written in Python 3.6+. The SWORDS framework implements the reproducible project structure as proposed by [Wilson et al. (2017)](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510) for each step of the framework. For more information about the phases, please look into the corresponding subfolders for further information on installation and usage. All output data is stored in output folders. There is also a corresponding interactive data analysis jupyter notebook file in each step. This notebook can be used for results. 

## Results

The results of the three steps of the SWORDS framework can be found in each of the subfolders of the project. Each step contains a Jupyter notebook with results, code, and narrative. Output data files can be found in the output folder in each step.  

Results of step 1: **Find user profiles associated to organisation**
- :open_book: [Analysis (Jupyter notebook)](collect_users/analyze_users.ipynb) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :inbox_tray: [Input data](collect_users/methods) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :outbox_tray: [Results (data)](collect_users/results) 

Results of step 2: **Collect relevant repositories**
- :open_book: [Analysis (Jupyter notebook)](collect_repositories/analyze_repositories.ipynb) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :inbox_tray: [Input data](collect_users/results) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :outbox_tray: [Results (data)](collect_repositories/results) 

Results of step 3: **Study and analyze repositories**
- :open_book: [Analysis (Jupyter notebook)](collect_variables/analyze_metrics.ipynb) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :inbox_tray: [Input data](collect_repositories/results) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :outbox_tray: [Results (data)](collect_variables/results) 

## Usage

In the normal use case, you start with phase 1 and go through each phase as the output of phase 1 can be used for phase 2. However, if you already have collected a list of users for example, you can skip phase 1 and use that collected list as input for phase 2. Since the phases are independent of each other, this approach is possible. For details on how to execute each phase, look into the corresponding subfolder.

## Citation

Use this citation for citing the SWORDS implementation of {INSERT YOUR ORGANISATION NAME}.
```
{INSERT CITATION DETAILS}
```

To cite the SWORDS project and the SWORDS template, use 

```
@software{de_Bruin_Scan_and_revieW_2022,
author = {de Bruin, Jonathan and Quach, Keven and Slewe, Christopher and Lamprecht, Anna-Lena},
month = {2},
title = {{Template of Scan and revieW of Open Research Data and Software}},
url = {https://github.com/UtrechtUniversity/SWORDS-template},
version = {1.0.0},
year = {2022}
}
```

## License

The SWORDS project template is published under the [MIT License](/license).

Icons of the banner are made by [Freepik](https://www.freepik.com ""Freepik"") from [www.flaticon.com](https://www.flaticon.com/ ""Flaticon"").

## Contact

For contact about this implementation of the SWORDS project, SWORDS@{INSERT YOUR ORGANISATION NAME ABBRAVATION}, post an issue on the [issue tracker](../../issues) or contact {INSERT YOUR EMAIL}. 

For general questions and remarks about the SWORDS project and the template can be posted on the [issue tracker of the SWORDS template](https://github.com/UtrechtUniversity/SWORDS-template/issues). You can also contact either [Jonathan de Bruin](https://github.com/J535D165) or [Keven Quach](https://github.com/kequach). 

The SWORDS project is an idea of the FAIR Data and Software working group of the [Utrecht University Open Science Programme](https://www.uu.nl/en/research/open-science).
",2022-08-12
https://github.com/UtrechtUniversity/SWORDS-UU,"# SWORDS@UU 

![banner](docs/banner.png)


This repository implements for the [**S**can and revie**W** of **O**pen **R**esearch **D**ata and **S**oftware (SWORDS)](https://github.com/UtrechtUniversity/SWORDS-template) framework. SWORDS is a powerful tool to gain insights in the open source activities of your, but not limited to, university or research institute. Studies show that open source contributions can be very benificial for organisations and society. SWORDS is divided into 3 stages that can be exectued and analyzed standalone: finding user profiles associated to your organisation, extract relevant repositories, and study the contents of the respositories. 

<p align=""center"">
  This repository is an implementation of SWORDS for <b>Utrecht University</b>.
   <!-- For example Utrecht University -->
</p>
<p align=""center"">
  <b>SWORDS@UU</b>
  <!-- For example SWORDS@UU -->
</p>

<p align=""center"">
  <img src=""https://github.com/UtrechtUniversity/.github/raw/main/profile/ubd-afsluiting-logo-pay-off-eng.png?raw=true"">
</p>

## Background

Conducting science in an open and collaborative way is very important for greater scientific and societal impact (https://www.nwo.nl/en/open-science). The Open Science movement is therefore actively promoting and advancing openness in academic publications, data, software, and other types of output. Major steps have been taken in the last years, however, there is still a lot to improve and win. A result of this movement towards more open research output is the increase of code and software published by researchers on online platforms. The use and reuse of open source (research) software and code is widespread nowadays. 

For (research) organisations, it can be interesting to have insights into the contributions and activities in open source software projects. Especially because activities of members, researchers, or employees can be divided over multiple user profiles and platforms (e.g. GitHub and GitLab). Having an overview of the activity, contents, and quality can be useful for various reasons like connecting initiatives, improve quality, and reward and recognize contributions. The SWORDS framework was introduced to help with collecting insights and assessing quality. 

<!-- Introduce FAIR -->

## The SWORDS framework


- Its goal is to analyze the FAIRness of GitHub repositories of Utrecht University researchers and see how they develop and manage software. While the analysis and data collection is done for Utrecht University researchers only, the purpose of this research is to serve as a template for other researchers to scan and review repositories for their university as well.



The SWORDS framework consists of three mostly-independent steps. Each step is described below and detailed information and instructions can be found in the links. 

1. **[Find user profiles associated to organisation](collect_users/)**. In this first step of the framework, multiple strategies are available to find users on GitHub and GitLab that are members or employees of your organisation. By using multiple collection strategies, the recall is considered to be high. The findability of members or employees is one of the output variables of this step. A structured list of users is used as input for the next step.
2. **[Collect relevant repositories](collect_repositories/)**. In the second step, the repositories of the users collected in the previous step are collected and filtered. Not all repositories are research output. These repositories are filted out automatically and/or manually.
3. **[Study and analyze repositories](collect_variables/)**. In this step, further variables of research repositories are gathered. These are then examined and studied on various quantitiative and qualitative properties. One can think about quality assesments, documentation availablity, FAIRness scores. 

<p align=""center"">
  <img src=""docs/SWORDS_basic_flow.drawio.png"">
</p>

The SWORDS framework is written in Python 3.6+. The SWORDS framework implements the reproducible project structure as proposed by [Wilson et al. (2017)](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510) for each step of the framework. For more information about the phases, please look into the corresponding subfolders for further information on installation and usage. All output data is stored in output folders. There is also a corresponding interactive data analysis jupyter notebook file in each step. This notebook can be used for results. 

## Results

The results of the three steps of the SWORDS framework can be found in each of the subfolders of the project. Each step contains a Jupyter notebook with results, code, and narrative. Output data files can be found in the output folder in each step.  

Results of step 1: **Find user profiles associated to organisation**
- :open_book: [Analysis (Jupyter notebook)](collect_users/analyse_users.ipynb) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :inbox_tray: [Input data](collect_users/data) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :outbox_tray: [Results (data)](collect_users/results) 

Results of step 2: **Collect relevant repositories**
- :open_book: [Analysis (Jupyter notebook)](collect_repositories/analyse_repositories.ipynb) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :inbox_tray: [Input data](collect_repositories/data) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :outbox_tray: [Results (data)](collect_repositories/results) 

Results of step 3: **Study and analyze repositories**
- :open_book: [Analysis (Jupyter notebook)](collect_variables/analyse_metrics.ipynb) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :inbox_tray: [Input data](collect_variables/data) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; :outbox_tray: [Results (data)](collect_variables/results) 

## Dashboard

A Kibana dashboard has been implemented to work with the data in addition to the Jupyter notebooks. The exported dashboard can be found in the subfolder [`kibana_dashboard/data-dashboard.ndjson`](kibana_dashboard/data-dashboard.ndjson). In order to run the dashboard on your local machine, first install ElasticSearch and Kibana as stated [on the Elastic website](https://www.elastic.co/start). To import the data and dashboard, follow [the documentation](https://www.elastic.co/guide/en/kibana/current/saved-objects-api-import.html). Alternatively, there is an easier method of importing in version 7.16:

1. Open the *hamburger menu*
2. Scroll down to Management --> click *Stack management*
3. Under Kibana, click on *Saved Objects*
4. Click on the *import* button and select the file [`data-dashboard.ndjson`](kibana_dashboard/data-dashboard.ndjson).

After following their documentation or these steps, the dashboard with the related data in the indices *repos* and *users* should be loaded.

## Usage

In the normal use case, you start with phase 1 and go through each phase as the output of phase 1 can be used for phase 2. However, if you already have collected a list of users for example, you can skip phase 1 and use that collected list as input for phase 2. Since the phases are independent of each other, this approach is possible. For details on how to execute each phase, look into the corresponding subfolder.

## Citation

Use this citation for citing the SWORDS implementation of Utrecht University.

```
@software{de_Bruin_Scan_and_revieW_2021,
author = {de Bruin, Jonathan and Quach, Keven and Slewe, Christopher and Lamprecht, Anna-Lena},
month = {9},
title = {{Scan and revieW of Open Research Data and Software at Utrecht University}},
url = {https://github.com/UtrechtUniversity/SWORDS-UU},
version = {1.0.0},
year = {2021}
}
```

To cite the SWORDS project and the SWORDS template, use 

```
@software{de_Bruin_Scan_and_revieW_2022,
author = {de Bruin, Jonathan and Quach, Keven and Slewe, Christopher and Lamprecht, Anna-Lena},
month = {2},
title = {{Template of Scan and revieW of Open Research Data and Software}},
url = {https://github.com/UtrechtUniversity/SWORDS-template},
version = {1.0.0},
year = {2022}
}
```

## License

The SWORDS project template is published under the [MIT License](/license).

Icons of the banner are made by [Freepik](https://www.freepik.com ""Freepik"") from [www.flaticon.com](https://www.flaticon.com/ ""Flaticon"").

## Contact

For contact about this implementation of the SWORDS project, SWORDS@UU, post an issue on the [issue tracker](../../issues) or contact [Jonathan de Bruin](https://github.com/J535D165) or [Keven Quach](https://github.com/kequach). 

For general questions and remarks about the SWORDS project and the template can be posted on the [issue tracker of the SWORDS template](https://github.com/UtrechtUniversity/SWORDS-template/issues). You can also contact either [Jonathan de Bruin](https://github.com/J535D165) or [Keven Quach](https://github.com/kequach). 

The SWORDS project is an idea of the FAIR Data and Software working group of the [Utrecht University Open Science Programme](https://www.uu.nl/en/research/open-science).
",2022-08-12
https://github.com/UtrechtUniversity/temporal-network-synthesis,"# temporal-network-synthesis
Utilities to generate temporal networks and analyze them
",2022-08-12
https://github.com/UtrechtUniversity/transferice,"
<a href=""https://utrechtuniversity.github.io/transferice/""><img src=""man/figures/oceanice-logo.jpg"" align=""right"" height=""56"" /></a>
<!-- README.md is generated from README.Rmd. Please edit that file -->

# transferice

<!-- badges: start -->

[![Project Status: Concept – Minimal or no implementation has been done
yet, or the repository is only intended to be a limited example, demo,
or
proof-of-concept.](https://www.repostatus.org/badges/latest/concept.svg)](https://www.repostatus.org/#concept)
[![license](https://img.shields.io/github/license/mashape/apistatus.svg)](https://choosealicense.com/licenses/mit/)
[![Last-changedate](https://img.shields.io/badge/last%20change-2022--07--28-yellowgreen.svg)](/commits/master)
[![Codecov test
coverage](https://codecov.io/gh/UtrechtUniversity/transferice/branch/master/graph/badge.svg)](https://app.codecov.io/gh/UtrechtUniversity/transferice?branch=master)
<!-- badges: end -->

The goal of transferice is to reconstruct past oceanographic conditions
using fossils. All steps of data selection, model construction, and
final predictions are implemented in a *shiny* (Chang et al. 2021)
interface to provide a visual representation of the machine learning
pipeline.

<figure>
<img src=""man/figures/transferice-demo.gif"" style=""width:95.0%""
alt=""Demo of the transferice app"" />
<figcaption aria-hidden=""true"">Demo of the transferice app</figcaption>
</figure>

## Installation

You can install the development version of transferice from GitHub with
devtools:

``` r
# Install tranferice from GitHub: 
# install.packages(""devtools"")
devtools::install_github(""UtrechtUniversity/transferice"")
```

## Shiny app

Run the app as follows:

``` r
# load package
library(transferice)
# run app
transferice_app()
```

## Funding

This project was funded by ERC Starting grant number 802835, OceaNice,
awarded to Peter Bijl.

## Credits

The construction of the R (R Core Team 2022) package *transferice* and
associated documentation was aided by the packages; *devtools* (Wickham
et al. 2021), *roxygen2* (Wickham, Danenberg, et al. 2022), *testthat*
(Wickham 2022), *knitr* (Xie 2014 ; Xie 2015), *rmarkdown* (Xie,
Allaire, and Grolemund 2018; Xie, Dervieux, and Riederer 2020), and the
superb guidance in the book: *R packages: organize, test, document, and
share your code*, by Wickham (2015).

Data transformation, cleaning and visualization is performed with:
*dplyr* (Wickham, François, et al. 2022), *tibble* (Müller and Wickham
2022), *stringr* (Wickham 2019), and *rlang* (Henry and Wickham 2022).

The app is build with *shiny* (Chang et al. 2021) and the guidance in
the book: *Mastering Shiny: Build Interactive Apps, Reports &
Dashboards* (Wickham 2020) was a great help in learning how to develop
such applications. Furthermore, the packages *shinyjs* (Attali 2021),
*shinyWidgets* (Perrier, Meyer, and Granjon 2022), *shinycssloaders*
(Sali and Attali 2020), *bslib* (Sievert and Cheng 2021) and *thematic*
(Sievert, Schloerke, and Cheng 2021) ensure user friendliness and
visually pleasing graphics.

## References

<div id=""refs"" class=""references csl-bib-body hanging-indent"">

<div id=""ref-shinyjs"" class=""csl-entry"">

Attali, Dean. 2021. *Shinyjs: Easily Improve the User Experience of Your
Shiny Apps in Seconds*. <https://deanattali.com/shinyjs/>.

</div>

<div id=""ref-shiny"" class=""csl-entry"">

Chang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke,
Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara
Borges. 2021. *Shiny: Web Application Framework for r*.
<https://shiny.rstudio.com/>.

</div>

<div id=""ref-rlang"" class=""csl-entry"">

Henry, Lionel, and Hadley Wickham. 2022. *Rlang: Functions for Base
Types and Core r and Tidyverse Features*.
<https://CRAN.R-project.org/package=rlang>.

</div>

<div id=""ref-tibble"" class=""csl-entry"">

Müller, Kirill, and Hadley Wickham. 2022. *Tibble: Simple Data Frames*.
<https://CRAN.R-project.org/package=tibble>.

</div>

<div id=""ref-shinyWidgets"" class=""csl-entry"">

Perrier, Victor, Fanny Meyer, and David Granjon. 2022. *shinyWidgets:
Custom Inputs Widgets for Shiny*.
<https://CRAN.R-project.org/package=shinyWidgets>.

</div>

<div id=""ref-rversion"" class=""csl-entry"">

R Core Team. 2022. *R: A Language and Environment for Statistical
Computing*. Vienna, Austria: R Foundation for Statistical Computing.
<https://www.R-project.org/>.

</div>

<div id=""ref-shinycssloaders"" class=""csl-entry"">

Sali, Andras, and Dean Attali. 2020. *Shinycssloaders: Add Loading
Animations to a Shiny Output While It’s Recalculating*.
<https://github.com/daattali/shinycssloaders>.

</div>

<div id=""ref-bslib"" class=""csl-entry"">

Sievert, Carson, and Joe Cheng. 2021. *Bslib: Custom Bootstrap ’Sass’
Themes for Shiny and Rmarkdown*.
<https://CRAN.R-project.org/package=bslib>.

</div>

<div id=""ref-thematic"" class=""csl-entry"">

Sievert, Carson, Barret Schloerke, and Joe Cheng. 2021. *Thematic:
Unified and Automatic Theming of Ggplot2, Lattice, and Base r Graphics*.
<https://CRAN.R-project.org/package=thematic>.

</div>

<div id=""ref-Wickham2015"" class=""csl-entry"">

Wickham, Hadley. 2015. *R Packages: Organize, Test, Document, and Share
Your Code*. O’Reilly Media, Inc. <https://r-pkgs.org/>.

</div>

<div id=""ref-stringr"" class=""csl-entry"">

———. 2019. *Stringr: Simple, Consistent Wrappers for Common String
Operations*. <https://CRAN.R-project.org/package=stringr>.

</div>

<div id=""ref-Wickham2020"" class=""csl-entry"">

———. 2020. *Mastering Shiny: Build Interactive Apps, Reports &
Dashboards.* O’Reilly Media, Inc. <https://mastering-shiny.org/>.

</div>

<div id=""ref-testthat"" class=""csl-entry"">

———. 2022. *Testthat: Unit Testing for r*.
<https://CRAN.R-project.org/package=testthat>.

</div>

<div id=""ref-roxygen2"" class=""csl-entry"">

Wickham, Hadley, Peter Danenberg, Gábor Csárdi, and Manuel Eugster.
2022. *Roxygen2: In-Line Documentation for r*.
<https://CRAN.R-project.org/package=roxygen2>.

</div>

<div id=""ref-dplyr"" class=""csl-entry"">

Wickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022.
*Dplyr: A Grammar of Data Manipulation*.
<https://CRAN.R-project.org/package=dplyr>.

</div>

<div id=""ref-devtools"" class=""csl-entry"">

Wickham, Hadley, Jim Hester, Winston Chang, and Jennifer Bryan. 2021.
*Devtools: Tools to Make Developing r Packages Easier*.
<https://CRAN.R-project.org/package=devtools>.

</div>

<div id=""ref-knitr2014"" class=""csl-entry"">

Xie, Yihui. 2014. “Knitr: A Comprehensive Tool for Reproducible Research
in R.” In *Implementing Reproducible Computational Research*, edited by
Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman;
Hall/CRC. <http://www.crcpress.com/product/isbn/9781466561595>.

</div>

<div id=""ref-knitr2015"" class=""csl-entry"">

———. 2015. *Dynamic Documents with R and Knitr*. 2nd ed. Boca Raton,
Florida: Chapman; Hall/CRC. <https://yihui.org/knitr/>.

</div>

<div id=""ref-rmarkdown2018"" class=""csl-entry"">

Xie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. *R Markdown: The
Definitive Guide*. Boca Raton, Florida: Chapman; Hall/CRC.
<https://bookdown.org/yihui/rmarkdown>.

</div>

<div id=""ref-rmarkdown2020"" class=""csl-entry"">

Xie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. *R Markdown
Cookbook*. Boca Raton, Florida: Chapman; Hall/CRC.
<https://bookdown.org/yihui/rmarkdown-cookbook>.

</div>

</div>
",2022-08-12
https://github.com/UtrechtUniversity/tweet_collector,"<!-- Parts of this template are inspired by https://github.com/othneildrew/Best-README-Template -->

# tweet_collector

<!-- Include Github badges here (optional) -->
<!-- e.g. Github Actions workflow status -->

Twitter forms a rich source of information for researchers interested in studying 'the public conversation'. 
The [Academic Research product track](https://developer.twitter.com/en/products/twitter-api/academic-research) is designed to serve the needs of the academic research community.
It provides researchers with special levels of access to public Twitter data without any cost.
This project is aimed to help researchers to use Academic Research product track to collect tweets of their interest and analyze them.

<!-- TABLE OF CONTENTS -->
## Table of Contents

- [Project Title](#project-title)
  - [Table of Contents](#table-of-contents)
  - [About the Project](#about-the-project)
    - [Built with](#built-with)
    - [License](#license)
  - [Getting Started](#getting-started)
    - [Prerequisites](#prerequisites)
    - [Installation](#installation)
  - [Usage](#usage)
  - [Links](#links)
  - [Contributing](#contributing)
  - [Contact](#contact)
  - [Acknowledgements](#Acknowledgements)

<!-- ABOUT THE PROJECT -->
## About the Project
This project provides a set of guidelines + tool, to facilitate the process of collecting tweets and analysing them.

**Date**: Sep 2021

**Researcher**:

- Abby Waysdorf (a.s.waysdofr@uu.nl)

**Research Software Engineer(s)**:

- Parisa Zahedi (p.zahedi@uu.nl)
- Roos Voorvaart (r.voorvaart@uu.nl)

### Built with
- [searchtweets-v2](https://pypi.org/project/searchtweets-v2/): It is a python package serves as a wrapper for the all Twitter API v2 search endpoints. We use this package to collect tweets.
- [elasticsearch/kibana](https://www.elastic.co/) : The collected tweets are visualized using elasticsearch and kibana.
- docker

<!-- Do not forget to also include the license in a separate file(LICENSE[.txt/.md]) and link it properly. -->
### License

The code in this project is released under [MIT License](LICENSE).

<!-- GETTING STARTED -->
## Getting Started
The followings are the main steps. 
1. Apply for Academic Research product track
2. Setup the environment
3. Search tweets
4. Visualize tweets

Each step is elaborted in details in **guideline** folder.

### Prerequisites

* To install and run this project locally, you need to have the following prerequisites installed.

  - Python 3.8
  - Docker desktop

### Installation

To get a local copy up and running follow these simple steps.

1. Clone the repo:
```sh
git clone https://github.com/UtrechtUniversity/tweet_collector.git
```
2. (Optional but recommended) Create and activate a virtual environment
```
   python -m venv [myenvname]
   source [myenvname]/bin/activate
```
3. Create package file:
```
poetry build
```
4. Install package through pip:
```
pip install ./dist/tweet_collector_version.whl
```

<!-- USAGE -->
## Usage
If the installation through pip was followed:
1. Make sure you are inside the virtual environment tweet_collector was installed in
1. Run the `tweet_collector` command to start collecting the tweets
1. Run the `tweet_collector_elastic` command to visualise the results in Kibana.

<!-- LINKS -->
## Links

An overview of interesting links related to the project.
- [Install Anaconda Python](https://www.anaconda.com/download/)

<!-- CONTRIBUTING -->
## Contributing

Contributions are what make the open source community an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

To contribute:

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

<!-- CONTACT -->
## Contact

Parisa Zahedi - (p.zahedi@uu.nl)

Project Link: [https://github.com/UtrechtUniversity/tweet_collector](https://github.com/UtrechtUniversity/tweet_collector)

<!-- ACKNOWLEDGEMENTS -->
## Acknowledgements
* Special thanks to [Ton Smeele](A.P.M.smeele@uu.nl) from Research IT team
* Special thanks to [Sytse Groenwold](s.groenwold@uu.nl) from Research IT team

",2022-08-12
https://github.com/UtrechtUniversity/usethis,# usethis,2022-08-12
https://github.com/UtrechtUniversity/uu-jekyll-theme,"jekyll-blank-theme
",2022-08-12
https://github.com/UtrechtUniversity/uucls,"# uucls - LaTeX classes for the UU house style

This is a library of latex classes for creating documents according to the [Utrecht University style guide](https://www.uu.nl/en/organisation/corporate-identity). 
Currently, the package contains: 

+ `lib/classes/uuletter2.cls` - a letter class that implements the [UU template](https://www.uu.nl/en/organisation/corporate-identity/downloads/letter)
+ `lib/classes/uureport.cls` - a wrapper around the standard report class that typesets a [UU report](https://www.uu.nl/en/organisation/corporate-identity/downloads/word-document)

I plan to implement at least one more class, `uuarticle.cls`, but there's no timeline yet.

Under `lib/examples` you can find some templates/code examples that illustrate the use of the classes.
They should compile once you've [installed](#installation) the classes.
Currently, there is no further documentation.

## Installation

I assume that if you're here, you already know how to use a custom latex class.
Please note the following things: 

+ The packages assume that the UU logos (found in `lib/resources`) are somewhere where latex can find them (ideally somewhere in the [TeX Directory Structure (TDS)](https://en.wikipedia.org/wiki/TeX_Directory_Structure)).
+ Dependencies are kept to a minimum but you need to have the following packages installed:

  - `opensans` and `merrieweather` for the fonts
  - `babel` (with Dutch language support) for localization
  - `graphicx` for the logos and signatures
   
The easiest way to install the packages is to use `git clone` somewhere in your TDS.
If you're on MacOS/Linux, you can use the shell script included in the project to install the project by typing the following into your command line:

``` shell

  git clone https://github.com/UtrechtUniversity/uucls.git && ./uucls/lib/scripts/uucls.sh install

```

**Note**: You can `rm -rf uucls` afterwards to delete the cloned repo (it was cloned just for the install anyways).
A nicer `curl` implementation is currently not possible since this is an internal repo.

Before you do this, please inspect the script under `lib/script/uucls.sh` (as a matter of principle, don't just run scripts from the internet).
It assumes that you have `git` and either MacTeX or TeXlive installed.
The script will `git clone` the package into your TDS and install itself as a command line utility for updating and uninstalling the script.

## Contributing

Initially, I created these classes for my personal use.
I'm sharing them here in the hope that this perhaps saves some of you the work creating your own files.
I'm very happy about PRs and/or comments.
Ideally, raise an issue or just get in touch via my UU email.

## Disclaimer

The UU branding is protected by copyright, with Utrecht University having the right of use. 
This is why the corporate style and the content of this repository cannot be used and/or applied by third parties without permission granted in advance.
",2022-08-12
https://github.com/UtrechtUniversity/uupeeps,"# uupeeps

This package is an interface to the public staff information on https://www.uu.nl/staff/. This package was created by Class CoTaPP in a live coding session led by Jonathan de Bruin. The aim was to create and publish a Python package in 30 minutes. 

## Installation

Instal the package with 

```
pip install git+https://github.com/UtrechtUniversity/uupeeps
```

## Usage

Import the package:

```
import uupeeps
```
Get employees per faculty:

```
uupeeps.get_employees_url(5)
```

Get employee info (e.g. https://www.uu.nl/staff/ALLamprecht/):

```
>>> uupeeps.get_employee_info(""ALLamprecht"")

{
   ""Guid"":""30936d707a9546e6b768c707473c4366"",
   ""Activities"":[
      
   ],
   ""ActivitiesFreeText"":""None"",
   ""AdditionalFunctions"":""None"",
   ""Availability"":""None"",
   ""Bio"":""None"",
   ""CV"":""<p>Short biography:</p>\n<ul>\n<li>Since 2017: Assistant Professor of Software Technology (here)</li>\n<li>2015 - 2017: Research Fellow at Lero - The Irish Software Research Centre at the University of Limerick, Ireland</li>\n<li>2012 - 2015: Postdoctoral Researcher at the Chair of Service and Software Engineering, Institute of Computer Sciene, University of Potsdam, Germany</li>\n<li>2007 - 2012: PhD Student and Research Assistant at the Chair of Programming Systems, Faculty of Computer Science, TU Dortmund University, Germany. My thesis <a href=\""https://doi.org/10.1007/978-3-642-45389-2\"">\""User-Level Workflow Design. A Bioinformatics Perspective.\""</a> was graded \""with distinction\"", honored with the faculty\\'s dissertation award and published in Springer\\'s LNCS series.</li>\n<li>2002 - 2007: B.Sc./M.Sc. student of Applied Computer Science at the University of G&ouml;ttingen, Germany</li>\n</ul>\n<p>If you are interested in more elaborate version of my CV, please contact me by e-mail and I will be happy to send you an up-to-date document.&nbsp;</p>"",
   ""CVUrl"":""None"",
   ""Chair"":""None"",
   ""ContactDetails"":[
      {
         ""Guid"":""06c7175e58b34c3ea59cea51f43bb1d8"",
         ""Address"":""Princetonplein 5"",
         ""Building"":""Buys Ballotgebouw"",
         ""City"":""Utrecht"",
         ""Id"":398232,
         ""Phone"":""030 253 3261"",
         ""PhoneDepartment"":""None"",
         ""Room"":""5.65"",
         ""Url"":""None"",
         ""Zipcode"":""3584 CC""
      }
   ],
   ""ContactFreeText"":""None"",
   ""Courses"":[
      {
         ""Guid"":""168305df221e4eb3bb501f92d2271e98"",
         ""Code"":""INFOMCTH"",
         ""ECTS"":""None"",
         ""Id"":232243,
         ""Level"":""None"",
         ""Name"":""Computational thinking"",
         ""Type"":""None"",
         ""Url"":""https://osiris.uu.nl/osiris_student_uuprd/OnderwijsCatalogusSelect.do?selectie=cursus&cursus=INFOMCTH&collegejaar=2020&taal=nl""
      },
      {
         ""Guid"":""6035a5b64a3d4f8290b71baa559a8ebf"",
         ""Code"":""INFOMPSV"",
         ""ECTS"":""None"",
         ""Id"":232244,
         ""Level"":""None"",
         ""Name"":""Programmasemantiek en -verificatie"",
         ""Type"":""None"",
         ""Url"":""https://osiris.uu.nl/osiris_student_uuprd/OnderwijsCatalogusSelect.do?selectie=cursus&cursus=INFOMPSV&collegejaar=2020&taal=nl""
      },
      {
         ""Guid"":""3e3e174cac624cb1a3c806421a3cefdc"",
         ""Code"":""BETA-B1PYT"",
         ""ECTS"":""None"",
         ""Id"":232245,
         ""Level"":""None"",
         ""Name"":""Programmeren met Python"",
         ""Type"":""None"",
         ""Url"":""https://osiris.uu.nl/osiris_student_uuprd/OnderwijsCatalogusSelect.do?selectie=cursus&cursus=BETA-B1PYT&collegejaar=2020&taal=nl""
      },
      {
         ""Guid"":""195b18705fb949a7a328c4cf0646bcec"",
         ""Code"":""INFOB2PWD"",
         ""ECTS"":""None"",
         ""Id"":232241,
         ""Level"":""None"",
         ""Name"":""Programming with data"",
         ""Type"":""None"",
         ""Url"":""https://osiris.uu.nl/osiris_student_uuprd/OnderwijsCatalogusSelect.do?selectie=cursus&cursus=INFOB2PWD&collegejaar=2020&taal=nl""
      },
      {
         ""Guid"":""3f9d95542948442fa28a36eca2215eca"",
         ""Code"":""INFOSP"",
         ""ECTS"":""None"",
         ""Id"":232242,
         ""Level"":""None"",
         ""Name"":""Softwareproject"",
         ""Type"":""None"",
         ""Url"":""https://osiris.uu.nl/osiris_student_uuprd/OnderwijsCatalogusSelect.do?selectie=cursus&cursus=INFOSP&collegejaar=2020&taal=nl""
      }
   ],
   ""CoursesFreeText"":""<p>Teaching is an essential part of academic life that I enjoy very much. I have more than 15 years of experience teaching courses on various topics at different places, in several study programs and with many formats and group sizes. I possess a University Teaching Qualification (BKO) from Utrecht University and a Senior Teaching Professionals certificate from the Potsdam University Graduate School. In Utrecht I teach different courses related to programming, software engineering and formal methods to both ICS and non-ICS students. English is my main medium of instruction, but students are also welcome to approach me in Dutch or German.</p>\n<p><strong>Projects<br /></strong>Feel free to contact me if you are interested in doing a Master thesis, Bachelor thesis or any other kind of project in the context of <a href=\""https://www.uu.nl/staff/allamprecht/Research\"">my research topics</a>. There are always interesting things to do! Some concrete ideas for thesis projects are available at <a href=\""https://uu.konjoin.nl/profile/anna-lena-lamprecht\"">my KonJoin page</a>, but there are always more and I am happy to discuss any other ideas.&nbsp;</p>"",
   ""CustomTab1"":""None"",
   ""CustomTab2"":""None"",
   ""CustomTab3"":""None"",
   ""DateInauguralLecture"":""None"",
   ""DateOfAppointment"":""None"",
   ""Email"":""a.l.lamprecht@uu.nl"",
   ""EndowedBy"":""None"",
   ""Expertises"":[
      {
         ""Guid"":""a63e139b270a413885f3cd11a7dc63ce"",
         ""Id"":17583,
         ""Name"":""Onderzoekssoftware"",
         ""Url"":""/medewerkers/organogram?expertise=10355""
      },
      {
         ""Guid"":""9a70e1e62d7441669722cd45ba964ebf"",
         ""Id"":17582,
         ""Name"":""Softwareontwikkeling"",
         ""Url"":""/medewerkers/organogram?expertise=2493""
      },
      {
         ""Guid"":""c4608d5a62df4f0aa714ef8fafdfd0c4"",
         ""Id"":18652,
         ""Name"":""Formele methoden"",
         ""Url"":""/medewerkers/organogram?expertise=10392""
      },
      {
         ""Guid"":""dfffa59b8abb48a3b18f2fa676bb0b52"",
         ""Id"":18653,
         ""Name"":""Open Science"",
         ""Url"":""/medewerkers/organogram?expertise=10286""
      }
   ],
   ""Faculties"":[
      {
         ""Guid"":""2c451e20740b444ea7e298f8eab586f4"",
         ""ChildCount"":""None"",
         ""Code"":""None"",
         ""Faculty"":""Bètawetenschappen"",
         ""FacultyId"":5,
         ""FacultyUrl"":""https://www.uu.nl/organisatie/faculteit-betawetenschappen"",
         ""MainPosition"":true
      }
   ],
   ""FocusAreas"":[
      {
         ""Guid"":""2d3ca2c6ea5045bfbde732a22bada7c4"",
         ""Id"":11897,
         ""Name"":""Applied Data Science"",
         ""Url"":""/medewerkers/organogram?focusarea=29""
      },
      {
         ""Guid"":""1552833042ae4797ae5b27dacb5c1a55"",
         ""Id"":13675,
         ""Name"":""Human-centered Artificial Intelligence"",
         ""Url"":""/medewerkers/organogram?focusarea=32""
      },
      {
         ""Guid"":""c3307ce0d07a4883bd6c66f0e5305329"",
         ""Id"":5657,
         ""Name"":""Integrative Bioinformatics"",
         ""Url"":""/medewerkers/organogram?focusarea=13""
      },
      {
         ""Guid"":""9a3d6d779fc64c3684cc942785fd83aa"",
         ""Id"":13676,
         ""Name"":""Life Sciences"",
         ""Url"":""/medewerkers/organogram?focusarea=19""
      },
      {
         ""Guid"":""40f92b8726e54775ad579f2b4974c212"",
         ""Id"":5484,
         ""Name"":""Utrecht Platform for Applied Data Science"",
         ""Url"":""/medewerkers/organogram?focusarea=27""
      }
   ],
   ""Id"":33086,
   ""Images"":[
      
   ],
   ""KeyPublications"":{
      ""Guid"":""7cba9f92ee1f487b9ac890a554b4d1de"",
      ""GroupType"":""None"",
      ""Groupname"":""Sleutelpublicaties"",
      ""Publications"":[
         
      ]
   },
   ""LastUpdate"":""/Date(1613590448380+0100)/"",
   ""Links"":[
      
   ],
   ""LinksSocialMedia"":[
      {
         ""Guid"":""929d43d61ca849a09253842f84a58e78"",
         ""IconUrl"":""https://www.uu.nl/medewerkers/static/SocialMedia/linkedin.com.png"",
         ""Id"":12783,
         ""Name"":""LinkedIn"",
         ""Url"":""https://www.linkedin.com/in/alamprecht/""
      },
      {
         ""Guid"":""99a9884609994ca7a5147b785452060d"",
         ""IconUrl"":""https://www.uu.nl/medewerkers/static/SocialMedia/orcid.org.png"",
         ""Id"":12828,
         ""Name"":""ORCID"",
         ""Url"":""https://orcid.org/0000-0003-1953-5606""
      },
      {
         ""Guid"":""8ed25440a9ea4befbe1b7a36be8ecf73"",
         ""IconUrl"":""https://www.uu.nl/medewerkers/static/SocialMedia/twitter.com.png"",
         ""Id"":14851,
         ""Name"":""Twitter"",
         ""Url"":""https://twitter.com/al_lamprecht""
      }
   ],
   ""Media"":""None"",
   ""MediaFreeText"":""None"",
   ""MobileNumber"":""None"",
   ""Name"":""Dr. A.L. (Anna-Lena) Lamprecht"",
   ""NameShort"":""A.L. (Anna-Lena) Lamprecht"",
   ""OnlyEN"":false,
   ""Organisation"":""Universiteit Utrecht"",
   ""PhotoUrl"":""/Public/GetImage?Employee=33086&_t=08042021213657&t="",
   ""Positions"":[
      {
         ""Guid"":""9469fae59d514b6689476fdb1ead5bdb"",
         ""Level1"":""Bètawetenschappen"",
         ""Level1Url"":""https://www.uu.nl/medewerkers/organogram/beta"",
         ""Level2"":""Informatica"",
         ""Level2Url"":""https://www.uu.nl/medewerkers/organogram/beta/87"",
         ""Level3"":""Intelligent Software Systems"",
         ""Level3Url"":""https://www.uu.nl/medewerkers/organogram/beta/87/849"",
         ""Level4"":""Software Technology"",
         ""Level4Url"":""https://www.uu.nl/medewerkers/organogram/beta/87/849/858"",
         ""Order"":0,
         ""Position"":""Universitair docent"",
         ""PositionId"":93
      }
   ],
   ""PostalAddress"":""None"",
   ""Prizes"":[
      
   ],
   ""PrizesFreeText"":""None"",
   ""Profile"":""<p>Hi, I am Anna-Lena Lamprecht. I work as an assistant professor in the <a href=\""https://www.uu.nl/en/research/software-systems/software-technology\"">Software Technology group</a> at the Department of Information and Computing Sciences. I conduct research at the interface of (research) software engineering and applied formal methods, currently focusing on FAIR software and automated composition of scientific workflows. I teach courses on programming, software engineering and formal methods in the department\\'s study programs and beyond. I am a <a href=\""https://www.uu.nl/en/organisation/faculty-of-science/about-us/westerdijk-fellowship\"">Westerdijk Fellow</a>, Faculty Ambassador of the <a href=\""https://openscience-utrecht.com/\"">Open Science Community Utrecht</a>, and co-founder and steering committee member of the <a href=\""https://www.uu.nl/en/news/diversity-inclusion-award-for-young-women-in-geoscience-and-for-women-in-information-and-computing\"">award-winning</a> Women in Information and Computing Sciences (<a href=\""https://wics.sites.uu.nl/\"">WICS</a>) network. I am active in several national and international communities, including <a href=\""https://de-rse.org/\"">de-RSE</a>, <a href=\""http://easst.aulp.co.uk/\"">EASST</a>, <a href=\""https://elixir-europe.org/\"">ELIXIR</a>, <a href=\""http://www.fmeurope.org/\"">FME</a>, <a href=\""https://ict-research.nl/\"">IPN</a>, <a href=\""https://nl-rse.org/\"">NL-RSE</a> and <a href=\""https://www.versen.nl/\"">VERSEN</a>. I am an NL-RSE core team member, Netherlands representative at the International Council of RSE Associations, secretary of the IPN EDI working group, and co-editor-in-chief of the open-access journal <a href=\""https://journal.ub.tu-berlin.de/eceasst\"">ECEASST</a>.</p>\n<p>More about me:</p>\n<ul>\n<li>Article about me and my work for the <a href=\""https://ict-research.nl/wordpress/wp-content/uploads/2019/01/IO-magazine-NR04-december-2018_v4-_web.pdf\"">I/O Magazine December 2018</a> (last page), by <a href=\""https://amandaschrijft.nl/\"">Amanda Verdonk</a>.</li>\n<li><a href=\""https://ubc.uu.nl/anna-lenas-expertise-software-technology/\"">Interview</a> for the May 2020 newsletter of the <a href=\""https://ubc.uu.nl/\"">Utrecht Bioinformatics Center</a>, by <a href=\""https://www.uu.nl/medewerkers/CJNijenhuis\"">Celia Nijenhuis</a>.&nbsp;</li>\n<li><a href=\""https://openscience-utrecht.com/2020/09/19/introducing-the-oscu-faculty-ambassadors-v-anna-lena-lamprecht-faculty-of-science/\"">Interview</a> for the <a href=\""https://openscience-utrecht.com/r2os/\"">Road to Open Science blog</a> in September 2020, by <a href=\""https://stefangaillard.com/\"">Stefan Gaillard</a>.</li>\n</ul>\n<p>&nbsp;</p>"",
   ""Projects"":[
      
   ],
   ""ProjectsCompleted"":[
      
   ],
   ""ProjectsFreeTextBottom"":""None"",
   ""ProjectsFreeTextTop"":""<p>I conduct research at the interface of <strong><a href=\""https://en.wikipedia.org/wiki/Research_software_engineering\"">(research) software engineering</a></strong> and <strong>applied <a href=\""https://en.wikipedia.org/wiki/Formal_methods\"">formal methods</a></strong>. Research software engineering (RSE) is an emerging discipline that studies research software and its development processes, and develops methods, techniques and tools to improve them. As research in most scholarly disciplines today critically depends on software, RSE is rapidly gaining importance. In particular, research-specific quality assurance methods for software are urgently needed to ensure correct, reliable and reproducible research results. My goal is to leverage the potential of formal methods to help researchers develop higher-quality software with less effort.&nbsp;</p>\n<p>My current research focus is on the use of process synthesis techniques for the exploration of computational pipelines (a.k.a. scientific workflows). <a href=\""https://www.uu.nl/staff/VKasalica\"">Vedran Kasalica</a> has joined me in Utrecht as a PhD candidate in this area. We have developed <strong>the</strong> <strong><a href=\""https://github.com/sanctuuary/APE\"">Automated Pipeline Explorer (APE)</a></strong> a command line tool and Java API for the automated exploration of possible computational pipelines from large collections of computational tools. We use APE in different collaborative projects in bioinformatics and the geosciences, most notably:&nbsp;</p>\n<ul>\n<li>\n<p><strong>Automated workflow discovery and composition in mass spectrometry-based proteomics. </strong>This is a collaboration with <a href=\""https://orcid.org/0000-0001-6666-1520\"">Jon Ison</a> (AstraZeneca), <a href=\""https://orcid.org/0000-0002-5865-8994\"">Magnus Palmblad</a> (University of Leiden Medical Center), <a href=\""https://orcid.org/0000-0002-9708-6722\"">Veit Schw&auml;mmle</a> (University of Southern Denmark) and other collaborators from the <a href=\""https://elixir-europe.org/\"">ELIXIR</a> network. We use APE for the composition of bioinformatics tools for the analysis of mass spectrometry data into purpose-specific workflows. The tools (from ms-utils.org and <a href=\""https://bio.tools/\"">bio.tools</a>) are semantically annotated with terms from the <a href=\""http://edamontology.org/page\"">EDAM</a> ontology.</p>\n</li>\n<li>\n<p><strong>Automated workflow discovery and composition in QuAnGIS.&nbsp;</strong><a href=\""https://questionbasedanalysis.com/\"">QuAnGIS</a> (Question-based Analysis of Geographic Information with Semantic Queries) is an ERC Starting Grant project led by <a href=\""https://orcid.org/0000-0002-2267-4810\"">Simon Scheider</a> (Faculty of Geosciences), which aims at developing a novel theory of interrogative spatial concepts to turn geoanalytical questions into a machine-readable form using semantic queries. In this form, questions can directly be matched with the capacity of major analytic GIS tools and data on the Web. Often the answer to the queries does however not yet exists and needs to be computed first, meaning that a suitable workflow has to be constructed to answer the query. We started to work together to solve this problem with workflow synthesis technology.</p>\n</li>\n</ul>\n<p>Furthermore, I am actively involved in the international collaborative work on <strong><a href=\""https://www.rd-alliance.org/groups/fair-research-software-fair4rs-wg\"">FAIR Principles for Research Software</a></strong> and lead author of <a href=\""https://doi.org/10.3233/DS-190026\"">one of the first major papers on the topic</a>, and in the discussion around research challenges within RSE.&nbsp;</p>"",
   ""Publications"":[
      {
         ""Guid"":""5bd97e63e7c2445bacd66844b9486632"",
         ""GroupType"":""None"",
         ""Groupname"":""2020 - Artikelen"",
         ""Publications"":[
            ""Kruiger, J.F., Kasalica, V., Meerlo, Rogier, Lamprecht, A.L., Nyamsuren, E. &amp; Scheider, S. (26-10-2020). <i><a href=\""https://doi.org/10.1111/TGIS.12692\"" target=\""_blank\"">Loose programming of GIS workflows with geo-analytical concepts</a></i>.  <i>Transactions in GIS</i> "",
            ""Scheider, S., Meerlo, Rogier, Kasalica, V. &amp; Lamprecht, A.L. (2020). <i><a href=\""https://doi.org/10.5311/JOSIS.2020.20.555\"" target=\""_blank\"">Ontology of core concept data types for answering geo-analytical questions</a></i>.  <i>Journal of Spatial Information Science</i>, 2020 (20) (35 p.). "",
            ""Kasalica, V. &amp; Lamprecht, A.L. (2020). <i><a href=\""http://dspace.library.uu.nl/handle/1874/396824\"" target=\""_blank\"">Workflow Discovery with Semantic Constraints: The SAT-Based Implementation of APE</a></i>.  <i>Electronic Communications of the EASST</i>, 78 (25 p.). ""
         ]
      },
      {
         ""Guid"":""78f42eae494c42c4abd85ebfa0433787"",
         ""GroupType"":""None"",
         ""Groupname"":""2020 - Artikelen in bundels / proceedings"",
         ""Publications"":[
            ""Kasalica, V. &amp; Lamprecht, A.L. (2020). <i><a href=\""https://doi.org/10.1007/978-3-030-50436-6_34\"" target=\""_blank\"">APE: A Command-Line Tool and API for Automated Workflow Composition</a></i>.  <i>Proceedings of the International Conference on Computational Science (ICCS 2020)</i> (12 p.). Springer."",
            ""Akhuseyinoglu, Kamil, Barria-Pineda, Jordan, Sosnovsky, Sergey, Lamprecht, Anna-Lena, Guerra, Julio &amp; Brusilovsky, Peter (2020). <i><a href=\""https://doi.org/10.1007/978-3-030-57717-9_18\"" target=\""_blank\"">Exploring Student-Controlled Social Comparison</a></i>. In Carlos Alario-Hoyos, Mar&#237;a Jes&#250;s Rodr&#237;guez-Triana, Maren Scheffel, Inmaculada Arnedillo-S&#225;nchez &amp; Sebastian Maximilian Dennerlein (Eds.),  <i>Addressing Global Challenges and Quality Education - 15th European Conference on Technology Enhanced Learning, EC-TEL 2020, Heidelberg, Germany, September 14–18, 2020, Proceedings</i> (pp. 244-258) (15 p.). Cham: Springer, Cham.""
         ]
      },
      {
         ""Guid"":""8c0e63de30b54b21a80f2c612c6653a5"",
         ""GroupType"":""None"",
         ""Groupname"":""2020 - Overige resultaten"",
         ""Publications"":[
            ""Sufi, Shoaib, Ortiz, Carlos Martinez, Hof, Cees, Aerts, Patrick, Klinkenberg, Adriaan, Lamprecht, Anna-Lena, Sierman, Barbara, Willigen, Bettine van, Olivier, Brett, Willing, Carol, Thiel, Carsten, Leeuwen, Caspar van, Jones, Catherine, Flach, Christina von, Katz, Daniel S., Hansen, Dominique, Plomp, Esther, Coen, Gerard, Steptoe, Hamish, Bosma, Hannah, Andrews, Heather, Davenport, James, Shiers, Jamie, Vos, Jesse de, Knijff, Johan van der, Spaaks, Jurriaan H., Kavoussanakis, Kostas, Garcia, Leyla, Behn, Mario, David, Mario, Kuzak, Mateusz, Hong, Neil Chue, Dintzner, Nicolas, Orviz, Pablo, Lavanchy, Paula Martinez, Doorn, Peter, Kotarski, Rachael, Silva, Raniere, Haines, Robert, Cosmo, Roberto di, Overgoor, Skip, Druskat, Stephan, Sandt, Stephanie van de, Boom, Tim van den, Letizia, Viviana, Seuskens, Wiel &amp; Moranville, Yoann (01-07-2020). <i><a href=\""https://doi.org/10.5281/zenodo.3922155\"" target=\""_blank\"">Report on the Workshop on Sustainable Software Sustainability 2019 (WOSSS19)</a></i>.  <i></i> ""
         ]
      },
      {
         ""Guid"":""c40b384427664809b0389557daf87456"",
         ""GroupType"":""None"",
         ""Groupname"":""2019 - Artikelen"",
         ""Publications"":[
            ""Palmblad, Magnus, Lamprecht, A.L., Ison, Jon &amp; Schw&#228;mmle, Veit (2019). <i><a href=\""http://dspace.library.uu.nl/handle/1874/379482\"" target=\""_blank\"">Automated workflow composition in mass spectrometry based proteomics</a></i>.  <i>Bioinformatics</i>, 35 (4), (pp. 656-664). "",
            ""Ison, Jon, M&#233;nager, Herv&#233;, Brancotte, Bryan, Jaaniso, Erik, Salumets, Ahto, Raček, Tom&#225;š, Lamprecht, Anna-Lena, Palmblad, Magnus, Kalaš, Mat&#250;š, Chmura, Piotr, Hancock, John M, Schw&#228;mmle, Veit &amp; Ienasescu, Hans-Ioan (2019). <i><a href=\""http://dspace.library.uu.nl/handle/1874/385733\"" target=\""_blank\"">Community curation of bioinformatics software and data resources</a></i>.  <i>Briefings in Bioinformatics</i> (9 p.). "",
            ""Lamprecht, Anna-Lena, Garcia, Leyla, Kuzak, Mateusz, Martinez, Carlos, Arcila, Ricardo, Martin Del Pico, Eva, Dominguez Del Angel, Victoria, van de Sandt, Stephanie, Ison, Jon, Martinez, Paula Andrea, McQuilton, Peter, Valencia, Alfonso, Harrow, Jennifer, Psomopoulos, Fotis, Gelpi, Josep Ll, Chue Hong, Neil, Goble, Carole &amp; Capella-Gutierrez, Salvador (13-11-2019). <i><a href=\""https://doi.org/10.3233/DS-190026\"" target=\""_blank\"">Towards FAIR principles for research software</a></i>.  <i>EPJ Data Science</i>, Preprint (Preprint), (pp. 1-23) (23 p.). ""
         ]
      },
      {
         ""Guid"":""f857dc7a4e7f4526be0e184ce1928f69"",
         ""GroupType"":""None"",
         ""Groupname"":""2019 - Boekdelen / hoofdstukken"",
         ""Publications"":[
            ""Margaria, Tiziana &amp; Lamprecht, Anna-Lena (2019). <i><a href=\""https://doi.org/10.1007/978-3-319-60013-0_209-1\"" target=\""_blank\"">Modeling of Games and Game Strategies</a></i>. In Arthur Tatnall (Eds.),  <i>Encyclopedia of education and information technologies</i> (12 p.). Cham: Springer International Publishing."",
            ""Lamprecht, Anna-Lena &amp; Margaria, Tiziana (2019). <i><a href=\""https://doi.org/10.1007/978-3-319-60013-0_210-1\"" target=\""_blank\"">Modeling of Scientific Workflows</a></i>. In Arthur Tatnall (Eds.),  <i>Encyclopedia of education and information technologies</i> (pp. 1-8) (8 p.). Cham: Springer.""
         ]
      },
      {
         ""Guid"":""e7cb811ceb4343d2b843b7614a9c4c48"",
         ""GroupType"":""None"",
         ""Groupname"":""2019 - Artikelen in bundels / proceedings"",
         ""Publications"":[
            ""Kasalica, Vedran &amp; Lamprecht, Anna-Lena (2019). <i><a href=\""https://doi.org/10.1007/978-3-030-24302-9_34\"" target=\""_blank\"">Workflow Discovery Through Semantic Constraints -  A Geovisualization Case Study</a></i>. In Sanjay Misra, Osvaldo Gervasi, Beniamino Murgante, Elena Stankova, Vladimir Korkhov, Carmelo Torre, Ana Maria A.C. Rocha, David Taniar, Bernady O. Apduhan &amp; Eufemia Tarantino (Eds.),  <i>Computational Science and Its Applications -- ICCSA 2019 - 19th International Conference, Saint Petersburg, Russia, July 1-4, 2019, Proceedings</i> (pp. 473-488) (16 p.). Cham: Springer.""
         ]
      },
      {
         ""Guid"":""7b156da4e7ed48a881c59d3e340dcaa3"",
         ""GroupType"":""None"",
         ""Groupname"":""2019 - Overige resultaten"",
         ""Publications"":[
            ""Erdmann, Christopher, Simons, Natasha, Otsuji, Reid, Labou, Stephanie, Johnson, Ryan, Castelao, Guilherme, Boas, Bia Villas, Lamprecht, Anna-Lena, Ortiz, Carlos Martinez, Garcia, Leyla, Kuzak, Mateusz, Martinez, Paula Andrea, Stokes, Liz, Honeyman, Tom, Wise, Sharyn, Quan, Josh, Peterson, Scott, Neeser, Amy, Karvovskaya, Lena, Lange, Otto, Witkowska, Iza, Flores, Jacques, Bradley, Fiona, Hettne, Kristina, Verhaar, Peter, Companjen, Ben, Sesink, Laurents, Schoots, Fieke, Schultes, Erik, Kaliyaperumal, Rajaram, T&#243;th-Czifra, Erzs&#233;bet, Azevedo, Ricardo de Miranda, Muurling, Sanne, Brown, John, Chan, Janice, Quigley, Niamh, Federer, Lisa, Joubert, Douglas, Dillman, Allissa, Wilkins, Kenneth, Chandramouliswaran, Ishwar, Navale, Vivek, Wright, Susan, Giorgio, Silvia Di, Fasemore, Mandela, F&#246;rstner, Konrad, Sauerwein, Till, Seidlmayer, Eva, Zeitlin, Ilja, Bacon, Susannah, Hannan, Katie, Ferrers, Richard, Russell, Keith, Whitmore, Deidre &amp; Dennis, Tim (01-02-2019). <i><a href=\""http://dspace.library.uu.nl/handle/1874/390076\"" target=\""_blank\"">Top 10 FAIR Data  Software Things</a></i>.  <i></i> ""
         ]
      },
      {
         ""Guid"":""581ad517c294414ba8121c627e54af26"",
         ""GroupType"":""None"",
         ""Groupname"":""2019 - Abstract"",
         ""Publications"":[
            ""Lamprecht, A.L. (2019). <i><a href=\""https://www.iccs-meeting.org/iccs2019/wp-content/schedule/pages/WTCS1.html#abstract253\"" target=\""_blank\"">Computational Thinking and Programming with Python for Aspiring Data Scientists</a></i>.  <i></i> ""
         ]
      },
      {
         ""Guid"":""a342324bbb25444f83a708340523dbe6"",
         ""GroupType"":""None"",
         ""Groupname"":""2019 - Prize (including medals and awards)"",
         ""Publications"":[
            ""Kasalica, V. &amp; Lamprecht, Anna-Lena 2019 Recipient ICCSA 2019 Best Paper Award ""
         ]
      },
      {
         ""Guid"":""dad38315b24f4b4b9ae59ac6baa5d4e3"",
         ""GroupType"":""None"",
         ""Groupname"":""2018 - Boekdelen / hoofdstukken"",
         ""Publications"":[
            ""Margaria, Tiziana, Lamprecht, A.L. &amp; Steffen, Bernhard (2018). <i>Continuous Model-Driven Engineering</i>. In Mike Hinchey (Eds.),  <i>Software Technology - 10 Years of Innovation in IEEE Computer</i> Wiley.""
         ]
      },
      {
         ""Guid"":""8c8162c2e04946db8cc24eaf31b735f8"",
         ""GroupType"":""None"",
         ""Groupname"":""2018 - Artikelen in bundels / proceedings"",
         ""Publications"":[
            ""Gossen, Frederik, K&#252;hn, Dennis, Margaria, Tiziana &amp; Lamprecht, A.L. (2018). <i><a href=\""https://doi.org/10.1109/COMPSAC.2018.00175\"" target=\""_blank\"">Computational Thinking: Learning by Doing with the Cinco Adventure Game Tool</a></i>.  <i>Proceedings of the 42nd IEEE International Conference on Computers, Software and Applications</i> "",
            ""McInerney, Clare, Lamprecht, A.L. &amp; Margaria, Tiziana (2018). <i><a href=\""https://doi.org/10.1007/978-3-319-74310-3_50\"" target=\""_blank\"">Computing Camps for Girls - A First-Time Experience at the University of Limerick</a></i>. In Arthur Tatnall &amp; Mary Webb (Eds.),  <i>Tomorrow’s Learning: Involving Everyone - Learning with and about technologies and computing</i> (pp. 494-505). Heidelberg: Springer."",
            ""Wickert, Alexander, Lamprecht, A.L. &amp; Margaria, Tiziana (2018). <i><a href=\""https://doi.org/10.1145/3193992.3194002\"" target=\""_blank\"">Domain-specific Design of Patient Classification in Cancer-Related Cachexia Research</a></i>.  <i>Proceedings of the 6th Conference on Formal Methods in Software Engineering, FormaliSE 2018, collocated with ICSE 2018, Gothenburg, Sweden, June 2, 2018.</i> (pp. 60-63). ACM.""
         ]
      },
      {
         ""Guid"":""6f9444e283ea41dba57319c6f61a11e4"",
         ""GroupType"":""None"",
         ""Groupname"":""2018 - Editorial"",
         ""Publications"":[
            ""Maglyas, Andrey &amp; Lamprecht, A.L. (2018). <i><a href=\""https://doi.org/10.1016/j.jss.2017.10.003\"" target=\""_blank\"">Introduction to the special issue on “Software Business”</a></i>.  <i>Journal of Systems and Software</i>, 135, (pp. 105-106) (2 p.). ""
         ]
      },
      {
         ""Guid"":""0ab9f98cbefa44378d4ddf0b48922622"",
         ""GroupType"":""None"",
         ""Groupname"":""2018 - Poster"",
         ""Publications"":[
            ""Lamprecht, A.L., Palmblad, Magnus, Ison, Jon &amp; Schw&#228;mmle, Veit (2018). <i><a href=\""https://doi.org/10.1109/eScience.2018.00098\"" target=\""_blank\"">Automated Composition of Scientific Workflows in Mass Spectrometry-Based Proteomics</a></i>.  <i></i> "",
            ""Kasalica, V. &amp; Lamprecht, A.L. (2018). <i><a href=\""https://doi.org/10.1109/eScience.2018.00099\"" target=\""_blank\"">Automated Composition of Scientific Workflows: A Case Study on Geographic Data Manipulation</a></i>.  <i></i> ""
         ]
      },
      {
         ""Guid"":""f5272cfb7b054c2a880569f5c0e78425"",
         ""GroupType"":""None"",
         ""Groupname"":""2017 - Abstract"",
         ""Publications"":[
            ""Lamprecht, A.L. (2017). <i>Bridging Gaps with Scientific Workflows: Experiences from an Interdisciplinary Project Course</i>.  <i></i> "",
            ""McInerney, Clare, Lamprecht, A.L. &amp; Margaria, Tiziana (2017). <i>Computing Camps for Girls - A First-Time Experience at the University of Limerick</i>.  <i></i> ""
         ]
      },
      {
         ""Guid"":""6ae1db4306dc400ba098bc6cd0dfbaf0"",
         ""GroupType"":""None"",
         ""Groupname"":""2017 - Editorial activity"",
         ""Publications"":[
            ""Lamprecht, A.L. (2017) Guest editor Electronic Communications of the EASST (Journal) <em>Guest Editor for post-conference proceedings of the 7th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation - Doctoral Symposium, 2016 (ISoLA DS 2016)</em>""
         ]
      },
      {
         ""Guid"":""2dcb6a2ebaa84138807787bea8576d90"",
         ""GroupType"":""None"",
         ""Groupname"":""2016 - Editorial activity"",
         ""Publications"":[
            ""Lamprecht, A.L. (2016) Editorial board member Electronic Communications of the EASST (Journal) <em>Co-Editor in Chief</em>"",
            ""Lamprecht, A.L. (2016) Guest editor Journal of Systems and Software (Journal) <em>Special Issue on &quot;Software Business&quot;</em>""
         ]
      }
   ],
   ""PublicationsFreeBottom"":""None"",
   ""PublicationsFreeTop"":""<p><a title=\""Google Scholar Anna-Lena Lamprecht\"" href=\""https://scholar.google.de/citations?user=d-GAM1oAAAAJ&amp;hl=de\"" target=\""_blank\"" rel=\""noopener\"">Google Scholar</a> has a rather complete record of my publications, dating back to 2006. See below for research outputs since joining Utrecht University (June 2017).</p>\n<p>&nbsp;</p>"",
   ""Skills"":[
      
   ],
   ""State"":""None"",
   ""StudyProgrammes"":[
      {
         ""Guid"":""d1433cd0e7b44ddebce51fee6ddb213b"",
         ""Addition"":""None"",
         ""FacultyId"":""None"",
         ""Id"":4235,
         ""Name"":""Business Informatics"",
         ""Url"":""None""
      },
      {
         ""Guid"":""16cd2af6bf4d4f66ad9cc9a8579eff67"",
         ""Addition"":""None"",
         ""FacultyId"":""None"",
         ""Id"":4236,
         ""Name"":""Computing Science"",
         ""Url"":""None""
      },
      {
         ""Guid"":""bc28a2954357463ca9a6e802561baf59"",
         ""Addition"":""None"",
         ""FacultyId"":""None"",
         ""Id"":4233,
         ""Name"":""Informatica"",
         ""Url"":""None""
      },
      {
         ""Guid"":""c33ae14fef0241809ce655f88d6ec9c6"",
         ""Addition"":""None"",
         ""FacultyId"":""None"",
         ""Id"":4234,
         ""Name"":""Informatiekunde"",
         ""Url"":""None""
      }
   ],
   ""Tabs"":{
      ""Guid"":""2d8711878eb141a7927fde68fae1810b"",
      ""Activities"":{
         ""Guid"":""47643a6177fe40da80bbcf92d7530a32"",
         ""Title"":""Activiteiten"",
         ""Visible"":true
      },
      ""AdditionalFunctions"":{
         ""Guid"":""7cb2a133ae1f4c3eac81e44bbd94ed90"",
         ""Title"":""Nevenfuncties"",
         ""Visible"":false
      },
      ""Admin"":{
         ""Guid"":""9afe8b533d4345a4b2fd6fc577e42184"",
         ""Title"":""Beheer"",
         ""Visible"":true
      },
      ""CV"":{
         ""Guid"":""79d603ae2e204f84b76ba9c5b8818a86"",
         ""Title"":""CV"",
         ""Visible"":true
      },
      ""Contact"":{
         ""Guid"":""3e82322480d3414d92dab443bcb8c4f3"",
         ""Title"":""Contact"",
         ""Visible"":true
      },
      ""Courses"":{
         ""Guid"":""136f5727aea84ab2a0c948998e595a09"",
         ""Title"":""Onderwijs"",
         ""Visible"":true
      },
      ""CustomTab1"":{
         ""Guid"":""aa58da75692d492483143c78991d8ae7"",
         ""Title"":"""",
         ""Visible"":false
      },
      ""CustomTab2"":{
         ""Guid"":""92d0b4ae0aea4ba08dc65194efce98cd"",
         ""Title"":"""",
         ""Visible"":false
      },
      ""CustomTab3"":{
         ""Guid"":""86826210e36b4e07b4b7eb79c000d891"",
         ""Title"":"""",
         ""Visible"":false
      },
      ""CustomTabs"":{
         ""Guid"":""5b5834c4ba8443c6b7122ce0d201617e"",
         ""Title"":""Vrije pagina's"",
         ""Visible"":true
      },
      ""General"":{
         ""Guid"":""e15255cdcd704e4da4a0b7eab5232557"",
         ""Title"":""Algemeen"",
         ""Visible"":true
      },
      ""Help"":{
         ""Guid"":""e6544c7a76c941af80fbe7a9dd199f7d"",
         ""Title"":""Help"",
         ""Visible"":true
      },
      ""Media"":{
         ""Guid"":""1714e8d08817443c92d5a412ba5314f0"",
         ""Title"":""In de media"",
         ""Visible"":false
      },
      ""Prizes"":{
         ""Guid"":""00b221a669a14dbcb6338f70fd35212f"",
         ""Title"":""Prijzen"",
         ""Visible"":true
      },
      ""Profile"":{
         ""Guid"":""e23ce4bd4fa543929be713d3d69df63e"",
         ""Title"":""Profiel"",
         ""Visible"":true
      },
      ""Publications"":{
         ""Guid"":""84a2e546ca414cc085fa0be5ae43d95d"",
         ""Title"":""Publicaties"",
         ""Visible"":true
      },
      ""Research"":{
         ""Guid"":""31f4bb8a959c4136b1a02a5b85466bcc"",
         ""Title"":""Onderzoek"",
         ""Visible"":true
      },
      ""Statistics"":{
         ""Guid"":""678669356e024ea3826ccdeb728dd16b"",
         ""Title"":""Statistiek"",
         ""Visible"":true
      }
   }
}

```

## Contact

This package was created by Class CoTaPP in a live coding session. Please contact Jonathan de Bruin for info j.debruin1@uu.nl. 
",2022-08-12
https://github.com/UtrechtUniversity/vagrant-irods,"# vagrant-irods

This repository contains Vagrant configurations for local iRODS VMs.

# Prerequisites

* Install [VirtualBox](https://www.virtualbox.org/wiki/Downloads)
* Install [Vagrant 2.x](https://www.vagrantup.com/downloads.html)
* Install the vagrant-env and vagrant-disksize plugin:  _vagrant plugin install vagrant-env vagrant-disksize_

# Included configurations

- irods-single-server: a basic plain vanilla iRODS server for local testing. It can run on either a CentOS 7 image or a Ubuntu 18.04 LTS image.
- irods-provider-consumer: an iRODS zone consisting of a provider and a single consumer. It can run on either a CentOS 7 image or a Ubuntu 18.04 LTS image. The VMs are meant for local testing, and run with default key values.
- irods-icommands: a VM which contains the icommands tools for remote administration of iRODS. It can run on either a CentOS 7 image or a Ubuntu 18.04 LTS image.

These scripts should support the 4.2.x iRODS versions that are available through the package repositories. As of 20 December 2021, versions 4.2.2 through 4.2.11 are available in the repositories.

# Usage

- If you use Windows, ensure that core.autocrlf is set to false in your git client before you clone the Vagrant-irods
  repository: _git config --global core.autocrlf false_
- Clone the vagrant-irods repository: _git clone https://github.com/utrechtuniversity/vagrant-irods.git_
- Go to the configuration directory. For example : _cd vagrant-irods/irods-single-server_
- Optionally adjust the settings in the .env file. You might want to change the image of the VM, the amount of memory assigned to the VM or the iRODS version to be installed.
- Start and provision the VM(s): _vagrant up_
- After the VM is provisioned, you should be able to log in using _vagrant ssh_. In case of the provider-consumer setup, use _vagrant ssh provider_ or _vagrant ssh consumer_.
",2022-08-12
https://github.com/UtrechtUniversity/workshop-computational-reproducibility,"# Best Practices for Writing Reproducible Code

_If you are looking at a GitHub README, please note that the slides are best viewed [via GitHub pages](https://utrechtuniversity.github.io/workshop-computational-reproducibility/)._

_A gitbook is under development as well: [check it out](https://utrechtuniversity.github.io/workshop-computational-reproducibility/docs/index.html)!_

Ensuring your research is reproducible can be a difficult task.
Scripting your analysis is a start, but this in and of itself is no guarantee that you, or someone else, can faithfully repeat your work at a later stage.
In this workshop, we will help you not only to make your work reproducible, but also to increase the efficiency of your workflow.
We do this by teaching you a few good programming habits: how to set up a good project structure, how to code and comment well, and how to document your code so that it can be used by others.
We will furthermore introduce you to Git and GitHub, which are essential tools in managing and publishing code.
Reproducibility requires extra effort, but we will focus on teaching you skills that will save you much more time in the long run than they cost to implement.

## Preparation

If you are attending this workshop, you can prepare a few things so we can hit the ground running!
We have [more details on this page](preparations).


## Schedule

### Day 1

| Time  | Activity |
|-------:|----------|
| 9:00 | Welcome & introduction ([slides](slides/slides_introduction.html))| 
| 9:30 | [Project setup & version control with git](exercises/project-setup.md) |
| 10:45 | Questions & discussion |
| _11:00_ | _Break_ |
| 11:15 | [Code quality](exercises/code-quality.md)  |
| 12:45 | Questions & discussion |
| _13:00_ | _End_ |


### Day 2

| Time  | Activity |
|-------:|----------|
| 9:00 | Code quality discussion | 
| 9:15 | [Documentation](exercises/documentation.md)  |
| 10:45 | Questions & discussion |
| _11:00_ | _Break_ |
| 11:15 | [Accessibility & reproducibility](exercises/reproducibility.md)  |
| 12:15 | [Reproduction of projects](slides/slides_reproducibility.html#18) |
| _13:00_ | _End_ |


## External resources, recommended reading, and developer inspiration

- [Software Development Guide by the Netherlands eScience Center](https://guide.esciencecenter.nl/)
- [The Turing Way: a guide to reproducible data science by the Turing Institute](https://the-turing-way.netlify.app/welcome)
- [Software Carpentry lessons](https://github.com/swcarpentry/swcarpentry)
- [Pro GIT](https://www.git-scm.com/book/en/v2)
- [Good Enough Practices in Scientific Computing](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510)

## Course materials (for reference, these are also linked in the schedule above!)

### Slides
1. [Introduction](slides/slides_introduction.html)
1. [Project setup and version control](slides/slides_project-setup.html)
1. [Code quality](slides/slides_code-quality.html)
1. [Project documentation](slides/slides_documentation.html)
2. [Accessibility & Reproducibility: Dependency Management](slides/slides_environment.html)
3. [Accessibility & Reproducibility: Archiving](slides/slides_reproducibility.html)


### Exercises
1. [Project setup & version control with git](exercises/project-setup.md)
1. [Code quality](exercises/code-quality.md) 
1. [Project documentation](exercises/documentation.md) 
1. [Accessibility & reproducibility](exercises/reproducibility.md) 


## License

All workshop material is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/). [View the license here](https://github.com/UtrechtUniversity/workshop-computational-reproducibility/blob/master/LICENSE.md).


## History and acknowledgements

This course was developed at Utrecht University, supported by the [Open Science Community Utrecht (OSCU)](https://openscience-utrecht.com) and [Research Data Management (RDM) support](https://www.uu.nl/en/research/research-data-management).

Workshop development is being coordinated by [Neha Moopen](https://github.com/nehamoopen), and the course received contributions from (in alphabetical order):
[Armel Lefebvre](https://github.com/armell)
| [Barbara Vreede](https://github.com/bvreede)
| [Bianca Kramer](https://github.com/bmkramer)
| [Cedric Thieulot](https://github.com/cedrict)
| [Erik van Sebille](https://github.com/erikvansebille)
| [Jeroen Bosman](https://github.com/JeroenBosman)
| [Jeroen Ooms](https://github.com/jeroen)
| [Jonathan de Bruin](https://github.com/J535D165)
| [Lukas van de Wiel](https://github.com/hooiberg)
| [Mateusz Kuzak](https://twitter.com/matkuzak)
| [Menno Fraters](https://github.com/MFraters)
| [Philippe Delandmeter](https://github.com/delandmeterp)
| [Renato Alves](https://github.com/unode)

",2022-08-12
https://github.com/UtrechtUniversity/workshop-introduction-to-R-and-data,"# Workshop: Introduction to R & data

This repository contains the documentation, exercises and datasets of the single day workshop ['Introduction to R & data'](https://www.uu.nl/en/research/research-data-management/training-workshops/introduction-to-r-data) at Utrecht University.
The workshop is organised by [Research Data Management Support](https://www.uu.nl/en/research/research-data-management).

## Workshop description

R is a powerful scripting language for data handling, data visualization, and statistics.
In this workshop, we aim to give you the tools to start exploring R and all it has to offer by yourself.

The course will take you from the very basics in R syntax, to data handling and visualization using a set of packages designed for data science, known as the ‘tidyverse’.
We will also take some time to understand datasets and their architecture, preparing you to handle your own data in a clean, robust, and reproducible manner.
We will work in RStudio and introduce R scripts as well as the R Markdown format: the latter is a great way to combine code and its output with text, allowing you to code in a narrative and intuitive way.
Moreover, this way you produce a human-readable document with which you can easily share and showcase your work.

## Upcoming workshops
Check [uu.nl/rdm](https://www.uu.nl/en/research/research-data-management/training-workshops/introduction-to-r-data) for upcoming workshops.
Registration is mandatory, and opens 2 months in advance of the course.

## Installation requirements
This course requires three things to be installed:
- The language **R**
- The IDE **Rstudio**
- The packages in **tidyverse**

Information on how to install these (and troubleshoot the installation) is on our [installation guide](installation.md).

## Schedule

Most activities are done independently, using videos and exercises provided.
We are always available for 1:1 support during these times, and reconvene in the main channel at stated times.

| Time | Activity |
|---:|:---|
| 9:00 | Walk-in, tech support |
| 9:30 | Introductions |
| 10:00 | Base R: _Exercises 1-6_ |
| 11:25 | Recap & Questions |
| **11:30** | **Coffee break** |
| 11:45 | Programming: _Exercises 7-9_ |
| 12:40 | Recap & Questions |
| **12:45** | **Lunch break** |
| 13:30 | Introduction to Tidyverse & Importing data: _Exercises 1-4_ |
| 14:15 | Subsetting and mutating data: _Exercises 5-8_ |
| 14:55 | Recap & Questions |
| **15:00** | **Coffee break** |
| 15:15 | Transformations & tidy data: _Exercises 9-12_ |
| 16:00 | Data visualization: _Exercises 13-15_ |
| 16:55 | Final recap and closing |

## Material
[Download a zipped file of the workshop material](https://github.com/UtrechtUniversity/workshop-introduction-to-R-and-data/archive/master.zip).
Move it to an accessible place on your system, and unzip.

### Slides
- [Introduction to R](slides/slides_introduction.html)
- [Data science with Tidyverse](slides/slides_tidyverse.pdf)

### Exercises & solutions
- Exercises morning: [baseR_exercises.Rmd](baseR_exercises.Rmd)
- Solutions morning: see [slides](slides/slides_introduction.html)
- Exercises afternoon: [datascience_exercises.Rmd](datascience_exercises.Rmd)
- Solutions afternoon: [datascience_solutions.html](datascience_solutions.html)

### Videos

#### Morning: Introduction to R
The morning session (Introduction to R) has been recorded for self-study.
Links to the videos and corresponding slides are below.

| Subject | Slides | Video | Exercise | Answer slide | Answer video |
|:--------|:-------|:------|:---------|:-------------|:-------------|
| Introduction to Rstudio | [Slides](slides/slides_introduction.html#12) | [Video](https://youtu.be/FFYSAUJ305o) | - | - | - |
| R Syntax & data types | [Slides](slides/slides_introduction.html#18) | [Video](https://youtu.be/S8zTmEvpYYk) | Exercise 1 | [Answer slide](slides/slides_introduction.html#27) | [Answer video](https://youtu.be/V8za6GR7u8Q) |
| Vectors in R | [Slides](slides/slides_introduction.html#28) | [Video](https://youtu.be/XMFjteCdHbQ) | Exercise 2 | [Answer slide](slides/slides_introduction.html#40) | [Answer video](https://youtu.be/rIieuoYT4z0) |
| Data structures | [Slides](slides/slides_introduction.html#41) | [Video](https://youtu.be/Ffk2Kxa_b_M) | Exercise 3 | [Answer slide](slides/slides_introduction.html#50) | [Answer video](https://youtu.be/A1GsHC6pIio) |
| Missing data | [Slides](slides/slides_introduction.html#53) | [Video](https://youtu.be/4gVvlg1Itzs) | Exercise 4 | [Answer slide](slides/slides_introduction.html#63) | - |
| Indexing vectors & lists | [Slides](slides/slides_introduction.html#66) | [Video](https://youtu.be/e10nO2swYIE) | Exercise 5 | [Answer slide](slides/slides_introduction.html#79) | [Answer video](https://youtu.be/4BZGGq-nBos) |
| Indexing a data frame | [Slides](slides/slides_introduction.html#80) | [Video](https://youtu.be/m15hbXG6I-Y) | Exercise 6 | [Answer slide](slides/slides_introduction.html#91) | [Answer video](https://youtu.be/DUk9bfcSwbA) |
| Programming | - | [Video (external)](https://www.youtube.com/watch?v=eSYeHlwDCNA&) | - | - | - |
| Programming: if statements | [Slides](slides/slides_introduction.html#98) | [Video](https://youtu.be/ASVKW4dyLZI) | Exercise 7 | [Answer slide](slides/slides_introduction.html#104) | [Answer video](https://youtu.be/CV3uOPfnnt4) |
| Programming: functions | [Slides](slides/slides_introduction.html#105) | [Video](https://youtu.be/P_qSXHyIUpQ) | Exercise 8 | [Answer slide](slides/slides_introduction.html#117) | [Answer video](https://youtu.be/EezEmFFbRow) |
| Programming: loops | [Slides](slides/slides_introduction.html#118) | [Video](https://youtu.be/K4KSjizSJFk) | Exercise 9 | [Answer slide](slides/slides_introduction.html#123) | [Answer video](https://youtu.be/7iKKwP3aFuA) |

#### Afternoon: Data science with Tidyverse
Lectures and demos have been recorded for self-study.
Links to the videos are below.
The solution to all exercises can be found in the [solutions document](datascience_solutions.html).

| Subject | Video | Exercises |
|:--------|:-------|:------|
| Introduction to Tidyverse | [Video](https://vimeo.com/470831693) | - |
| Importing data | [Video](https://vimeo.com/470836273) | Exercise 1-4 |
| Subsetting & mutating | [Video](https://vimeo.com/470859983) | Exercise 5-8 |
| Transformations & tidy data | [Video](https://vimeo.com/470864363) | Exercise 9-12 |
| Data visualization | [Video](https://vimeo.com/470862707) | Exercise 13-15 |


### External resources
- Book: [R for Data Science](https://r4ds.had.co.nz/)
- [R cheat sheets](https://www.rstudio.com/resources/cheatsheets/) provided by the R community and RStudio, describing common procedures and packages. We use the following cheat sheets during our workshop:
    - [Base R](http://github.com/rstudio/cheatsheets/blob/main/base-r.pdf)
    - [R Markdown](https://raw.githubusercontent.com/rstudio/cheatsheets/main/rmarkdown.pdf)
    - [Data Import](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-import.pdf)
    - [Data Transformation with dplyr](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-transformation.pdf)
    - [Data Visualization with ggplot2](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-visualization.pdf)



## Acknowledgements
The workshop 'Introduction to R & data' is developed and maintained by [Research Data Management Support](https://www.uu.nl/en/research/research-data-management) at Utrecht University.
All videos were recorded by [Jacques Flores](https://www.uu.nl/medewerkers/jpflores).
The material was written by [Barbara Vreede](https://github.com/bvreede), based on earlier versions by [Jonathan de Bruin](https://github.com/J535D165) and [Tessa Pronk](https://github.com/TessaPr).
",2022-08-12
https://github.com/UtrechtUniversity/Workshop-IRAS,"---
output:
  html_document: default
  pdf_document: default
---
## Workshop HPC 
This repo contains all the documents, scripts and data files used in the workshop HPC for researchers affiliated to Utrecht University. The workshop consists of 2 hands-on sessions of 4 hours each. If you want to organize this workshop for your research group, send us [an email with subject HPC Workshop](mailto:info.rdm@uu.nl?subject=HPC%20Workshop) 

The next workshop sessions are scheduled for researchers of IRAS on Januari 15 and 22, 2019. Sign up for this workshop before December 11, 2018 by sending an email to [Mariana Simões](mailto:m.simoes@uu.nl?subject=HPC%20Workshop%202019-01-11).

Follow [this link](./docs/overview.md) for full description of objectives and contents of the workshop
",2022-08-12
https://github.com/UtrechtUniversity/workshop-r-for-humanities,"# workshop-r-for-humanities

## Workshop Description

R is a powerful scripting language for data handling, data visualization, and statistics. In this workshop, we aim to give you the tools to start exploring R and all it has to offer by yourself. We focus on learning the basics of R and applying your new found R knowledge and skills to texts. This workshop specifically targets researchers working within the Humanities, focusing on the analysis of textual data (as found in poems and novels, for example). However, it is open to and useful for all UU employees who want to perform computational textual analyses.

The course will take you from the very basics in R syntax, to data handling and visualisation using a set of tools known as the ‘tidyverse’. You will learn how to perform text mining using the ‘tidytext’ package, enabling you to handle your own data in a clean, robust, and reproducible manner. We will work in RStudio and introduce R as well as R Markdown: this is a great way to combine code and its output with text, allowing you to code in a narrative and intuitive way. Moreover, this way you produce a human-readable document with which you can easily share and showcase your work.

At the end of the course you will be able to:

- read and write lines of R code (even if you do not understand all functions, you know how to look them up);
- understand what ‘tidy’ text data is, how to generate it, and work with it;
- perform basic text mining and sentiment analysis, calculate important terms in a (set of) texts and relationships between words;
- use RStudio, and use it to write an R script and an R markdown document.

## Schedule

| Time | Activity |
| --- | --- |
| 9:00 | Walk-in, tech support |
| 9:30 | Introductions |
| 10:00 | Short intro to the course (on text mining & Tidyverse) |
| 10:15 | Base R: _Exercises 1-3_ |
| 11:15 | Recap & Questions |
| **11:30** | **Coffee break** |
| 11:45 | Base R: _Exercises 4-6_ |
| 12:45 | Recap & Questions |
| **13:00** | **Lunch break** |
| 13:45 | The Tidy Text Format & Sentiment Analysis: _Exercises 7-9_ |
| 14:45 | Recap & Questions |
| **15:00** | **Coffee break** |
| 15:15 | Analyzing word and document frequency & Relationships between words: _Exercises 10-12_ |
| 16:15 | Recap & Questions |
| 16:30 | If needed, extra time for Recap & Questions + individual assistance |
| 17:00 | Doei |",2022-08-12
https://github.com/UtrechtUniversity/yoda,"<br/>
<p align=""center"">
  <a href=""https://github.com/UtrechtUniversity/yoda"">
    <img src=""docs/yoda.svg"" alt=""Yoda logo"" width=""340"" height=""140"">
  </a>

  <p align=""center"">
    A system for reliable, long-term storing and archiving large amounts of research data during all stages of a study.
    <br/>
    <br/>
    <a href=""https://utrechtuniversity.github.io/yoda/""><strong>Explore the docs »</strong></a>
    <br/>
    <br/>
    <a href=""https://github.com/UtrechtUniversity/yoda/issues/new?assignees=&labels=bug+%3Abug%3A&template=bug_report.md&title=%5BBUG%5D"">Report Bug</a>
    .
    <a href=""https://github.com/UtrechtUniversity/yoda/issues/new?assignees=&labels=enhancement&template=feature_request.md&title=%5BFEATURE%5D"">Request Feature</a>
    .
    <a href=""https://github.com/UtrechtUniversity/yoda/discussions/categories/ideas"">Share an idea</a>
    .
    <a href=""https://github.com/UtrechtUniversity/yoda/discussions/categories/q-a"">Ask a question</a>
  </p>
</p>

[![Release](https://img.shields.io/github/v/tag/UtrechtUniversity/yoda?sort=semver)](https://github.com/UtrechtUniversity/yoda/releases) [![License](https://img.shields.io/github/license/UtrechtUniversity/yoda.svg?maxAge=2592000)](/LICENSE)

## About the project
Yoda is a research data management solution developed by [Utrecht University](https://www.uu.nl/) and used by multiple institutes around the world.
It enables researchers and their partners to securely deposit, share, publish and preserve large amounts of research data during all stages of a research project.

This is the main repository of Yoda. It contains the [Ansible](https://docs.ansible.com) playbook for automated deployment, as well as the [documentation](https://utrechtuniversity.github.io/yoda/).

## Requirements
### Control machine requirements
* [Ansible](https://docs.ansible.com/ansible/intro_installation.html) (>= 2.11.x)
* [VirtualBox](https://www.virtualbox.org/manual/ch02.html) or [libvirt](https://libvirt.org/)
* [Vagrant](https://www.vagrantup.com/docs/installation/) (>= 2.2.x)

### Managed node requirements
* [CentOS](https://www.centos.org/) (>= 7.4)

## Documentation
Documentation is hosted on: https://utrechtuniversity.github.io/yoda/

## Contributing
### Code
Instructions on how to setup your development environment and other useful instructions for development can be found in the [documentation](https://utrechtuniversity.github.io/yoda/development/setting-up-development-environment.html).

### Bug reports
We use GitHub for bug tracking.
Please search existing [issues](https://github.com/UtrechtUniversity/yoda/issues) and create a new one if the issue is not yet tracked.

### Questions and ideas
The best place to reach us about questions and ideas regarding Yoda is on our [GitHub Discussions page](https://github.com/utrechtuniversity/yoda/discussions).

## License
This project is licensed under the GPL-v3 license.
The full license can be found in [LICENSE](LICENSE).
",2022-08-12
https://github.com/UtrechtUniversity/yoda-clienttools,"# Yoda-clienttools

The Yoda client tools are a set of commandline utilities for Yoda. They are mainly
intended for data managers and key users.

## Installation

The Yoda clienttools require Python 3. They are compatible with Yoda 1.7 and Yoda 1.8.
The tools need to be used with the '-y 1.8' option in order to work with
Yoda 1.8 instances.

It is recommended to install the tools in a virtual environment, like this:

```
virtualenv --python /usr/bin/python3.6 --no-site-packages venv
source venv/bin/activate
pip3 install --upgrade git+https://github.com/UtrechtUniversity/yoda-clienttools.git
```

If your zone has data objects or collections with nonstandard characters, you should probably use
a version of python-irodsclient that has a fix for known issues when dealing with such objects:

```
pip3 install --upgrade git+https://github.com/cjsmeele/python-irodsclient@xml-compatibility
```

## Overview of tools

### ycleanup\_files

```
usage: ycleanup_files [-h] -r ROOT [-y {1.7,1.8}]

Recursively finds data objects in a collection that will typically have to be
cleaned up when a dataset is archived, and deletes them.

optional arguments:
  -h, --help            show this help message and exit
  -r ROOT, --root ROOT  Delete unwanted files in this collection, as well as
                        its subcollections
  -y {1.7,1.8}, --yoda-version {1.7,1.8}
                        Yoda version on the server (default: 1.7)
```

Overview of files to be removed:

| file      | meaning                        |   |   |   |
|-----------|--------------------------------|---|---|---|
| .\_*      | MacOS resource fork            |   |   |   |
| .DS_Store | MacOS custom folder attributes |   |   |   |
| Thumbs.db | Windows thumbnail data         |   |   |   |

### yensuremembers

```
usage: yensuremembers [-h] -i INTERNAL_DOMAINS [-y {1.7,1.8}]
                      [--offline-check | --online-check] [--verbose]
                      [--dry-run]
                      userfile groupfile

Ensures each research group in a list has a common set of members with a particular role. For example:
   one user has a manager role in all groups.

positional arguments:
  userfile              Name of the user file
  groupfile             Name of the group file (""-"" for standard input)

optional arguments:
  -h, --help            show this help message and exit
  -i INTERNAL_DOMAINS, --internal-domains INTERNAL_DOMAINS
                        Comma-separated list of internal email domains to the Yoda server
  -y {1.7,1.8}, --yoda-version {1.7,1.8}
                        Yoda version on the server (default: 1.7)
  --offline-check, -c   Only checks user file format
  --online-check, -C    Check mode (online): Verifies that all users in the user file exist.
  --verbose, -v         Verbose mode: print additional debug information.
  --dry-run, -d         Dry run mode: show what action would be taken.

        The user file is a text file. Each line has a role and an existing user account name,
        separated by ':':

        Roles are:

        'manager:'    = user that will be given the role of manager
        'member:'     = user that will be given the role of member with read/write
        'viewer:'     = user that will be given the role of viewer with read

        Example lines:
        manager:m.manager@uu.nl
        viewer:v.viewer@uu.nl

        The group file should have one group name on each line.
```
### ygrepgroups

```
usage: ygrepgroups [-h] [-y {1.7,1.8}] [-a] searchstring

Searches for groups by a search string

positional arguments:
  searchstring          The string to search for

optional arguments:
  -h, --help            show this help message and exit
  -y {1.7,1.8}, --yoda-version {1.7,1.8}
                        Yoda version on the server (default: 1.7)
  -a, --all             Show all groups (not just research and vault groups)
```

### ygroupinfo

Prints the category and subcategory of a Yoda research group.

```
usage: ygroupinfo [-h] [-y {1.7,1.8}] groupname

Shows information about a Yoda research group

positional arguments:
  groupname

optional arguments:
  -h, --help            show this help message and exit
  -y {1.7,1.8}, --yoda-version {1.7,1.8}
                        Yoda version on the server (default: 1.7)
```

### yimportgroups

```
usage: yimportgroups [-h] [-y {1.7,1.8}] -i INTERNAL_DOMAINS
                     [--offline-check | --online-check] [--allow-update]
                     [--delete] [--verbose]
                     csvfile

Creates a list of groups based on a CSV file

positional arguments:
  csvfile               Name of the CSV file

optional arguments:
  -h, --help            show this help message and exit
  -y {1.7,1.8}, --yoda-version {1.7,1.8}
                        Yoda version on the server (default: 1.7)
  -i INTERNAL_DOMAINS, --internal-domains INTERNAL_DOMAINS
                        Comma-separated list of internal email domains to the Yoda server
  --offline-check, -c   Check mode (offline): verify CSV format only. Does not connect to iRODS and does not create groups
  --online-check, -C    Check mode (online): verify CSV format and that groups do not exist. Does not create groups.
  --allow-update, -u    Allows existing groups to be updated
  --delete, -d          Delete group members not in CSV file
  --verbose, -v         Show information as extracted from CSV file

        The CSV file is expected to include the following labels in its header (the first row):
        'category'    = category for the group
        'subcategory' = subcategory for the group
        'groupname'   = name of the group (without the ""research-"" prefix)

        The remainder of the columns should have a label that starts with a prefix which
        indicates the role of each group member:

        'manager:'    = user that will be given the role of manager
        'member:'     = user that will be given the role of member with read/write
        'viewer:'     = user that will be given the role of viewer with read

        Notes:
        - Columns may appear in any order
        - Empty data cells are ignored: groups can differ in number of members

        Example:
        category,subcategory,groupname,manager:manager,member:member1,member:member2
        departmentx,teama,groupteama,m.manager@example.com,m.member@example.com,n.member@example.com
        departmentx,teamb,groupteamb,m.manager@example.com,p.member@example.com,
```

### yreport\_collectionsize

Shows a report of the size of all data objects in a (set of) collections.

```
usage: yreport_collectionsize [-y {1.7,1.8}] [--help] [-h] [-r] [-R]
                              [-g GROUP_BY]
                              (-c COLLECTION | -H | -C COMMUNITY)

Shows a report of the size of all data objects in a (set of) collections

optional arguments:
  -y {1.7,1.8}, --yoda-version {1.7,1.8}
                        Yoda version on the server (default: 1.7)
  --help                show help information
  -h, --human-readable  Show sizes in human readable format, e.g. 1.0MB
                        instead of 1000000
  -r, --count-all-replicas
                        Count the size of all replicas of a data object. By
                        default, only the size of one replica of each data
                        object is counted.
  -R, --include-revisions
                        Include the size of stored revisions of data objects
                        in the collection (if available).
  -g GROUP_BY, --group-by GROUP_BY
                        Group collection sizes by resource or by location.
                        Argument should be 'none' (the default), 'resource' or
                        'location'. Grouping by resource or location implies
                        --count-all-replicas. If a collection has no
                        dataobjects and --group-by resource / location is
                        enabled, its size will be printed with group 'all'.
  -c COLLECTION, --collection COLLECTION
                        Show total size of data objects in this collection and
                        its subcollections
  -H, --all-collections-in-home
                        Show total size of data objects in each collection in
                        /zoneName/home, including its subcollections. Note:
                        you will only see the collections you have access to.
  -C COMMUNITY, --community COMMUNITY
                        Show total size of data objects in each research and
                        vault collection in a Yoda community. Note: you will
                        only see the collections you have access to.
```

### yreport\_dataobjectspercollection

Prints a report of the number of subcollections and data objects
per collection. The output is in CSV format.

```
usage: yreport_dataobjectspercollection [-h] [-y {1.7,1.8}] [-r ROOT] [-e]

Shows a report of number of data objects and subcollections per collection

optional arguments:
  -h, --help            show this help message and exit
  -y {1.7,1.8}, --yoda-version {1.7,1.8}
                        Yoda version on the server (default: 1.7)
  -r ROOT, --root ROOT  show only collections in this root collection
                        (default: show all collections
  -e, --by-extension    show number of data objects by extension for each
                        collection
```

List of columns in regular mode:
1. Number of subcollections in collection (nonrecursive).
2. Number of data objects in collection (nonrecursive).
3. Total number of subcollections and data objects in collection (nonrecursive)
4. Name of collection

List of columns if --by-extension is enabled:
1. Number of subcollections in collection (nonrecursive).
2. Number of data objects in collection with the listed extension (nonrecursive).
3. Total number of subcollections and data objects with the listed extension
   in collection (nonrecursive)
4. Extension
5. Name of collection

### yreport\_intake

Prints an intake collection report. This report is only relevant for environments
that use the intake module.

On systems with a significant number of datasets, it is recommended to use the
--cache parameter to keep a local cache of dataset statistics in order to speed
up report generation.

```
usage: yreport_intake [-h] [-y {1.7,1.8}] [-p] -s STUDY [-c CACHE]

Generates a report of the contents of an intake collection.

optional arguments:
  -h, --help            show this help message and exit
  -y {1.7,1.8}, --yoda-version {1.7,1.8}
                        Yoda version on the server (default: 1.7)
  -p, --progress        Show progress updates.
  -s STUDY, --study STUDY
                        Study to process
  -c CACHE, --cache CACHE
                        Local cache directory. Can be used to retrieve
                        previously collected information on datasets, in order
                        to speed up report generation. The script will also
                        store newly collected dataset information in the
                        cache.
```

### yreport\_linecount

Prints a report of the number of lines per data object.

```
usage: yreport_linecount [-h] [-y {1.7,1.8}] (-c COLLECTION | -d DATA_OBJECT)

Shows a report of the line counts of data objects.

optional arguments:
  -h, --help            show this help message and exit
  -y {1.7,1.8}, --yoda-version {1.7,1.8}
                        Yoda version on the server (default: 1.7)
  -c COLLECTION, --collection COLLECTION
                        show line counts of all data objects in this
                        collection (recursive)
  -d DATA_OBJECT, --data-object DATA_OBJECT
                        show line count of only this data object
```

### ywhichgroups

Prints a list of all groups a user is a member of.

```
usage: ywhichgroups [-h] [-y {1.7,1.8}] username

Returns a list of groups of which a user is a member

positional arguments:
  username              The username

optional arguments:
  -h, --help            show this help message and exit
  -y {1.7,1.8}, --yoda-version {1.7,1.8}
                        Yoda version on the server (default: 1.7)
```
",2022-08-12
https://github.com/UU-ComputerScience/ag-pictgen,"# ag-pictgen
Attribute Grammar picture generator

# About ag-pictgen
Ag-pictgen is a small tool for generating Attribute Grammar pictures in tikz format (http://sourceforge.net/projects/pgf/).
It is used by the UUAGC (http://foswiki.cs.uu.nl/foswiki/HUT/AttributeGrammarSystem) tutorial.
The package is extracted from the repo https://svn.science.uu.nl/repos/project.STEC.uuagc where
the tutorial can be found, to allow independent use.
For now, there is no documentation, but the `examples` subdir contains an example `cons_toc.ppd` (and its use in `cons_toc_use.tex`)

",2022-08-12
https://github.com/UU-ComputerScience/js-asteroids,"JS Asteroids
============

An implementation of wxAsteroids in Javascript using UHC (Utrecht Haskell Compiler).
See the [github page](http://uu-computerscience.github.com/js-asteroids/) for a short
intro.

Navigate into the following directories for more elaborate information:

* msc-thesis     : Master thesis 
* lightoo        : A lightweight DSL for OO programming in Haskell
* uhc-js         : A helper library for accessing browser functionality using Haskell
* wxasteroids    : A port of wxAsteroids to the web browser
* thesis-snippets: The code snippets contained in the thesis 


",2022-08-12
https://github.com/UU-ComputerScience/ruler,"ruler
=====

Ruler tool used by UHC (Utrecht Haskell Compiler)",2022-08-12
https://github.com/UU-ComputerScience/ruler-systems,"# ruler-systems
Ruler system variations developed as part of the PhD of Arie Middelkoop

The repo is cloned from https://svn.science.uu.nl/repos/project.ruler.systems,
which now will become obsolete.
",2022-08-12
https://github.com/UU-ComputerScience/shuffle,"shuffle
=======

Shuffle tool used by UHC (Utrecht Haskell Compiler)",2022-08-12
https://github.com/UU-ComputerScience/uhc,"The Utrecht Haskell Compiler (UHC)
==================================

When installing from this (https://github.com/UU-ComputerScience/uhc)
repo also use the repo for uhc-util
(https://github.com/UU-ComputerScience/uhc-util) as these are kept in
sync. For further installation instructions see EHC/README in the
subdirectory EHC holding the actual compiler.

Alternatively, and with less features, a lightweight variant of UHC is
available on hackage: https://hackage.haskell.org/package/uhc-light. Its
version lags behind the master of this repo.

Supported platforms
===================

UHC currently compiles/runs on MacOSX (10.11) with GHC 7.10.
Compiling on other Unix platforms in general does not give problems,
but is somewhat unpredictable due the great variation in combinations.
",2022-08-12
https://github.com/UU-ComputerScience/uhc-pubs,"uhc-pubs
========

Publications
",2022-08-12
https://github.com/UU-ComputerScience/uhc-util,"uhc-utils
=========

Utilities required by UHC (Utrecht Haskell Compiler)


CHR part
========

Has been moved to chr-* packages (https://github.com/atzedijkstra/chr)


status/disclamer
================

Currently the source code is just factored out of UHC, minimally
commented, all modules in UHC.Util, not yet properly spread using naming
conventions. Also, some of the modules have become obsolete over time,
so will someday be removed, to be replaced by other libraries. In other
words, the library is intended for UHC only.
",2022-08-12
https://github.com/UU-ComputerScience/uu-cco,"uu-cco
======

Tools for the CCO (Compiler Construction) course at the UU (Utrecht University)
",2022-08-12
https://github.com/UU-ComputerScience/uu-interleaved,# uu-interleaved,2022-08-12
https://github.com/UU-ComputerScience/uu-options,# uu-options,2022-08-12
https://github.com/UU-ComputerScience/uulib,"[![Build Status](https://travis-ci.org/UU-ComputerScience/uulib.svg?branch=master)](https://travis-ci.org/UU-ComputerScience/uulib)

uulib
=====

This is a conventional cabal package and can be
installed accordingly.

See also http://foswiki.cs.uu.nl/foswiki/HUT/WebHome
",2022-08-12
https://github.com/UU-Hydro/fieldwork_france,"# fieldwork_france
This repo contains the model scripts for the field work of DFGUU in France. 
",2022-08-12
https://github.com/UU-Hydro/PCR-GLOBWB_input_example,"PCR-GLOBWB_input_example
========================

This repo contains (small) sample datasets for running PCR-GLOBWB. 

Contact: E.H.Sutanudjaja@uu.nl

Note that the quality of the data in this repo has not been verified (they were taken from an old archive).

",2022-08-12
https://github.com/UU-Hydro/PCR-GLOBWB_model,"# PCR-GLOBWB

PCR-GLOBWB (PCRaster Global Water Balance) is a large-scale hydrological model intended for global to regional studies and developed at the Department of Physical Geography, Utrecht University (Netherlands). This repository holds the model scripts of PCR-GLOBWB. 

contact: Edwin H. Sutanudjaja (E.H.Sutanudjaja@uu.nl).

Please also see the file README.txt.

Main reference/paper: Sutanudjaja, E. H., van Beek, R., Wanders, N., Wada, Y., Bosmans, J. H. C., Drost, N., van der Ent, R. J., de Graaf, I. E. M., Hoch, J. M., de Jong, K., Karssenberg, D., López López, P., Peßenteiner, S., Schmitz, O., Straatsma, M. W., Vannametee, E., Wisser, D., and Bierkens, M. F. P.: PCR-GLOBWB 2: a 5 arcmin global hydrological and water resources model, Geosci. Model Dev., 11, 2429-2453, https://doi.org/10.5194/gmd-11-2429-2018, 2018.

## Input and output files (including OPeNDAP-based access)

PCR-GLOBWB input and output files for the runs made in Sutanudjaja et al. (2018, https://doi.org/10.5194/gmd-11-2429-2018) are available on https://geo.data.uu.nl/research-pcrglobwb/pcr-globwb_gmd_paper_sutanudjaja_et_al_2018/. For requesting access, please send an e-mail to E.H.Sutanudjaja@uu.nl.

The input files (and some output files) are also available on the OPeNDAP server: https://opendap.4tu.nl/thredds/catalog/data2/pcrglobwb/catalog.html. The OPeNDAP protocol (https://www.opendap.org) allow users to access PCR-GLOBWB input files from the remote server and perform PCR-GLOBWB runs **without** the need to download the input files (with total size ~250 GB for the global extent).

## How to install

Please follow the following steps required to install PCR-GLOBWB:

 1. You will need a working Python environment, we recommend to install Miniconda, particularly for Python 3. Follow their instructions given at https://docs.conda.io/en/latest/miniconda.html. The user guide and short reference on conda can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/cheatsheet.html).

 2. Get the requirement or environment file from this repository [conda_env/pcrglobwb_py3.yml](conda_env/pcrglobwb_py3.yml) and use it to install all modules required (e.g. PCRaster, netCDF4) to run PCR-GLOBWB:

    `conda env create --name pcrglobwb_python3 -f pcrglobwb_py3.yml`

    The requirements file will create a environment named *pcrglobwb_python3*.

 3. Activate the environment in a command prompt:

    `conda activate pcrglobwb_python3`

 4. Clone or download this repository. We suggest to use the latest version of the model, which should also be in the default branch. 

    `git clone https://github.com/UU-Hydro/PCR-GLOBWB_model.git`

    This will clone PCR-GLOBWB into the current working directory.


## PCR-GLOBWB configuration .ini file

For running PCR-GLOBWB, a configuration .ini file is required. Some configuration .ini file examples are given in the *config* directory. To be able to run PCR-GLOBWB using these .ini file examples, there are at least two things that must be adjusted. 

First, please make sure that you edit or set the *outputDir* (output directory) to the directory that you have access. You do not need to create this directory manually.  

Moreover, please also make sure that the *cloneMap* file is stored locally in your computing machine. The *cloneMap* file defines the spatial resolution and extent of your study area and must be in the pcraster format. Some examples are given in this repository [clone_landmask_maps/clone_landmask_examples.zip](clone_landmask_maps/clone_landmask_examples.zip).

By default, the configuration .ini file examples given in the *config* directory will use PCR-GLOBWB input files from the 4TU.ResearchData server, as set in their *inputDir* (input directory). 

`inputDir = https://opendap.4tu.nl/thredds/dodsC/data2/pcrglobwb/version_2019_11_beta/pcrglobwb2_input/`

This can be adjusted to any (local) locations, e.g. if you have the input files stored locally in your computing machine. 


## How to run

Please make sure that the correct conda environment in a command prompt:

`conda activate pcrglobwb_python3`

Go to to the PCR-GLOBWB *model* directory. You can start a PCR-GLOBWB run using the following command: 

`python deterministic_runner.py <ini_configuration_file>`

where <ini_configuration_file> is the configuration file of PCR-GLOBWB. 
",2022-08-12
https://github.com/UU-Hydro/PCR_BMI,"PCR-GLOBWB with BMI
====================

This repository contains code of the global hydrologic model PCR-GLOBWB extended with Basic Model Interface (BMI) functionality, hereafter named PCR-BMI.
The code is developed and applied with GLOFRIM, a globally applicable computational framework for integrated hydrological–hydrodynamic modelling (https://glofrim.readthedocs.io/).

The default PCR-GLOBWB model *without* BMI functionality can be found here as Zenodo release (https://doi.org/10.5281/zenodo.595656).

Basic Model Interface (BMI)
-------------------------------

The Basic Model Interface (BMI) is a library specification to simplify the coupling of models.
The BMI is a non-invasive interface that allows for accessing model code via a defined set of function to simplify conversion of an existing model to a reusable, plug-and-play model component.

More info can be found here: https://csdms.colorado.edu/wiki/BMI_Description.

Important note
---------------

Maintanence and development of the code are independent of the original PCR-GLOBWB source code. It may thus be that certain features of the original version are not (yet) covered here.

For instance, the here provided code was only tested at 30 arcmin spatial resolution. Current efforts are untertaken to merge the different developments.

To use the package, it is required to have pcraster 4.1 or higher installed. The code and installation instructions can be found here: https://pcraster.geo.uu.nl/getting-started/.

For installation of PCR-BMI, it is possible to do this with

.. code-block:: console

    pip install git+https://github.com/UU-Hydro/PCR_BMI.git

or 

.. code-block:: console

    python path/to/PCR_BMI/setup.py develop

It is furthermore important to know that the DynRout extension, which is used in some of the test cases of GLOFRIM with PCR-GLOBWB, is not part of the code stored in this repository. 
If you want to receive the code of DynRout (which does not have a BMI, unfortunately), please contact the developers.

Usage
------

If you want to use the code, you are free to download, apply, and share it within the GNU GPL 3.0 license.

In case of application of this code or parts of it in any reports, please refer to the articles in the reference section as well as the Zenodo-release (version and DOI).

To make use of the BMI functions in PCR-GLOBWB, a typical workflow would look like this:

.. code-block:: python

    from pcrglobwb_bmi_v203 import pcrglobwb_bmi

    config_pcr = 'path/to/a/model/configurations/file.cfg'

    model_pcr = pcrglobwb_bmi.pcrglobwbBMI()

    # initiate PCR-GLOBWB
    model_pcr.initialize(config_pcr)

    # spin-up PCR-GLOBWB
    model_pcr.spinup()

    # update PCR-GLOBWB for 1 timestep (i.e. 1 day)
    model_pcr.update(1)

    # retrieve the values of a model variable as array
    Q = model_pcr.get_var('discharge')

    # overwrite values of a model variable
    model_pcr.set_var('discharge', Q)

Exposed variables
------------------

Note: Not all PCR-GLOBWB variables are exposed and can be retrieved/overwritten with BMI functions. 
Depending on modelling requirements, exposing additional variables may be needed.

Currently exposed variables are:

- discharge
- landSurfaceRunoff
- topWaterLayer
- channelStorage
- waterBodyStorage
- cellArea
- lddMap

Contributing
-------------

For inquiries, feedback, criticism, and research ideas please create an issue in this repository.

Contact
--------

Jannis Hoch PhD

email: j.m.hoch@uu.nl

References
-----------

- Hoch et al., 2017, https://doi.org/10.5194/gmd-10-3913-2017

- Hoch et al., 2019, https://doi.org/10.5194/nhess-19-1723-2019 



",2022-08-12
https://github.com/UU-Hydro/RiverScape,"# RiverScape


This repository holds a set of Jupyter notebooks for the interactive evaluation of river management measures, the RiverScape Python package and necessary input data.

For a detailed description of the concepts, models and study area we refer to the reference publications
[Straatsma and Kleinhans (2018)](https://doi.org/10.1016/j.envsoft.2017.12.010),
[Straatsma et al. (2017)](https://advances.sciencemag.org/content/3/11/e1602762) and
[Straatsma et al. (2019)](https://doi.org/10.5194/nhess-19-1167-2019).


## How to install

A few steps are required to run the Jupyter notebooks:

 1. You will need a working Python environment, we recommend to install Miniconda. Follow their instructions given at:

    [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)

 2. Download the [requirements file](https://github.com/UU-Hydro/RiverScape/blob/master/requirements.yaml) and use it to install all modules required to run the Jupyter notebooks:

    `conda env create -f requirements.yaml`

    The requirements file will create a environment named *riverscape* using Python 3.8. In case you prefer a different name or Python version you need to edit the *requirements.yaml* file.

 3. Activate the environment in a command prompt:

    `conda activate riverscape`

 4. Clone or download this repository:

    `git clone https://github.com/UU-Hydro/RiverScape.git`

    This will clone RiverScape into the current working directory.

General information on Jupyter notebooks and manuals can be found [here](https://jupyter.readthedocs.io/en/latest/).
The user guide and short reference on Conda can be found [here](https://docs.conda.io/projects/conda/en/latest/user-guide/cheatsheet.html).

## How to run

Activate the environment in a command prompt:

`conda activate riverscape`

Change to the RiverScape *scripts* directory.
You can start Jupyter from the command prompt and afterwards select a notebook in your browser:

`jupyter-notebook`



You can also open individual notebooks directly by specifying the filename, e.g. the intervention planning with:

`jupyter-notebook intervent_parameter.ipynb`



## Available notebooks

The following Jupyter notebooks are provided:

  1. Intervention planning: [intervent_parameter.ipynb](scripts/intervention_planning.ipynb)

  2. Evaluation of biodiversity: [biodiversity_evaluation.ipynb](scripts/biodiversity_evaluation.ipynb)

  3. Evaluation of implementation costs: [cost_evaluation.ipynb](scripts/cost_evaluation.ipynb)

  4. Evaluation of landowner involvement: [landowner_evaluation.ipynb](scripts/landowner_evaluation.ipynb)

  5. Evaluation of an ensemble of pre-defined measures: [measure_evaluation.ipynb](scripts/measure_evaluation.ipynb)




## Exemplary output set of measures

In case you want to run the evaluation notebooks without explicitly defining your own set of measures first you can load output data from a pre-defined set of measures.
A single measure is included in the *output* folder.
An ensemble of measures is provided as compressed file.
Extract the file *example_measures_waal.zip* in the *outputs* folder.
You'll get a *example_measures_waal* subfolder containing outputs of 17 measures.



",2022-08-12
https://github.com/UUDigitalHumanitieslab/AASP,"# Automatic Analysis of Speech Prosody
An application to analyze speech prosody, using two approaches:
- AuToDI (Automatic ToDI) - suited especially for Dutch language files
- FDA (Functional Discriminative Analysis)

For a description of the system, see Hu (2020).

--- Hu, N., Janssen, B., Hanssen, J., Gussenhoven, C., & Chen, A. (2020). Automatic Analysis of Speech Prosody in Dutch. In Proc. Interspeech 2020 (pp. 155–159). https://doi.org/10.21437/Interspeech.2020-2142

## Usage
Add a selection of files from your computer to the analysis set. These files should be pairs of .TextGrid and .wav files of the same name. Add a label (called ""Speaker name"", but it can be any label which helps to distinguish files), and click ""Upload"".

In the next step, select files for analysis, and whether to apply AuToDI or FDA. After that, you will be asked which tier in the .TextGrid files should be used for analysis.

In the final step, the results of the analysis can be downloaded as a .zip file.

## AuToDI
This part of the code reuses [AuToBI](https://github.com/AndrewRosenberg/AuToBI) (Rosenberg, 2010), a Java application to automatically annotate prosody with ToBI labels. The Java applciation is used for generating descriptors of the frequency development only; custom classifiers were trained for the ToDI annotation system for the Dutch language. These classifiers can be found in `/AuToDI/classifiers`. They are pickled `sklearn` models.

--- Rosenberg, A. (2010). Autobi-a tool for automatic tobi annotation. In Eleventh Annual Conference of the International Speech Communication Association.

## FDA
This part of the application extracts f0, f1 and f2 from the audio files with the Python wrapper around Praat, Parselmouth. Then it uses R scripts modified from the [FDA R scripts](https://github.com/uasolo/FDA-DH) by Gubian (cf. Gubian, 2015), which fit [B-splines](https://en.wikipedia.org/wiki/B-spline) to the frequency shapes, and list their principal components.

In order to use FDA, you need to specify how many knots (i.e., how many different curves are ""attached"" to each other) and which smoothing factor lambda should be used.

--- Gubian, M., Torreira, F., & Boves, L. (2015). Using functional data analysis for investigating multidimensional dynamic phonetic contrasts. Journal of Phonetics, 49, 16-40.

# Run
Download and extract this repository.

## Using Docker
Using Docker is the easiest way to run this application locally. This will work on Linux, Mac OS, and Windows 10 Professional. Warning: Docker and the images created for this application will take several GBs of disk space.

First, download and install [Docker Desktop](https://docs.docker.com/desktop/). When Docker is running, you can use a command line utilitiy (Terminal on Mac, CMD.exe on Windows) to change to the directory where you extracted this repository:
```
cd /path/to/directory
```

Then you can start the Docker containers by running:
```
docker-compose up
```
This will take a long time to start up the first time, but restarting at a later time should be fast.

Stop the application with crtl-C. If you added items to the AASP database, these will be retained and available on your next start of the application.

If there is a new version of this software, download and extract to the same location again, then run:
```
docker-compose up --build
```

## Without Docker
Required software:
- [PostgreSQL](https://www.postgresql.org/)
- [R](https://www.r-project.org/)
- [Java](https://openjdk.java.net/)
- [Python 3.6](https://www.python.org/downloads/release/python-3615/)

Create a PostgreSQL database with the name 'aasp', and a user 'aasp'. To do that, open a command line utility, and enter the following, replacing `/path/to/your/pgsql/data` with the path in which your Postgres data is saved. This is typically `/usr/local/pgsql/data` on a Linux machine. On Mac OS, you may have installed Postgres through the [Postgres App](https://postgresapp.com/). In that case, you can open the app, and find out the location in the `Server Settings...` menu.
```
postgres -D /path/to/your/pgsql/data
create user aasp with createdb password 'aasp';
create database aasp;
grant all on database aasp to aasp;
```


To start the app, create a virtual environment, install requirements and initialize the database:
```
pip install -r requirements.txt
python manage.py makemigrations
python manage.py migrate
python manage.py createsuperuser
```

To start the application on `localhost:8400`, run:
```
python manage.py runserver --port 8400
```
",2022-08-12
https://github.com/UUDigitalHumanitieslab/AASP-processing,"AASP-processing
===============================================================================
Custom scripts for processing arff and TextGrid files to be used in the Automatic Analysis of Speech Prosody project

To run:
1. Make sure you have Python 3.6 installed (e.g. via [Anaconda](https://www.anaconda.com/distribution/))
2. If you do not want to install the required libraries globally, make a conda environment (see instructions [here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html))
3. Navigate to this directory and run `pip install -r requirements.txt`
4. In this directory, run the script: `python process_arff.py -a PATH/TO/YOUR/ARFF/FEATURE/FILES -t PATH/TO/YOUR/TEXTGRID/FILES` This will:
- adjust the headers of the arff feature files and write new files into a directory. By default, this will be subdirectory called 'processed' in the source file directory, you can set another directory with the -c flag.
- step through the directory with TextGrid files (including subdirectories) and collect the annotated intonations, and combine them with the features from the arff files in a large output file. By default, the complete table is written to  `combined.arff`, and a table without string parameters will be written to `combined_nostring.arff` in this directory. You can set another filename with the -o flag.
- For help with the script, you can pass `python process_arff.py --help`
",2022-08-12
https://github.com/UUDigitalHumanitieslab/alpino-query,"# Alpino Query

```bash
pip install alpino-query
```

When running locally without installing, instead of `alpino-query` use `python -m alpino_query`.

## Mark

Mark which part of the treebank should selected for filtering. It has three inputs:

1. [Lassy/Alpino XML](https://www.let.rug.nl/~vannoord/Lassy/)
2. the tokens of the sentence
3. for each token specify the properties which should be marked

For example:

```bash
alpino-query mark ""$(<tests/data/001.xml)"" ""Dit is een voorbeeldzin ."" ""pos pos pos pos pos""
```

It is also possible to mark multiple properties for a token, this is done by separating them with a comma. Each of these can also be specified to be negated. These will then be marked as 'exclude' in the tree.

```bash
alpino-query mark ""$(<tests/data/001.xml)"" ""Dit is een voorbeeldzin ."" ""pos pos,-word,rel pos pos pos""
```

## Subtree

Generates a subtree containing only the marked properties. It will also contain additional attributes to mark that properties should be excluded and/or case sensitive.

The second argument can be empty, `cat`, `rel` or both (i.e. `catrel` or `cat,rel`). This indicates which attributes should be removed from the top node. When only one node is left in the subtree, this argument is ignored.

```bash
alpino-query subtree ""$(<tests/data/001.marked.xml)"" cat
```

## XPath

Generates an XPath to query a treebank from the generated subtree. Second argument indicates whether a query should be generated which is order-sensitive.

```bash
alpino-query xpath ""$(<tests/data/001.subtree.xml)"" 0
```

## Using as Module

```python
from alpino_query import AlpinoQuery

alpino_xml = ""<Alpino xml as string>""
tokens = [""Dit"", ""is"", ""een"", ""voorbeeldzin"", "".""]
attributes = [""pos"", ""pos,-word,rel"", ""pos"", ""pos"", ""pos""]

query = AlpinoQuery()
query.mark(alpino_xml, tokens, attributes)
print(query.marked_xml) # query.marked contains the lxml Element

query.generate_subtree([""rel"", ""cat""])
print(query.subtree_xml) # query.subtree contains the lxml Element

query.generate_xpath(False) # True to make order sensitive
print(query.xpath)
```

## Considerations

### Exclusive

When querying a node this could be exclusive in multiple ways.
For example:

* a node should not be a noun `node[@pos!=""noun""]`
* it should not have a node which is a noun `not(node[@pos=""noun""])`

The first statement does *require* the existence of a node, whereas the second also holds true if there is no node at all. When a token is only exclusive (e.g. not a noun) a query of the second form will be generated, if a token has both inclusive and exclusive properties a query of the first form will be generated.

### Relations

`@cat` and `@rel` are always preserved for nodes which have children. The only way for this to be dropped is for when all the children are removed by specifying the `na` property for the child tokens.

## Upload to PyPi

```bash
pip install twine
python setup.py sdist
twine upload dist/*
```
",2022-08-12
https://github.com/UUDigitalHumanitieslab/angular-karma-module-config-testcase,"This is a reduced testcase for an open question: how to test the `config` function of an `angular.module`?

To try the attempt in the testcase:

    npm test

You will get:

    Error: No pending request to flush !

If you find the solution, please make a fork and publish your solution in a pull request. Thanks in advance.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/angular-karma-ng-google-maps-testcase,"This is a reduced testcase for an open question: why does the spec in `test.js` fail?

To try the testcase:

    npm test

You will get:

    Error: [$injector:unpr] Unknown provider: uiGmapGoogleMapApiProviderProvider <- uiGmapGoogleMapApiProvider

If you find the solution, please make a fork and publish your solution in a pull request. Thanks in advance.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/AnnCor-scripts,"# AnnCor-scripts

It is recommended to create a `virtualenv` using Python 3.5 (or higher) before running any of the code.

Then simply:
```bash
$ pip install -r requirements/deploy.txt
$ ./run_tests.sh
```

Run `pip install -r requirements/develop.txt` for setting up the development environment.

All provided tests should run without issues.

## Adding Morphological Information to CHAT Files

Use `./add_mor_to_cha.py` for adding `%mor` tiers to an existing CHAT file. The format expected for the file containing the morphological information is a [Lassy XML](https://www.let.rug.nl/vannoord/Lassy/) file (as produced by [Alpino](https://www.let.rug.nl/vannoord/alp/Alpino/)) which are (a) merged in a single XML under some root node (e.g. `<Treebank>`) and (b) contain a `sentid` attribute in the `<sentence>` tag with a one-based utterance index.

The result will be output to the console. When there are no issues the output can simply be targeted to the desired output file.

```bash
$ ./add_mor_to_cha.py -c example.cha -p example.xml > enriched.cha
```

Missing tag mapping will be output in the results in the form `???|[word]-[lassy tag]`, e.g. `???|niet-BW(init)` if this one for the word ""niet"" is missing. At the end of the conversion an overview of missing tags is rendered to console as an error. The output file can then be inspected for more contextual information.

```bash
$ ./add_mor_to_cha.py -c example.cha -p example.xml -c incomplete_mapping.csv > enriched.cha
> ERROR 9 sentence(s) have no tag mapping defined!
> ERROR Missing mapping(s):
> BW()
> TSW()
```

These errors could also be written to a file if needed:

```bash
$ ./add_mor_to_cha.py -c example.cha -p example.xml -c incomplete_mapping.csv > enriched.cha 2> errors.log
```

Converting multiple files at once can be done as such:

```bash
$ ./add_mor_to_cha.py -c *.cha -p example.xml -o example_output
```

All the converted files would be written to the directory `example_output`.

### POS Mapping

The POS mapping can be provided as a comma-separated CSV file using `--mapping` (`-m`). This is used to map each node denoting as single word in the Lassy XML file to a well-formed CHAT tag e.g. `V|help-inf`. The CSV file should contain a header and the following columns:

1. The CHAT tag to place before the word in the `%mor` tier, e.g. `V`. *Optionally* a delimiter (e.g. `-`, `#` or `&`) followed by the affix to use.
2. The ID (ignored).
3. The value in the Lassy `postag` attribute to match.
4. The word form to use (`lemma` or `root`).

Additional columns are ignored.

#### Separable Verbs

Separable verbs are automatically detected and mapped: either placing the preposition separately if its included in the node (e.g. `aan$ V|schaats-PASP`) or removing it when it's not embedded in this node. See `test_separable_verb` for precise details.

#### Punctuation Mapping

Punctuation performed is done when a word is mapped to `PUNCT` e.g.: `PUNCT|period`. Instead of the symbol itself, a mapping is used. A tab-separated CSV file can be provided using `--punctuation` (`-u`), containing:

1. The punctuation symbol to match.
2. The word to use in its place.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/auchann,"# AuChAnn

[![Actions Status](https://github.com/UUDigitalHumanitieslab/auchann/workflows/Unit%20tests/badge.svg)](https://github.com/UUDigitalHumanitieslab/auchann/actions)

[pypi auchann](https://pypi.org/project/auchann)

AuChAnn is a python package that provides Automatic CHAT Annotation based on a transcript string and an interpretation (or 'corrected') string. For example, when given:
Transcript:      'Ik wilt nu eh na huis'
Correction:      'Ik wil nu naar huis'

AuChAnn produces:
CHAT-Annotation: 'ik wilt [: wil] nu &-eh na(ar) [* s:r:prep] huis'

CHAT is an annotation convention that was developed for the CHILDES corpus (MacWinney, 2000) and is used by many linguists to annotate speech. For more information on CHAT,  you can read their manual: https://talkbank.org/manuals/CHAT.html.

AuChAnn was specifically developed to enhance linguistic data in the form of a transcript and interpretation by a linguist for use with SASTA (https://github.com/UUDigitalHumanitieslab/sasta)

## Getting Started

You can install AuChAnn using pip:
```bash
pip install auchann
```

When installed, the program can be run interactively from the console using the command `auchann` .

## Import as Library

To use AuChAnn in your own python applications, you can import the align_words function from align_words, see below. This is the main functionality of the package.

```python
from auchann.align_words import align_words

transcript = input(""Transcript: "")
correction = input(""Correction: "")
alignment = align_words(transcript, correction)
print(alignment)
```

### Settings

Various settings can be adjusted. Default values are used for every unchanged property.

```python
from auchann.align_words import align_words, AlignmentSettings
import editdistance

settings = AlignmentSettings()

# Return the edit distance between the original and correction
settings.calc_distance = lambda original, correction: editdistance.distance(original, correction)

# Return an override of the distance and the error type;
# if error type is None the distance returned will be ignored
# Default method detects inflection errors
settings.detect_error = lambda original, correction: (1, ""m"") if original == ""geloopt"" and correction == ""liep"" else (0, None)

# How many words could be split from one?
# e.g. das -> da(t) (i)s requires a lookahead of 2
# hoest -> hoe (i)s (he)t requires a lookahead of 3
settings.lookahead = 5

# Allow detection of replacements within a group
# e.g. swapping articles this will then be marked with
# the specified key

# EXAMPLE:
# Transcript: de huis
# Correction: het huis
# de [: het] [* s:r:gc:art] huis
settings.replacements = {
    's:r:gc:art': ['de', 'het', 'een'],
    's:r:gc:pro': ['dit', 'dat', 'deze'],
    's:r:prep': ['aan', 'uit']
}

# Other lists to adjust
settings.fillers = ['eh', 'hm', 'uh']
settings.fragments = ['ba', 'to', 'mu']

### Example usage
transcript = input(""Transcript: "")
correction = input(""Correction: "")
alignment = align_words(transcript, correction, settings)
print(alignment)
```

## How it Works

The `align_words` function scans the transcript and correction and determines for each token whether a correction token is copied exactly from the transcript, replaces a token from the transcript, is inserted, or whether a transcript token has been omitted. Based on which of these operations has occurred, the function adds the appropriate CHAT annotation to the output string.

The algorithm uses edit distance to establish which words are replacements of each other, i.e. it links a transcript token to a correction token. Words with the lowest available edit distance are matched together, and based on this match the operations COPY and REPLACE are determined. If two candidates have the same edit distance to a token, word position is used to determine the match. The operations REMOVE and INSERT are established if no suitable match can be found for a transcript and correction token respectively.

In addition to establishing these four operations, the function detects several other properties of the transcript and correction which can be expressed in CHAT. For example, it determines whether a word is a filler or fragment, whether a conjugation error has occurred, or if a pronoun, preposition, or article has been used incorrectly.

## Development

To install the requirements:

```bash
pip install -r requirements.txt
```

To run the AuChAnn command-line function from the console:

```bash
python -m auchann
```

### Run Tests

```bash
pip install pytest
pytest
```

### Upload to PyPi

```bash
pip install pip-tools twine
python setup.py sdist
twine upload dist/*.tar.gz
```

## Acknowledgments

The research for this software was made possible by the CLARIAH-PLUS project financed by NWO (Grant 184.034.023).

## References

MacWhinney, B. (2000).  The CHILDES Project: Tools for Analyzing Talk. 3rd Edition.  Mahwah, NJ: Lawrence Erlbaum Associates
",2022-08-12
https://github.com/UUDigitalHumanitieslab/catharijneverhalen,"# Catharijneverhalen

Frontend to the online storybase of the Dutch museum Catharijneconvent.
https://catharijneverhalen.hum.uu.nl/app/

This is a web application consisting of two subapplications. The `backend` operates at the server side, based on Django/Python. The `frontend` operates at the client side, based on Angular/JavaScript. The backend and frontend know very little about each other; they communicate through a resource-oriented JSON API (which isn't [HATEOAS](https://en.wikipedia.org/wiki/HATEOAS)).

The `backend` and `frontend` directories contain separate Readme documents with further details on the respective subapplications.

The project has employed [git flow branching](http://nvie.com/posts/a-successful-git-branching-model/). You are advised to use the [git-flow tool](https://github.com/nvie/gitflow) if adopting the codebase.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/cfh-lectures,"# Centre for the Humanities video archive

This repository contains the code of the Centre for Humanities video archive, located at http://cfh-lectures.wp.hum.uu.nl. 
It's a [WordPress child theme](https://codex.wordpress.org/Child_Themes) of the [default Utrecht University theme](https://github.com/frietboer/UU2014).
",2022-08-12
https://github.com/UUDigitalHumanitieslab/chamd,"[![Python package](https://github.com/UUDigitalHumanitieslab/chamd/actions/workflows/python-package.yml/badge.svg?branch=develop)](https://github.com/UUDigitalHumanitieslab/chamd/actions/workflows/python-package.yml)

Conversion and cleaning of CHILDES CHA files into PaQu Plaintext
Metadata Format (to convert to Alpino).

`pypi chamd
<https://pypi.org/project/chamd/>`_

.. code:: bash

   pip install chamd
   chamd --help

Running from project:

.. code:: bash

    python -m chamd --help

Import as Library
=================

This way the library can be used to read CHAT file (contents) from an external application.

.. code:: python

    from chamd import ChatReader
    reader = ChatReader()
    chat = reader.read_file('example.cha') # or read_string
    
    for item in chat.metadata:
        print(item)
    for line in chat.lines:
        for item in line.metadata:
            print(item)
        print(line.text)


Upload to PyPi
==============

.. code:: bash

   python setup.py sdist
   twine upload dist/*

Run Tests
=========

.. code:: bash

    python -m unittest discover tests/
",2022-08-12
https://github.com/UUDigitalHumanitieslab/compound-splitter,"# Compound Splitter

This is a basic wrapper for multiple Dutch compound splitters.

## Requirements

Python 3.6+

``` bash
pip install -r requirements.txt
python retrieve.py
python prepare.py
```

## Tests

``` bash
python -m unittest discover tests/
```

## Evaluate Different Compound Algorithms

This will evaluate the different algorithms using the reference files in `test_sets` .

``` bash
python -m compound_splitter.evaluate
```

## Run Web API

``` bash
python -m compound_splitter.api_web
```

### JSON Interface

 `GET /list`

Lists the splitting methods.

 `GET /split/<method_name>/<compound>`

Splits the compound using the specified method.

## Run Simple Socket Server

``` bash
python -m compound_splitter.socket_server
```

``` bash
$ telnet localhost 7005
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
bedrijfsaansprakelijkheidsverzekering,secos
bedrijfs,aansprakelijkheids,verzekeringConnection closed by foreign host.
```

## Install

Make sure the requirements are installed and prepared ( `prepare.py` ).

``` bash
python setup.py install
compound-splitters-nl-api # starts the web API
compound-splitters-nl-socket # start the socket server
```
",2022-08-12
https://github.com/UUDigitalHumanitieslab/cookiecutter-webapp-deluxe,"# cookiecutter-webapp-deluxe

[![Actions Status](https://github.com/UUDigitalHumanitiesLab/cookiecutter-webapp-deluxe/workflows/Tests/badge.svg)](https://github.com/UUDigitalHumanitiesLab/cookiecutter-webapp-deluxe/actions)

A boilerplate for full-fledged web applications with [Django][1] backend, [Angular][2] frontend and [Selenium][3] functional tests.

[1]: https://www.djangoproject.com
[2]: https://angular.io
[3]: https://www.selenium.dev/documentation/webdriver/


## Before you start

You need to install *at least* the following software:

 - Python 3.8 - 3.10
 - [Cookiecutter][4] (install using pip in a virtualenv using Python 3)
 - virtualenv
 - Git (if you use this cookiecutter directly from GitHub)

This is the minimum for Cookiecutter to be able to do its work, i.e., generate a tree of source files. However, cookiecutter-webapp-deluxe includes a post-generation script that automates the other work that comes with starting up a new project, such as creating a new database and installing packages. In order for it to be able to do as much as possible for you, it is recommended that you also install all of the following software before generating a project:

 - Git
 - [gitflow][5] (by default included with [Git for Windows][6])
 - PostgreSQL >= 10, client, server and C libraries
 - [Visual C++ for Python][7] (Windows only)
 - Node.js >= 12
 - Yarn

After generating a new project with this cookiecutter, you'll find a README in the root of the project that mentions two more dependencies not yet listed above. The post-generation script does not depend on them, but you'll likely need them at some later point while developing or deploying the generated project.

 - [WebDriver][8] for at least one browser (for functional testing)
 - WSGI-compatible webserver (for deployment)

These are all the external dependencies you'll need during or after project generation that have to be installed manually.

[4]: https://pypi.python.org/pypi/cookiecutter
[5]: https://github.com/nvie/gitflow
[6]: https://gitforwindows.org
[7]: https://wiki.python.org/moin/WindowsCompilers
[8]: https://pypi.org/project/selenium/#drivers


## Usage

### Quickstart

```console
$ cd to/parent/directory/that/contains/all/your/projects/
$ cookiecutter gh:UUDigitalHumanitieslab/cookiecutter-webapp-deluxe --checkout develop
# (the plan is to change the latter command into `dh init`)
```

This will first ask you for the template parameter values, then generate a file tree and finally run the post-generation script.


### Template parameters that will be asked

#### project_title

The ""pretty"" name of your project, intended for human audiences.


#### slug

The ""technical"" name of your project, intended for file names, package names, variable names, etcetera. Should match the regular expression `^[a-z][a-z0-9_]*$`. The root directory of the generated project will have this name, too.

### app_prefix

The [app prefix](https://angular.io/guide/styleguide#component-custom-prefix) to use in Angular.

#### description

A one-liner that describes what your project is about.


#### author

The name(s) of the person(s) or legal entity that will initially own the copyright over the generated project. Please note that a BSD 3-Clause license is generated; if you wish to use a different license, you have to replace it manually.


#### origin

The URL of the main public Git repository where your project will be hosted and where you'll be pushing your local branches to. The repository need not exist yet; the post-generation script will not attempt to push to it. You can use the [NPM package.json repository URL shorthand][9] for GitHub.

[9]: https://docs.npmjs.com/files/package.json#repository


#### database_{name,user,password}

Respectively the database name for your local development database and the name and password of the local database user that will be granted access to that database. The names and password will be hardcoded into your source tree, so other developers will be using the same values for their local development databases. The values are not used for deployment, so you can keep different, secret names and password in production.

There is, in fact, no reason why you'd need to override these values; the parameters will likely be removed from the interactive asker in the future.


#### localizations

Frontend languages for which you'd like to generate localization files initially. You can still add more languages later. The parameter is encoded as `code:name` pairs separated by commas, where each `code` must be an IETF language tag that [i18next can recognize][10] and each `name` a speaking, human-recognizable slug that can be used as a variable name.

[10]: https://www.i18next.com/principles/translation-resolution#languages

#### frontend

The frontend framework to use.

#### {frontend,backend}_port

The default port to use when locally running a frontend or backend server.

#### psql_command

A command that can be called in order to run SQL queries against your local PostgreSQL server. This will be used to set up your local development database.
This command is not stored in the repository; other people who clone your project will have to enter their own `psql_command`.

It is important that this command can be run with your own account, i.e., without elevated privileges. In other words, it should not start with `sudo`.

The default of `psql` only works if `psql` is in your `PATH` environment variable and you have configured the PostgreSQL server such, that your own OS username is also your PostgreSQL username, you have a *default database* by the same name *and* you can login with peer authentication. Otherwise, you may have to provide the fully-qualified path to your `psql` executable and/or use options such as `-U` and `-d`.

If all of this is abacadabra to you, don't worry and just ignore the parameter. If the command fails, it is easy to fix afterwards. This is discussed in the section on the [post-generation script](#post-generation-script).


#### virtualenv

Path to the virtualenv that will be created for your local clone of the project. If you provide a relative path, it will be resolved against the root of the generated project. An absolute path is also allowed. The path is not stored in the repository; people who clone your project can put their own virtualenvs in different locations.

Many people create a `.env` inside the root of each project. For this reason, this path is suggested as the default. `.env` is also in the `.gitignore` of the generated project, so it will not be committed to VCS by accident. However, any other path will work equally well, especially if you place it outside of the project root. If you have a central place for storing all your virtualenvs, by all means use it.

The post-generation script needs to know the path to your virtualenv, but you are still in full control of how the virtualenv will be created. For example, you can use Anaconda if you so wish. This is the purpose of the next parameter.


#### virtualenv_command

The command that will generate the virtualenv in the location that you provided in the previous parameter. This command will run with the root of the generated project as its working directory. It is important that you can run this command as yourself, i.e., without `sudo`.

The virtualenv must have Python version >= 3.8, <= 3.10. If your virtualenv-creating command uses a different version by default, make sure to add an option to rectify this. The most commonly used command, `virtualenv`, has the `-p` option, so you can for example append `-p python3.10`.


### Post-generation script

After asking the parameters, Cookiecutter generates your project in an instant and then immediately continues executing the post-generation script. Since you don't get the opportunity to inspect the project before the script starts, we discuss the script first.

The post-generation script attempts to execute all of the following steps for you automatically, so you can get started developing your new project as soon as possible:

 - generate the initial translation files
 - create a virtualenv
 - install pip-tools
 - run `pip-compile`
 - install Python packages
 - install Node packages
 - run `git init` and `git flow init`
 - make the initial commit
 - add the `origin` remote to your repository
 - create a local development database for you
 - run initial database migrations
 - create a backend superuser (will prompt for a name and password)

The script is capable of *graceful degradation*. If any step fails, it will continue with other steps that don't depend on it. A log of all automated commands is kept in case you need to debug failed steps (`bootstrap.log`).

In the end, the script will list the commands that still have to be executed by you. There will be at least four such commands, because the script always leaves four steps to you. If any of the automated steps failed, there will be more commands for you to be executed. More on this below.

Some of the automated steps, such as creating a virtualenv, installing packages and creating a development database, also have to be executed by teammates when they clone your project. For them, a similar script called `bootstrap.py` is included in the root of the generated project. In fact, the post-generation script imports common functionality from the generated `bootstrap.py`.


#### Running the post-post-generation commands

The commands that the script lists for you are supposed to work if you copy them verbatim in the order shown. You should execute them **one at a time**, because a command may depend on external conditions which haven't been met or which the script cannot check for you. In some cases, you may also need to modify a command in order to make it work.

The commands all rely on external software and documenting them in full detail is beyond the scope of this README. The most important gotchas are listed below.

 - `yarn django migrate` is a command that the post-generation script would normally automate for you. If it is listed for you to execute manually, this means that automatic database migration failed. In nearly all cases, this is due to the previous step (database *creation*) also having failed. The post-generation script cannot be aware of this because `psql` does not follow the convention of returning a non-zero exit status on failure. So, before you run `yarn django migrate`, first search for a line starting with `psql` in the `bootstrap.log`, read the debug output following that line and get the database created. You may wish to consult the [PostgreSQL documentation][11] as you go.
 - `git push -u origin main develop` requires the `origin` remote repository to exist. If it doesn't, create it first. If you create a new repository on GitHub or a similar hosting service, opt out of initializing it with a README, a `.gitignore`, a license or anything like that; the cookiecutter already takes care of all of those things.

[11]: https://www.postgresql.org/docs/10/index.html


### The generated project

The generated project is a directory with the name that you entered as the `slug`. This directory is placed within the working directory in which you invoked the cookiecutter.

The root directory of the generated project contains a `bootstrap.py` which teammates can use after cloning the repository. If you ran either the post-generation script or the `bootstrap.py`, the project root also contains a `bootstrap.log` with details about the automated setup steps.

The project root contains many more things, one of them being the project's very own README. Please consult that README for further information on working with the project.


## Development

While in theory, it is possible to develop this cookiecutter with a test-driven approach, it is almost certainly less work to just make your changes, generate a test project and keep adjusting until you get the desired result. This may change once the design is modularized into multiple smaller cookiecutters within an overarching command line utility (`dh init`). For now, some tips are provided for the ""trial and error"" approach.

In any case, you will need to make a local clone of the cookiecutter. When generating a test project, use your local clone instead of the one on GitHub. It's as simple as passing the path to the Cookiecutter command:

```console
$ cookiecutter path/to/your/local/clone/of/cookiecutter-webapp-deluxe
```

If you are going to generate many test projects in quick succession, the following tricks can help to make your work efficient and cleanup easy:

 - Make a dedicated directory to contain your test projects, i.e., a directory that doesn't contain anything else.
 - Use short, systematic project names, such as p1, p2, etcetera.
 - Press the return key on all the other asked parameters in order to use the default values. Besides being fast, this ensures that the database and database user have the same name as the project directory and that the virtualenv is placed inside the project directory.
 - If you do all of the above, you can use the following command or something similar to quickly cleanup all generated test projects in one go:
   ```
   $ for name in `ls`; do dropdb $name ; dropuser $name ; rm -rf $name ; done
   ```
 - If you want database creation to work while using the default `psql_command`, as well as the above cleanup command, make sure to configure PostgreSQL correctly so the client commands are in your `PATH` and you can use peer authentication.

",2022-08-12
https://github.com/UUDigitalHumanitieslab/corpus-scraper,"CorpusScraper
=============

By Digital Humanities Lab, Utrecht University


Motivation
----------

Some online text concordance corpora, such as Corpus del Español, do not offer a button to download your search results in a practical format, effectively expecting you to copy, paste and edit the data manually page by page. CorpusScraper is a bookmarklet that automates that work for you, so saving your search results becomes nearly as easy as if there would be such a button.

See https://uudigitalhumanitieslab.github.io/corpus-scraper/ for a full explanation of the usage.


What’s included
---------------

The bookmarklet is written for Chrome and appears to work in Safari and Firefox as well. Internet Explorer seems not to work for most corpora, but your mileage may vary (either way, you should have at least version 9 installed). 

Currently, the script can scrape data from [all byu.edu corpora](http://corpus.byu.edu/) and from Real Academia Española ([CREA](http://corpus.rae.es/creanet.html)/[CORDE](http://corpus.rae.es/cordenet.html)). As of version 2.0, corpora from the Fundación Rafael Lapesa are supported as well ([CORPES](http://web.frl.es/CORPES/view/inicioExterno.view), [CREA](http://web.frl.es/CREA/view/inicioExterno.view), [CNDHE](http://web.frl.es/CNDHE/view/inicioExterno.view)). It is written such that you can add your own implementations for other online concordance corpora.


How to add support for a new online text corpus
-----------------------------------------------

The top anonymous function of the script contains a variable declaration that looks like this:

    var domains = {
        ...
    };

containing key-value pairs that look like this:

    'corpus.example.org': {...}

add such a key-value pair for each domain that you want to add support for, using an existing pair as an example. Note that you can create aliases afterwards if the same implementation works on multiple domains, like in the following line:

    domains['www.corpusdelespanol.org'] = domains['corpus.byu.edu'];

At the very least, the value part should contain the following four members:

    columns: [...]

> An array of strings, representing the names of the columns that will be extracted. This is considered a promise. Every data row that your domain implementation outputs, should have the same length as this array and contain the corresponding fields in the same order.

    init: function ( ) {...}

> Initializes the `target` and `progressSteps` top-level variables. `target` must be set to the innermost frame or window that contains the first page of data, while `progressSteps` should be set to the number of pages to extract from. Make sure to have a special case for when there is only a single page of results with no navigation available (see the Corpus del Español example). 

    getNext: function (doc) {...}

> Returns either the URL to the next page in `doc`, which is a Document object, or a function which accepts a callback, fetches the next document and finally calls the callback with the next document. If there is no next page it returns `undefined`.
> Note that you cannot return the URL simply as `anchor.href`, because Chrome does not add the `href` property to anchor elements when the containing document is parsed in the background without rendering it. Instead, you should return `anchor.getAttribute('href')`.

    scrape1page: function (doc) {...}

> Extracts the data from the page represented by `doc` (again a Document object) and appends those data to the top-level `data` array, one nested array per row (so `data` becomes an array of arrays of strings or numbers). Note that each row should contain the same number of fields in the same order as described in your `columns` member.

Please take note that your custom domain implementation should only use the `target`, `data` and `progressSteps` identifiers from the top-level function. Do not use `window.document` or `window.jQuery`, because that will probably not work out as you expect. You can however use `doc.querySelector` and `doc.querySelectorAll` instead. See http://www.w3.org/TR/selectors-api/ for a specification of these selectors. 
",2022-08-12
https://github.com/UUDigitalHumanitieslab/corpus2alpino,"[![Actions Status](https://github.com/UUDigitalHumanitiesLab/corpus2alpino/workflows/Unit%20tests/badge.svg)](https://github.com/UUDigitalHumanitiesLab/corpus2alpino/actions)

[PyPi/corpus2alpino](https://pypi.org/project/corpus2alpino/)

# CHAT, FoLiA, PaQu metadata, plaintext and TEI to Alpino XML or PaQu metadata format

Converts [CHAT](https://childes.talkbank.org/), [FoLiA](https://proycon.github.io/folia/), [PaQu metadata](https://dspace.library.uu.nl/bitstream/1874/356078/1/AnnCor_Annotation_2017_05_11_2017_05_11.pdf), plaintext and [TEI](http://www.tei-c.org) XML files to [Alpino](https://www.let.rug.nl/vannoord/alp/Alpino) XML files. Each sentence in the input file is parsed separately.

## Usage

### Command Line

```bash
pip install corpus2alpino
corpus2alpino -s localhost:7001 folia.xml -o alpino.xml
```

Or from project root:

```bash
python -m corpus2alpino -s localhost:7001 folia.xml -o alpino.xml
```

### Library

```python
from corpus2alpino.converter import Converter
from corpus2alpino.annotators.alpino import AlpinoAnnotator
from corpus2alpino.collectors.filesystem import FilesystemCollector
from corpus2alpino.targets.memory import MemoryTarget
from corpus2alpino.writers.lassy import LassyWriter

alpino = AlpinoAnnotator(""localhost"", 7001)
converter = Converter(FilesystemCollector([""folia.xml""]),
    # Not needed when using the PaQuWriter
    annotators=[alpino],
    # This can also be ConsoleTarget, FilesystemTarget
    target=MemoryTarget(),
    # Set to merge treebanks, also possible to use PaQuWriter
    writer=LassyWriter(True))

# get the Alpino XML output, combined into one treebank XML file
parses = converter.convert()
print(''.join(parses)) # <treebank><alpino_ds ... /></treebank>
```

### Enrichment

It is possible to add custom properties to (existing) Lassy/Alpino files. This is done using a csv-file containing the node attributes and values to look for and the custom properties to assign.

For example:

```bash
python -m corpus2alpino tests/example_lassy.xml -e tests/enrichment.csv -of lassy
```

See [`corpus2alpino.annotators.enrich_lassy`](corpus2alpino/annotators/enrich_lassy.py) for more information.

## Development

### Unit Test

```bash
python -m unittest
```

### Upload to PyPi

See: https://packaging.python.org/tutorials/packaging-projects/#generating-distribution-archives

Make sure `setuptools` and `wheel` are installed. Then from the virtualenv:

```bash
python setup.py sdist bdist_wheel
twine upload dist/*
```

## Requirements

* [Alpino parser](http://www.let.rug.nl/vannoord/alp/Alpino) running as a server: `Alpino batch_command=alpino_server -notk server_port=7001`
* Python 3.7 or higher
* [libfolia-dev](https://packages.ubuntu.com/bionic/libfolia-dev)
* [libxml2-dev](https://packages.ubuntu.com/bionic/libxml2-dev)

### Installation Instructions for Ubuntu

```bash
sudo apt install libfolia-dev libxml2-dev
pip install -r requirements.txt
```
",2022-08-12
https://github.com/UUDigitalHumanitieslab/daycare-ethics,"# Doordenkertjes (Dutch for ""Brain Teasers"")

Version 1.0.2
Code (c) 2014-2016 Digital Humanities Lab, Utrecht University
Illustrations (c) 2015 Birgit Gorter

This is an application meant to inspire daycare workers in ethical reflection. It is available both as a website and as a phone application (soon to be released). The homepage is at https://doordenkertjes.hum.uu.nl/. For more information, see https://doordenkertjes.hum.uu.nl/#about (Dutch).


## Technical information

This is a hybrid [Flask](http://flask.pocoo.org) web application/[PhoneGap](http://phonegap.com) cross-platform phone application. The client side consists of a single HTML file that can run either inside a browser or inside a web view within a native mobile phone application, without modification. The server side is a Flask application that responds to JSON requests and offers a web-based administration interface.

The Git repository uses the git-flow branching model.


### Project overview

The inner `daycare_ethics` directory (the one that contains `config.xml` and `__init__.py`) serves both as the top-level Python package and as the PhoneGap project directory. Inside it are several subdirectories, some of which belong to the Flask application and some of which belong to PhoneGap.

`admin`, `database` and `server` belong to the Flask part. They define the administration interface, database and public JSON interface, respectively. The administration interface is based on the Flask-Admin package. The database definition uses SQLAlchemy declarative models.

`test` also belongs to the Flask part and defines unit tests for the entire Python application. Its internal layout mimics that of the application.

`hooks` and `res` belong to the PhoneGap part. The former does not contain anything important. The latter contains multimedia that are specifically meant for the PhoneGap native application wrapper, i.e. icons and splash screens in multiple formats.

When you start using PhoneGap, this will also create `platforms` and `plugins` directories.

The `www` directory belongs to both; its name is dictated by the PhoneGap framework but it serves at the same time as what would usually be the `static` directory for a Flask application. It contains the regular client-side code: scripts, style sheets and images. All JavaScript libraries are stored locally, because a PhoneGap application must still work when there is no internet connection. Note that the `index.html` is not a template; the public client side does not use templates because that would be impractical for web view deployment.

`www/spec` contains a [Jasmine](https://jasmine.github.io)-based test suite for the JavaScript part.


### How to install and test locally

It is recommended that you create a Python 2.7 virtualenv with the pip-tools package installed. Activate the virtualenv and run `pip-sync` in order to install all dependencies. **Note:** at the time of writing, pip-tools does not support pip version 8 or later. Run `pip install -U pip==7.1.2` first to be sure that you have the latest compatible version of pip.

In order to run the server side test suite, simply run `python test.py`. This automatically runs all test suites in the `daycare_ethics/tests` directory. The test suite is entirely self-contained; you do not need anything other than the Python packages inside the virtualenv.

In order to run the client side test suite, open `daycare_ethics/www/spec/SpecRunner.html`. This only requires a local copy of the code. Once you have the server running locally as detailed below, you can also visit http://127.0.0.1:5000/spec/SpecRunner.html.

In order to run a local version of the server side, you need a persistent local database, a JSON data file to supply the CAPTCHA system and a configuration file from which the Python application can read its parameters.

Before first use you have to create an empty database, possibly with an associated user; consult the documentation of your RDBMS for instructions. The application is known to work with recent versions of MySQL and SQLite and it will probably work with other RDBMSs.

The CAPTCHA JSON file should define an object in which every key names a category and the corresponding value is an array of words that belong to that category. See `daycare_ethics/tests/data/test_captcha.json` for an example. You should not use that example, because it has been published publicly. More data makes the captcha safer; we recommend at least 10 categories with at least 30 words per category. A slight overlap between the categories is allowed. Words must not contain whitespace. It is wise to stick to the ASCII character set and to write all words in lowercase (unlike in the example).

The configuration file should at least contain the following fields:

    SQLALCHEMY_DATABASE_URI = 'mysql://user@localhost/dbname?charset=utf8&use_unicode=0'
    SECRET_KEY = '...' # long random string
    CAPTCHA_DATA = '/absolute/path/to/captcha/data.json'
    SQLALCHEMY_TRACK_MODIFICATIONS = False

You may optionally set the `SESSION_COOKIE_NAME` if you want it to be something other than ""session"". Consult the SQLAlchemy documentation for details on the database URI. If you call the configuration file `config.py` it will be automatically ignored by Git.

Once you have the above requirements in place, run your local test server with the following command:

    python run.py path/to/config.py

The path should be either relative to the inner `daycare_ethics` directory or absolute.

Running the test server creates a new `instance` directory next to `daycare_ethics`. Media that you upload to the application will be saved there. Debugging output will go straight to your terminal.

You can open the client side interface either by opening `daycare_ethics/www/index.html` in your browser (this does not work in Google Chrome) or by visiting http://127.0.0.1:5000. The administration interface is at http://127.0.0.1:5000/admin. However, at this point the `index.html` is still ""phoning home"" to doordenkertjes.hum.uu.nl. In order to see your own content in the public interface, patch the bottom of `index.html` as follows:

    -    app.origin = 'https://doordenkertjes.hum.uu.nl/';
    +    app.origin = 'http://127.0.0.1:5000/';


### How to deploy to a server

As above, you need a virtualenv with the required Python packages, a database, a JSON CAPTCHA file and a configuration file. Patch the `app.origin` in your `index.html` with the server address. Deployment is otherwise completely regular for a Flask application; consult the Flask documentation for instructions. Your WSGI interface file should import and call the application factory roughly as follows:

    from daycare_ethics import create_app

    application = create_app(config_file=path_to_config_py,
                              instance=custom_instance_folder_path )

    # optionally add handlers to application.logger here

The instructions for logging can also be found in the Flask documentation.

By default, the administration interface is **unprotected**. You have to take your own measures to prevent unauthorized tinkering. We recommend that you use SSL and arrange LDAP or a similar authentication method at the server level to protect `/admin`.

The illustrations are **copyrighted** and you cannot reuse them without explicit permission by Birgit Gorter. Please contact us for permission or use your own illustrations instead.


### How to deploy the PhoneGap application

Install PhoneGap or Cordova through the Node Package Manager (npm). See the official documentation for instructions. Since this is a cross-platform application, you have to follow the instructions for the command line interface (CLI). You have to set your working directory to the inner `daycare_ethics` whenever you issue commands to PhoneGap or Cordova, because it depends on the `config.xml` in there.

The application depends on the following plugins, which you can install with `phonegap plugin add`:

  * `cordova-open`
  * `cordova-plugin-inappbrowser`
  * `cordova-plugin-whitelist`

In order to build the application and run it in an emulator, use the following command:

    phonegap run <platform>

Note that only the platforms `ios` and `android` have been tested so far. You may add the `--device` option to install to a connected physical device instead. Consult the manual of your vendor IDE for further instructions on how to connect your device.

In order to just build without installing, replace `run` by `build`. For publication to an app store, use the `--release` flag.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/dhl-packages,"# DHL-packages
Packages for python. Contains handy frequently used code.


## Requirements

Works with: python 3

## Creating a new package
See: https://python-packaging.readthedocs.io/en/latest/minimal.html

## Upload

1. Create dist
    1. `python setup.py sdist`
1. Upload dist with twine
    1. `twine upload dist/*`

## Install
`pip install dhlUtils`




## What is in it?

1. utilities for managing csv files
    1. loading and saving
    2. manipulating entries
1. Filesystem
    1. Currently only a function that can replace all strings in a file
",2022-08-12
https://github.com/UUDigitalHumanitieslab/digital-atlas,"# Digital Atlas

[![Actions Status](https://github.com/UUDigitalHumanitieslab/digital-atlas/workflows/Unit%20tests/badge.svg)](https://github.com/UUDigitalHumanitieslab/digital-atlas/actions)

The Digital ATLAS of postcolonial Europe visualizes the sites, archives, galleries, museums, monuments, organizations and events, collecting information on postcolonial intellectuals and the main migrant organizations/manifestations in Europe.

Presented at [Postcolonial Publics: Art and Citizen Media in Europe
Ca' Foscari University of Venice, VIU (May 26th, 2022)](https://www.unive.it/data/33113/1/60248).


## Before you start

You need to install the following software:

 - Python >= 3.8, <= 3.10
 - virtualenv
 - [Visual C++ for Python][1] (Windows only)
 - Node.js >= 8
 - Yarn
 - [WebDriver][2] for at least one browser (only for functional testing)
 - WSGI-compatible webserver (deployment only)
 - PostgreSQL >= 10, client, server and C libraries (not used yet)
 

[1]: https://wiki.python.org/moin/WindowsCompilers
[2]: https://pypi.org/project/selenium/#drivers


## How it works

This project integrates four isolated subprojects, each inside its own subdirectory with its own code, package dependencies and tests:

- **backend**: the server side web application based on [Django][3] and [DRF][4]. Within this project this only serves the Angular frontend

- **data**: reads the Excel sheet with all the collected data and exports it to a format which can be used by the frontend

 - **frontend**: the client side web application based on [Angular](https://angular.io)

 - **functional-tests**: the functional test suite based on [Selenium][6] and [pytest][7]

[3]: https://www.djangoproject.com
[4]: https://www.django-rest-framework.org
[6]: https://www.selenium.dev/documentation/webdriver/
[7]: https://docs.pytest.org/en/latest/

Each subproject is configurable from the outside. Integration is achieved using ""magic configuration"" which is contained inside the root directory together with this README. In this way, the subprojects can stay truly isolated from each other.

If you are reading this README, you'll likely be working with the integrated project as a whole rather than with one of the subprojects in isolation. In this case, this README should be your primary source of information on how to develop or deploy the project. However, we recommend that you also read the ""How it works"" section in the README of each subproject.


## Development

### Quickstart

First time after cloning this project:

```console
$ cd frontend
$ yarn
$ yarn build
```

Running the application in [development mode][8] (hit ctrl-C to stop):

```console
$ yarn start
```

This will run the frontend and watch all source files for changes. You can visit the frontend on http://localhost:4200/.


### Commands for common tasks

The `package.json` next to this README defines several shortcut commands to help streamline development. In total, there are over 30 commands. Most may be regarded as implementation details of other commands, although each command could be used directly. Below, we discuss the commands that are most likely to be useful to you. For full details, consult the `package.json`.

Install the pinned versions of all package dependencies in all subprojects:

```console
$ yarn
```

Run backend and frontend in [production mode][8]:

```console
$ yarn start-p
```

Run the functional test suite:

```console
$ yarn test-func [FUNCTIONAL TEST OPTIONS]
```

The functional test suite by default assumes that you have the application running locally in production mode (i.e., on port `4200`). See [Configuring the browsers][10] and [Configuring the base address][11] in `functional-tests/README` for options.

[10]: functional-tests/README.md#configuring-the-browsers
[11]: functional-tests/README.md#configuring-the-base-address

Run *all* tests (mostly useful for continuous integration):

```console
$ yarn test [FUNCTIONAL TEST OPTIONS]
```

Run an arbitrary command from within the root of a subproject:

```console
$ yarn back  [ARBITRARY BACKEND COMMAND HERE]
$ yarn front [ARBITRARY FRONTEND COMMAND HERE]
$ yarn func  [ARBITRARY FUNCTIONAL TESTS COMMAND HERE]
```

For example,

```console
$ yarn back less README.md
```

is equivalent to

```console
$ cd backend
$ less README.md
$ cd ..
```

Run `python manage.py` within the `backend` directory:

```console
$ yarn django [SUBCOMMAND] [OPTIONS]
```

`yarn django` is a shorthand for `yarn back python manage.py`. This command is useful for managing database migrations, among other things.

Manage the frontend package dependencies:

```console
$ yarn fyarn (add|remove|upgrade|...) (PACKAGE ...) [OPTIONS]
```



### Notes on Python package dependencies

Both the backend and the functional test suite are Python-based and package versions are pinned using [pip-tools][13] in both subprojects. For ease of development, you most likely want to use the same virtualenv for both and this is also what the `bootstrap.py` assumes.

[13]: https://pypi.org/project/pip-tools/

This comes with a small catch: the subprojects each have their own separate `requirements.txt`. If you run `pip-sync` in one subproject, the dependencies of the other will be uninstalled. In order to avoid this, you run `pip install -r requirements.txt` instead. The `yarn` command does this correctly by default.

Another thing to be aware of, is that `pip-compile` takes the old contents of your `requirements.txt` into account when building the new version based on your `requirements.in`. You can use the following trick to keep the requirements in both projects aligned so the versions of common packages don't conflict:

```console
$ yarn back pip-compile
# append contents of backend/requirements.txt to functional-tests/requirements.txt
$ yarn func pip-compile
```


### Development mode vs production mode

The purpose of development mode is to facilitate live development, as the name implies. The purpose of production mode is to simulate deployment conditions as closely as possible, in order to check whether everything still works under such conditions. A complete overview of the differences is given below.

dimension  |  Development mode  |  Production mode
-----------|--------------------|-----------------
command  |  `yarn start`  |  `yarn start-p`
base address  |  http://localhost:8000  |  http://localhost:4200
backend server (Django)  |  in charge of everything  |  serves backend only
frontend server (angular-cli)  |  serves  |  watch and build
static files  |  served directly by Django's staticfiles app  |  collected by Django, served by gulp-connect
backend `DEBUG` setting  |  `True`  |  `False`
backend `ALLOWED_HOSTS`  |  -  |  restricted to `localhost`
frontend sourcemaps  |  yes  |  no
frontend optimization  |  no  |  yes


## Deployment

Both the backend and frontend applications have a section dedicated to deployment in their own READMEs. You should read these sections entirely before proceeding. All instructions in these sections still apply, though it is good to know that you can use the following shorthand commands from the integrated project root:

```console

# collect static files of both backend and frontend, with overridden settings
$ yarn django collectstatic --settings SETTINGS --pythonpath path/to/SETTINGS.py
```

You should build the frontend before collecting all static files.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/filter-components,"**These components are part of [GrETEL](https://github.com/UUDigitalHumanitieslab/gretel/tree/master/web-ui/src/app/components/filters) now.**

# Filters

This project was generated with [Angular CLI](https://github.com/angular/angular-cli) version 1.6.5.

## Development server

Run `ng serve` for a dev server. Navigate to `http://localhost:4200/`. The app will automatically reload if you change any of the source files.

## Code scaffolding

Run `ng generate component component-name` to generate a new component. You can also use `ng generate directive|pipe|service|class|guard|interface|enum|module`.

## Build

Run `ng build` to build the project. The build artifacts will be stored in the `dist/` directory. Use the `-prod` flag for a production build.

## Running unit tests

Run `ng test` to execute the unit tests via [Karma](https://karma-runner.github.io).

## Running end-to-end tests

Run `ng e2e` to execute the end-to-end tests via [Protractor](http://www.protractortest.org/).

## Further help

To get more help on the Angular CLI use `ng help` or go check out the [Angular CLI README](https://github.com/angular/angular-cli/blob/master/README.md).
",2022-08-12
https://github.com/UUDigitalHumanitieslab/gretel,"[![Build Status](https://travis-ci.org/UUDigitalHumanitieslab/gretel.svg?branch=develop)](https://travis-ci.org/UUDigitalHumanitieslab/gretel)

# GrETEL 4

This is currently under active development. The stable predecessor can be found at http://gretel.ccl.kuleuven.be/gretel3 (and the source at https://github.com/CCL-KULeuven/gretel/).

## Info

* v4.2.0 August 2019: federated search, improved configuration and state management, download results with node properties and again [many more fixes](https://github.com/UUDigitalHumanitieslab/gretel/compare/v4.1.0...v4.2.0).
* v4.1.0 February 2019: Fixed support for GrInded corpora, [many more fixes](https://github.com/UUDigitalHumanitieslab/gretel/compare/v4.0.2...v4.1.0), feature complete replacement of version 3.
* v4.0.2 October 2018: GrETEL 4 release with many bugfixes and improvements.
* v4.0.0 June 2018: First GrETEL 4 release with new interface.
* v3.9.99 November 2017: GrETEL 4 currently under development!
* v3.0.2 July 2017: Show error message if the BaseX server is down  
* v3.0. November 2016: GrETEL 3 initial release. Available at http://gretel.ccl.kuleuven.be/gretel3

### Branches

master: official version of GrETEL 4, available at http://gretel.hum.uu.nl/gretel3/
dev: development version  
gretel2.0: official version of GrETEL 2.0, available at http://gretel.ccl.kuleuven.be/gretel-2.0  

## Installation

### Prerequisites

Next to a standard LAMP server (with a PHP version > 5.4), GrETEL requires the following packages to be installed on your machine:

* [BaseX](https://packages.debian.org/jessie/database/basex)
* [SimpleXML](http://php.net/manual/en/book.simplexml.php)
* [alpino-query](https://github.com/UUDigitalHumanitieslab/alpino-query)

### Next steps

1. Download (or clone) GrETEL from GitHub.
2. Download the Alpino dependency parser. Current binary used in the live version: `Alpino-x86_64-linux-glibc2.5-20548-sicstus` (available [here](http://www.let.rug.nl/vannoord/alp/Alpino/versions/binary)). 

> It is recommended to use the same version used for creating the treebanks. This way an example based search will result in the same search structure as stored in the database.

3. Create BaseX databases containing the treebanks you want to make available (not necessary when using GrETEL-upload).
4. Adapt `config.example.php` file and change name to `config.php`, and then:
  * Set the path to the Alpino dependency parser in the variable `$alpinoDirectory` (by default: directory `parsers`)
  * Set BaseX variables (machine names, port numbers, password and username)
  * Set path for the Python virtual environment or other place where the required commands are installed.
5. Install [composer](https://getcomposer.org/) to be able to install PHP dependencies.
6. Enable the rewrite module (e.g. `sudo a2enmod rewrite && sudo systemctl restart apache2`).
7. Set `AllowOverride` to `All` to allow `.htaccess` to set the settings for the rewrite module.
8. Run `pip install -r requirements.txt`.
9. Run `npm run build` to compile all the remaining dependencies.
10. Make sure `tmp` and `log` folders exist in the root and can be accessed by Apache.

## Notes for users

Only the properties of the first node matched by an XPATH variable is returned for analysis. For example:

A user searches for `//node[node]`. Two variables are found in this query: `$node1 = //node` and `$node2 = $node1[node]`.

The following sentence would match this query: 

`node[np] (node[det] node[noun])`

The node found for `$node1` will then be `node[np]`. 
The node found for `$node2` will then be `node[det]`. The properties of `node[noun]` will not be available for analysis using this query.

When searching for a more specific structure, this is unlikely to occur.

## Notes for developers

### Front-end

The [Angular](https://angular.io) front-end can be found under `web-ui` and run from there: `npm run start`. You can also use `npm run start:live` to use the production back-end.

### Back-end

* The results that are flushed to the user at a time as well as the maximum results that will be fetched is stored in variables in `config.php`. Change `$flushLimit` and `$resultsLimit` to the values that you want.
* Scripts are organised according to their function:
  * `api/`: entry point for server calls from the front-end (through `api/src/router.php`).
  * `basex-search-scripts/`: scripts that are required to do the actual searching for results. However, the `basex-client.php` is sometimes needed in other cases as well to open up a BaseX session.
  * `preparatory-scripts/`: scripts that run functions on the input leading up to but not including the actual fetching of results. These scripts manipulate do things such as creating XPath, generating breadth-first patterns, parsing the input, and modifying input examples.
  * `functions.php`: contains general functions that are often required but that are not specific to any part of the process

## Credits

* [Liesbeth Augustinus](http://www.ccl.kuleuven.be/~liesbeth/) and [Vincent Vandeghinste](http://www.ccl.kuleuven.be/~vincent/ccl): concept and initial implementation;
* [Bram Vanroy](http://bramvanroy.be/): GrETEL 3 improvements and design;
* [Martijn van der Klis](http://www.uu.nl/staff/MHvanderKlis): initial GrETEL 4 functionality and improvements;
* [Sheean Spoel](http://www.uu.nl/staff/SJJSpoel), [Gerson Foks](https://www.uu.nl/staff/GFoks) and [Jelte van Boheemen](https://www.uu.nl/medewerkers/JvanBoheemen): additional GrETEL 4 functionality and improvements;
* [Koen Mertens](https://github.com/KCMertens): federated search at [Instituut voor de Nederlandse taal](https://ivdnt.org).
* Colleagues at the [Centre for Computational Linguistics at KU Leuven](http://www.arts.kuleuven.be/ling/ccl), and [Utrecht University Digital Humanities Lab](https://dig.hum.uu.nl) for their feedback.

## License

This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (cc-by-sa-4.0). See the [LICENSE](LICENSE) file for license rights and limitations.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/GrETEL-upload,"# GrETEL-upload

GrETEL-upload is an extension package for [GrETEL](http://gretel.ccl.kuleuven.be/gretel3/) that allows to upload your own corpus or dataset.
The application will then automatically transform your corpus in an Alpino XML-treebank. 
After processing, the treebanks are searchable in GrETEL, and if you supply metadata, you can use these for filtering and analysis.

## Local installation

### Requirements

On top of a default LAMP installation (with PHP 7.\*; PHP 8 is currently not working), the following packages are required:

* [basex](https://packages.debian.org/stretch/basex): Storing processed treebanks into a XML-database.
* [php-zip](https://packages.debian.org/stretch/php-zip): Required to process .zip-files.
* [php-ldap](https://packages.debian.org/stretch/php-ldap): Authentication via LDAP.
* [php-sqlite3](https://packages.debian.org/stretch/php-sqlite3): SQLite3 module for PHP, allows tests with in-memory database.
* [php-libxml](https://packages.debian.org/stretch/php-xml)

GrETEL-upload also requires the following external programs to be installed:

* [Alpino](http://www.let.rug.nl/vannoord/alp/Alpino/). Download and then unpack (preferably) into `/opt/Alpino/` . You can change the installation directory in the `application/config/database_default.php` . It also need to be changed in `alpino.sh` .
* [Corpus2alpino](https://github.com/UUDigitalHumanitieslab/corpus2alpino). This can be installed globally using `sudo -H pip3 install corpus2alpino` . This requires Python 3.6+.

It is also possible to install using pip:

``` bash
pip install -r requirements.txt
```

Make sure to modify [config/common.php] (see below) to point to the install location of corpus2alpino.

### Configuration

You will have to provide configuration details in four files:

* `application/config/common.php` : Paths and other common settings.
* `application/config/config.php` : CodeIgniter settings.
* `application/config/database.php` : Settings for your database connection to both the relational database (e.g. MySQL) and the XML-database (basex).
* `application/config/ldap.php` : Settings for LDAP authentication.

An example configuration for each can be found in `application/config/{NAME}_default.php` .

Update the apache config, to allow read-write access to gretel-upload (and gretel).

### Database schema

Create the mysql database gretel_upload 
You can use the command `php index.php migrate` in the source directory to create/migrate the database schema.
See `docs/schema.png` for the current database schema (exported from [phpMyAdmin](https://www.phpmyadmin.net/)).

### Permissions

Make sure the `uploads` directory is writable for the user running the Apache daemon (usually `www-data` ). Also create a writable `sessions` directory and refer to its absolute path in `application/config/config.php` if using the default `files` session driver.

### Start-up

Start both Alpino and BaseX as server instances by running the following two commands:

	basexserver -S
	./alpino.sh

Then, navigate to the installation directory in your web browser (e.g. `localhost/gretel-upload/` ) to start using GrETEL-upload.

#### Production: Cron Task

For production servers, a cron job is required for processing uploaded treebanks. Schedule the following e.g. every 5 minutes:

	/usr/bin/php {root}/index.php cron process

## Uploading corpora

### Formats

Currently, three formats are supported: [LASSY-XML](https://www.let.rug.nl/vannoord/Lassy/), [CHAT](http://childes.talkbank.org/) and plain text (UTF-8 encoded).
When you upload a set of texts (always in a zipped folder, possibly consisting of multiple directories), 
you can specify whether the text is already sentence- and/or word-tokenized.
If not, the application will do this for you.

### Metadata

GrETEL-upload allows metadata annotation using the [PaQu metadata format](http://zardoz.service.rug.nl:8067/info.html#cormeta).
This metadata will be converted to LASSY-XML during import.

The GrETEL-upload interface then allows you to select which facet you would want to use to filter the data in GrETEL.
You can e.g. choose to display a metadata column called 'year' as a slider, dropdown list or set of checkboxes.
You can also choose to hide certain columns.

## Libraries

### PHP

GrETEL-upload is written in PHP and created with [CodeIgniter 3.1.11](https://www.codeigniter.com/).
The application uses the following libraries:

* `application/libraries/Alpino.php` : Wrapper around Alpino's dependency parser and tokenisation scripts.
* `application/libraries/BaseX.php` : [BaseX PHP connector](https://github.com/BaseXdb/basex/blob/master/basex-api/src/main/php/BaseXClient.php). Slightly modified to work in CodeIgniter.
* `application/libraries/Format.php` : Helper to convert between various formats such as XML, JSON, CSV, etc. Part of CodeIgniter Rest Server (see below).
* `application/libraries/Ldap.php` : Authentication via LDAP. Inspired by the [LDAP Authentication library](https://github.com/gwojtak/Auth_Ldap).
* `application/libraries/REST_Controller.php` : [CodeIgniter Rest Server](https://github.com/chriskacerguis/codeigniter-restserver), turns controllers into REST APIs.

### Javascript

GrETEL-upload uses the following JavaScript libraries:

* [jQuery](https://jquery.com/)
* [qTip2](http://qtip2.com/)

### CSS

GrETEL-upload is created with [Pure CSS](http://purecss.io/).

### Images

GrETEL-upload uses the [FamFamFam silk icon set](http://www.famfamfam.com/).

## API

GrETEL-upload has an API for retrieving data from the database:

* treebank/: Returns all publicly available treebanks.
* treebank/show/[title]: Returns the components of the treebank given by title.
* treebank/metadata/[title]: Returns the metadata of the treebank given by title.
* treebank/user/[user_id]: Returns all treebanks available to the currently logged in user. This might include private treebanks.

## Tests

The test suite is created using [ci-phpunit-test](https://github.com/kenjis/ci-phpunit-test).
This uses [PHPUnit](https://phpunit.de/).
You can run the tests by navigating to the `application/tests` directory and calling `phpunit` .

## Demo

A working version is available on http://gretel.hum.uu.nl.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/gretel-vagrant-box,"# GrETEL Vagrant Box

A vagrant box for GrETEL, including the Upload functionality.

## Usage

Run `vagrant up`

To finish the installation and make gretel work do the following:

* Go to `localhost:8080/gretel-upload` in your browser
* Upload a corpus with a guest account
* Go to `localhost:8080/gretel` Gretel works now

Both `gretel` and `gretel_upload` are cloned to `vagrant_data` and you can start working on it.

## Troubleshooting

Make sure that all users have full access (777) to the content of vagrant data. This is necessary because of a quirk with the shared directories.

## Explanation

Vagrant is used to create a virtual machine in a repeatable manner.
This Vagrant file does the following interesting things:

1) It creates a `ubuntu/bionic64` box.
1) It links the `localhost:8080` to port `80` on the guest machine.
1) It runs a bunch of scripts that install packages and copies config files.
    1) In particular `install_mysql.sh` is interesting because it makes use of a preseed: `/vagrant/vagrant_data/scripts/mysql-server.preseed`.

See `VagrantFile` for a more detailed explanation.

See `vagrant_data/scripts` for the scripts that are used.

See `vagrant_data/config` for the config files that are used.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/handlebars-i18next,"# handlebars-i18next

[![npm](https://img.shields.io/npm/dt/handlebars-i18next)](https://badge.fury.io/js/handlebars-i18next) [![npm version](https://badge.fury.io/js/handlebars-i18next.svg)](https://badge.fury.io/js/handlebars-i18next)

[Handlebars][handlebars] helper that lets you translate with [i18next][i18next] inside your templates.

*New!* Need to automatically collect the `{{i18n}}` tags from your Handlebars templates for your translation JSON? Look no further than our sister package [handlebars-i18next-parser][parser].

[handlebars]: https://handlebarsjs.com
[i18next]: https://www.i18next.com
[parser]: https://www.npmjs.com/package/handlebars-i18next-parser


## Quickstart

Installation

```console
$ npm i handlebars-i18next
```

Glue code

```js
import Handlebars from 'handlebars';  // runtime also possible
import i18next from 'i18next';
import registerI18nHelper from 'handlebars-i18next';

// Prepare your i18next instance (can be a custom instance)
i18next.init({
    resources: {
        en: {
            translation: {
                greeting: 'Hello, {{name}}!',
            }
        },
        fr: {...},
    },
    ...
}, function(error, t) {
    // Once this callback is called, you can start rendering templates
    // that depend on the helper (if `error` is undefined).
});

registerI18nHelper(Handlebars, i18next);
```

Template

```hbs
{{i18n 'greeting'}}
```

Template call

```js
template({name: 'Alice'});
```

Result

```
Hello, Alice!
```

Properties in the context of the helper are automatically available as interpolation values to i18next. It just works!


## Advanced usage

### Block helper: templated default value

You can use the helper as a section. The nested template block will be rendered as usual with the same context and passed to `i18next.t` as the `defaultValue` option.

```hbs
{{#i18n 'greeting'}}Please be welcome, {{name}}!{{/i18n}}
```

So if the `greeting` key is not found in any of the selected languages in the current namespace, this will be rendered:

```
Please be welcome, Alice!
```

Otherwise, the following or one of its translations.

```
Hello, Alice!
```


### Providing explicit interpolation values

You can pass an `i18next.replace` property in the root context of the template call in order to provide interpolation values for all helpers in the template.

```js
template({
    name: Alice,
    i18next: {
        replace: {
            name: 'Bob',
        },
    },
});
```

will result in

```
Hello, Bob!
```

You can also pass arbitrary keyword arguments to the helper. These will be passed as options to `i18next.t` and be available as interpolation values.

```hbs
{{i18n 'greeting' name='Cynthia'}}
```

will result in

```
Hello, Cynthia!
```

Keyword arguments take precedence over `root.i18next.replace`, which in turn takes precedence over the current context of the helper.


### Passing other options to `i18next.t`

See [the i18next documentation][i18n-doc] for available options.

[i18n-doc]: https://www.i18next.com/translation-function/essentials#overview-options

In order to provide default options for **all** occurrences of the helper in your template, pass the options hash as the `i18next` property of the root context to the template call.

```js
template({name: 'Alice', i18next: {
    lng: 'fr',
    interpolate: {...},
    ...
}});
```

In order to override options for a **single** occurrence of the helper, pass them directly as keyword arguments to the helper.

```hbs
{{i18n 'greeting' lng='fr' interpolate='{...}'}}
```

Some notes:

 - The options `lngs`, `fallbackLng`, `ns`, `postProcess` and `interpolation` must be JSON-encoded strings when passed as keyword arguments.
 - The `returnObjects` option is forced to be `false`, since Handlebars helpers must return a string. You can pass another value, but it will be ignored.
 - The `replace` option is not supported as keyword argument. Pass the interpolation values individually as keyword arguments instead, as described in the previous section.


### Changing the name of the helper

You can override the helper name by passing the name of your choice as the optional third argument to the exported helper registering function.

```js
import registerI18nHelper from 'handlebars-i18next';

// ...

registerI18nHelper(Handlebars, i18next, 't');
```

```hbs
{{t 'greeting'}}
```


*Made by*

[![Digital Humanities Lab](http://dhstatic.hum.uu.nl/logo-lab/png/dighum-logo.png)](https://dig.hum.uu.nl)
",2022-08-12
https://github.com/UUDigitalHumanitieslab/handlebars-i18next-parser,"# handlebars-i18next-parser

Parser/lexer for the combination of [`handlebars-i18next`][handlebars-i18next] and [`i18next-parser`][i18next-parser].

[handlebars-i18next]: https://www.npmjs.com/package/handlebars-i18next
[i18next-parser]: https://www.npmjs.com/package/i18next-parser

If you are using [`handlebars-i18next`][handlebars-i18next] to insert translation strings into your Handlebars templates, then this package will enable you to extract the keys (and default values) into JSON files for your translators.

## Quickstart

Installation:

```
npm install --save-dev handlebars-i18next-parser
yarn add -D handlebars-i18next-parser
```

Configuration for [`i18next-parser`][i18next-parser] (by default this assumes you use `i18n` as the name for the helper; see below on how to override):

```js
import HbsI18nLexer from 'handlebars-i18next-parser';

{
    lexers: {
        hbs: [HbsI18nLexer],
        handlebars: [HbsI18nLexer],
    }
}
```

Finally, run [`i18next-parser`][i18next-parser] as documented.

## Using alternative helper/function names

[`handlebars-i18next`][handlebars-i18next] lets you [override the helper name][helper-override]. If you use this feature, for example to use the name `t` instead, you can notify the lexer by changing the [`i18next-parser` `lexers` configuration][lexer-conf] as follows:

[helper-override]: https://www.npmjs.com/package/handlebars-i18next#changing-the-name-of-the-helper
[lexer-conf]: https://www.npmjs.com/package/i18next-parser#lexers

```js
{
    lexers: {
        hbs: [{
            lexer: HbsI18nLexer,
            functions: ['t'],
        }],
    }
}
```

*Made by*

[![Digital Humanities Lab](http://dhstatic.hum.uu.nl/logo-lab/png/dighum-logo.png)](https://dig.hum.uu.nl)
",2022-08-12
https://github.com/UUDigitalHumanitieslab/HistoLexic,"# HistoLexic

HistoLexic is a demonstrator for relaying to [INT LexiconService](http://sk.taalbanknederlands.inl.nl/LexiconService/) for lemmatization. The repository currently consists of the following: 

- `histolexic.py`: A standalone Python script for demonstrating the various uses of the LexiconService webservice. Uses the [requests](http://docs.python-requests.org/en/latest/) package. 
- `histolexic.html`: An example implementation of lemmatization in JavaScript. Uses [jQuery](http://jquery.com/) and the plugin [qTip](http://qtip2.com/). 

## Demo

There's an online demo of the lemmatization available [here](http://uudigitalhumanitieslab.github.io/HistoLexic/).
",2022-08-12
https://github.com/UUDigitalHumanitieslab/historic-hebrew-dates,"[![Build Status](https://travis-ci.com/UUDigitalHumanitieslab/historic-hebrew-dates.svg?token=gbE1yWiPSuz64uDZEWzs&branch=develop)](https://travis-ci.com/UUDigitalHumanitieslab/historic-hebrew-dates)

WARNING: this library is very rough around the edges

# Historic Hebrew Dates

Python library and console application for extracting Hebrew and Aramaic dates from historic texts. It includes a graphical editor to specify, modify and test the search patterns.

## Running from the Console

```bash
$ python -m historic_hebrew_dates שבע מאות וחמישים וארבע
> 7*100+5*10+4
> 754
```

## From Code

```python
from historic_hebrew_dates import create_parsers
hebrew = create_parsers('hebrew')

result = hebrew['numerals'].parse('שבע מאות וחמישים וארבע')
print(result[0][0].value) # ((7*100+5*10)+4)
print(result[0][0].evaluated) # 754
```

# Getting the Editor to Work

## Using Vagrant

On a machine without developer tools it's probably most convenient to use [Vagrant](https://www.vagrantup.com/docs/installation/) for running the editor.

Once this has been setup run the following from a terminal at the root directory of this project:

```bash
vagrant up
```

Wait for everything to be ready (can take a few minutes), then it should be possible to go
to http://localhost:4200. Changes made to the patterns can be send back
to this repository using [Git](https://git-scm.com/).

## Locally

* Install [Python 3.6](https://www.python.org) or newer and make sure to include pip.
* Install [node](https://nodejs.org).
* Install [yarn](https://yarnpkg.com).

(If you want you could setup a [virtual environment](https://virtualenv.pypa.io) first).

```
yarn
yarn start
```

Go to `http://localhost:4200`.

# How Does it Work?

Dates consist of different formats and constituent parts, e.g.:

* <u>thirteenth</u> of **September**, _2019_
* **September** <u>13</u>, _twenty-nineteen_

These different formats can be matched using a list of patterns:

* `{day:ordinal} of {month}, {year:number}`
* `{month} {day:number}, {year:number}`

Patterns can also be derived automatically using an annotated corpus (see `annotated_corpus.py`).

The patterns are text strings with optional placeholder references for other patterns. Those references consist of a name (e.g. `month`, `day`, `year`) and a type (`number`, `ordinal`, `month`...). It is also possible to reference to preceding patterns by their type name or all preceding patterns using a numbered reference (e.g. `{1}`).

The matched values are then available for the expression, which can be evaluated using the evaluation function which has been specified for the pattern type.

For example for numbers:

| type | pattern | value |
| ---- | ------- | ----- |
| A | one | `1` |
| A | two | `2` |
| A | ... | ...|
| A | nine | `9` |
| B | twenty | `20` |
| B | thirty | `30` |
| B | ... | ... |
| B | ninety | `90` |
| C | `{big:B}-{small:A}` | `({big}+{small})` |

This could match and evaluate forty-two.

The text is tokenized using all the tokens found in the patterns. If a word is of an unknown token, the system will try to split it up in multiple tokens. This way it can work with words which are written together.

It is also possible to specify in the search text that parts are missing by using a question mark. Those will be filled with all the possible known tokens.

Search is then done using a chart parser. The parser goes through all the patterns, matches them against all the possible (sub) token interpretations and finally returns all the possible matches.

The patterns are specified in `historic_hebrew_dates/patterns` and can be edited using a graphical web interface.

# Supporting Another Language

Copying, renaming and editing the `.json` file of another language is enough to get started. Once this has been done you can specify the patterns using the editor.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/issues-exporter,"# issues-exporter
Exports GitHub issues (currently only to .csv)
",2022-08-12
https://github.com/UUDigitalHumanitieslab/jquery-promise,"# jquery-promise

A lightweight Promise polyfill based on Promises/A+ compliant $.Deferred from jQuery 3.

[![Build Status](https://travis-ci.com/UUDigitalHumanitieslab/jquery-promise.svg?branch=develop)](https://travis-ci.com/UUDigitalHumanitieslab/jquery-promise) [![Promises/A+ 1.1 compliant](https://promisesaplus.com/assets/logo-small.png)](https://promisesaplus.com/)


## Use case

jquery-promise is for you if the following conditions apply.

 - You are already using jQuery 3+ for independent reasons.
 - You need a polyfill for the ES6 `Promise` interface, for example because you are transpiling `async`/`await` syntax.
 - You want your polyfill to be as small as possible.


## Quickstart

```console
$ npm i @dhl-uu/jquery-promise
```

```js
import '@dhl-uu/jquery-promise';
```

This will only put the polyfill in global scope if there is no `Promise` global already. The polyfill implementation is also the default export of the module.


## Limitations

This is a very thin wrapper around [`jQuery.Deferred`][2]. Our polyfill `Promise` interface has the same constructor, static methods and instance methods as [the standard][4]. However, objects returned after chaining `.then`, `.catch` and `.finally` lack the `.finally` method themselves. This is beyond our control, because jQuery constructs promise objects as plain objects in a closure and there is no prototype that we can extend. This is unlikely to cause problems in practice. You still get Promises/A+ compliant then-able objects.

[2]: https://api.jquery.com/category/deferred-object/
[4]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise


*Made by*

[![Digital Humanities Lab](http://dhstatic.hum.uu.nl/logo-lab/png/dighum-logo.png)](https://dig.hum.uu.nl)
",2022-08-12
https://github.com/UUDigitalHumanitieslab/kbkrant-harvest,"# kbkrant-harvest

working environment for harvesting the Databank Digitale Dagbladen


## Initial setup

Create and activate a virtualenv, then

```console
$ pip install pip-tools
$ pip-sync
$ python -m oaiharvest.registry add -p didl kb http://services.kb.nl/mdo/oai/KEY
    # enter destination directory in the prompt (not inside this directory)
```

where `KEY` should be a valid API key.


## Harvesting the metadata

This would normally harvest the entire set in one go:

```console
$ python -m oaiharvest.harvest -s DDD kb
```

To restrict harvesting to the night, we add the `--between` argument, like this:

```console
$ python -m oaiharvest.harvest -s DDD -b 19:00 06:00 kb
```

We wrap this in a script of our own in order to capture the resumption tokens from `pyoai`:

```console
$ python harvest_metadata.py /path/to/tokens.log -s DDD -b 19:00 06:00 kb
```

Run the latter command in a `screen` session so you can logout from the harvesting server without interrupting the process.


## Preventing filesystem problems

Run the `organize.py` over the harvesting directory at least every 15 minutes using a cron job. You need to have a crontab entry that looks like this:

    14,29,44,59 0-5,19-23 * * * /absolute/path/to/virtualenv/bin/python /absolute/path/to/organize.py /absolute/path/to/harvesting/dir

This will compress the newspapers while moving them into subdirectories by the last two digits of their serial number. Every time it runs, it creates a manifest file that lists all newly organized newspapers. These manifests are fed into the OCR harvester.


## Harvesting the OCR

Harvesting the OCR is the slowest process, as every article has to be retrieved individually. We automate this with the `harvest_ocr.py` script. Invoke it like this:

```console
$ python harvest_ocr.py /path/to/harvesting/dir
```

This depends on there being manifests with downloaded metadata in the harvesting directory. You can run the OCR harvest in parallel with the metadata harvest and the `organize.py` cleaner.

Again, it is advised that you do this in a `screen` session so you can logout from the server while the process is running.


## Diagnostics

You should periodically check your `screen` session to verify that the harvesting scripts are still running properly. In addition to this, the contents of the harvesting directory carry information about the progress of the harvest.

The `manifests`, `in_progress` and `ocr_complete` directories contain manifest files in various stages of processing. Manifests in the first directory list newspapers of which the metadata have been downloaded but the OCR still have to be retrieved. Manifests move through `in_progress` and finally into `ocr_complete` complete as the article texts are collected. Each line in a manifest represents a single newspaper, so a crude indication of OCR harvesting progress can be obtained by comparing the total at the bottom of

```console
$ wc -l /path/to/harvesting/dir/manifests/*
```

to the total at the bottom of

```console
$ wc -l /path/to/harvesting/dir/ocr_complete/*
```

Keep in mind that this does not tell you how many of the newspapers in the manifest in `in_progress` have been processed. For a more precise indication of progress, you should compare the number of `*.didl.xml.gz` files (no OCR, minus one that should be in progress) to the number of `*.tgz` files (OCR complete), inside the two-digit subdirectories.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/khatt,"# KHATT

[![Build Status](https://travis-ci.org/UUDigitalHumanitieslab/khatt.svg?branch=develop)](https://travis-ci.org/UUDigitalHumanitieslab/khatt)

Knowledge Hyperlinking and Text Transcription

## To Run
### Without Docker
1. Install [Node](https://nodejs.org/en/), [Yarn](https://classic.yarnpkg.com/en/docs/install), [PostgreSQL](https://www.postgresql.org/) and  [Python 3.6](https://www.python.org/downloads/) on your computer.
2. Start PostgreSQL
3. To keep your Python packages isolated, install [virtualenv] (https://virtualenv.pypa.io/en/stable/installation.html)
4. Download this directory (using git clone or downloading a zip and extracting it)
5. Navigate to `{name-of-topdirectory}/backend`
6. Run `python bootstrap.py`
7. Follow the instructions of the script: setting up a virtual environment, creating the database, creating a superuser
8. Enable your virtual environment (in the typical case, by running `source activate .env`)
9. Navigate to `{name-of-topdirectory}`
10. Run `yarn`. This will install all frontend and backend dependencies. Grab a cup of coffee, tea or other beverage of your choice.
11. Run `yarn start`. This will start the server.
12. Open your browser. At `localhost:8000`, you should be able to see the application.
13. As there are no books in the database yet, create them. Navigate to `localhost:8000/api/books/` (NB: don't omit the last slash!)
14. There is a form at the bottom of the page which allows you to define a book with title and author.
15. Go to `localhost:8000/upload` to upload a manuscript page for the book (for now: .jpg, .png only).
16. Start marking and annotating
17. To download, go to `localhost:8000/download`. By clicking the download link, you will get *all* annotations in the database which have been marked as complete, with information about the associated book, manuscript and page.

### With Docker (requires 10 GB+ hard disk space)
1. NB: does not run on all Windows licenses, please check [this link](https://docs.docker.com/docker-for-windows/install/)
2. Download [Docker](https://www.docker.com/products/docker-desktop)
3. Download this directory (using git clone or downloading a zip and extracting it)
4. Navigate to `{name-of-your-khatt-directory}`
5. Run `docker-compose up`. This will build and start the application. Wait until all four containers (frontend, db, backend, nginx) have been started. Grab a cup of coffee in the meantime.
6. As there are no books in the database yet, create them. Navigate to `localhost/api/books/` (NB: don't omit the last slash!)
7. There is a form at the bottom of the page which allows you to define a book with title and author.
8. Go to `localhost/upload` to upload a manuscript page for the book (for now: .jpg, .png only).
9. Start marking and annotating
10. To download, go to `localhost/download`. By clicking the download link, you will get *all* annotations in the database which have been marked as complete, with information about the associated book, manuscript and page.
11. To stop the container at any point, press crtl-c.
12. To start the container again, repeat steps 4 and 5. Startup should go much faster (since the build only needs to be done once).

## For developers

You need to install the following software:

 - PostgreSQL >= 9.3, client, server and C libraries
 - Python >= 3.4, <= 3.7
 - virtualenv
 - WSGI-compatible webserver (deployment only)
 - [Visual C++ for Python][1] (Windows only)
 - Node.js >= 8
 - Yarn
 - [WebDriver][2] for at least one browser (only for functional testing)

[1]: https://wiki.python.org/moin/WindowsCompilers
[2]: https://pypi.org/project/selenium/#drivers


### Architecture

This project integrates three isolated subprojects, each inside its own subdirectory with its own code, package dependencies and tests:

 - **backend**: the server side web application based on [Django][3] and [DRF][4]

 - **frontend**: the client side web application based on [Angular](https://angular.io)

 - **functional-tests**: the functional test suite based on [Selenium][6] and [pytest][7]

[3]: https://www.djangoproject.com
[4]: https://www.django-rest-framework.org
[6]: https://www.seleniumhq.org/docs/03_webdriver.jsp
[7]: https://docs.pytest.org/en/latest/

Each subproject is configurable from the outside. Integration is achieved using ""magic configuration"" which is contained inside the root directory together with this README. In this way, the subprojects can stay truly isolated from each other.

If you are reading this README, you'll likely be working with the integrated project as a whole rather than with one of the subprojects in isolation. In this case, this README should be your primary source of information on how to develop or deploy the project. However, we recommend that you also read the ""How it works"" section in the README of each subproject.

### Quickstart

First time after cloning this project:

```console
$ yarn
```

Running the application in [development mode][8] (hit ctrl-C to stop):

```console
$ yarn start
```

This will run the backend and frontend applications, as well as their unittests, and watch all source files for changes. You can visit the frontend on http://localhost:8000/, the browsable backend API on http://localhost:8000/api/ and the backend admin on http://localhost:8000/admin/. On every change, unittests rerun, frontend code rebuilds and open browser tabs refresh automatically (livereload).

[8]: #development-mode-vs-production-mode


### Recommended order of development

For each new feature, we suggested that you work through the steps listed below. This could be called a back-to-front or ""bottom up"" order. Of course, you may have reasons to choose otherwise. For example, if very precise specifications are provided, you could move step 8 to the front for a more test-driven approach.

Steps 1–5 also include updating the unittests. Only functions should be tested, especially critical and nontrivial ones.

 1. Backend model changes including migrations.
 2. Backend serializer changes and backend admin changes.
 3. Backend API endpoint changes.
 4. Frontend model changes.
 5. Other frontend unit changes (templates, views, routers, FSMs).
 6. Frontend integration (globals, event bindings).
 7. Run functional tests, repair broken functionality and broken tests.
 8. [Add functional tests][9] for the new feature.
 9. Update technical documentation.

[9]: functional-tests/README.md#writing-tests

For release branches, we suggest the following checklist.

 1. Bump the version number in the `package.json` next to this README.
 2. Run the functional tests in production mode, fix bugs if necessary.
 3. Try using the application in production mode, look for problems that may have escaped the tests.
 4. Add regression tests (unit or functional) that detect problems from step 3.
 5. Work on the code until new regression tests from step 4 pass.
 6. Optionally, repeat steps 2–5 with the application running in a real deployment setup (see [Deployment](#deployment)).


### Commands for common tasks

The `package.json` next to this README defines several shortcut commands to help streamline development. In total, there are over 30 commands. Most may be regarded as implementation details of other commands, although each command could be used directly. Below, we discuss the commands that are most likely to be useful to you. For full details, consult the `package.json`.

Install the pinned versions of all package dependencies in all subprojects:

```console
$ yarn
```

Run backend and frontend in [production mode][8]:

```console
$ yarn start-p
```

Run the functional test suite:

```console
$ yarn test-func [FUNCTIONAL TEST OPTIONS]
```

The functional test suite by default assumes that you have the application running locally in production mode (i.e., on port `4200`). See [Configuring the browsers][10] and [Configuring the base address][11] in `functional-tests/README` for options.

[10]: functional-tests/README.md#configuring-the-browsers
[11]: functional-tests/README.md#configuring-the-base-address

Run *all* tests (mostly useful for continuous integration):

```console
$ yarn test [FUNCTIONAL TEST OPTIONS]
```

Run an arbitrary command from within the root of a subproject:

```console
$ yarn back  [ARBITRARY BACKEND COMMAND HERE]
$ yarn front [ARBITRARY FRONTEND COMMAND HERE]
$ yarn func  [ARBITRARY FUNCTIONAL TESTS COMMAND HERE]
```

For example,

```console
$ yarn back less README.md
```

is equivalent to

```console
$ cd backend
$ less README.md
$ cd ..
```

Run `python manage.py` within the `backend` directory:

```console
$ yarn django [SUBCOMMAND] [OPTIONS]
```

`yarn django` is a shorthand for `yarn back python manage.py`. This command is useful for managing database migrations, among other things.

Manage the frontend package dependencies:

```console
$ yarn fyarn (add|remove|upgrade|...) (PACKAGE ...) [OPTIONS]
```



### Notes on Python package dependencies

Both the backend and the functional test suite are Python-based and package versions are pinned using [pip-tools][13] in both subprojects. For ease of development, you most likely want to use the same virtualenv for both and this is also what the `bootstrap.py` assumes.

[13]: https://pypi.org/project/pip-tools/

This comes with a small catch: the subprojects each have their own separate `requirements.txt`. If you run `pip-sync` in one subproject, the dependencies of the other will be uninstalled. In order to avoid this, you run `pip install -r requirements.txt` instead. The `yarn` command does this correctly by default.

Another thing to be aware of, is that `pip-compile` takes the old contents of your `requirements.txt` into account when building the new version based on your `requirements.in`. You can use the following trick to keep the requirements in both projects aligned so the versions of common packages don't conflict:

```console
$ yarn back pip-compile
# append contents of backend/requirements.txt to functional-tests/requirements.txt
$ yarn func pip-compile
```


### Development mode vs production mode

The purpose of development mode is to facilitate live development, as the name implies. The purpose of production mode is to simulate deployment conditions as closely as possible, in order to check whether everything still works under such conditions. A complete overview of the differences is given below.

dimension  |  Development mode  |  Production mode
-----------|--------------------|-----------------
command  |  `yarn start`  |  `yarn start-p`
base address  |  http://localhost:8000  |  http://localhost:4200
backend server (Django)  |  in charge of everything  |  serves backend only

frontend server (angular-cli)  |  serves  |  watch and build

static files  |  served directly by Django's staticfiles app  |  collected by Django, served by gulp-connect
backend `DEBUG` setting  |  `True`  |  `False`
backend `ALLOWED_HOSTS`  |  -  |  restricted to `localhost`

frontend sourcemaps  |  yes  |  no
frontend optimization  |  no  |  yes


## Deployment

Both the backend and frontend applications have a section dedicated to deployment in their own READMEs. You should read these sections entirely before proceeding. All instructions in these sections still apply, though it is good to know that you can use the following shorthand commands from the integrated project root:

```console

# collect static files of both backend and frontend, with overridden settings
$ yarn django collectstatic --settings SETTINGS --pythonpath path/to/SETTINGS.py
```

You should build the frontend before collecting all static files.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/lassy-xpath,"[![Build Status](https://travis-ci.org/UUDigitalHumanitieslab/lassy-xpath.svg?branch=develop)](https://travis-ci.org/UUDigitalHumanitieslab/lassy-xpath)

## LASSY XPath

Module for working with XPath queries on [LASSY XML](https://www.let.rug.nl/vannoord/Lassy/) files. It includes a graphical editor including auto completion, macros and validation based on [Ace](https://ace.c9.io/), a parser and validator based on [ts-xpath](https://github.com/UUDigitalHumanitieslab/ts-xpath) and an ""extractinator"" for determining XPaths to get each node from the returned tree separately. It also has a ""reconstructor"" to create an XML structure representing the query tree. The functionality can be used as an Angular module.

## Compatibility

- v0.12.x is for Angular 12
- v0.4.3 and down should work with Angular 6 and JQuery

## Angular

Import the module:

```typescript
import { LassyXPathModule } from 'lassy-xpath';

@NgModule({
    imports: [LassyXPathModule]
})
export class AppModule {}
```

Includes the services:

```typescript
import { MacroService, ExtractinatorService, ValueEvent } from 'lassy-xpath';


@Component()
export class ExampleComponent {
    constructor(
        macroService: MacroService,
        private extractinatorService: ExtractinatorService) {
        // set the macros to use in the editor
        macroService.loadDefault();
    }

    inputChanged(event: ValueEvent) {
        this.valid = !event.error;
        this.value = event.xpath;
        console.log(this.extractinatorService.extract(event.xpath));
    }
}
```

Embeds an editor:

```html
<lx-editor [value]=""value"" (onChange)=""inputChanged($event)"" autofocus=""true""></lx-editor>
```

Use the `ParserService` for parsing/validating a LASSY XML XPath.

## Publishing a new version

*Run `npm run publish` from root or follow these steps:*

1. Compile using `npm run build`
2. `cd dist/lassy-xpath`
3. Optionally run `npm pack` to test the package locally
4. Remove the `lassy-xpath-x.xx.x.tgz` file (if generated in 3)
5. Run `npm publish`
",2022-08-12
https://github.com/UUDigitalHumanitieslab/LoCMiner,"# LoCMiner

The LoCMiner is a simple web application that allows users to search the 
[Chronicling America collection](http://chroniclingamerica.loc.gov/) of the 
[Library of Congress](http://www.loc.gov/) and then easily export their data 
to text mining tools of their choice.

## Back-end

The LoCMiner is written in the [Flask microframework](http://flask.pocoo.org/) 
with the [SQLAlchemy extension](https://pythonhosted.org/Flask-SQLAlchemy/) for ORM. 
All requests to the Chronicling America collection are processed using the 
[Requests: HTTP for Humans](http://docs.python-requests.org/) package. 

For a local installation, the following steps should be sufficient:

    > sudo apt install postgresql

Setup a PostgreSQL database user and place the database configuration in `config.py`.

    > git clone https://github.com/UUDigitalHumanitieslab/LoCMiner.git
    > cd LoCMiner
    > pip install -r requirements.txt
    > python2 run.py
    
This will start the web interface. To process searches, you should start 
[Redis](http://redis.io/) (usually booted on start-up) and 
[Celery](http://www.celeryproject.org/) (in a separate shell):

    > celery -A LoCMiner.tasks worker
    
You can specify your settings in `LoCMiner/config.py`. 
If you want to use the *DevelopmentConfig*, be sure to change this in 
both `run.py` and `LoCMiner/factories.py`. 

The user interface should now be reachable from `http://localhost:5000`.

## Front-end

On the front-end the [PureCSS](http://purecss.io/) package is used primarily for the lay-out. 
The following JavaScript libraries are employed:

- [jQuery](http://jquery.com/)
- [DataTables](http://datatables.net/)
- [HighCharts](http://www.highcharts.com/)

## Text Mining

### Texcavator

The application allows for synergy with the text mining tool [Texcavator](https://github.com/UUDigitalHumanitieslab/texcavator). 
Your saved searches can be indexed to an [Elasticsearch](http://www.elasticsearch.org/) cluster via the 
[pyelasticsearch](http://pyelasticsearch.readthedocs.org/en/latest/) package. 
You can then freely search your results with Texcavator. 

### Voyant

The application can return a simple output file for use in the online text mining tool [Voyant](http://voyant-tools.org/). 

### Export to .csv and .txt

Finally, the application allows for simple exports of both metadata (to a .csv-file) and full-text (to .txt-files). 

## Demo

A demonstrator is available [here](https://dhtest2.hum.uu.nl/locminer). Currently access is limited to a select
number of Utrecht University students and employees. If you want a peek, contact the 
[Digital Humanities lab](http://digitalhumanities.wp.hum.uu.nl/).
",2022-08-12
https://github.com/UUDigitalHumanitieslab/Map-your-Heroine,"# Map your Heroine

[![Build Status](https://travis-ci.org/UUDigitalHumanitieslab/map_your_heroine.svg?branch=develop)](https://travis-ci.org/UUDigitalHumanitieslab/map_your_heroine)

Map your Heroine will humanize all your digits!


## Before you start

You need to install the following software:

 - PostgreSQL >= 9.3, client, server and C libraries
 - Python >= 3.4, <= 3.7
 - virtualenv
 - WSGI-compatible webserver (deployment only)
 - [Visual C++ for Python][1] (Windows only)
 - Node.js >= 8
 - Yarn
 - [WebDriver][2] for at least one browser (only for functional testing)

[1]: https://wiki.python.org/moin/WindowsCompilers
[2]: https://pypi.org/project/selenium/#drivers


## How it works

This project integrates three isolated subprojects, each inside its own subdirectory with its own code, package dependencies and tests:

 - **backend**: the server side web application based on [Django][3] and [DRF][4]
 
 - **frontend**: the client side web application based on [Angular](https://angular.io)
 
 - **functional-tests**: the functional test suite based on [Selenium][6] and [pytest][7]

[3]: https://www.djangoproject.com
[4]: https://www.django-rest-framework.org
[6]: https://www.seleniumhq.org/docs/03_webdriver.jsp
[7]: https://docs.pytest.org/en/latest/

Each subproject is configurable from the outside. Integration is achieved using ""magic configuration"" which is contained inside the root directory together with this README. In this way, the subprojects can stay truly isolated from each other.

If you are reading this README, you'll likely be working with the integrated project as a whole rather than with one of the subprojects in isolation. In this case, this README should be your primary source of information on how to develop or deploy the project. However, we recommend that you also read the ""How it works"" section in the README of each subproject.


## Development

### Quickstart

First time after cloning this project:

```console
$ python bootstrap.py
```

Running the application in [development mode][8] (hit ctrl-C to stop):

```console
$ yarn start
```

This will run the backend and frontend applications, as well as their unittests, and watch all source files for changes. You can visit the frontend on http://localhost:8000/, the browsable backend API on http://localhost:8000/api/ and the backend admin on http://localhost:8000/admin/. On every change, unittests rerun, frontend code rebuilds and open browser tabs refresh automatically (livereload).

[8]: #development-mode-vs-production-mode


### Recommended order of development

For each new feature, we suggested that you work through the steps listed below. This could be called a back-to-front or ""bottom up"" order. Of course, you may have reasons to choose otherwise. For example, if very precise specifications are provided, you could move step 8 to the front for a more test-driven approach.

Steps 1–5 also include updating the unittests. Only functions should be tested, especially critical and nontrivial ones.

 1. Backend model changes including migrations.
 2. Backend serializer changes and backend admin changes.
 3. Backend API endpoint changes.
 4. Frontend model changes.
 5. Other frontend unit changes (templates, views, routers, FSMs).
 6. Frontend integration (globals, event bindings).
 7. Run functional tests, repair broken functionality and broken tests.
 8. [Add functional tests][9] for the new feature.
 9. Update technical documentation.

[9]: functional-tests/README.md#writing-tests

For release branches, we suggest the following checklist.

 1. Bump the version number in the `package.json` next to this README.
 2. Run the functional tests in production mode, fix bugs if necessary.
 3. Try using the application in production mode, look for problems that may have escaped the tests.
 4. Add regression tests (unit or functional) that detect problems from step 3.
 5. Work on the code until new regression tests from step 4 pass.
 6. Optionally, repeat steps 2–5 with the application running in a real deployment setup (see [Deployment](#deployment)).


### Commands for common tasks

The `package.json` next to this README defines several shortcut commands to help streamline development. In total, there are over 30 commands. Most may be regarded as implementation details of other commands, although each command could be used directly. Below, we discuss the commands that are most likely to be useful to you. For full details, consult the `package.json`.

Install the pinned versions of all package dependencies in all subprojects:

```console
$ yarn
```

Run backend and frontend in [production mode][8]:

```console
$ yarn start-p
```

Run the functional test suite:

```console
$ yarn test-func [FUNCTIONAL TEST OPTIONS]
```

The functional test suite by default assumes that you have the application running locally in production mode (i.e., on port `4200`). See [Configuring the browsers][10] and [Configuring the base address][11] in `functional-tests/README` for options.

[10]: functional-tests/README.md#configuring-the-browsers
[11]: functional-tests/README.md#configuring-the-base-address

Run *all* tests (mostly useful for continuous integration):

```console
$ yarn test [FUNCTIONAL TEST OPTIONS]
```

Run an arbitrary command from within the root of a subproject:

```console
$ yarn back  [ARBITRARY BACKEND COMMAND HERE]
$ yarn front [ARBITRARY FRONTEND COMMAND HERE]
$ yarn func  [ARBITRARY FUNCTIONAL TESTS COMMAND HERE]
```

For example,

```console
$ yarn back less README.md
```

is equivalent to

```console
$ cd backend
$ less README.md
$ cd ..
```

Run `python manage.py` within the `backend` directory:

```console
$ yarn django [SUBCOMMAND] [OPTIONS]
```

`yarn django` is a shorthand for `yarn back python manage.py`. This command is useful for managing database migrations, among other things.

Manage the frontend package dependencies:

```console
$ yarn fyarn (add|remove|upgrade|...) (PACKAGE ...) [OPTIONS]
```



### Notes on Python package dependencies

Both the backend and the functional test suite are Python-based and package versions are pinned using [pip-tools][13] in both subprojects. For ease of development, you most likely want to use the same virtualenv for both and this is also what the `bootstrap.py` assumes.

[13]: https://pypi.org/project/pip-tools/

This comes with a small catch: the subprojects each have their own separate `requirements.txt`. If you run `pip-sync` in one subproject, the dependencies of the other will be uninstalled. In order to avoid this, you run `pip install -r requirements.txt` instead. The `yarn` command does this correctly by default.

Another thing to be aware of, is that `pip-compile` takes the old contents of your `requirements.txt` into account when building the new version based on your `requirements.in`. You can use the following trick to keep the requirements in both projects aligned so the versions of common packages don't conflict:

```console
$ yarn back pip-compile
# append contents of backend/requirements.txt to functional-tests/requirements.txt
$ yarn func pip-compile
```


### Development mode vs production mode

The purpose of development mode is to facilitate live development, as the name implies. The purpose of production mode is to simulate deployment conditions as closely as possible, in order to check whether everything still works under such conditions. A complete overview of the differences is given below.

dimension  |  Development mode  |  Production mode
-----------|--------------------|-----------------
command  |  `yarn start`  |  `yarn start-p`
base address  |  http://localhost:8000  |  http://localhost:4200
backend server (Django)  |  in charge of everything  |  serves backend only

frontend server (angular-cli)  |  serves  |  watch and build

static files  |  served directly by Django's staticfiles app  |  collected by Django, served by gulp-connect
backend `DEBUG` setting  |  `True`  |  `False`
backend `ALLOWED_HOSTS`  |  -  |  restricted to `localhost`

frontend sourcemaps  |  yes  |  no
frontend optimization  |  no  |  yes


## Deployment

Both the backend and frontend applications have a section dedicated to deployment in their own READMEs. You should read these sections entirely before proceeding. All instructions in these sections still apply, though it is good to know that you can use the following shorthand commands from the integrated project root:

```console

# collect static files of both backend and frontend, with overridden settings
$ yarn django collectstatic --settings SETTINGS --pythonpath path/to/SETTINGS.py
```

You should build the frontend before collecting all static files.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/microcontact,"Microcontact
============
**Microcontact. Language Variation and Change from the Italian heritage perspective** is an ERC-funded research project that aims to understand language change from a syntactic microvariational perspective, by observing core syntactic aspects of Romance languages in contact. More information can be found [here](https://microcontact.sites.uu.nl/project/).

Configuration
-------------
There are default settings in `microcontact.settings`. It is recommended that you take some time to review these settings and override them where necessary, especially for production use. See the ""Database"" section below for comments on the `DATABASES` setting in particular.

For local development use, create a Python module in the project root that starts with `from microcontact.settings import *` and then selectively override settings. Your file will be automatically ignored by git if you call it `config.py`.

For production use, copy `microcontact/settings.py` to a separate file. Put this file elsewhere in the filesystem under restrictive access rights, then modify its contents. There are many settings that need to be overridden because of security considerations; please see the comments.

In either case, set the environment variable `DJANGO_SETTINGS_MODULE` to the module name of your customized settings in order to use it. Depending on location of your settings module, you may also need to set `PYTHONPATH`. For documentation, see the [Django documentation on settings][1].

Google Maps API key
-------------------
You need to obtain an API key for Google Maps in order for the frontend application to work. Save the key (with nothing else) in a file named `.gmapikey` in the project root directory. This file is ignored by Git.

Dependencies
------------

Make sure that the following packages are installed using `sudo apt-get install`:

* libldap2-dev (needed for pyldap)
* libsasl2-dev (needed for pyldap)
* libmagic-dev (needed for [python-magic][22])
* python3-dev (needed for Anaconda3)

For the Python dependencies, create a Python 3 virtualenv and activate it. `pip install pip-tools` and then run `pip-sync`.
For the JavaScript dependencies, install NPM, Bower and Grunt, then run `npm install` and `bower install`. For deployment, you can run `bower install` with the `--production` flag in order to skip development-only packages.

For audio conversion, get ffmpeg. See instructions for Windows, OS X and Ubuntu [here][21].

For file type detection we use the [python-magic][22] package, which depends on `libmagic` being installed on your system.

Database
--------
The default configuration in `microcontact.settings` assumes a SQLite database. SQLite and PostgreSQL are also the only database backends supported by the `requirements.txt`. If you choose a different backend, you will need to install additional libraries. For most backends, including PostgreSQL, you will also need to create a dedicated database and a dedicated user with all privileges on that database. For local development, the database user should also be able to create new databases in order to enable testing. See the [Django settings reference][2] for instructions on setting `DATABASES`.

In order to bootstrap your local database before first running the application, run `python manage.py migrate`. Run this command again after defining new migrations. In order to define a new migration (after modifying the database schema), run `python manage.py makemigrations` and edit the generated file. See the [Django documentation on migrations][14] for details.

Local development
-----------------
Make sure that your virtual environment is activated, then run `grunt`. This will do many things:

  - Compile the CoffeeScript, Sass and Mustache sources to browser-ready static assets (these are hidden in `/.tmp` under the project root).
  - Run all unit tests once (this relies on you having installed `pytest`).
  - Run the Django-based development server on `localhost:8000/`.
  - Watch all sources for changes, automatically recompiling static assets, re-running unit tests and reloading any open browser tabs where applicable.
  - Recompile and rerun the functional tests when changed. Note that the functional tests are not automatically rerun when you change source files. Functional tests are further discussed below.

All subprocesses log to the same single terminal that you run `grunt` from, so the output will be a mess at first. After that, however, you will be grateful for the combined output, as any server hiccups, test failures and compilation errors will automatically come to your attention from a single window. This process keeps running until you kill it with `ctrl-c`.

At your option, you may also run any of the tasks above separately. The required commands are listed below with pointers to further documentation. Refer to the `Gruntfile.coffee` for the definitions and configuration of the Grunt-based tasks.

  - `grunt handlebars` to precompile all Mustache templates except for the `index.mustache` to a single `templates.js` ([grunt-contrib-handlebars][13]).
  - `grunt coffee` to compile all CoffeeScript sources to JavaScript ([grunt-contrib-coffee][3]). `grunt coffee:compile` to limit compilation to the client side scripts or `grunt coffee:functional` to limit compilation to the functional tests.
  - `grunt newer:coffee`, `grunt newer:coffee:compile` or `grunt newer:coffee:functional` to do the same but only with changed sources ([grunt-newer][15]).
  - `grunt clean:develop compile-handlebars:develop` to generate the `index.html` ([grunt-compile-handlebars][4]). The `clean` step is necessary because `compile-handlebars` appends to output files instead of replacing them.
  - `grunt sass postcss` to compile the stylesheets from Sass to CSS ([grunt-sass][18], [grunt-postcss][19]).
  - `grunt symlink` to ensure that the `/bower_components` are accessible from within the `/.tmp` ([grunt-contrib-symlink][6]).
  - `grunt compile` to do all of the above.
  - `grunt server` or `python manage.py runserver` to run just the development server ([django-admin][12]). Keeps running.
  - `grunt watch` to automatically recompile, retest and reload when files change ([grunt-contrib-watch][10]). This includes the functional tests as described above. Note that if you start the development server before the watch task, livereload will not work until you manually refresh your browser tab. Keeps running.
  - `grunt casperjs` to run all functional tests. This assumes that you already compiled the CoffeeScript sources. ([grunt-casperjs][16], [CasperJS][17])
  - `grunt newer:casperjs` to run only the functional tests that you edited and recompiled.

In order to remove generated files but not the `node_modules` or the `bower_components`, run `grunt clean:all` ([grunt-contrib-clean][11]).

If you want to verify the optimized assets (see below) during development, you can override the `STATICFILES_DIRS` setting in your `config.py` as follows:

    STATICFILES_DIRS = (
        os.path.join(BASE_DIR, 'dist'),
    )

this will cause the Django application to take the `index.html` and other static assets from `/dist` instead of `/.tmp`. The development server will pick up the change while running, so you don't need to restart it. Note that livereload does not work when you serve from `/dist`.

Deployment
----------
An optimized version of the static assets can be obtained by running `grunt dist`. The optimized files are put in the `/dist` project subdirectory. In this case, most of the external libraries are fetched from CDNs in their minified forms.

You are advised to run the Django-based backend as a WSGI application from your favourite HTTP server. See [Deploying Django][20].

Serve the static assets under `/static/`.

Directory reference
-------------------

    project root                 (whatever you called it)
    ├── .editorconfig            notation conventions, in-VCS
    ├── .functional-tests        functional tests compiled to JS, out-of-VCS
    ├── .gitignore
    ├── .tmp/                    generated static assets for dev., out-of-VCS
    ├── Gruntfile.coffee         task configuration, in-VCS
    ├── README.md                this file, in-VCS
    ├── bower.json               listing of JS deps, in-VCS
    ├── bower_components/        JS deps during development, out-of-VCS
    ├── client                   static asset sources, in-VCS
    │   ├── script
    │   │   ├── *.coffee
    │   │   └── *_test.coffee    unit tests belonging to *.coffee
    │   ├── style/
    │   └── template/
    ├── config.py                presumed to be written by you, out-of-VCS
    ├── dist/                    generated static assets for depl., out-of-VCS
    ├── functional-tests         functional test sources in Coffee, in-VCS
    ├── manage.py                backend manager, in-VCS
    ├── media/                   file uploads, out-of-VCS
    ├── microcontact             the Django project package, in-VCS
    │   ├── *.py
    │   └── *_test.py            unit tests belonging to *.py
    ├── node_modules/            Node and Grunt dependencies, out-of-VCS
    ├── package.json             listing of Node/Grunt deps, in-VCS
    ├── recordings               the Django API application package, in-VCS
    │   ├── *.py
    │   └── *_test.py            unit tests belonging to *.py
    ├── requirements.in          top-level Python package requirements, in-VCS
    └── requirements.txt         generated pinned requirements, in-VCS


(c) 2016 Julian Gonggrijp
(c) 2017 Digital Humanities Lab, Utrecht University


[1]: https://docs.djangoproject.com/en/1.8/topics/settings/
[2]: https://docs.djangoproject.com/en/1.8/ref/settings/#databases
[3]: https://www.npmjs.com/package/grunt-contrib-coffee
[4]: https://www.npmjs.com/package/grunt-compile-handlebars
[6]: https://www.npmjs.com/package/grunt-contrib-symlink
[7]: https://www.npmjs.com/package/grunt-contrib-connect
[8]: https://www.npmjs.com/package/http-proxy
[9]: https://www.npmjs.com/package/grunt-concurrent
[10]: https://www.npmjs.com/package/grunt-contrib-watch
[11]: https://www.npmjs.com/package/grunt-contrib-clean
[12]: https://docs.djangoproject.com/en/1.8/ref/django-admin/
[13]: https://www.npmjs.com/package/grunt-contrib-handlebars
[14]: https://docs.djangoproject.com/en/1.8/topics/migrations/
[15]: https://www.npmjs.com/package/grunt-newer
[16]: https://www.npmjs.com/package/grunt-casperjs
[17]: http://docs.casperjs.org/
[18]: https://www.npmjs.com/package/grunt-sass
[19]: https://www.npmjs.com/package/grunt-postcss
[20]: https://docs.djangoproject.com/en/1.8/howto/deployment/
[21]: https://github.com/adaptlearning/adapt_authoring/wiki/installing-ffmpeg
[22]: https://github.com/ahupp/python-magic
",2022-08-12
https://github.com/UUDigitalHumanitieslab/panel-randomizer,"[![Build Status](https://travis-ci.org/UUDigitalHumanitieslab/panel-randomizer.svg?branch=develop)](https://travis-ci.org/UUDigitalHumanitieslab/panel-randomizer)

Manual: [dhlab-manuals.sites.uu.nl/handleiding/panel-randomizer-manual](https://dhlab-manuals.sites.uu.nl/handleiding/panel-randomizer-manual/)

# Panel Randomizer #

Panel Randomizer is a Python/Django application to redirect participants to LimeSurvey questionnaires. It has the following features:

* Anonymises participant ID based on a student number.
* Limits participation to only one time, over multiple surveys.
* Detects device type and redirect to a mobile version of a survey.
* Helps rotating different question groups in a survey. 
* Helps displaying YouTube videos in a survey: the YouTube player is started without controls. After playing the video the participant is sent to the next page.

How it works:

* Participants fill out their (student) ID, which is saved AES encoded in the database.
* Participants are only forwarded one first time to a questionnaire. If the respondent's ID is already present in the database, the participant is redirected to a screenout page.
* Recognizes the device of the participant. Desktop users wil be forwarded to a survey that is specified in the admin settings. Mobile users can be forwarded to an other specified survey that is optimized for mobile devices. If in the admin no mobile version is specified, all users are forwarded to the desktop version  
* The resulting forwarding URL contains two GET variables (key-value pairs) that can be used to pre-fill questions in the Lime Survey questionnaire:
  * Respondent ID. The URL key can be named arbitrarily in the admin (see: Manual)
  * Branching. The URL key can be named arbitrarily in the admin. The value in the URL is a rotating number, which runs from 1 to the limit value that is defined in the admin. Using this as a pre-answered value, a specified group of questions can be shown in LimeSurvey using LimeSurvey's so called 'relevance equation'.

## Prerequisites ##

* Python >= 3.4
* LimeSurvey

## Development ##

Optional: Create a virtual environment using `virtualenv` for Python 3.

```
pip install -r requirements.txt`
python manage.py migrate
python manage.py createsuperuser
python manage.py runserver
```

Go to `http://127.0.0.1:8000/admin`.

## Manual ##

See: [dhlab-manuals.sites.uu.nl/handleiding/panel-randomizer-manual](https://dhlab-manuals.sites.uu.nl/handleiding/panel-randomizer-manual/)

1. Create a survey in LimeSurvey (optional: with different groups)
2. Put one or two questions in the first group to be pre-filled by the URL of Panel Randomizer: student number and (optional) branching. These questions can be made 'hidden' optionally
3. Optional: create other groups that will be shown if the answer on the routing question is 1 to ... [number of groups]. Fill out in 'relevance equation' of this group: `[questionid]==[rotation number]`. For example `Q001==1`. In this case this particular group will only show when the question `Q001` was pre-filled with 1 by the forwarding url made by Panel Randomizer.
4. in 'Panel integration' in Lime Survey settings, add the url parameters that are used for pre-filling the answers. Choose a name for the student number parameter that needs to be taken from the url and assign the parameter to the target question from step 2.<br> Choose a name for the branching parameter (taken form the url of Panel Randomizer with the rotating group number) and assign the parameter to the target question for the branching from step 2.
5. Enter the Admin of Panel Randomizer. (https://panel-randomizer.hum.uu.nl/admin/)
6. Right from 'Surveys'  choose 'add'.
7. Fill out in the new screen:
	* A survey name (required). This name will be used as the last part of the link for the participants. E.g. https://panel-randomizer.hum.uu.nl/survey1. The name you entered was 'survey1'. Now all your settings for this survey will be retrieved and used.
	* Survey desktop url (required). This is the url you see in your LimeSurvey for desktop users under 'Survey URL'. Panel Randomizer wil generate this url with extra GET variables.
	* Survey mobile url (optional). The LimeSurvey url for the users of mobile devices.
	* Expected completion time. This will be shown on the welcome screen of Panel Randomizer
	* Group count. The number of groups that need to be rotated. The group number is passed as a GET variable (integer) in the forwarding URL generated by Panel Randomizer. For every new participant this integer will be incremented with 1 until the max number is reached, and starts again at 1.
	* Integration parameter student enc (required). The parameter name you choose (in step 4) in LimeSurveys 'panelintegration' to save the (encrypted) student number. Please fill out the exact same name. The encrypted student number wil be passed as a GET variable in the URL to LimeSurvey with this name. I.e when you choose the name 'student' in panelintegration as the url parameter that prefills the student number question, please fill out the same name here. The forwarding URL to Limesurvey will then contain this: `&student=[encrypted number]`
	* Integration parameter branching (optional). The parameter name you choose in LimeSurveys 'panelintegration' to save the answer needed for showing a particular group. Please fill out the exact same name. I.e when you choose the name 'branch' in panelintegration as the url parameter that prefills the question that decides which group of questions is shown next, fill out 'branch' here as well. In the generated url the GET parameter will then contain this: `&branch=1`, and a `1` will be prefilled as the question answer in LimeSurvey. For the next participant the number will be '2' until the number you filled in 'Group count' is met.
	* Click 'save'<br><br>
8. In order to test the connection to your LimeSurvey survey, use as a student number `123`. This number is (for the moment) available for testing purposes and will not be saved in the database of Panel Randomizer, as all other numbers will allow only one entry.
9. When you get 'Onderzoeksnaam ontbreekt of is onjuist', you probably forgot to add to the url the name of the survey you just created in Panel Randomizer admin. Please use a slash `/` and the survey name after the base url in the browser address bar.
10. To add a YouTube movie you need to add two lines of code to a question:

```html
<script src=""https://panel-randomizer.hum.uu.nl/static/js/youtube_helper.js"" async></script>
<div class=""single-play-video"" data-video-url=""https://www.youtube.com/watch?v=9RTaIpVuTqE""> </div>`
```

The first line starting with `<script>` will retrieve the javascript code that calls `youtube_helper.js`.
The second line will place a YouTube player, which will start automatically playing the url you entered. The url after `data-video-url=` needs to be adjusted for the aimed video.

## ROCKY ##

This application was built as part of the ERC Advanced Grant project _""Forests and Trees: the Formal Semantics of Collective Categorization (ROCKY)""_. More information about this project can be found here: https://rocky.sites.uu.nl/.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/perfectextractor,"# PefectExtractor

![GitHub](https://img.shields.io/github/license/UUDigitalHumanitieslab/perfectextractor?style=plastic)
![Travis (.org)](https://img.shields.io/travis/UUDigitalHumanitieslab/perfectextractor?style=plastic)
![PyPI](https://img.shields.io/pypi/v/perfectextractor?style=plastic)

*Extracting Perfects (and related forms) from parallel corpora*

This command-line application allows for extraction of Perfects (and related forms, like the Recent Past construction in French and Spanish) from part-of-speech-tagged, lemmatized and sentence-aligned parallel corpora encoded in XML.
 
## Installation

First, create a [virtual environment](https://docs.python.org/3/library/venv.html) and activate it:

    $ python -m venv venv
    $ source venv/bin/activate

Then, install the requirements in this virtual environment via:

    $ pip install -r requirements.txt

Finally, create the executable `extract` via:

    $ pip install --editable .

## Recognizing Perfects 

In English, a *present perfect* is easily recognizable as a present form of *to have* plus a past participle, like in (1):

    (1) I have seen that movie twenty times.

However, one difficulty in finding Perfects in most languages is that there might be words between the auxiliary and the past participle, like in (2):

    (2) Nobody has ever climbed that mountain.

Furthermore, languages have passive forms that generally require the past participle of *to be* to be interjected, like in (3):

    (3) The bill has been paid by John.
     
In English, there is the additional issue of the *present perfect continuous*, which in form shares the first part of the construction with the *present perfect*, like in (4):

    (4) He has been waiting here for two hours.
    
In some languages (e.g. French, German, and Dutch), the Perfect can be formed with both Have and Be. 
The past participle governs which auxiliary verb is used, as (5) and (6) show.

    (5) J'ai vu quelque chose [lit. I have seen some thing]
    (6) Elle est arrivé [lit. She is arrived]
    
For French, this is a closed list 
([DR and MRS P. VANDERTRAMP](https://en.wikipedia.org/wiki/Pass%C3%A9_compos%C3%A9#Auxiliary_.22.C3.8Atre.22)), 
but for other languages, this might be a more open class.

The last common issue with finding Perfects is that in e.g. Dutch and German, the Perfect might appear before the auxiliary verb in subordinate clauses. (7) is an example: 

    (7) Dat is de stad waar hij gewoond heeft. [lit. This is the city where he lived has]
    
The extraction script provided here takes care of all these issues, and can have language-specific settings. 

### Implementation 

The extraction script (`perfectextractor/apps/extractor/perfectextractor.py`) is implemented using the [lxml XML toolkit](http://lxml.de/). 

The script looks for auxiliary verbs (using a [XPath expression](https://en.wikipedia.org/wiki/XPath)), and for each of these, 
it tries to find a past participle on the right hand side of the sentence (or left hand side in Dutch/German), allowing for words between the verbs, 
though this lookup stops at the occurrence of other verbs, punctuation and coordinating conjunctions.

The script also allows for extraction of *present perfect continuous* forms. 

The script handles these by a list of verbs that use Be as auxiliary. 
The function *get_ergative_verbs* in `perfectextractor/apps/extractor/wiktionary.py` extracts these verbs from [Wiktionary](https://en.wiktionary.org) for Dutch.
This function uses the [Requests: HTTP for Humans](http://docs.python-requests.org/) package.
For German, the list is compiled from [this list](https://deutsch.lingolia.com/en/grammar/verbs/sein-haben).

## Recognizing Recent Pasts

Most Romance languages share a grammaticalized construction to refer to events in the recent past, e.g. the *passé récent* in French and the *pasado reciente* in Spanish.
In English, typically a *present perfect* alongside the adverb *just* is used to convey this meaning, commonly referred to as *perfect of recent past* (Comrie 1985) or *hot news perfect* (McCawley 1971).

The French *passé récent* is formed with a present tense of *venir* 'come' followed by the particle *de* and an infinitive, as in (8) below.
 
    (8) Je viens de voir Marie. [lit. I come DE see Mary] 
    
The Spanish *pasado reciente* is (quite similarly) formed with a present tense of *acabar* 'finish' followed by the particle *de* and an infinitive, as in (9) below.

    (9) Acabo de ver a María. [lit. I finish DE see Mary]

The extraction script (`perfectextractor/apps/extractor/recentpastextractor.py`) provided here allows export of these constructions from parallel corpora.  

## Other extractors

This application also allows extraction from parallel corpora based on part-of-speech tags or regexes. 

## Corpora

### Dutch Parallel Corpus

The extraction was first tested with the [Dutch Parallel Corpus](http://www.kuleuven-kulak.be/DPC).
This corpus (that uses the [TEI format](http://www.tei-c.org/)) consists of three languages: Dutch, French and English. 
The configuration for this corpus can be found in `perfectextractor/corpora/dpc/base.cfg` and `perfectextractor/corpora/dpc/perfect.cfg`.
Example documents from this corpus are included in the `perfectextractor/tests/data/dpc` directory.
The data for this corpus is **closed source**, to retrieve the corpus, you'll have to contact the authors on the cited website.
After you've obtained the data, you can run the extraction script with:

    extract <folder> en fr nl --corpus=dpc --extractor=perfect

### OPUS Corpora

The extraction has also been implemented for the open parallel corpus collection [OPUS](http://opus.nlpl.eu/), that contains most notably the [Europarl Corpus](http://opus.nlpl.eu/Europarl.php) and the [OpenSubtitles Corpus](http://opus.nlpl.eu/OpenSubtitles.php).
This corpus (that uses the [XCES format](http://www.tei-c.org/) for alignment) consists of a wide variety of languages. 
The configuration for this corpus can be found in `perfectextractor/corpora/opus/base.cfg` and `perfectextractor/corpora/opus/perfect.cfg`: implementations have been made for Dutch, English, French, German and Spanish. 
Example documents from this corpus are included in the `perfectextractor/tests/data/europarl` directory.
The data for this corpus is **open source**: you can download the corpus and the alignment files from the cited website.
After you've obtained the data, you can run the extraction script with:

    extract <folder> en de es --corpus=opus --extractor=perfect

### British National Corpus (BNC)

The extraction has also been implemented for the monolingual [British National Corpus](http://www.natcorp.ox.ac.uk/).
The data for this corpus is **open source**: you can download the corpus from the linked website.
After you've obtained the data, you can run the extraction script with:

    extract <folder> en --corpus=bnc --extractor=perfect

### Implementing your own corpus

If you want to implement the extraction for another corpus, you'll have to create: 

  * An implementation of the corpus in the `perfectextractor/corpora` directory (see `perfectextractor/corpora/opus` for an example).
  * A configuration file in this directory (see `perfectextractor/corpora/opus/base.cfg` for an example).
  * An entry in the main script (see `perfectextractor/extract.py`)

## Other options to the extraction script

You can view all options of the extraction script by typing:

    extract --help

Do note that at this point in time, not all options are available in all corpora.
Feel free to send a pull request once you have implemented an option, or to request one by creating an issue. 

## Other scripts

These scripts can be found in `perfectextractor/scripts`.

### pick_alignments

This script allows to filter the alignment file based on (for example) a file prefix.
This is helpful in the case of large alignment files, as is e.g. the case for the Europarl corpus.
Example usage:

    python pick_alignments.py 

### merge_results

This script allows merging results from various files.
Example usage:

    python merge_results.py 

### splitter

This script allows to split a big corpus into subparts and then to run the extractors.
Example usage:

    python splitter.py 

## Tests

The unit tests can be run using: 

    python -m unittest discover -b

A coverage report can be generated (after installing [coverage.py](https://coverage.readthedocs.io/en/coverage-4.2/)) using:

    coverage run --source . -m unittest discover -b
    coverage html

## Citing

If you happen to have used (parts of) this project for your research, please refer to this paper:

[van der Klis, M., Le Bruyn, B., de Swart, H. (2017)](http://www.aclweb.org/anthology/E17-2080). Mapping the Perfect via Translation Mining. *Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers* 2017, 497-502.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/persistent-forking,"=== Persistent Forking (WordPress plugin) ===
Contributors: UUDigitalHumanitiesLab
Tags: forking, forks, posts
Requires at least: 4.0
Tested up to: 4.3
License: MIT
License URI: http://opensource.org/licenses/MIT

Enable contributors to create permanent forks of existing posts.
Visualise the family of any fork.


== Description ==

Persistent Forking allows contributors of your site to create
permanent ""forks"" of existing posts. Every fork is a full-fledged
post in its own right and can be published as such. This
distinguishes Persistent Forking from the *Post Forking* plugin,
where forks are a collaborative tool in order to arrive at a single
definitive version. The parent-child relationships between posts are
tracked and it is possible to visualise the entire family tree of a
post.


== Installation ==

Just copy the plugin to your wp-content/plugins directory, then
enable the plugin in your admin panel.


== Screenshots ==

1. Forking controls are added to the top of every post. This is only visible for members of your site who can create posts.
2. When you click the ""Fork"" control, a new post is created and you are redirected to the editing form. The direct parent and the family are displayed in a metabox (red circle). The title of the parent post is copied with ""[fork]"" in front. As you can see, post contents are not copied over to the fork.
3. When you click the ""Show family"" control, all parent-child relations within the family are displayed in a tree diagram. You can click on a node to go to the corresponding post.


== Support ==

Please report any issue that you find while using the plugin at our
GitHub repository:

https://github.com/UUDigitalHumanitieslab/persistent-forking/issues

The first priority of the Digital Humanities Lab is to service
requests from Humanities researchers at Utrecht University. However,
the plugin is open source with a liberal license, so anyone can
provide a patch if we don't follow up on your issue.

If you are interested in (Digital) Humanities, please have a look at
our website:

http://digitalhumanities.wp.hum.uu.nl
",2022-08-12
https://github.com/UUDigitalHumanitieslab/programming-in-python,"# Programming in Python

The entry level course 'Programming in Python', by the [Utrecht Centre for Digital Humanities](https://cdh.uu.nl/) aims to teach the basics of the Python programming language. Special attention is given to best practises in coding, e.g.: writing clean code and documentation. 

The course was first taught 15-16 November, 2021. 

This repository contains all teaching materials, exercises, solutions and extra resources relevant to the course. Where possible, all files are provided as Python Notebooks. ",2022-08-12
https://github.com/UUDigitalHumanitieslab/R-lookup-and-merge,"# R-lookup-and-merge
Allows one to lookup translation and categorization from a dictionary and merge that with the original data.

The script `lookup.R` uses `lookup.txt` as a dictionary to retrieve translations and categorizations to be merged into the source file (`G2top200-rightcontext.csv` is included as an example). The script uses the [plyr](https://cran.r-project.org/web/packages/plyr/index.html) library.

This script was tailor-made for [Yipu Wei](http://www.uu.nl/staff/YWei1) of Utrecht University.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/Reader-responses-to-translated-literature,"# Reader responses to translated literature

This repo contains work on the DIOPTRA-L project (Haidee Kotze, Gys-Walt van Egdom, Corina Koolen, UU Digital Humanities Lab).

It contains the following code:
- scraper: Python scripts used to scrape reviews from Goodreads
- preprocessing: Python scripts used to clean the data
- analysis: Python scripts to collect and count translation lemmas
- collocations: Python scripts for finding collocations surrounding translation lemmas
- embeddings: Jupyter notebooks for training and evaluating word embeddings using word2vec
- sentiment: Python scripts to count positive / negative and hedge terms in collocations
- model: R scripts used to generate statistics and visualizations of the data
 

## Setup
The anlysis was tested in Python 3.6. To install dependencies, run
`pip install -r requirements.txt`
",2022-08-12
https://github.com/UUDigitalHumanitieslab/readit-interface,"# READ-IT

Public interface for READ-IT


## Before you start

You need to install the following software:

 - PostgreSQL >= 9.3, client, server and C libraries
 - Python >= 3.4, <= 3.6
 - virtualenv
 - [Apache Jena Fuseki][fuseki] (see [notes in the backend README](backend/README.md#notes-for-setting-up-fuseki))
 - Elasticsearch 7 (see [notes in the backend README](backend/README.md#setting-up-elasticsearch))
 - RabbitMQ or other message broker and Celery (see [notes](backend/README.md#setting-up-celery))
 - WSGI-compatible webserver (deployment only)
 - [Visual C++ for Python][1] (Windows only)
 - Node.js >= 8
 - Yarn
 - [WebDriver][2] for at least one browser (only for functional testing)

[1]: https://wiki.python.org/moin/WindowsCompilers
[2]: https://pypi.org/project/selenium/#drivers
[fuseki]: https://jena.apache.org/documentation/fuseki2/


## How it works

This project integrates three isolated subprojects, each inside its own subdirectory with its own code, package dependencies and tests:

 - `backend`, the server side web application based on [Django][3] and [DRF][4];
 - `frontend`, the client side web application based on [Backbone][5];
 - `functional-tests`, the functional test suite based on [Selenium][6] and [pytest][7].

[3]: https://www.djangoproject.com
[4]: https://www.django-rest-framework.org
[5]: https://backbonejs.org
[6]: https://www.seleniumhq.org/docs/03_webdriver.jsp
[7]: https://docs.pytest.org/en/latest/

Each subproject is configurable from the outside. Integration is achieved using ""magic configuration"" which is contained inside the root directory together with this README. In this way, the subprojects can stay truly isolated from each other.

If you are reading this README, you'll likely be working with the integrated project as a whole rather than with one of the subprojects in isolation. In this case, this README should be your primary source of information on how to develop or deploy the project. However, we recommend that you also read the ""How it works"" section in the README of each subproject.


## Development

### Quickstart

First time after cloning this project:

```console
$ python bootstrap.py
```

Running the application in [development mode][8] (hit ctrl-C to stop):

```console
$ yarn start
```

This will run the backend and frontend applications, as well as their unittests, and watch all source files for changes. You can visit the frontend on http://localhost:8000/, the browsable backend API on http://localhost:8000/api/ and the backend admin on http://localhost:8000/admin/. On every change, unittests rerun, frontend code rebuilds and open browser tabs refresh automatically (livereload).

For live debugging of the frontend unittests, visit http://localhost:8000/specRunner.html. For live debugging of the backend unittests, run `yarn watch-test-back` and either manually set a breakpoint using your editor, pass `--pdb` to automatically break into the debugger when a test fails or pass `--trace` to automatically break into the debugger at the start of each test. In the latter case, you probably want to pass additional arguments to restrict which tests will be run. You can also combine these options.

[8]: #development-mode-vs-production-mode

To temporarily work only on the backend or frontend, respectively, run

```console
# backend
$ yarn start-back
# frontend
$ yarn gulp
```

In the former case, please note that there is no index page; you need to go directly to http://localhost:8000/admin/ etcetera. Obviously, there is no `specRunner.html`, either. In the latter case, the port number is `8080` instead of `8000` and the unittests are at http://localhost:8080/static/specRunner.html. Of course, backend pages like `/admin/` don’t exist in this case.


### Recommended order of development

For each new feature, we suggested that you work through the steps listed below. This could be called a back-to-front or ""bottom up"" order. Of course, you may have reasons to choose otherwise. For example, if very precise specifications are provided, you could move step 8 to the front for a more test-driven approach.

Steps 1–5 also include updating the unittests. Only functions should be tested, especially critical and nontrivial ones.

 1. Backend model changes including migrations.
 2. Backend serializer changes and backend admin changes.
 3. Backend API endpoint changes.
 4. Frontend model changes.
 5. Other frontend unit changes (templates, views, routers, FSMs).
 6. Frontend integration (globals, event bindings).
 7. Run functional tests, repair broken functionality and broken tests.
 8. [Add functional tests][9] for the new feature.
 9. Update technical documentation.

[9]: functional-tests/README.md#writing-tests

For release branches, we suggest the following checklist.

 1. Bump the version number in the `package.json` next to this README.
 2. Run the functional tests in production mode, fix bugs if necessary.
 3. Try using the application in production mode, look for problems that may have escaped the tests.
 4. Add regression tests (unit or functional) that detect problems from step 3.
 5. Work on the code until new regression tests from step 4 pass.
 6. Optionally, repeat steps 2–5 with the application running in a real deployment setup (see [Deployment](#deployment)).


### Commands for common tasks

The `package.json` next to this README defines several shortcut commands to help streamline development. In total, there are over 30 commands. Most may be regarded as implementation details of other commands, although each command could be used directly. Below, we discuss the commands that are most likely to be useful to you. For full details, consult the `package.json`.

Install the pinned versions of all package dependencies in all subprojects:

```console
$ yarn
```

Run backend and frontend in [production mode][8]:

```console
$ yarn start-p
```

Run the functional test suite:

```console
$ yarn test-func [FUNCTIONAL TEST OPTIONS]
```

The functional test suite by default assumes that you have the application running locally in production mode (i.e., on port `8080`). See [Configuring the browsers][10] and [Configuring the base address][11] in `functional-tests/README` for options.

[10]: functional-tests/README.md#configuring-the-browsers
[11]: functional-tests/README.md#configuring-the-base-address

Run *all* tests (mostly useful for continuous integration):

```console
$ yarn test [FUNCTIONAL TEST OPTIONS]
```

Run an arbitrary command from within the root of a subproject:

```console
$ yarn back  [ARBITRARY BACKEND COMMAND HERE]
$ yarn front [ARBITRARY FRONTEND COMMAND HERE]
$ yarn func  [ARBITRARY FUNCTIONAL TESTS COMMAND HERE]
```

For example,

```console
$ yarn back less README.md
```

is equivalent to

```console
$ cd backend
$ less README.md
$ cd ..
```

Run `python manage.py` within the `backend` directory:

```console
$ yarn django [SUBCOMMAND] [OPTIONS]
```

`yarn django` is a shorthand for `yarn back python manage.py`. This command is useful for managing database migrations, among other things.

Manage the frontend package dependencies:

```console
$ yarn fyarn (add|remove|upgrade|...) (PACKAGE ...) [OPTIONS]
```

Run [frontend Gulp commands][12]:

```console
$ yarn gulp [SUBCOMMAND ...] [OPTIONS]
```

[12]: frontend/README.md#gulp-tasks-and-options

Extract translation strings in the frontend:

```console
$ yarn localize
```


### Notes on Python package dependencies

Both the backend and the functional test suite are Python-based and package versions are pinned using [pip-tools][13] in both subprojects. For ease of development, you most likely want to use the same virtualenv for both and this is also what the `bootstrap.py` assumes.

[13]: https://pypi.org/project/pip-tools/

This comes with a small catch: the subprojects each have their own separate `requirements.txt`. If you run `pip-sync` in one subproject, the dependencies of the other will be uninstalled. In order to avoid this, you run `pip install -r requirements.txt` instead. The `yarn` command does this correctly by default.

Another thing to be aware of, is that `pip-compile` takes the old contents of your `requirements.txt` into account when building the new version based on your `requirements.in`. You can use the following trick to keep the requirements in both projects aligned so the versions of common packages don't conflict:

```console
$ yarn back pip-compile
# append contents of backend/requirements.txt to functional-tests/requirements.txt
$ yarn func pip-compile
```


### Development mode vs production mode

The purpose of development mode is to facilitate live development, as the name implies. The purpose of production mode is to simulate deployment conditions as closely as possible, in order to check whether everything still works under such conditions. A complete overview of the differences is given below.

dimension  |  Development mode  |  Production mode
-----------|--------------------|-----------------
command  |  `yarn start`  |  `yarn start-p`
base address  |  http://localhost:8000  |  http://localhost:8080
backend server (Django)  |  in charge of everything  |  serves backend only
frontend server (gulp-connect)  |  does not run  |  primary gateway
static files  |  served directly by Django's staticfiles app  |  collected by Django, served by gulp-connect
backend `DEBUG` setting  |  `True`  |  `False`
backend `ALLOWED_HOSTS`  |  -  |  restricted to `localhost`
livereload  |  yes  |  no
frontend sourcemaps  |  yes  |  no
frontend optimization  |  no  |  yes
HTML embedded libraries  |  taken from `frontend/node_modules`  |  taken from CDN


## Deployment

Both the backend and frontend applications have a section dedicated to deployment in their own READMEs. You should read these sections entirely before proceeding. All instructions in these sections still apply, though it is good to know that you can use the following shorthand commands from the integrated project root:

```console
# build the frontend with overridden settings
$ yarn gulp dist --production --config path/to/your/config-override.json
# collect static files of both backend and frontend, with overridden settings
$ yarn django collectstatic --settings SETTINGS --pythonpath path/to/SETTINGS.py
```

You should execute these commands in the order shown, i.e., build the frontend before collecting all static files.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/semantometrics-python,"# semantometrics-python

*semantometrics-python* is a Python toolset that tries to mimic the semantometrics research contribution measure, 
as being described by Petr Knoth and Drahomira Herrmannova in their 
[2014 article](http://www.dlib.org/dlib/november14/knoth/11knoth.html). 
This repository contains tools for calculating this and other measures, 
as well as preprocessing scripts to get from .html/.pdf to .txt-files.   

## Preprocessing 

### PDF to text

* **File:** 
*pdf2txt.py*
* **Usage:** 
Just move the script to a folder with .pdf-files and run it. It will output .txt-files.  
* **Packages used:** 
To get from PDF files (the de facto standard for research papers) to plain text files, 
we use the [PDFMiner](https://euske.github.io/pdfminer/) package. 
An alternative would be the Java package [Apache Tika](http://tika.apache.org), 
which is actually used by Knoth and Herrmanova.

### HTML to text

* **Files:** 
*html2txt_cultmach.py*, 
*html2txt_theoryandevent.py*
* **Usage:** 
Just move the script to a folder with .html-files and run it. It will output .txt-files. 
* **Packages used:** 
To get from HTML files to plain text files, we use the [BeautifulSoup](https://euske.github.io/pdfminer/) package.
The HTML format is used by a few research papers, usually with different lay-out (YMMV), 
that's why there are a few of these converters in the repository.   
Currently supported journals: 
    - [Culture Machine](http://www.culturemachine.net/)
    - [Theory and Event](http://muse.jhu.edu/journals/theory_and_event/)

## Tools

### Semantic Similarity

* **File:** 
*semantometrics.py*
* **Usage:** 
The script is set up to work with two sets of files: references of the research article in question 
(A-set, .txt-files starting with the letter 'a') and citations of the research article 
(B-set, .txt-files starting with the letter 'b'). The script expects these files to be in the same directory. 
The script returns the result of the contribution function defined by Knoth and Herrmannova.
* **Packages used:** 
For tf-idf calculation (the measure used in Knoth and Herrmannova's article) we use the 
[scikit-learn](http://scikit-learn.org/) implementation with a modified tokenization: 
[NLTK](http://www.nltk.org/)'s implementation of the [Porter2 (Snowball) stemmer](http://snowball.tartarus.org/). 
We also opted to remove punctuation and tokens that contain digits (e.g. '1987' and 'a2c').  

### Gephi output

* **File:** 
*gephi_input.py*
* **Usage:** 
We can also use tf-idf to calculate semantic similarity between files instead of sets of files. 
We can then use this measure (called *cosine similiarity*) as weights on an undirected graph. 
This allows us to do all sorts of cluster analysis with tools like [Gephi](http://gephi.github.io/). 
The script creates two input files for Gephi: 
    - *nodes.csv*, which contains the nodes of the network 
    - *edges.csv*, which contains the weighted edges of the network. 
* **Packages used:** 
We use the same measure of similarity as in the above semantic similarity tool. 

### Topic extraction 

* **File:** 
*topic_extraction.py*
* **Usage:** 
This script can be used to generate clusters from a corpus based on semantic similarity. 
The script uses [*k*-means clustering](http://en.wikipedia.org/wiki/K-means_clustering). 
Per cluster, the top terms can be identified, which makes it a bit more insightful than the clustering with Gephi above. 
The script creates two output files per *k* number of clusters: 
    - *clusters<k>.txt*, which contains the top terms per cluster
    - *clusters<k>.csv*, which contains the cluster association for each input file. 
Also, a graph is plotted with the inertia per *k* number of clusters, so that a value for *k* can be chosen
with the [elbow method](http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method).
* **Packages used:** 
The script uses the [scikit-learn](http://scikit-learn.org/) implementation of K-means clustering, 
and again the tf-idf calculation from the same package, this time with the built-in tokenization. 
For the most part, the script is adapted from 
[an example on the scikit-learn website](http://scikit-learn.org/stable/auto_examples/document_clustering.html) 
to which the looping over *k*, the output files and the plotting of the inertia were added.  
To plot the inertia, the script uses [matplotlib](http://matplotlib.org/). 

### Corpus counting

* **File:** 
*corpus_count.py*
* **Usage:** 
This script can be used to count the number of occurrences of a certain list of words in a corpus. 
* **Packages used:** 
The script uses the [scikit-learn](http://scikit-learn.org/) implementation of word counting. 
",2022-08-12
https://github.com/UUDigitalHumanitieslab/tei_reader,"[![Build Status](https://travis-ci.org/UUDigitalHumanitieslab/tei_reader.svg?branch=master)](https://travis-ci.org/UUDigitalHumanitieslab/tei_reader)

# Python 3 Library for Reading the Text Content and Metadata of TEI P5 (Lite) Files

The library focuses on extracting the main text content from a file and provide the available metadata about the text.

# TL;DR

```bash
pip install tei-reader
```

```python
from tei_reader import TeiReader
reader = TeiReader()
corpora = reader.read_file('example-tei.xml') # or read_string
print(corpora.text)

# show element attributes before the actual element text
print(corpora.tostring(lambda x, text: str(list(a.key + '=' + a.text for a in x.attributes)) + text))
```

# More Explanation
A reader can be opened using `TeiReader()`. It is then possible to either call `read_file(file_name)` or `read_string(str)`. Both will return a `Corpora` object containing the following properties:

| Property | Description |
| --- | --- |
| `corpora[]` |  A corpora can contain sub-corpora. |
| `documents[]` | The `Document` objects directly part of this corpora. |

`Corpora` and `Document` all inherit from `Element`. In all objects deriving from this it is possible to call:

| Property | Description
| --- | --- |
| `attributes{}` | Contain attributes applicable to this element. If an attribute contains attributes these are also returned. (e.g. `encodingDesc::editorialDecl::normalization`) |
| `text` | Get the entire text content as `str` |
| `divisions[]` | Recursively get all the text divisions in document order. If an element contains parts or text without tag. Those will be returned in order and wrapped with a `PlaceholderDivision`. |
| `all_parts[]` | Recursively get the parts in document order constituting the entire text e.g. if something has emphasis, a footnote or is marked as foreign. Text without a container element will be returned in order and wrapped with a `PlaceholderPart`. |
| `parts[]` | Get the parts in document order directly below the current element. |

`Attribute`, `PlaceholderDivision` and `PlaceholderPart` all support the same properties as `Element`.

# Upload to PyPi

```bash
python setup.py sdist
twine upload dist/*
```
",2022-08-12
https://github.com/UUDigitalHumanitieslab/texcavator,"::

     _____                             _             
    |_   _|____  _____ __ ___   ____ _| |_ ___  _ __ 
      | |/ _ \ \/ / __/ _` \ \ / / _` | __/ _ \| '__|
      | |  __/>  < (_| (_| |\ V / (_| | || (_) | |   
      |_|\___/_/\_\___\__,_| \_/ \__,_|\__\___/|_|   


Copyright '`Digital Humanities lab` @ Utrecht University',`Netherlands eScience Center`_, `University of Amsterdam`_. 

From 2015 onwards developed by the `Digital Humanities lab`_, Utrecht University.

.. _`Netherlands eScience Center`: https://www.esciencecenter.nl/
.. _`University of Amsterdam`: http://www.uva.nl/en/
.. _`Digital Humanities lab`: http://dig.hum.uu.nl/

Developer quick-start
=====================

************
Dependencies
************

Before installing Texcavator, make sure your packages are up-to-date and
a relational database (we use MySQL_) and Redis_ server are present on the system.
In apt-based Linux distros like Ubuntu/Debian, issue::

    sudo apt-get update
    sudo apt-get upgrade
    sudo apt-get install mysql-server redis-server

Make sure both servers are running. Furthermore, you will need a few development packages::

    sudo apt-get install libmysqlclient-dev libxml2-dev libxslt-dev python-dev

.. _MySQL: https://www.mysql.com/
.. _Redis: http://redis.io/

************
Installation
************

To install Texcavator, clone the repository (using git_) in your home directory
and make a virtualenv_, activate it, and install the requirements::

    sudo apt-get install git python-pip
    pip install virtualenv
    cd ~
    git clone https://github.com/UUDigitalHumanitieslab/texcavator.git
    mkdir .virtualenvs
    virtualenv .virtualenvs/texc
    source .virtualenvs/texc/bin/activate
    pip install -r texcavator/requirements.txt

In ``texcavator/settings.py``, you can change the path to the log file, if you like.

Copy ``texcavator/settings_local_default.py`` to ``texcavator/settings_local.py``.
The latter file is not kept under version control.

In ``texcavator/settings_local.py``, set up the database; for a quick test, you can use SQLite::

    DATABASES = {
        'default': {
            'ENGINE': 'django.db.backends.sqlite3',
            'NAME': os.path.join(PROJECT_PARENT, 'db.sqlite3')
        }
    }

.. _git: https://git-scm.com/
.. _virtualenv: http://virtualenv.readthedocs.io/

*************
Elasticsearch
*************

The next step is to load your data into an elasticsearch_ index.
To get started using elasticsearch see the quickstart_.

In ``texcavator/settings_local.py``, you can specify the elasticsearch host and port
(typically elasticsearch runs on ``localhost:9200``).
Texcavator assumes by default the data is in an index called ``kb`` and
that the documents are stored in a type ``doc`` that has at least the following fields:

* article_dc_subject
* article_dc_title
* identifier
* paper_dc_date
* paper_dc_title
* paper_dcterms_spatial
* paper_dcterms_temporal
* text_content

We use the mapping specified in ``mapping.rst``.

.. _elasticsearch: https://www.elastic.co/
.. _quickstart: https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started.html

*********************
Prerequisite commands
*********************

Texcavator requires you to install some external packages and management commands in order to function correctly.
Before issuing the commands below, make sure Elasticsearch, MySQL and Redis are still running at the specified ports.

* Install ShiCo, which allows for visualizing shifting concepts over time::

    python install-shico.py

* Populate the database::

    python manage.py migrate

* Create a Django superuser. The username and password you pick will be the administrator account for Texcavator::

    python manage.py createsuperuser

* Run the management command ``gatherstatistics`` to be able to display timelines::

    python manage.py gatherstatistics

* Run the management command ``add_stopwords`` to add a default list of (Dutch) stop words::

    python manage.py add_stopwords stopwords/nl.txt

* Run the management command ``gathertermcounts`` to be able to create word clouds normalized for inverse document frequency::

    python manage.py gathertermcounts

* Run the management command ``add_guest_user`` to add a guest environment (with limited options)::

    python manage.py add_guest_user

.. _Dojo: http://dojotoolkit.org/

******************
Development server
******************

First, make sure Elasticsearch, MySQL and Redis are still running at the specified ports.
Then, start Celery and Django's built-in webserver::

    celery --app=texcavator.celery:app worker --loglevel=info
    # In a separate terminal
    python manage.py runserver

Texcavator is now ready for use at ``http://127.0.0.1:8000``.

Downloading of query data requires a running SMTP server; you can use Python's built-in server for that::

    # In a separate terminal
    python -m smtpd -n -c DebuggingServer localhost:1025

Deployment
==========

You can find instructions for deploying Texcavator in ``deployment.rst``

Documentation
=============

The documentation for Texcavator is in Sphinx_. You can generate the documentation by running::

    make html

in the /doc/ directory.

.. _Sphinx: http://sphinx-doc.org/index.html


License
=======

Texcavator is distributed under the terms of the Apache2 license. See ``LICENSE`` for details.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/theaterdramaturgiebank,"# TheaterDramaturgie.Bank

This repository contains the code of the Dramaturgy Database (also known as TheaterDramaturgie.Bank), located at http://theaterdramaturgiebank.sites.uu.nl/. 
It's a [WordPress child theme](https://codex.wordpress.org/Child_Themes) of the [default Utrecht University theme](https://github.com/ictenmediaUU/UU2014).

Below is a breakdown of the function of each file in this repository:

* docs: Contains the documentation.
* languages: Contains language files, but as the database is currently monolingual, these files are not important.
* parts: Contains small snippets of code that are included in other pages.
  * `brandbar.php`: Customization of the top brand bar. Added the site title to the bar and removed search functionality.
  * `colofon.php`: Customization of the bottom bar. Removed most items, only displays the Digital Humanities lab logo.
  * `record-loop.php`: Functionality for looping over a post with the 'Record' category. Referenced from `category-record.php`.
  * `record-random.php`: Functionality for looping over a post with the 'Record' category on the front page. References from `index.php`.
* libraries: Contains external libraries.
  * `Parsedown.php`: Markdown parser in PHP. Retrieved from https://github.com/erusev/parsedown.
* `category-collection.php`: Template for displaying a list of posts with the 'Collection' category.
* `category-record.php`: Template for displaying a list of posts with the 'Record' category.
* `functions.php`: Specific functions for this template.
* `header.php`: Custom header, removed the site title.
* `index.php`: Customized front page, displaying a search form and a random post with the 'Record' category.
* `page-authors.php`: Page for displaying all authors in an alphabetical listing.
* `page-keywords.php`: Page for displaying all tags in an alphabetical listing.
* `single-collection.php`: Template for displaying a single post with the 'Collection' category.
* `single-person.php`: Template for displaying a single post with the 'Person' category.
* `single-record.php`: Template for displaying a single post with the 'Record' category.
* `style.css`: Custom CSS, generated through [SaSS](http://sass-lang.com/). Generated.
* `style.scss`: Custom SCSS, will generate the CSS and the source mapping.

## Compiling with SaSS

Use `sass style.scss style.css` to compile the .scss file to CSS.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/timealign,"# TimeAlign

TimeAlign allows you to easily annotate similar forms in aligned phrases.

## Installation

TimeAlign is created with the [Django web framework](https://www.djangoproject.com/) and requires Python 3.
After installing the dependencies for the MySQL database driver (see below), you can install the required python packages by running `pip install -r requirements.txt`

### MySQL Dependencies
If you want to use MySQL as your database backend (recommended) use the following commands to install a database server and the required packages for the python client.

#### CentOS 7.7
    sudo yum install mariadb-server mariadb-devel python3-devel
    sudo yum groupinstall 'Development Tools'

#### Ubuntu 18.04
    sudo apt-get install python3-dev default-libmysqlclient-dev libssl-dev mysql-server

### Setting up TimeAlign in a virtual environment
    # Clone the repository
    git clone [repository URL]
    cd timealign/
    
    # NOTE! When using Pycharm, .env cannot be recognized as a virtual environment folder. Use 'venv' instead.
    # Create a virtual environment
    sudo apt-get install virtualenv
    virtualenv .env
    source .env/bin/activate
    pip install --upgrade pip wheel
    pip install -r requirements.txt

    # Create a database and change the databases section in timealign/settings.py accordingly
    ## Setup database: https://dev.mysql.com/doc/mysql-getting-started/en/#mysql-getting-started-installing
    ## Create user: https://dev.mysql.com/doc/refman/8.0/en/creating-accounts.html

    # Migrate the database
    ## Create project db setting
    cp ./timealign/settings_secret_default.py ./timealign/settings_secret.py
    ## Update information in the 'settings_secret.py', then execute migrate script
    python manage.py migrate

    # Initialize revisions
    python manage.py createinitialrevisions

    # Run the tests
    python manage.py test

If the test runs OK, you should be ready to roll! Run the webserver using:

    # Start the (local) web server
    python manage.py runserver

During debugging, we additionally use the [Django Debug Toolbar](https://django-debug-toolbar.readthedocs.io/). Install it with:

    pip install django-debug-toolbar

And then uncomment the lines referring to the toolbar in `timealign/settings.py`.

## Documentation

You can find ERD diagrams of the applications in [`doc/models`](doc/models/README.md).

General information on the Time in Translation-project can be found on [our website](https://time-in-translation.hum.uu.nl/).

## Citing

If you happen to have used (parts of) this project for your research, please refer to this paper:

[van der Klis, M., Le Bruyn, B., de Swart, H. (2017)](http://www.aclweb.org/anthology/E17-2080). Mapping the Perfect via Translation Mining. *Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers* 2017, 497-502.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/utils,"DHLabUtils
=======================



----

This is the README file for the project.

The file should use UTF-8 encoding and be written using `reStructuredText
<http://docutils.sourceforge.net/rst.html>`_. It
will be used to generate the project webpage on PyPI and will be displayed as
the project homepage on common code-hosting services, and should be written for
that purpose.

Typical contents for this file would include an overview of the project, basic
usage examples, etc. Generally, including the project changelog in here is not
a good idea, although a simple ""What's New"" section for the most recent version
may be appropriate.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/wisselwerking,"# Toewijzen wisselwerkingen

1. Download de resultaten als puntkomma-gescheiden CSV-bestand.
2. Werk eventueel capacities.csv bij. Het is ook mogelijk het bestand te verwijderen: het script zal dan bij een onbekende keuze vragen wat de capaciteit is, en een nieuw bestand opslaan.
3. Installeer Python 3 en draai vanuit een command line (powershell, bash, cmd):

```bash
python magic.py Aanmeldformulier_Wisselwerking.csv
```
",2022-08-12
https://github.com/UUDigitalHumanitieslab/wurzburg-glosses-extraction,"# Würzburg glosses extraction

Allows one to extract grammatical information on glosses from the Würzburg glosses lexicon (Kavanagh 2001).

## Usage

Starting from the PDF version of the lexicon, one can use `pdf2html.py` to convert to HTML 
(uses [PDFMiner](https://euske.github.io/pdfminer/)), 
and then `cleanhtml.py` to remove tags (uses [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/)). 

After preprocessing, `run.py` allows one to extract grammatical information out of the lexicon. 

### Web application

This project comes with a small web application (build in [Flask](http://flask.pocoo.org/)) that allows you to
run the extraction for a single gloss, in case something went wrong during the automatic phase. The web application
can be started by running `web.py`. 

## References

Kavanagh, Seamus (2001).
*A lexicon of the Old Irish glosses in the Würzburg Manuscript of the Epistles of St. Paul.* 
Edited by Dagmar S. Wodtko.
Österreichische Akademie der Wissenschaften.
",2022-08-12
https://github.com/UUDigitalHumanitieslab/youtube-comment-collector,"# youtube-comment-collector
The script *youtube.py* collects comments and metadata from [YouTube](https://www.youtube.com) 
videos using the (now deprecated) [V2 API](https://developers.google.com/youtube/2.0/developers_guide_protocol). 
The script uses the [feedparser package](https://pythonhosted.org/feedparser/) for parsing the YouTube feeds, 
and the [requests](http://docs.python-requests.org/) library for parsing the Google Plus JSON responses. 
Run the script with `python youtube.py` after you have set your API key in `example.cfg`. ",2022-08-12
https://github.com/UUGRASP/UUComputingDocs,"# UU GRASP Computing manuals


Please check the published form of this web site is at:
[https://uugrasp.github.io/UUComputingDocs/](https://uugrasp.github.io/UUComputingDocs/)
",2022-08-12
https://github.com/vankesteren/blim,"# blim
Univariate Bayesian Linear Modelling made approachable for anyone: `blim()` works just like `lm()`!

## how to install

Has been tested on 64 bit R 3.2.5. Not guaranteed to work on other versions of R! Rtools is required for installation as a package. If you don't have it, you can download it from https://cran.r-project.org/bin/windows/Rtools/

```R
# install and load blim
library(devtools) # Required for installation
install_github(""vankesteren/blim"") # Install my package from github
library(blim) # Load this package
library(MASS) # forgot to add MASS in the package
# for usage:
?blim
```
",2022-08-12
https://github.com/vankesteren/bloctheme,"# bloctheme
",2022-08-12
https://github.com/vankesteren/blog,"<img src=""https://vankesteren.github.io/blog/img/header-img.svg"" width=""78%"">  <img src=""https://vankesteren.github.io/blog/img/bloglogo.svg"" width=""20%"">
[![GitHub license](https://img.shields.io/badge/status-online-green.svg)](https://vankesteren.github.io/blog)
[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://raw.githubusercontent.com/vankesteren/blog/master/LICENSE)
[![GitHub commits](https://img.shields.io/github/commits-since/vankesteren/blog/v1.0.svg)](https://github.com/vankesteren/blog/commits/master)

# Erik-Jan van Kesteren - blog -
To view the blog, go to [the website](https://vankesteren.github.io/blog).

# How I made it
In the near future I will probably create a blogpost about how I made this website. Until then, have fun looking around on this repo! I generated all the files simply in `RStudio` using the `rmarkdown` functionality that is built into it.
",2022-08-12
https://github.com/vankesteren/blog-source,"# Personal blog
[vankesteren.github.io](https://vankesteren.github.io)

Made using the great [cocoa enhanced](https://github.com/mtn/cocoa-eh-hugo-theme) hugo theme.

See also the [site repository](https://github.com/vankesteren/vankesteren.github.io).",2022-08-12
https://github.com/vankesteren/brainatlasrender,"# Render brain atlases using Blender 
This repository contains the .blend file (`DKT_atlas.blend`) and python scripts to map data (`factors.csv`) to colours in in a pial brain representation in blender. Using a nice lighting setup with ray tracing leads to nice results!

## How to run
```bash
blender -b -Y DKT_atlas.blend -P scripts/execute.py
```

## Output
![](output/DK-segmentation-2.png)
![](output/3fac_bottom.png)
![](output/3fac_top.png)",2022-08-12
https://github.com/vankesteren/cmfilter,"<p align=""center"">
  <img src=""cmfilter.png"" width=""300px""></img>
  <br/>
  <span>
    <a href=""https://vankesteren.r-universe.dev""><img src=""https://vankesteren.r-universe.dev/badges/cmfilter""></img></a>
    <a href=""https://github.com/vankesteren/cmfilter/actions/workflows/r.yml""><img src=""https://github.com/vankesteren/cmfilter/actions/workflows/r.yml/badge.svg""></img></a>
    <a href=""https://ci.appveyor.com/project/vankesteren/cmfilter""><img src=""https://ci.appveyor.com/api/projects/status/f0hbgmqlgkqhdstj?svg=true""></img></a>
  </span>
  <h5 align=""center"">Coordinate-wise Mediation Filter</h5>
</p>
<br/>

## Description
An `R` package for simultaneous discovery of multiple mediators in an _x → M → y_ system using Coordinate-wise Mediation Filtering.

Keywords: `high-dimensional data`, `feature selection`, `structural equation modeling`, `mediation analysis`

## Installation
The package can be installed from the `r-universe`:

```r
# Enable repository from vankesteren
options(repos = c(
  vankesteren = ""https://vankesteren.r-universe.dev"",
  CRAN = ""https://cloud.r-project.org""
))

# Download and install cmfilter in R
install.packages(""cmfilter"")
```


To install the development version of `cmfilter`, run

```r
devtools::install_github(""vankesteren/cmfilter@devel"")
```

## Usage
The built-in documentation (run `help(cmf)`) gives information on how to use this package.

## Citation

```
van Kesteren, E. J., & Oberski, D. L. (2019). Exploratory mediation analysis with many potential mediators. Structural Equation Modeling: A Multidisciplinary Journal, 26(5), 710-723.
```
",2022-08-12
https://github.com/vankesteren/csstrials,"<div style=""text-align:center; font-size:1.1rem;font-family:monospace;"">https://vankesteren.github.io/csstrials </div>
",2022-08-12
https://github.com/vankesteren/dav_practicals,"# Data Analysis and Visualisation practicals
Here you can find all information and files for the practicals of the elective master's course _Data Analysis and Visualisation_ at Utrecht University (course code `201600038` in Osiris).

Download the practicals folder <a href=""https://github.com/vankesteren/dav_practicals/archive/master.zip""><button>here</button></a>

<table>
  <tr>
    <th> # </th>
    <th> Name </th>
    <th> Link </th>
    <th> Hand in? </th>
  </tr>
  <tr>
    <td> 00 </td>
    <td> Prerequisites </td>
    <td> <a href=""http://uudav.nl/00_Prerequisites/prerequisites.html""> link </a> </td>
    <td> No </td>
  </tr>
  <tr>
    <td> 01 </td>
    <td> R basics for DAV </td>
    <td> <a href=""http://uudav.nl/01_R_basics_for_DAV/r_basics.html""> link </a> </td>
    <td> No </td>
  </tr>
  <tr>
    <td> 02 </td>
    <td> Data manipulation & EDA </td>
    <td> <a href=""http://uudav.nl/02_Data_manipulation/data_manipulation.html""> link </a> </td>
    <td> Yes </td>
  </tr>
  <tr>
    <td> 03 </td>
    <td> Data Visualisation using ggplot2 </td>
    <td> <a href=""http://uudav.nl/03_Data_visualisation/data_visualisation.html""> link </a> </td>
    <td> Yes </td>
  </tr>
  <tr>
    <td> 04 </td>
    <td> Assignment EDA </td>
    <td> <a href=""http://uudav.nl/04_Assignment_Exploratory_data_analysis/assignment_eda.html""> link </a> </td>
    <td> <strong>Yes</strong> </td>
  </tr>
  <tr>
    <td> 05 </td>
    <td> Supervised learning: Regression 1 </td>
    <td> <a href=""http://uudav.nl/05_Supervised_learning_Regression_1/regression_1.html""> link </a> </td>
    <td> Yes </td>
  </tr>
  <tr>
    <td> 06 </td>
    <td>  Supervised learning: Regression 2 </td>
    <td> <a href=""http://uudav.nl/06_Supervised_learning_Regression_2/regression_2.html""> link </a> </td>
    <td> Yes </td>
  </tr>
  <tr>
    <td> 07 </td>
    <td> Supervised learning: Classification 1 </td>
    <td> <a href=""http://uudav.nl/07_Supervised_learning_Classification_1/classification_1.html""> link </a> </td>
    <td> Yes </td>
  </tr>
  <tr>
    <td> 08 </td>
    <td> Supervised learning: Classification 2 </td>
    <td> <a href=""http://uudav.nl/08_Supervised_learning_Classification_2/classification_2.html""> link </a> </td>
    <td> Yes </td>
  </tr>
  <tr>
    <td> 09 </td>
    <td> Supervised learning: Regression 3 </td>
    <td> <a href=""http://uudav.nl/09_Supervised_learning_Regression_3/regression_3.html""> link </a> </td>
    <td> Yes </td>
  </tr>
  <tr>
    <td> 10 </td>
    <td> Assignment Prediction Model </td>
    <td> <a href=""http://uudav.nl/10_Assignment_Prediction_model/assignment_prediction.html""> link </a> </td>
    <td> <strong>Yes</strong> </td>
  </tr>
  <tr>
    <td> 11 </td>
    <td> Unsupervised learning: PCA & Correspondence Analysis </td>
    <td> <a href=""http://uudav.nl/11_Unsupervised_learning_PCA_CA/unsupervised_learning_1.html""> link </a> </td>
    <td> Yes </td>
  </tr>
  <tr>
    <td> 12 </td>
    <td> Unsupervised learning: Clustering </td>
    <td> <a href=""http://uudav.nl/12_Unsupervised_learning_Clustering/unsupervised_learning_2.html""> link </a> </td>
    <td> Yes </td>
  </tr>
</table>
",2022-08-12
https://github.com/vankesteren/Densy-Develop,"# Densy-Develop
Play around with probability distributions! Go to my [shinyapps.io](https://erikjan.shinyapps.io/Densy/) to see a deployed version.

# Package Dependencies
- shiny
- plotly
- markdown
",2022-08-12
https://github.com/vankesteren/efast,"<h3 align=""center""> EFAST </h3>
<p align=""center""> 
  <img src=""./img/efast_icon.png"" width=""150px""></img>
</p>



<h4 align=""center"">Exploratory Factor Analysis with Structured Residuals</h4>
<p align=""center"">
<a href=""https://travis-ci.org/vankesteren/efast""><img src=""https://travis-ci.org/vankesteren/efast.svg?branch=master""></img></a>
<a href=""https://zenodo.org/badge/latestdoi/197017520""><img src=""https://zenodo.org/badge/197017520.svg"" alt=""DOI""></a><br/>
</p>

Create and fit exploratory factor analysis with structured residuals (EFAST) models for data with feature covariance due to symmetry structure.
<p align=""center"">
  <img src=""./img/efa_uncorr_met.png"" width=""500px""></img><br/>
</p>

#### Installing efast
Run the following code to install `efast` with the tutorial vignette.
```r
remotes::install_github(""vankesteren/efast"")
```

#### Quickstart
To quickly fit an EFAST model and see the structure of the data required, you can use simulated data from the package:
```r
library(efast)
simdat <- simulate_efast()
fit_sim <- efast_hemi(
  data   = simdat, 
  M      = 4, 
  lh_idx = 1:17, 
  rh_idx = 18:34
)
efast_loadings(fit_sim, symmetry = TRUE)
lateralization(fit_sim)
```

#### Tutorial
An extended tutorial for using EFAST can be found on the [GitHub pages site](https://vankesteren.github.io/efast).


#### Support
Questions can be asked on the [`issues page`](https://github.com/vankesteren/efast/issues). I'll try to answer them as soon as possible!
",2022-08-12
https://github.com/vankesteren/efast_code,"<p align=""center"">
  <h3 align=""center"">Exploratory Factor Analysis with Structured Residuals for Brain Imaging Data</h3>
  <h4 align=""center"">Code repository</h4>
</p>

<p align=""center"">
  <a href=""https://zenodo.org/badge/latestdoi/260201823""><img src=""https://zenodo.org/badge/260201823.svg"" alt=""DOI""></a>
</p>
<br/>

This repository contains the code to produce the figures and tables in the manuscript titled _""Exploratory Factor Analysis with Structured Residuals for Brain Imaging Data""_ by Erik-Jan van Kesteren and Rogier Kievit.
  
### Prerequisites
This repository depends on the [`efast`](https://github.com/vankesteren/efast) package. Please install `efast` by following the instructions there.

### Contents
| Folder                                     | Description                                                      |
| :----------------------------------------- | :--------------------------------------------------------------- |
| [Simulations](./simulations)               | Perform and analyze the simulations from Section 3 of the paper. |
| [Empirical examples](./empirical_examples) | Perform and analyze the examples from Section 4 of the paper.    |",2022-08-12
https://github.com/vankesteren/ema_simulations,"# Simulations for the manuscript _Exploratory Mediation Analysis with Multiple Potential Mediators_
",2022-08-12
https://github.com/vankesteren/f1model,"# Bayesian Analysis of Formula One Race Results

[![DOI](https://zenodo.org/badge/352695980.svg)](https://zenodo.org/badge/latestdoi/352695980)

Repository containing code, data & presentation accompanying the manuscript _Bayesian Analysis of Formula One Race Results: Disentangling Driver Skill and Constructor Advantage_. The scripts contain the following:

| Script                                     | Contents                                                         |
| :----------------------------------------- | :--------------------------------------------------------------- |
| [`01_prep_data.R`](./01_prep_data.R)       | Data preparation, data joining from database `f1db_csv`          |
| [`02_process_data.R`](./02_process_data.R) | Data processing, status filtering, outcome computation, some EDA |
| [`03_model.R`](./03_model.R)               | Creating and estimating models                                   |
| [`04_compare.R`](./04_compare.R)           | Performing model comparison                                      |
| [`05_check.R`](./05_check.R)               | MCMC validation, posterior predictive checks                     |
| [`06_infer.R`](./06_infer.R)               | Inferences using posteriors of parameters                        |
| [`07_predict.R`](./07_predict.R)           | Counterfactual predictions                                       |

Data [`f1db_csv`](dat/f1db_csv) obtained from [Ergast developer API](http://ergast.com/mrd/db/) on 2022-02-17 uploaded with permission. All data objects (`.rds` and `.csv` files) are [CC BY 4.0](http://creativecommons.org/licenses/by/4.0/) licensed. Code is [MIT](LICENSE) licensed.

> Disclaimer: the ratings shown below are the result of a statistical model and its accompanying simplifying assumptions, estimated using only position data from 2014-2021. Please do not take the ratings as absolute truth.

![driver talent plot](img/plt_skill_trajectories.png)

![constructor advantage plot](img/plt_advantage_avg.png)

![constructor form plot](img/plt_advantage_trajectory.png)

NB: Presentation picture sources are in the notes.

",2022-08-12
https://github.com/vankesteren/fair_measurement,"## Fair inference on error-prone outcomes

_L. Boeschoten, E.-J. van Kesteren, A. Bagheri, & D.L. Oberski_

[![DOI](https://zenodo.org/badge/246888221.svg)](https://zenodo.org/badge/latestdoi/246888221) [![License: CC BY 4.0](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/)

This is a reproducible code repository supporting the manuscript. By running the `R` scripts in this repository, all the figures and tables can be reproduced. This repository follows a CC-BY 4.0 license.

The dataset `data/data_new.csv` was obtained directly without making changes from the [dissecting bias](https://gitlab.com/labsysmed/dissecting-bias) repository on GitLab and is subject to their license terms. 


### Setup
Ensure you have `R` and `RStudio` installed and run the following lines to install the dependencies
- `install.packages(""tidyverse"", ""glmnet"", ""patchwork"", ""lavaan"", ""remotes"")`
- `remotes::install_github(""vankesteren/firatheme"")`

### Running the code
The structure of the folder follows loosely the structure of the _Experiments_ section in the manuscript. Clone this repository, open the `fair_measurement.Rproj` file and run the following files in order:


| Section | Main R file | Main output |
| :------ | :------ | :----- |
| Data preparation and feature selection | `01_feature_selection.R` | `output/01_feature_selection.png`|
| Fair inference on cost as a proxy of health | `02_parity_correction.R`| `output/02_parity_correction.pdf` |
| Fair inference on latent health | `03_measurement_model.R` | `output/03_measurement_model.pdf` |
| Investigating unfairness in proxies | `04_differential_item_functioning.R` | `Table 1 (in console)` |


These main R files depend on the remaining R files and output files in this repository. The main figures will appear in the `output/` folder.
",2022-08-12
https://github.com/vankesteren/firatheme,"<p align=""center""><img src=""./img/title.svg"" width=""400px""></img></p>
<h5 align=""center"">Ready-to-use <code>ggplot2</code> theme</h5>
<p align=""center"">
  <a href=""https://zenodo.org/badge/latestdoi/125283144""><img src=""https://zenodo.org/badge/125283144.svg"" alt=""DOI""></a>
  <a href=""https://github.com/vankesteren/firatheme/actions/workflows/r_cmd_check.yml""><img src=""https://github.com/vankesteren/firatheme/actions/workflows/r_cmd_check.yml/badge.svg"" alt=""test""></a>
</p>


<br>

This is a `ggplot2` theme I use for almost all my plots. You can use it and adapt it if you want. It's designed to be truly plug-and-play. See [below](#example-plots) for examples.

<p align=""center"">
  <br/>
  <img src=""./img/plot.png"" width=""70%""></img>
  <br/>
</p>


As with any software project, this has a collection of dependencies it builds on:

- The [trafford data lab theme](https://github.com/traffordDataLab/assets/blob/master/theme/ggplot2/theme_lab.R) was one of the main inspirations.
- It uses Mozilla's [Fira Sans](https://mozilla.github.io/Fira/) font because it is beautiful.
- Fonts in `R` graphics are notoriously difficult. The package [`extrafont`](https://github.com/wch/extrafont) solves this.
- Of course, one of the main dependencies is [`ggplot2`](https://github.com/tidyverse/ggplot2).
- I used [coolors](https://coolors.co/) to generate the colours.


## Installation
On mac computers, first install the [`Fira Sans`](https://fonts.google.com/specimen/Fira+Sans) font to the font book.


```R
remotes::install_github(""vankesteren/firatheme"")
```

The preferred way to save plots is via the `firaSave()` function. For png output, this works out of the box. 

### PDF output
For pdf output, `firaSave()` embeds the Fira Sans font by default using the `extrafont` package. The requirement for this is `ghostscript`:

#### On windows
1. Install [ghostscript](https://www.ghostscript.com/download/gsdnld.html).
2. Restart your `R` or `RStudio` application
3. Now you can output!

#### On mac
1. (install homebrew)
2. `brew install ghostscript`
3. Restart your `R` or `RStudio` application
4. Now you can output!

#### On linux
1. Install ghostscript so that it is on the `PATH`.
2. Restart your `R` or `RStudio` application
3. Now you can output!

## Example plots
Below you can find some example plots made using `theme_fira()`

### Dots and lines
```R
library(ggplot2)
library(firatheme)

ggplot(mtcars, aes(x = mpg*0.43, y = wt*0.4535924, colour = factor(cyl))) +
       geom_point(size = 2) + geom_smooth(se = FALSE) +
       labs(title = ""Car weight vs efficiency"",
            x = ""Efficiency (km/l)"",
            y = ""Weight (1000 kg)"",
            colour = ""Cylinders"") +
       theme_fira() +
       scale_colour_fira()

firaSave(""plot.png"", device = ""png"")
```
![plt](./img/plot.png)

### Bar graph

```R
library(ggplot2)
library(firatheme)

ggplot(chickwts, aes(x = feed, y = weight)) +
  geom_bar(stat = ""identity"", width=0.8, fill = firaCols[1]) +
  labs(title = ""Chicken weights by feed type"",
       y = ""Weight (grams)"",
       x = """") +
  theme_fira()
```

![chk](./img/chick.png)


### Boxplot

```R
library(ggplot2)
library(firatheme)

ggplot(iris, aes(y = Sepal.Length, x = Species, fill = Species)) +
  geom_boxplot(col = firaCols[4], width = 0.5, size = 1) +
  labs(y = ""Sepal Length"", title = ""Iris data"") +
  theme_fira() +
  scale_fill_fira() +
  theme(legend.position = ""none"")
```

![iris](./img/iris.png)

### More lines
```R
library(ggplot2)
library(firatheme)

ggplot(airquality, aes(y = Ozone, x = 1:nrow(airquality))) +
  geom_line(colour = firaCols[2], size = 0.7) +
  geom_point(colour = firaCols[2], size = 1.7) +
  geom_smooth(colour = firaCols[1], size = 0.7, se = FALSE) +
  labs(title = ""Ozone in New York"", x = ""Days"") +
  theme_fira()
```

![oz](./img/ozone.png)


## Colours
The colour palette of `firatheme` is available through `firaCols` and `firaPalette()`. In `ggplot` objects, you should use `scale_fill_fira()` and `scale_colour_fira()` for mapped variables. Optionally, the argument `continuous = TRUE` can be passed.
![col](./img/firacols.png)

Using the palette functions, you can get any number of colours from the palette, for example `firaPalette(n = 25)` --- the last image in the figure below:
![cols](./img/firapalette.png)

## Citation
If you want to cite `firatheme`, you can use the following `bibtex`:

```bibtex
@article{vankesteren2021firatheme, 
  title = {vankesteren/firatheme: firatheme version 0.2.4}, 
  DOI = {10.5281/zenodo.4679413}, 
  publisher = {Zenodo}, 
  author = {Erik-Jan van Kesteren}, 
  year = {2021}, 
  month = {apr}
}
```
",2022-08-12
https://github.com/vankesteren/fontfira,"# fontfira
 Fira font package for use with extrafont
",2022-08-12
https://github.com/vankesteren/guitzli,"<p align=""center"">
  <img src=""https://raw.githubusercontent.com/vankesteren/guitzli/master/build/icon.png"" width=""20%""></img>
  <h1 align=""center"">Guitzli</h1>
  <p align=""center"">
    <a href=""https://ci.appveyor.com/project/vankesteren/guitzli""><img src=""https://ci.appveyor.com/api/projects/status/24xoj95ytbf4kn0g?svg=true"" alt=""Appveyor Build""></a>
    <a href=""https://travis-ci.org/vankesteren/guitzli""><img src=""https://api.travis-ci.org/vankesteren/guitzli.svg?branch=master""></a>
    <a href=""https://github.com/vankesteren/guitzli/releases""><img src=""https://img.shields.io/github/release/vankesteren/guitzli.svg"" alt=""GitHub release""></a>
    <a href=""https://raw.githubusercontent.com/vankesteren/blog/master/LICENSE""><img src=""https://img.shields.io/badge/license-MIT-orange.svg"" alt=""GitHub license""></a>
  </p>
</p>
<br/>

## Optimise images for web
Convert image files for web using google's [`guetzli`](https://github.com/google/guetzli) algorithm. This app is built on [`electron`](https://electron.atom.io/) with help from [`electron-forge`](https://beta.electronforge.io/). Also uses [`JQuery`](https://jquery.com/) and [`Material Design Lite`](http://getmdl.io) for the user interface.

![screenshot](https://raw.githubusercontent.com/vankesteren/guitzli/master/screenshot.png)

## When do I use it?
### The good
Guitzli uses [`guetzli`](https://github.com/google/guetzli) to compress your images to a fraction of their original size __without visual quality loss__. This is due to a [really interesting](https://arxiv.org/abs/1703.04421) human-visual-comparison algorithm developed by google. You're not convinced? Download the [before](https://raw.githubusercontent.com/vankesteren/guitzli/master/before.jpg) (8.62 MB) and [after](https://raw.githubusercontent.com/vankesteren/guitzli/master/after.jpg) (1.94 MB) photos from the above screenshot and compare them in as much detail as you want!

### The bad
__Guetzli conversion is slow!__ Because of this, there are only a handful of web-related cases where guitzli is valuable:
1. You are serving an image with limited bandwith (e.g., self-hosting).
2. You are serving an image many times (e.g., a landing page).
3. You are adamant about having the absolute smallest file size.
4. Any combination of the above.

[Here](https://github.com/google/guetzli#using) you can find some more information on using guetzli from the people at google.

## How to install

### Windows
Download the latest version for your system from [Releases](https://github.com/vankesteren/guitzli/releases). Only recent versions of windows are supported at this time. I'll try to support linux and osx as soon as possible!
Install directory will be `%LOCALAPPDATA%\guitzli`. From there you can create a desktop shortcut should you so desire.

### Debian/Ubuntu Linux
```shell
# Optional cd to downloads
cd ~/Downloads
wget -O guitzli.deb ""https://github.com/vankesteren/guitzli/releases/download/V0.3.0/guitzli_linux_64.deb""
sudo dpkg -i guitzli.deb
```

## How to build from source

1. Install [Node.js](https://nodejs.org/en/download/) 6.10.2.

2. On linux, also install `libgconf-2-4`.

3. Clone, install, and start.
```bash
git clone https://github.com/vankesteren/guitzli.git
cd guitzli
npm install
npm start
```


## How to package and create binaries

It's simple: `npm run make` for to generate 32-bit and 64 bit binaries for your current architecture. You can also build only one architecture: `npm run make32` or `npm run make64`.


# To-do

sooner: Proper packaging. Mac ~& Linux~ support.

later: Multi-image support. (batch processing & multi-core)
",2022-08-12
https://github.com/vankesteren/JSephus,"<p align=""center"">
  <h1 align=""center"">JSephus</h1>
  <h3 align=""center"">A portable ES6 class for an attractive SVG loading element.
  </h3>
  <br>
  <h3 align=""center""> <a href=""https://vankesteren.github.io/JSephus""> Go to the website. </a> </h3>
</p>
",2022-08-12
https://github.com/vankesteren/koffie,"# koffie
webappje. dit is absoluut niet de beste manier van het maken van een top-framework, maar is makkelijk om mee te spelen.

# Link
ga naar [de github page](https://vankesteren.github.io/koffie) voor een live versie

# Geitjes
Hier is een geitjesfoto

![geitjes](assets/goats.jpg)
",2022-08-12
https://github.com/vankesteren/loratemp,"# loratemp
Read sensor data via LoRa. Go to my [shinyapps.io](https://erikjan.shinyapps.io/tempy/) to see a deployed version.

",2022-08-12
https://github.com/vankesteren/Massign,"<p align=""center"">
  <img src=""Massign.svg"" width=""250px""></img>
  <br>
  <h4 align=""center"">Simple matrix construction in R</h4>
  <p align=""center"">
    <a href=""https://travis-ci.org/vankesteren/Massign""><img src=""https://travis-ci.org/vankesteren/Massign.svg?branch=master""></a>
    <a href=""https://cran.r-project.org/package=Massign""><img src=""http://www.r-pkg.org/badges/version/Massign""></a>
    <a href=""https://cran.r-project.org/package=Massign""><img src=""https://cranlogs.r-pkg.org/badges/grand-total/Massign?color=1199aa""></a>
  </p>

</p>
<br>

### What is this?
An `R` package with a single function: the matrix assignment operator `%<-%`.

### Why is this?
What's simpler:

```R
# let's create a correlation matrix!
M <- matrix(c(1, 0.2, -0.3, 0.4,
              0.2, 1, 0.6, -0.4,
              -0.3, 0.6, 1, 0.4,
              0.4, -0.4, 0.4, 1),
            nrow = 4,
            ncol = 4,
            byrow = TRUE)
```
or

```R
# Showing off the lower triangular feature:
M %<-% ""   1
         0.2,    1
        -0.3,  0.6,    1
         0.4, -0.4,  0.4,    1""
```

I like the second better. Hence `Massign`.

### How do I install it?
```R
# Development version
devtools::install_github(""vankesteren/Massign"")
library(Massign)

# CRAN (release) version
install.packages(""Massign"")
library(Massign)
```

### Can you give a more formal description?
Constructing matrices for quick prototyping can be a nuisance,
requiring the user to think about how to fill the matrix with values using
the matrix() function. The %<-% operator solves that issue by allowing the
user to construct matrices using code that shows the actual matrices.

### Why the choice for `%<-%`?
R users may already be used to the other matrix operations like `%*%` and `%^%`
(from `expm`). I felt this was a logical choice in that context.

### What else can the package do?
```R
#' @examples
# Basic usage
M %<-% ""   1,  0.2, -0.3,  0.4
         0.2,    1,  0.6, -0.4
        -0.3,  0.6,    1,  0.4
         0.4, -0.4,  0.4,    1""

# Variables allowed!
phi <- 1.5
V %<-% ""1,     1,     1
        1,   phi, phi^2
        1, phi^2, phi^4""

# We can also assign to the right:
""   1
  0.5,   1
 -0.2, 0.2,   1"" %->% S

# Complex matrices work too:
C %<-% ""  1+2i, 2+1i, 3+4i
        4+0.5i, 5+2i, 6+4i""

# And lastly, if you're a fan of LaTeX and one-liners:
L %<-% ""1, 2, 3 \\ 4, 5, 6 \\ 7, 8, 9 \\ 10, 11, 12""
# (although this kind of defeats the WYSIWYG purpose of Massign)
```

### Who is the target audience?
Anyone who uses `R` for prototyping with matrices. For example, generating data with `lavaan` can be made easier using `Massign`, or trying out different matrix operations.
**NB: Massign is _not_ for programming**. Due to the way the assignment operator `%<-%` currently works, I do not guarantee it to work once environments become a little more difficult. **Use of this package as a dependency is at your own risk :)**

### How did you make that arrow in the logo?
That's the font [Fira Code](https://github.com/tonsky/FiraCode), where `<-` is a ligature. Check it out!
",2022-08-12
https://github.com/vankesteren/memplot,"<p align=""center"">
  <h3 align=""center"">memplot</h3>
  <h5 align=""center"">Marginal Effects at the Mean Plots</h5>
</p>

An intuitive way of interpreting coefficients in a model: take a 
typical case / sample, vary the variable of interest, and see how that changes 
the prediction. Works for categorical and continuous predictors.

#### How to install
```r
devtools::install_github(""vankesteren/memplot"")
```

#### Example plots
![qsec](img/wt.png)

![cyl](img/cyl.png)
",2022-08-12
https://github.com/vankesteren/pcalg,"# Discovering Causal Structure with the PC Algorithm

This is the repository for the data files of the [MSDSlab](https://msdatasciencelab.wordpress.com/) presentation by [Oisin Ryan](https://ryanoisin.github.io/) on 22/02/2018. Oisin will explain the background and implementation of the PC algorithm and show how it can discover causal structure in a network of variables through smart use of conditional independence rules.


## Package `pcalg` installation

The `pcalg` package has some dependencies which have been removed from `CRAN`. To install the package, run the following in `R`.

```R
source(""https://bioconductor.org/biocLite.R"")
biocLite(c(""graph"", ""RBGL"", ""Rgraphviz""))
install.packages(""pcalg"")
```

## Dataset loading

Here is how to load the data into `R`. Note that the `pcalg::pc()` function uses a list of sufficient statistics rather than the raw dataset.
```R
# Necessary git url for the location of the datasets (i.e. this repo)
gitUrl <- ""https://raw.githubusercontent.com/msdslab/pcalg/master/datasets/""

# Dataset loading - change 1.csv into your dataset of choice
dat <- read.csv(paste0(gitUrl, ""1.csv""))
suffStat <- list(C = cor(dat), n = nrow(dat))

```

## DAG estimation

For those who want a little head start, below an example analysis.

```R
library(pcalg)

res <- pc(suffStat, labels = names(dat),
          indepTest = gaussCItest, # the type of independence test
          alpha = 0.01) # the alpha level of the independence test
plot(res)

# More information about the pc function
?pc
```

Also try replacing `pc` with `skeleton` or `fci` (for the partial ancestral graph).
",2022-08-12
https://github.com/vankesteren/personalsite,"### [erikjanvankesteren.nl](http://erikjanvankesteren.nl)

A simpler personal website",2022-08-12
https://github.com/vankesteren/prettywebsites,"<h1 align=""center"">Pretty websites</h1>

Below are some websites I appreciate. They have nice design or nice organisation of content, or simply nice fonts. I made no effort to filter my selection based on their content or the companies behind them, I just looked at the design. If you have any to suggest, let me know.

### [blog.openai.com](https://blog.openai.com/)
[![openai](img/openai.png)](https://blog.openai.com/)

> Great design of their in-depth blog posts, really nice _journal_-like feeling for their cover images which are also sometimes moving images - but never excessively so.

<br/>

### [de wortel van drie](https://dwvd.nl/)
[![dwvd](img/dwvd.png)](https://dwvd.nl/)

> Yes, they are web designers, so of course their website is pretty. But dwvd does things a little differently: just look at the overlapping elements, the slab font, the layout of the main page. To fully enjoy this website, you should visit it.

<br/>

### [developer.mozilla.org](https://developer.mozilla.org)
[![mozilla](img/mdn.png)](https://developer.mozilla.org)

> Sparse use of colour, bold headings, and an incredible amount of content make this website one of my favourites. Usually when I go here, it's because I am annoyed at some web programming issue, but I always leave refreshed. Perhaps a bit too much _design over function_, but I totally forgive it in this case.

<br/>

### [GitHub](https://github.com)
[![github](img/github.png)](https://github.com)

> GitHub also belongs in this list for keeping an incredible number of features so clean. I like their use of colour to highlight important actions, such as the `new repository` button in the above screenshot. Also, subtle notifications are much better than in-your-face ones.

<br/>

### [The Upshot](nytimes.com/section/upshot)
[![upshot](img/upshot.png)](nytimes.com/section/upshot)

> The New York times website is beautiful overall, but _The Upshot_, the data visualisation section, is my favourite part. They have really consistent and consistently beautiful visualisations. Also, look at that pretty overhanging `SECTIONS` button in the top left.

<br/>

<br/>

### [Reaktor](https://www.reaktor.com/)
[![upshot](img/reaktor.png)](https://www.reaktor.com/)

> Aside from its general clean aesthetic, one specific little detail about the Reaktor website caught my eye: hovering over a link makes it slide similar to the logo with its vertical shift. This is a great detail that really brings home the simple style!

<br/>


<br/>

### [Monotype](https://monotype.com)
[![monotype](img/monotype.png)](https://monotype.com)

> I love the main page of Monotype, which really invites you to try out different fonts interactively. Also, the soft background, small top-left wordmark and lack of separators give a calm feel with space for the fonts.

<br/>

### [asn bank](https://www.asnbank.nl/home.html)
[![asn](img/asn.png)](https://www.asnbank.nl/home.html)

> ASN bank is a nice example of how a great logo, sparse use of colour and a beautiful font can together make a very traditional _menu bar_ layout really shine. Of course, a bank needs a traditional layout for accessibility, but that does not mean it needs to be [bootstrappy](https://getbootstrap.com/docs/4.0/examples/album/) like the rest of the internet.

<br/>

### [Ace & Tate](https://www.aceandtate.com)
[![ace](img/acetate.png)](https://www.aceandtate.com)

> Font, font, font. It's pretty. Also, I really like their recent redesign where they put the logo in the middle and make the ampersand (&) the divide between the left and right parts of the page. Great spacing of the elements, too.

<br/>

### [Medium](https://medium.com)
[![medium](img/medium.png)](https://medium.com)

> Although it is becoming a little too cluttered for my taste these days, the Medium website has always had fantastic visuals and a great font.

<br/>

### [Berkshire Hathaway](http://berkshirehathaway.com/)
[![berkshire](img/berkshire.png)](http://berkshirehathaway.com/)

> An honorary mention goes out to Berkshire Hathaway, one of the most valuable investment companies in the world. They managed to keep their website largely the same since 1997 (!), as you can see on the [wayback machine](https://web.archive.org/web/19970530212007/http://www.berkshirehathaway.com:80/). The part I love most: _""If you have any comments about our WEB page, you can write us at the address shown above. However, due to the limited number of personnel in our corporate office, we are unable to provide a direct response.""_

<br/>

### []()
[![]()]()

>

<br/>
",2022-08-12
https://github.com/vankesteren/privacy-preserving-glm,"# Supplementary material archive
Code and data for the submitted manuscript _Privacy-preserving generalized linear models using distributed block coordinate descent_

1. [`privreg-images`](privreg-images) holds standalone R-scripts for generating two of the figures in the manuscript
3. [`privreg-simulations`](privreg-simulations) contains all the code for the comparison of our privacy-preserving GLM method to full-data GLM using monte carlo simulations.
2. [`privreg-experiments`](privreg-experiments) holds all the preprocessing and analysis code for the privacy-preserving GLM analyses on three datasets from the UCI machine learning repository. The raw data can additionally be found on the UCI website here:
   1. [Forest fires](https://archive.ics.uci.edu/ml/datasets/forest+fires)
   2. [Hepatocellular carcinoma](https://archive.ics.uci.edu/ml/datasets/HCC+Survival)
   3. [Diabetes](https://archive.ics.uci.edu/ml/datasets/diabetes)
   
I suggest using the `.Rproj` files to open the folders as an `R` project in `RStudio`, where possible.
",2022-08-12
https://github.com/vankesteren/privreg,"<p align=""center"">
  <img src=""logo.png"" width=""300px""></img>
  <br/>
  <span>
    <a href=""https://CRAN.R-project.org/package=privreg""><img src=""http://www.r-pkg.org/badges/version/privreg""></img></a>
    <a href=""https://travis-ci.org/vankesteren/privreg""><img src=""https://travis-ci.org/vankesteren/privreg.svg?branch=master""></img></a>
  </span>
  <h5 align=""center"">Private regression using block coordinate descent</h5>
</p>
<br/>

## installation
```r
remotes::install_github(""vankesteren/privreg"")
```

## usage

```r
library(privreg)

# create test data
set.seed(45)
S <- cov2cor(rWishart(1, 10, diag(10))[,,1])
X <- cbind(MASS::mvrnorm(1000, rep(0, 10), S), rbinom(100, 1, 0.1))
b <- runif(11, -1, 1)
y <- X %*% b + rnorm(100, sd = sd(X %*% b))

# vertically partition test data
alice_data <- data.frame(y, X[, 1:5])
bob_data   <- data.frame(y, X[, 6:11])

# create alice and bob locations
alice <- PrivReg$new(
  formula   = y ~ .,
  data      = alice_data,
  family    = gaussian(),
  intercept = TRUE,
  verbose   = TRUE,
  name      = ""alice"",
  crypt_key = ""pre-shared-key-123""
)

bob <- PrivReg$new(
  formula   = y ~ .,
  data      = bob_data,
  family    = gaussian(),
  intercept = FALSE,
  verbose   = TRUE,
  name      = ""bob  "",
  crypt_key = ""pre-shared-key-123""
)

# connect the session
alice$listen()
bob$connect(url = ""127.0.0.1"")

# ...

# estimate the model
alice$estimate()

# ...

# disconnect
alice$disconnect()

# compare results to glm()
summary(glm(y ~ X))
alice$summary()
bob$summary()
```

```
> summary(glm(y ~ X))

Call:
glm(formula = y ~ X)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.17875  -0.73828  -0.05085   0.72709   2.45231  

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.10280    0.03419  -3.007 0.002708 ** 
X1           0.86003    0.19318   4.452 9.48e-06 ***
X2          -0.26996    0.12100  -2.231 0.025903 *  
X3           0.85155    0.10760   7.914 6.65e-15 ***
X4          -1.07249    0.13248  -8.096 1.67e-15 ***
X5           0.36944    0.11597   3.186 0.001490 ** 
X6           1.05666    0.16451   6.423 2.07e-10 ***
X7           0.37315    0.11277   3.309 0.000971 ***
X8          -0.61272    0.19582  -3.129 0.001806 ** 
X9           0.44891    0.10730   4.184 3.12e-05 ***
X10         -0.69175    0.18520  -3.735 0.000198 ***
X11         -0.65930    0.12914  -5.105 3.96e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 1.080014)

    Null deviance: 2142.7  on 999  degrees of freedom
Residual deviance: 1067.1  on 988  degrees of freedom
AIC: 2928.8

Number of Fisher Scoring iterations: 2

> alice$summary()

Privacy-preserving GLM
----------------------

family:      gaussian
formula:     y ~ .
iterations:  399

Coefficients:
             Estimate Std. Error     2.5%     97.5% t value  Pr(>|t|)    
(Intercept) -0.102797   0.034189 -0.16989 -0.035705 -3.0067  0.002708 ** 
X1           0.860031   0.193180  0.48094  1.239122  4.4520 9.478e-06 ***
X2          -0.269965   0.121005 -0.50742 -0.032509 -2.2310  0.025903 *  
X3           0.851555   0.107599  0.64041  1.062703  7.9142 6.653e-15 ***
X4          -1.072488   0.132478 -1.33246 -0.812517 -8.0956 1.666e-15 ***
X5           0.369441   0.115970  0.14186  0.597018  3.1857  0.001490 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

> bob$summary()

Privacy-preserving GLM
----------------------

family:      gaussian
formula:     y ~ .
iterations:  400

Coefficients:
   Estimate Std. Error     2.5%    97.5% t value  Pr(>|t|)    
X1  1.05666    0.16451  0.73382  1.37949  6.4229 2.074e-10 ***
X2  0.37315    0.11277  0.15185  0.59444  3.3089 0.0009705 ***
X3 -0.61272    0.19582 -0.99700 -0.22844 -3.1289 0.0018059 ** 
X4  0.44891    0.10730  0.23835  0.65947  4.1837 3.123e-05 ***
X5 -0.69175    0.18520 -1.05519 -0.32832 -3.7351 0.0001984 ***
X6 -0.65930    0.12914 -0.91272 -0.40588 -5.1053 3.961e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```
",2022-08-12
https://github.com/vankesteren/readuni,"# readuni
 Read unisens binary files
",2022-08-12
https://github.com/vankesteren/rijkspalette,"<p align=""center"">
  <img src=""./img/header.png"" width=""450px""></img>
  <p align=""center"">
    <a href=""https://travis-ci.org/vankesteren/rijkspalette""><img src=""https://travis-ci.org/vankesteren/rijkspalette.svg?branch=master""></a>
    <a href=""https://cran.r-project.org/package=rijkspalette""><img src=""http://www.r-pkg.org/badges/version/rijkspalette""></a>
    <a href=""https://cran.r-project.org/package=rijkspalette""><img src=""https://cranlogs.r-pkg.org/badges/grand-total/rijkspalette?color=1199aa""></a>
  </p>
</p>


### What is this
An R package to generate palettes based on famous paintings from the Rijksmuseum, using the fantastic [Rijksmuseum API](http://rijksmuseum.github.io/).

```R
# Install the development version:
devtools::install_github(""vankesteren/rijkspalette"")
```

### How to use

Let's make a barplot using a palette based on [Vermeer's famous painting of a woman reading a letter](https://www.rijksmuseum.nl/en/search/objects?q=vermeer+letter&p=1&ps=12&st=Objects&ii=0#/SK-C-251,0). The `rijksPalette()` function queries the collection of paintings of the [Rijksmuseum in Amsterdam](https://www.rijksmuseum.nl).
```R
library(rijkspalette)
letter <- rijksPalette(""Vermeer Letter"")
letter
```

A 16-bit impression of the palette is shown in the `R` console.

![console](./img/console.png)

Let's look at the palette a bit better:

```R
plot(letter)
```

![vermeer](./img/vermeer.png)

Now we can make a plot using the palette from these colours.

```R
barplot(iris$Sepal.Length,
        col = letter$palette(3)[iris$Species],
        border = NA, space = 0,
        main = ""Sepal length of 3 iris species"")
```
![iris](./img/iris.png)

Note that the `palette()` function performs interpolation: you can generate any number of colours!
```R
barplot(rep(1, 1500),
        col = letter$palette(1500),
        border = NA, space = 0,
        axes = FALSE, asp = 1)
```
![continuous](./img/continuous.png)


__Try it out for yourself! Post your palette on twitter with the `#rijkspalette` hashtag :thumbsup:__

<br/>

![spacer](./img/spacer.png)
<br>

### Details: tuning, exploring, and clustering
The palette works well for the above image. However, when a painter uses many colours, some prominent colours may be skipped:

```R
appel5 <- rijksPalette(""Karel Appel"")
plot(appel5)
```
![appel5](./img/appel5.png)

Here, the quite prominent red colour is skipped. Luckily, we can tune both the number of colours and the brightness of those colours:

```R
appel7 <- tune(appel5, brightness = 0.85, k = 7)
plot(appel7)
```
![appel5](./img/appel7.png)


That's better. But why? The `explore()` function can give us more detail:

```R
explore(appel)
```
![explore](./img/explore.png)

Here, we see the colours in the image plotted on the [`a*b*` space](https://en.wikipedia.org/w/index.php?title=Lab_color_space&oldid=830722208). The white squares are the cluster centroids used to generate the palette. Note that the two quite distinct arms in the top of the plot belong to the same cluster. By increasing the number of clusters (the number of colours in the palette), we can fix this issue:

![explore7](./img/explore7.png)

The better coverage of the colour space indicates that 7 clusters is better than 5. The `k` argument in the `tune()` function takes care of that.

To access the individual colours, use the `cols` slot:

```R
appel$cols

[1] ""#A8402D"" ""#969D4C"" ""#B5BDAC"" ""#7D817A"" ""#336D7F"" ""#235B6D"" ""#303344""
```

As before, the `palette` slot is a `colorRampPalette` function to be used in plots:

```R
barplot(1/sqrt(1:15), col = appel$palette(15))
```

![barplot](./img/barplot.png)
",2022-08-12
https://github.com/vankesteren/rpeaks,"<p align=""center"">
  <img src=""./img/logo.png"" width=""200px""></img>
</p>

Fast implementation of Pan & Tompkins (1985). It is programmed relatively efficiently, using `Rcpp` and `RcppArmadillo` to process long ecg data much faster than alternatives. Default processing parameters are taken directly from the original paper.

This package gratefully borrows parts of the processing code from `rsleep::detect_rpeaks()`.

_NB: use at your own risk. This method has not been officially validated!_

## Speed comparison
<p align=""center"">
  <img src=""./img/speedcomparison.png"" width=""700px""></img>
</p>

## Details
This algorithm uses a butterworth filter of order 1 for the band-pass step, and a 3rd-order length-5 Savitzky-Golay smoothing filter to compute the derivative of the band-passed signal. Peak detection on the preprocessed signal works in a simplified way: we take the first value above the lower bound (3 * the mean signal value) which is higher than its  neighbours, and not within the refractory period after the previous R peak.


### Reference
Pan, J., & Tompkins, W. J. (1985). A real-time QRS detection algorithm. _IEEE transactions on biomedical engineering, (3)_, 230-236.
",2022-08-12
https://github.com/vankesteren/sem-computationgraphs,"<p align=""center"">
  <h3 align=""center"">Flexible Extensions to Structural Equation Models Using Computation Graphs</h3>
  <h4 align=""center"">Code repository</h4>
</p>
<br/>

This repository contains the code to produce the figures and tables in the article titled ""Flexible Extensions to Structural Equation Models Using Computation Graphs"" by Erik-Jan van Kesteren and Daniel Oberski. [DOI:10.1080/10705511.2021.1971527](https://doi.org/10.1080/10705511.2021.1971527)

### Prerequisites
- This repository depends on the [`tensorsem`](https://github.com/vankesteren/tensorsem/tree/computationgraph). Please install `tensorsem` by following the instructions there.
- This repository depends on several other packages, managed by `renv`. The first time you open `sem-computationgraphs.Rproj`, use `renv::restore()` to install the required dependencies.

### Contents
| Folder | Description |
| :--- | :---------- |
| [Lavaan comparison](./lavaan_comparison) | Shows how structural equation models can be estimated using our method and compares it to [`lavaan`](http://lavaan.org). |
| [LAD estimation](./lad_estimation) | Shows how estimation can be done using the least absolute deviation objective function. |
| [LASSO regression](./lasso_regression) | Compares our method to [`regsem`](https://github.com/Rjacobucci/regsem) and [`glmnet`](https://cran.r-project.org/web/packages/glmnet/index.html). | 
| [High-dimensional mediation (Generated)](./mediation_generated) | Performs ULS and LASSO-penalized ULS estimation for generated high-dimensional mediation data. |
| [High-dimensional mediation (Example)](./mediation_hidim) | Performs ULS and LASSO-penalized ULS estimation for real-world high-dimensional mediation data. |
| [Sparse Factor Analysis](./sparse_factor) | Shows how sparsity can be induced in confirmatory factor analysis through Bayesian LASSO estimation. |
",2022-08-12
https://github.com/vankesteren/shapie,"# shapie: convert your text to a unique shape

```r
shiny::runGitHub(""vankesteren/shapie"", launch.browser = TRUE)
```
",2022-08-12
https://github.com/vankesteren/single_subject,"# Treatment effects in single-subject count data

Go to the [markdown document](./treatment_single_subject.md) or to the [pdf document](./treatment_single_subject.pdf) to view the method.

![](ppc.png)
",2022-08-12
https://github.com/vankesteren/streetcompare,"# streetcompare
Geospatial analysis at the street level

![](img/street.png)",2022-08-12
https://github.com/vankesteren/systematic-reviews,"# systematic-reviews


Committing guidelines:

  * Commit machine-readable and FOSS formats as much as possible
  * repect dir structure, use the Rstudio project, Makefile etc.
  * Don't commit huge datafiles (>20MB)
  * Communicate on issue tracker and close issues from commits
  * No spaces in filenames please
 
",2022-08-12
https://github.com/vankesteren/tensorsem,"<p align=""center"">
  <img src=""img/tensorsem.png"" width=""300px""></img>
  <br/>
  <span>
    <a href=""https://travis-ci.org/vankesteren/tensorsem""><img src=""https://travis-ci.org/vankesteren/tensorsem.svg?branch=master""></img></a>
    <a href=""https://zenodo.org/badge/latestdoi/168356695""><img src=""https://zenodo.org/badge/168356695.svg"" alt=""DOI""></a>
  </span>
  <h5 align=""center"">Structural Equation Modeling using Torch</h5>
</p>
<br/>

## Description
An `R` and `python` package for structural equation modeling using `Torch`. This package is meant for researchers who know their way around SEM, Torch, and lavaan. 
Structural equation modeling is implemented as a fully functional `torch.nn` module. A short example optimization loop would be:

```python
import tensorsem as ts
model = ts.StructuralEquationModel(opt = opts)  # opts are of class SemOptions
optim = torch.optim.Adam(model.parameters())  # init the optimizer
for epoch in range(1000):
    optim.zero_grad()  # reset the gradients of the parameters
    Sigma = model()  # compute the model-implied covariance matrix
    loss = ts.mvn_negloglik(dat, Sigma)  # compute the negative log-likelihood, dat tensor should exist
    loss.backward()  # compute the gradients and store them in the parameter tensors
    optim.step()  # take a step in the negative gradient direction using adam
```

## Installation
To install the latest version of `tensorsem`, run the following:

1. Install the `R` interface package from this repository:
    ```r
    remotes::install_github(""vankesteren/tensorsem"")
    ```
2. Install `pytorch` on your system. Use the [`pytorch` website](https://pytorch.org/get-started/locally/) to do this. For example, for a windows pip cpu version, use:
    ```shell script
    pip install torch==1.5.0+cpu torchvision==0.6.0+cpu -f https://download.pytorch.org/whl/torch_stable.html
    ```
3. Install the `tensorsem` `python` package from this repository.
    ```shell script
    pip install https://github.com/vankesteren/tensorsem/archive/master.zip
    ```
4. (Optional) Install `pandas` and `matplotlib` for plotting and parameter storing
    ```shell script
    pip install matplotlib pandas
    ```

## Usage
See the [example](example) directory for a full usage example, estimating the Holzinger-Swineford model using maximum likelihood, unweighted least squares, and diagonally weighted least squares.
",2022-08-12
https://github.com/vankesteren/vennvis,"# vennvis: venn visualisation of variable variances

[![Build Status](https://travis-ci.org/vankesteren/vennvis.svg?branch=master)](https://travis-ci.org/vankesteren/vennvis)

It's simple: the area of each circle is proportional to a variable's variance, the overlap is proportional to their covariance. 

```r
devtools::install_github(""vankesteren/vennvis"")
```

# Example

```r
set.seed(147289)
x <- rnorm(100)
y <- 0.5*x+rnorm(100, 0.25)
vennvis(x, y)
```

<p align=""center""><img src=""example.png"" width = ""65%""></img></p>

```r
z <- 0.3*x+0.4*y+rnorm(100, 0.15)
vennvis(x, y, z)
```
<p align=""center""><img src=""example2.png"" width = ""65%""></img></p>


# Ok cool, what's next?

Visual upgrades are on their way. Also, variable labels might be nice. Much to do. If you're interested in contributing, don't hesitate to fork, clone, or pull request. Any tips welcome!
",2022-08-12
https://github.com/vankesteren/wa2pdf,"<p align=""center"">
  <!--img src=""img.svg"" width=""250px""></img>
  <br-->
  <h1 align=""center"">wa2pdf</h1>
  <h4 align=""center"">Convert whatsapp logs to a presentable pdf report</h4>
  <p align=""center"">
    <a href=""https://travis-ci.org/vankesteren/wa2pdf""><img src=""https://travis-ci.org/vankesteren/wa2pdf.svg?branch=master""></a>
  </p>
</p>
<br>

## Go to the [website](http://wa2pdf.ddns.net)
[![](server.png)](http://wa2pdf.ddns.net)
NB: It runs on a raspberry pi at home so be gentle and be patient; it's slow :)

## Prerequisites
### Windows
1. Install [MiKTeX](https://miktex.org/).
2. Make sure you have the font `DejaVu Sans` installed. You can get it from `dist/dejavu-sans` in this repository or from [dejavu-fonts/dejavu-fonts](https://github.com/dejavu-fonts/dejavu-fonts/releases).
3. Make sure you have [Node.js](https://nodejs.org/) installed.

### Linux
1. Install a TeX distribution with XeLaTeX `sudo apt-get install texlive-full`.
2. Install the dejavu font `sudo apt-get install ttf-dejavu`.
3. Make sure you have [Node.js](https://nodejs.org/) installed.


### Mac
_NB: I don't have a mac. If you want to contribute, try out whether these steps work and send me a PR to remove this line :)_
1. Install [MacTeX](http://www.tug.org/mactex/) (OSX).
2.Make sure you have the font `DejaVu Sans` installed. You can get it from `dist/dejavu-sans` in this repository or from [dejavu-fonts/dejavu-fonts](https://github.com/dejavu-fonts/dejavu-fonts/releases).
3. Make sure you have [Node.js](https://nodejs.org/) installed.

## How to use
### Command line interface
One step: `npm install -g vankesteren/wa2pdf`
The command line interface (see folder `./cli`) works like this:

`wa2pdf whatsapplog.txt -o prettylog.pdf`

Other usage information:
```
Usage: wa2pdf [options] <file>


Options:

  -o, --output <filename>  The output file (.pdf)
  -d, --debug              Print debug information
  -s, --silent             Do not open the converted pdf.
  -n, --noprogress         Do not show progress indicator.
  -h, --help               output usage information
```

### Server
Two steps:
1. clone this repository: `git clone https://github.com/vankesteren/wa2pdf.git`
2. Run `npm start` in the main directory of the repo.

The server will then start at port 8080. If you want a different port, use `npm start <portnumber>`. Direct your browser to the webpage. From there, you can upload WhatsApp log files to the server and download / save the pdf output.

## To do
Send me a pull request for (improvements on) any of these!
- ~~Create [unit tests](https://travis-ci.org/vankesteren/wa2pdf)~~
- Server interface:
    - send your whatsapp log via email?
    - ~~upload your log to a webpage?~~
- Template: Make the template pretty.
- ~~CLI: use [chalk](https://www.npmjs.com/package/chalk.) to style output~~
- Output: Optionally, calculate statistics etc for the report to be shown on top
    - total number of messages per person
    - most common words
    - other summary info

CLI dev: `npm link`
",2022-08-12
https://github.com/vitality-data-center/development-API,"# **Data Access API**
----
  This API provides the access to the datasets and their metadata stored at the backend. Every user should log in and uses his/her account to browse and access the datasets available in the data center. 


* **Method:**
  
  <_The request type: `POST`_>

## **Request**

| Parameter | Description                                                                                 |
|:-------------------|:--------------------------------------------------------------------------------------------|
| `user_id`   | user id |
| `key_words`  | this is optional and still needs further discussions. Ideally the key words should be from users, and is a string with a set of words that are separated by commas, such as ""noise, running"" | 
   



 ## **Response:**
The response results may contain information of multiple datasets, so the response may be formatted into a [json array](https://stackoverflow.com/questions/12289844/difference-between-jsonobject-and-jsonarray), and each element in this array corresponds to a json object, which refers to a dataset. The following elements may be present in the response, and will be visualized in the [frontend](https://vitality-data-center.github.io/) 
 
| Value | data type | Description                                                                                 |
|:------|:------------|:------------------------------------------------------------------------------------------|
| `id`   | integer|the ID of the dataset. |
| `owner_id`  | integer | the user ID of the owner of the dataset. |
| `title`   | VARCHAR(255) |title of the dataset. |
| `time_c`  | TIMESTAMP| the time when the dataset was created. | 
| `time_u`  | TIMESTAMP|the time when the dataset was uploaded. | 
| `size`  | integer |size of the dataset.                |
| `means` |  VARCHAR(100)| the means that was used for collection of the dataset|
| `permission` |  VARCHAR(16) |access permission, Enum: `closed`, `need_permission`, `open`|
| `context` |  VARCHAR(16) | the context related to the dataset, Enum: `behavior`, `environment`, `unknown`|
| `activity` |  VARCHAR(8) |the physical activity covered by the dataset, Enum: `walking`, `running`, `biking`, 'other'|
| `description` | VARCHAR(255)| a short description of the dataset|
| `link` | VARCHAR(255)| the link access to the dataset (still needs to be decided, and can be skipped if necessary)|


",2022-08-12
https://github.com/vitality-data-center/environmental_factors,"# Environmental factors
Following is a list of environmental factors at 4- and 6- digit postcode level of the Netherlands. All these factors are developed in line with our research plan, and will be linked to travel behavior variables. It should be mentioned that the environmental factors of pc6 level are calculated given three different buffer sizes (300 meters, 600 meters, 1000 meters). If you use any of our codes or data for your research, plase cite the following papers:

*Xipei Ren, Zhiyong Wang, Carolin Nast, Dick Ettema, and Aarnout Brombacher. 2019. Integrating Industrial Design and Geoscience: a Survey on Data-Driven Research to Promote Public Health and Vitality. In 9th International Digital Public Health Conference (2019) (DPH’ 19), November 20–23, 2019, Marseille, France. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3357729.3357747* 

*Wang, Z., Ettema, D., & Helbich, M. (2021). Objective environmental exposures correlate differently with recreational and transportation walking: A cross-sectional national study in the Netherlands. Environmental research, 194, 110591.*

#### Number of crossings (representing street connectivity)
The number of crossings witihin the buffers (300, 600, and 1000) around each pc6 centroid. <br>
`crossing_1`: cul-de-sac <br>
`crossing_3`: 3-way crossings <br>
`crossing_4plus`: >=4-way crossings <br>


#### Number of ddresses (representing degree of urbanization)
The number of addresses witihin the buffers (300, 600, and 1000) around each pc6 centroid.<br>
It is indicated by the attribute `addr_num` in the datasets.

#### Street density (from OpenStreetMap)
It is indicated by the attribute `street_density` in the datasets. <br>
For pc4, its value is the total length of all walking streets divided by the area of pc4. <br>
For pc6, its value is the total length of all walking streets divided by the buffer area around the pc6 centroid.

<img width=""600"" height=""700""  src=""https://github.com/vitality-data-center/environmental_factors/blob/master/images/road.png"" />


#### Residential building density (from OpenStreetMap) 
It is indicated by the attribute `res_bldg_density` in the datasets.<br>
For pc4, its value is the total area of all residential buildings divided by the area of pc4. <br>
For pc6, its value is the total length of all residential buildings divided by the buffer area around the pc6 centroid.<br>

#### air pollution indicators (from European Environment Agency https://www.eea.europa.eu/)
Totally 4 air pollution indicators are generated: NO2 (1000m x 1000m), PM25 (1000m x 1000m), PM10 (1000m x 1000m), NOx (2000m x 2000m), which are indicated by the attributes `no2_avg`, `pm25_avg`,`pm10_avg`,`nox_avg`, respectively. <br>
For pc4 and each type of air pollution, their values are the average values of all cells in each pc4. <br>
For pc6 and each type of air pollution, their values are the average values of all cells in the buffer around pc6 centroid.<br>
<img width=""600"" height=""700""  src=""https://github.com/vitality-data-center/environmental_factors/blob/master/images/air.png"" />


#### noise pollution
The noise pollution data is provided by RIVM (Rijksinstituut voor Volksgezondheid en Milieu), and can be downloaded as GIS file from https://www.atlasnatuurlijkkapitaal.nl/kaarten. Some description of the noise data can be found below. 
<img width=""650""  src=""https://github.com/vitality-data-center/environmental_factors/blob/master/images/noise_map.png"" />

1 = zeer goed 		Lden<=45 dB <br>
2 = goed 			45<Lden<=50 dB <br>
3 = redelijk 		50<Lden<=55 dB <br>
4 = matig 			55<Lden<=60 dB <br>
5 = slecht 			60<Lden<=65 dB <br>
6,7,8 = zeer slecht 		Lden>65 dB <br>

De geluidklassen hebben betrekking op de cumulatieve geluidbelasting in Lden (jaar) als veroorzaakt door
- rijkswegen (2016)
- gemeentelijke en provinciale wegen (2011)
- railverkeer (2016)
- luchtvaart (2011)
- industrie (kentalraming)
- windturbines (2015)

For pc4, indicator for noise level 1 is represented by the attribute `dn_1`, which is the ratio of the area of noise level 1 in the pc4 area. The same for other noise levels 2, 3, 4, 5, 6. 
For pc6, indicator for noise level 1 is represented by `dn_1`, which is the ratio of the area of noise level 1 in the buffer area of the pc6 centroid. The same for other noise levels 2, 3, 4, 5, 6. 


#### Landuse mix entropy (derived from Bestand Bodemgebruik)
It is represented by the attribute `landuse_idx`. <br>
For pc4, it is calcuated based on the following three land use classification:<br>
- Group 1, residential, 20
- Group 2, recreational 40, 41, 42, 43, 44, 50, 51, 60, 61, 62, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83
- Group 3, other: 10, 11, 12, 21, 22, 23, 24, 30, 31, 32, 33, 34, 35

For pc6, it is calcuated based on the following five land use classification:<br>
- Group 1, residential, 20
- Group 2, recreational 40, 41, 42, 43, 44, 50, 51, 60, 61, 62, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83
- Group 3, other: 10, 11, 12, 22, 23, 30, 31, 32, 33, 34, 35
- Group 4, commercial, 21
- Group 5, industrial, 24

<img width=""600""  src=""https://github.com/vitality-data-center/environmental_factors/blob/master/images/landuse1.png"" />
<img width=""618""  src=""https://github.com/vitality-data-center/environmental_factors/blob/master/images/landuse2.png"" />

#### NDVI (Normalized Difference Vegetation Index)
The details of ndvi can be found via the following link:
https://gisgeography.com/ndvi-normalized-difference-vegetation-index/. <br>
For pc4, their values are the average values of all cells of in each pc4. <br>
For pc6, their values are the average values of all cells in the buffer around pc6 centroid.<br>
Note that negative values are excluded from the calculation.

<img width=""600"" height=""700""  src=""https://github.com/vitality-data-center/environmental_factors/blob/master/images/ndvi.png"" />



",2022-08-12
https://github.com/wooshrow/amp-plugin-adapter-java,"# AMP adapter for Lab Recruits

This adapter will allow you to run a test on the Lab Recruits from AMP. The adapter implements a set of AMP-labels. You can see the available labels in the model `demo.aml` in the project `Lab Recruit` in AMP.

When the adapter is launched, it will connect to a running instance of Lab Recruits and load a demo game-level. Then, it connects to the AMP server.

AMP will then control a player-agent in the Lab Recruits.

Currently implemented AMP-labels are:

  * stimulus `explore`: This will wipe the agent's memory and then send the agent to explore the currently loaded Lab Recruits level. `explore` will not push any button. Note that finishing exploration takes time, especially if the game-level is big.

  * response `observation`: contains information on what the Lab Recruits agent has been observing since the last `explore`. The observation will contain information about buttons and open doors that were seen during since the last exploration. A seen entity is, usually, also physically reachable (assuming the physical level does not have 'windows' that the agent can peek through).

  * stimulus `push_button(i)`: this will cause the agent to go to a button and interact with it. Again, note that going to a game entity takes time, as the agent needs to physically go there. Also, such a task can only be completed if the button is physically reachable from the current agent's location.

  * Stimulus `pass_door(i)`: this will cause an agent to go towards a door. Currently this will not cause the agent to actually pass the door; it will only come close enough so that it can see the door.

  * Stimulus 'finishlevel': I leave it to you to implement this :) It is supposed to make the agent to go to the goal-flag in the level (assuming there is exactly one in the level) and touch it.

## How to build this project

  For now, the most convenient way is to build it from the Eclipse IDE. From Eclipse, **import this project as a Maven project**. This will build the project.

## How to get the Lab Recruits game

You have to build it :) [Get the source from here](https://github.com/iv4xr-project/labrecruits). There are build-instructions there. To build it you need the UNITY game development environment. You will need a specific version of UNITY. This information is in the said build instructions.

## How try an AMP-demo on Lab Recruits

1. Go to AMP. Open the project `Lab Recruit`, and then choose the the model `demo.aml` in the in AMP.

1. Run the game Lab Recruits.

1. I assume you have imported this project into Eclipse. From Eclipse, run the main-method of the class `PluginAdapter` (in the package `ampPluginAdapter`). This will launch this Adapter, load a demo-level to Lab Recruits, and then connect to AMP.

1. You are now ready to run a test. From AMP, start a new test.
",2022-08-12
https://github.com/wooshrow/course-python,"# A Course on Python

This is a course to learn the Python programming language.

### Hardware and software requirement

You need to bring your own laptop with either Windows (at least Windows 10), or MacOS, or Linux.

To prepapre for the course, install the following on your laptop:

1. Download this course-repository to your laptop. For example you [can download its zip here](https://github.com/wooshrow/course-python/archive/refs/heads/main.zip).

1. Install **Python 3.x** from https://www.python.org/ , at least version 3.9. The latest now is 3.10.

    To check if you have the right version, open a **Command Line Interface (CLI)** (also called 'Command Prompt' in Windows). In Mac a CLI looks like this:

    ![Command Line Interface (CLI) in Mac](./Lecturenotes/img/CLI.png) 
    
    
    In the CLI, type one of these commands:

    `python3 --version`

    `python --version`

    Check if you also get `pip3`, which is Python's official package manager; you need this for installing packages which are not in the standard Python distributions (for example data-science related packages). To check if you have pip3, do this in your CLI:

    `pip3 --version`

1. Install the package `pandas` for Python. From CLI do one of these:

     `pip3 install pandas`

     `python3 -m pip install pandas`

1. Test your Python installation. At CLI, go to root directory of this course materials. Then run the program `testme.py` from the CLI:

     `python3 testme.py`

    The program should not crash.     


1. Install **Visual Studio Code (VSC)** from https://code.visualstudio.com/ ; we will use this later as a text editor to write programs.

    From VSC, install the _Python Extensions_. This shoukd also intsall Jupyter extension. If Jupyter extension is installed, you can just do `Open Folder` from VSC to open the directory `Lecturenotes` of this course. After this you can browse the lecture notes from VSC.

1. Install Jupyter. This gives you an easy to use working environment to do our exercises at the beginning.

    After you installed Python 3.x (above), you can install `Jupyter Notebook` from Python 3.x. From CLI, do these:

      `pip3 install --upgrade pip`

      `pip3 install jupyter`

    After installing Jupyter, to try that it works, from CLI, go to the directory where you unziped this course, and do:

      `jupyter notebook`

    This should open Jupyter in your web-browser. From there, browse the course materials, load one of the `*.ipynb` files in the folder `Lecturenotes`.  

    You can use either this web-browser-based Jupyter. Alternatively, use VSC to open and browse the folder `Lecturenotes`. From there just open one of the `*.ipynb` files in the folder, this will open the file in the Jupyter-mode in VSC. 

### [Course Plan, click here](./courseplan.md)

### Other stuffs

* Lecture notes and exercises (previously in PDF format, now as interactive Jupyter notebooks)
* Project assignments (for four two-week projects, done in groups of 3-4 students)

### License
<a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by/4.0/88x31.png"" /></a><br />This work is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/"">Creative Commons Attribution 4.0 International License</a>.

### Acknowledgement

This course is based on the materials from the course _Computational Thinking, Programming with Python and Programming with Data at Utrecht University (CoTaPP)_, developed by [Anna-Lena Lamprecht](https://github.com/annalenalamprecht) and [Amy van der Ham](https://github.com/amyvdham). Thank you!
",2022-08-12
https://github.com/wooshrow/gclparser,"# GCL Parser

A simple parser for the GCL language used in the course _Program Verification_. The parser is written using Happy and Haskell. A Cabal build file is included to build the library.

This parser admits a richer language than the base GCL to also accommodate optional assignments. You can ignore the parts of the parser/syntax that you don't need in your parts of assignments.

#### Prerequisites
To compile the tool, the following package are required:
* array
* containers
* optparse-applicative
* pretty.

#### Compilation
To compile the library, run the command `cabal build`.

#### Running/testing from interpreter

After building (see above), you can run ghci from ./src, passing the path to where
Cabal puts the generated files, and passing relevant language options:

```
ghci -i../dist/build -XNamedFieldPuns
```

From there you can load packages e.g. `:l GCLParser.Parser` and try out functions defined there in the interpreter.

#### Usage

The module `GCLParser.Parser` exports two functions:

```Haskell
parseGCLstring :: String -> ParseResult Program
parseGCLfile :: FilePath -> IO (ParseResult Program)
```
The first is to parse a GCL program from a string, and the second is to parse from a text file. The parser returns a value of type `ParseResult Program`, which is a synonym for `Either String Program`. Such value takes the form of either `Left e` where `e` is an error message if the parsing fails, or `Right p` if the parsing is successful. In the later case, `p` is a value of type `Program` which is a datatype used to structurally repfresent a GCL program. See the module `GCLParser.GCLDatatype` for its definition.

Note that the module `GCLParser.Parser` is generated by Happy. The source code is `src/GCLParser/Parser.y`, which is a Happy parser definition. So, if you need to change the parser, you should not edit `Parser.hs` manually, but instead edit `Parser.y`.

#### Other tools

##### Pretty printing

The module `GCLParser.PrettyPrint` provides a function to pretty print a value of type `Program`:

```Haskell
ppProgram2String :: Program -> String
```

##### interpreter (running a GCL program)

The module `GCLInterpreter` provides a function to execute a GCL program, given a starting state.

```Haskell
execProgram :: Program -> State -> Either (String,State) State
```

Be mindful that this tool is experimental, and is not made for performance. There is also some incompleteness when interpreting the ∀ and ∃ quantifiers. Obviously, we can't actually check such a quantifier over the whole space of integers, so some practical choice was made. See the module in-code documentation.

##### mutation test

The module `MuGCL` provides a function that generate mutants of a GCL Program.
Given a program P, the function below generates a bunch of so-called
mutants. Each is a program P', which is a copy of P, but where it is changed
in one place (e.g. some expression ""i<n"" is mutated to ""i<=n"").

```Haskell
mutateProgram :: Program -> [(MutationType,Program)]
```

You can use this mutants to test the completeness of your verification
tool (it should be able to kill all the mutants, but see also my note in
the documentation of `MuGCL`).

#### Supported GCL syntax

See in `/docs`.

#### Error message

Unfortunately almost none. But if some of you know how to make Happy produces a more friendly error message, do let me know.

#### GCL examples and verification benchmark

See in `/examples`

#### Credits

Many thanks to Stefan Koppier for providing the initial implementation of the parser.

**Contributors:** Stefan Koppier, Wishnu Prasetya
",2022-08-12
https://github.com/wooshrow/ooxClone,"# OOX
OOX comes in two parts: the language and a verification tool for this language. The following sections will describe each part separately.

## The OOX Language

---

OOX is an intermediate verification language (IVL) with objects and concurrency as first class citizens. The syntax of OOX will be familiar to those experienced with object-oriented languages as e.g. Java and C#.

For example, see the OOX program below. In this program, we have a counter that incremented the field `current` in the class `Counter` from its initial value `0` to `4` in a concurrent fashion.

```none
class Counter {
    int current;

    Counter(int initial) {
        this.current := initial;
    }

    void increment() {
        lock(this) {
            int value := this.count;
            this.current := value + 1;
        }
    }

    static void count(int initial, int N)
            requires(initial >= 0)
            exceptional(false) {
        Counter counter := new Counter(initial);
        int i := 0;
        while(i < N) {
            fork counter.increment();
            i := i + 1;
        }
        join;
        int value := counter.current;
        assertvalue == initial + N;
    }

    static void main(int initial) {
        count(initial, 4);
    }
}
```

For those interested, OOX also has a [formal semantics](https://dspace.library.uu.nl/bitstream/handle/1874/396688/thesis.pdf?sequence=1).

## The OOX Verification Tool

---

The OOX language comes with a tool to verify such programs with a tool also named OOX. It is based on symbolic execution and supports all language features as defined in the OOX language.

### Installation

OOX is developed with GHC, and is tested using GHC 8.8.4. It also requires the SMT solver [Z3](https://github.com/Z3Prover/z3), specifically version 4.8.8.

Simply use the command `cabal v2-build` to build or `cabal v2-repl` to interactively use OOX. Cabal will ensure that the required Haskell packages are installed. The [Z3 package](https://hackage.haskell.org/package/z3) will require a bit more work, as it requires the Z3 bindings. On Windows, the fields `extra-include-dirs` and `extra-lib-dirs` can be added to the Cabal configuration file located in the user-specific AppData.

### Usage

The OOX verification tool can be tweaked using the following set of parameters.

| Description | Long | Short | Mandatory | Example |
| -           | -    | -     | -         | -       |
| The file to verify |            |       | V         | ""program.oox"" |
| The entry function | `--function` | `-f`    | V | `-f ""Program.main` |
| Maximum verification depth | `--path-depth` | `-k` |  | `-k 150` |
| Ignore the post-condtions | `--no-ensures` | | | `--no-ensures` |
| Ignore the exceptional post-condtions | `--no-ensures` | | | `--no-exceptional` |
| Ignore the pre-condtions | `--no-ensures` | | | `--no-requires` |
| Do not consider symbolic `null`  values | `--no-symbolic-null` | | | `--no-symbolic-null` |
| Do not consider symbolic  aliases | `--no-symbolic-alias` | | | `--no-symbolic-alias` |
| The maximum symbolic array size to consider | `--symbolic-array-size` | | | `--symbolic-array-size 3` |
| Disable formula caching | `--no-cache` | | | `--no-cache` |
| Disable partial order reduction | `--no-por` | | | `--no-por` |
| Disable local evaluation | `--no-local-solver` | | | `--no-local-solver` |
| Disable random exploration | `--no-random-interleaving` | | | `--no-random-interleaving` |
| The logging level | `--inform` | `-i` | | `-i 1` |

For example, the command `oox ""program.oox"" -f ""Program.main"" -k 150 -i 0` verifies the function `main` in the class `Program` in the program `program.oox` up to a maximum depth of `150` with logging disabled.

### Paper

Stefan Koppier, [_The Path Explosion Problem in Symbolic Execution, an Approach to the Effects of Concurrency and Aliasing_](./doc/koppier_thesis.pdf). Master thesis, Utrecht University, 2020.

### Examples and Benchmark

They can be found under `examples`. They include two benchmarks: `benchmarkpv` contains programs from the PSV-course and `benchmarksvcomp` contains selected programs from the SV-Comp set.

There is a test script  `tests\Testoox.hs` from where you can run OOX on example-sets and benchmarks:

* Do `> cabal v2-repl` from CLI.
* Then from ghci `:l ./tests/Testoox.hs`
* Once loaded, do for example: `runTestSuite tsuite_simple1`


### Tool workflow

* Top level: `Main.hs`. This parses the source file, then does an analysis-phase, and then execution-phase.
   * The parsing is done by the function `parsingPhase` from `Parsing/Phase.hs`. The parser produces a value of type `Sem r CompilationUnit`. The type `Sem` is from _Polysemy_.
   * The analysis is done by the function `analysisPhase` from `Analysis/Phase.hs`

### Other notes

* Function that evaluates path-condition: `execAssume` in the module `Execution.Semantics.AssertAssume`. This in turn calls `evaluateAsBool` in the module `Execution.Semantics.Evaluation`.
",2022-08-12
https://github.com/wooshrow/wooshrow,"My Github-page: https://wooshrow.github.io./

# STOP WAR

<!--
**wooshrow/wooshrow** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- 🔭 I’m currently working on ...
- 🌱 I’m currently learning ...
- 👯 I’m looking to collaborate on ...
- 🤔 I’m looking for help with ...
- 💬 Ask me about ...
- 📫 How to reach me: ...
- 😄 Pronouns: ...
- ⚡ Fun fact: ...
-->
",2022-08-12
https://github.com/wouter-swierstra/Brainfuck,"Brainfuck
=========

A Brainfuck interpreter written in Agda

The heart of the interpreter is a `run` function that, given a
Brainfuck program and the stream of characters entered via stdin,
produces a (possibly infinite) execution trace.

This program demonstrates how a total language, such as Agda, can
still be *Turing complete* – the `run` function assigns semantics to
any Brainfuck program, even those that do not terminate. The key
insight (which is not particularly novel) is that (productive)
coinductive programs are total and sufficient to simulate Turing
machines. For a more precise definition of 'total', I'd refer to [David
Turner's work](https://uf-ias-2012.wikispaces.com/file/view/turner.pdf).



",2022-08-12
https://github.com/wouter-swierstra/concepts-slide-template,"# README

This repository contains a template for you to create your slides for your presentation.

The documentation in `slides.md` should be fairly self-explanatory.
",2022-08-12
https://github.com/wouter-swierstra/formal-methods-nl,"# formal-methods-nl
A research agenda for formal methods in the Netherlands -- abstracts of our Lorentz Center meeting
",2022-08-12
https://github.com/wouter-swierstra/fp-dag,"# FP Dag

Welcome to the repository hosting the FP Dag 2022 website.

",2022-08-12
https://github.com/wouter-swierstra/hbral,"# hbral
A library fof hetergeneous binary random-access lists in Haskell.

Ported from the Agda code presented in the eponymous paper.
",2022-08-12
https://github.com/wouter-swierstra/logic-notes,"
# Readme

These are the lecture notes for the course on 'Logic for Computer
Science' taught at Utrecht University.

I use these notes to cover some additional material after completing
the first part of Moller and Struth's *Modelling Computing Systems*.

Please create an issues or pull requests if you spot any errors.
",2022-08-12
https://github.com/wouter-swierstra/SoftwareProject,"
Table of Contents

* 00-slides.md – introduction to Scrum for supervisors

* 01-slides.md – introduction, marshmallow challenge, introduction to Scrum

* 02-slides.md – user stories and planning

* 03-slides.md – risks, constraints, architecture

* 04-slides.md – using git and GitHub effectively

* 05-slides.md – testing and quality assurance

* 06-slides.md – SCM build management (Make & Visual Studio), Bash scripting








",2022-08-12
https://github.com/wouter-swierstra/xmonad,"
This repository contains an implementation of xmonad's StackSet module
in Coq. Extracting Haskell from this Coq file produces a drop-in
replacement module for the original Haskell module (once it has been
massaged a bit by a few innocuous shell scripts).

The Makefile has mostly been generated by coq_makefile. It contains
several build targets:

  - make patched_xmonad -- downloads xmonad and applies several
  reasonably innocent patches to the xmonad source;

  - make extraction -- extract Haskell code from the Coq
    implementation of the StackSet module, and postprocess it with
    some final shell scripts;

  - make integration -- tries to build the patched xmonad with the
    extracted StackSet module;

  - make quickcheck -- also runs the QuickCheck testsuite on the
    resulting xmonad binary;

  - make theorems -- gives some indication of how many QuickCheck
    properties have been formalized.

The Coq code is in the src/ directory. Most of the Coq code is in the
StackSet.v module. The Extraction.v module has various extraction
commands to generate somewhat palatable Haskell code. Several
QuickCheck properties have alread been proven in the Properties.v file.

The necessary shell scripts and patches are all in the scripts/directory.
",2022-08-12
https://github.com/xiedanghan/MangroveVulnerabilityModel,"# Mangrove Vulnerability
*Mangrove Vulnerability* is a long-term modelling study investigating the reponses of mangrove vulnerability under sea-levle rise and human pressure in different coastal conditions, including tides, waves, sediment supply and coastal slopes. Results and methods are described in [Xie et al., 2020](https://doi.org/10.1088/1748-9326/abc122) and [Xie et al., 2022](https://doi.org/10.1029/2021JF006301).

## Schematic overview of the bio-morphodynamic modelling approach
<p align=""center"" width=""100%"">
<img src=""figs/Figure 1. Schematic figure 2.png"" width=""600"">
    
## Highlights
###### 1. Multi-species mangrove assemblages in response to sea-level rise and human pressure ([Xie et al., 2020](https://doi.org/10.1088/1748-9326/abc122))
<p align=""center"" width=""100%"">
    <img src=""figs/Figure 5. Conceptual diagram.png"" width=""500"">
</p>

<div align=""center"">
  Bio-morphodynamic feedbacks can decouple accretion rates from inundation time, altering mangrove habitat conditions and causing mangrove diversity loss even when total forest coverage remains constant or is increasing.
</div> 

###### 2. Mangrove vulnerability to sea-level rise in different coastal conditions ([Xie et al., 2022](https://doi.org/10.1029/2021JF006301).)
<p align=""center"" width=""100%"">
    <img src=""figs/Fig. 5 mangroves with 3 SLRs.png"" width=""550"">
</p>

<div align=""center"">
  Mangroves in micro-tidal conditions are more vulnerable to SLR and landward habitat can be created even with limited sediment supply and thus without complete infilling of the available accommodation space.
</div>

## Model Installation and Setup
When it is the first time for you to use this model, several processes should be done before the model simulations:
1)	***Installation***<br/>
Check whether both the Delft3D and Matlab are installed correctly, both of which should be successfully run independently on their own working environment.

2)	***Deltares OpenEarth Matlab Tools*** <br/>
Since the vegetation coupling relies on external MATLAB functions, it requires the installation of the OpenEarth tool box. Please install the OpenEarth Matlab Tool box through the link: https://publicwiki.deltares.nl/display/OET/MATLAB  

3)	***Settings of Startrun.bat***<br/>
For D3D open-source the Startrun.bat file needs to point to d_hydro.exe, which will generate the executable: ""call C:\... \Delft3D 4.01.00\win32\flow2d3d\bin\d_hydro.exe""
(tested with Delft3D open-source-tag: 7545, compiled with Intel Fortran on Windows 10, Matlab 2017a)

4)	***Directory settings in Matlab***<br/>
Change working directory and add Deltares OpenEarth matlab Tools to Matlab environment in the lines 16 and 26 of ""general_input.m"", respectively.

5)	***More detail instructions can be related to the technical document***

## References
Xie, D., Schwarz, C., Brückner, M. Z. M., Kleinhans, M. G., Urrego, D. H., Zhou, Z., and van Maanen, B. (2020), Mangrove diversity loss under sea-level rise triggered by bio-morphodynamic feedbacks and anthropogenic pressures, *Environmental Research Letters, 15*(11), 114033. https://doi.org/10.1088/1748-9326/abc122

Xie, D., Schwarz, C., Kleinhans, M. G., Zhou, Z., & van Maanen, B. (2022). Implications of coastal conditions and sea-level rise on mangrove vulnerability: A bio-morphodynamic modeling study. Journal of Geophysical Research: Earth Surface, 127, e2021JF006301. https://doi.org/10.1029/2021JF006301

## Contacts and Future Updates
I hope my model helps you well 😀...
For any potential collaborations, questions or suggestions, you are very much welcome to contact me through the email (xiedanghan@gmail.com) or follow my [ResearchGate](https://www.researchgate.net/profile/Danghan-Xie).


Sincerely,<br/>
Danghan Xie<br/>
28th Feb., 2022 

",2022-08-12
